{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4db5a8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39777b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROBES_DIM_X = 50\n",
    "PROBES_DIM_Y = 5\n",
    "PROBES_DIM_Z = 5\n",
    "PROBES_COUNT = PROBES_DIM_X * PROBES_DIM_Y * PROBES_DIM_Z\n",
    "\n",
    "INPUT_FLOAT_COUNT = 18 # probe_feature + pos + angle\n",
    "SH_FLOAT_COUNT = 27\n",
    "MLP_HIDDEN_LAYER_WIDTH = 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28963ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralSH(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralSH, self).__init__()\n",
    "        # self.probe_features = nn.Parameter(torch.rand(PROBES_COUNT), requires_grad=True)\n",
    "\n",
    "        self.hidden_layer = nn.Linear(INPUT_FLOAT_COUNT, MLP_HIDDEN_LAYER_WIDTH)\n",
    "        self.output_layer = nn.Linear(MLP_HIDDEN_LAYER_WIDTH, SH_FLOAT_COUNT)\n",
    "\n",
    "    def trigonometric_encoding(self, x: torch.Tensor, L: int):\n",
    "        assert x.ndim == 2\n",
    "        y = []\n",
    "        for i in range(L):\n",
    "            s = torch.sin(2**i * torch.pi * x)\n",
    "            c = torch.cos(2**i * torch.pi * x)\n",
    "            y.append(s)\n",
    "            y.append(c)\n",
    "        y = torch.cat(y, dim=1)\n",
    "        return y\n",
    "\n",
    "    def forward(self, pos, angle):\n",
    "        assert pos.ndim == 2\n",
    "        assert angle.ndim == 2\n",
    "\n",
    "        pos = pos.view(-1, 3)\n",
    "        angle = angle.view(-1, 1)\n",
    "\n",
    "        assert pos.shape[0] == angle.shape[0]\n",
    "        batch_size = pos.shape[0]\n",
    "\n",
    "        pos_enc = self.trigonometric_encoding(pos, L=2)\n",
    "        angle_enc = self.trigonometric_encoding(angle, L=3)\n",
    "\n",
    "        # x = torch.cat([pos_enc, angle_enc, self.probe_features.repeat(batch_size, 1)], dim=1)\n",
    "        x = torch.cat([pos_enc, angle_enc], dim=1)\n",
    "        x = self.hidden_layer(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "805dd4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1250, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx, yy, zz = np.meshgrid(\n",
    "    np.linspace(0.0, 1.0, PROBES_DIM_X),\n",
    "    np.linspace(0.0, 1.0, PROBES_DIM_Y),\n",
    "    np.linspace(0.0, 1.0, PROBES_DIM_Z),\n",
    ")\n",
    "xx = xx.reshape(-1, 1)\n",
    "yy = yy.reshape(-1, 1)\n",
    "zz = zz.reshape(-1, 1)\n",
    "pos_grid = np.hstack([xx, yy, zz])\n",
    "pos_grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2181dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1250, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "light_angle = np.zeros((pos_grid.shape[0], 1), dtype=float)\n",
    "light_angle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b624813f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1250, 27])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralSH()\n",
    "with torch.no_grad():\n",
    "    out = model(torch.FloatTensor(pos_grid), torch.FloatTensor(light_angle))\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac7d4824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from texture_sampler import Texture, load_texture_by_name, sample_uv\n",
    "\n",
    "DATA_DIR = Path(\"LightmapsData/\")\n",
    "\n",
    "\n",
    "def Texture3DSample(tex: Texture, uvw: np.ndarray):\n",
    "    uvw = uvw.reshape(-1, 3)\n",
    "    _, _, _, channels = tex.data.shape\n",
    "    results = []\n",
    "    for p in uvw:\n",
    "        color = sample_uv(tex, p[0], p[1], p[2])\n",
    "        color = color.reshape(-1, channels)\n",
    "        results.append(color)\n",
    "    results = np.concatenate(results, axis=0)\n",
    "    return results\n",
    "\n",
    "\n",
    "def GetVolumetricLightmapAmbient(BrickTextureUVs: np.ndarray):\n",
    "    tex = load_texture_by_name(DATA_DIR, \"AmbientVector\")\n",
    "    return Texture3DSample(tex, BrickTextureUVs)\n",
    "\n",
    "\n",
    "def GetVolumetricLightmapSHCoefficients0(BrickTextureUVs: np.ndarray):\n",
    "    AmbientVector = GetVolumetricLightmapAmbient(BrickTextureUVs)\n",
    "    SHCoefficients0Red = Texture3DSample(load_texture_by_name(DATA_DIR, \"SHCoefficients_0\"), BrickTextureUVs) * 2 - 1\n",
    "    SHCoefficients0Green = Texture3DSample(load_texture_by_name(DATA_DIR, \"SHCoefficients_2\"), BrickTextureUVs) * 2 - 1\n",
    "    SHCoefficients0Blue = Texture3DSample(load_texture_by_name(DATA_DIR, \"SHCoefficients_4\"), BrickTextureUVs) * 2 - 1\n",
    "    SHDenormalizationScales0 = np.array([\n",
    "        0.488603 / 0.282095,\n",
    "\t\t0.488603 / 0.282095,\n",
    "\t\t0.488603 / 0.282095,\n",
    "\t\t1.092548 / 0.282095\n",
    "    ])\n",
    "    SHCoefficients0Red = SHCoefficients0Red * AmbientVector[:, 0:1] * SHDenormalizationScales0\n",
    "    SHCoefficients0Green = SHCoefficients0Green * AmbientVector[:, 1:2] * SHDenormalizationScales0\n",
    "    SHCoefficients0Blue = SHCoefficients0Blue * AmbientVector[:, 2:3] * SHDenormalizationScales0\n",
    "    return AmbientVector, SHCoefficients0Red, SHCoefficients0Green, SHCoefficients0Blue\n",
    "\n",
    "\n",
    "def GetVolumetricLightmapSH3(BrickTextureUVs: np.ndarray):\n",
    "    AmbientVector, SHCoefficients0Red, SHCoefficients0Green, SHCoefficients0Blue = GetVolumetricLightmapSHCoefficients0(BrickTextureUVs)\n",
    "    SHCoefficients1Red = Texture3DSample(load_texture_by_name(DATA_DIR, \"SHCoefficients_1\"), BrickTextureUVs) * 2 - 1\n",
    "    SHCoefficients1Green = Texture3DSample(load_texture_by_name(DATA_DIR, \"SHCoefficients_3\"), BrickTextureUVs) * 2 - 1\n",
    "    SHCoefficients1Blue = Texture3DSample(load_texture_by_name(DATA_DIR, \"SHCoefficients_5\"), BrickTextureUVs) * 2 - 1\n",
    "    SHDenormalizationScales1 = np.array([\n",
    "\t\t1.092548 / 0.282095,\n",
    "\t\t4.0 * 0.315392 / 0.282095,\n",
    "\t\t1.092548 / 0.282095,\n",
    "\t\t2.0 * 0.546274 / 0.282095\n",
    "    ])\n",
    "    SHCoefficients1Red = SHCoefficients1Red * AmbientVector[:, 0:1] * SHDenormalizationScales1\n",
    "    SHCoefficients1Green = SHCoefficients1Green * AmbientVector[:, 1:2] * SHDenormalizationScales1\n",
    "    SHCoefficients1Blue = SHCoefficients1Blue * AmbientVector[:, 2:3] * SHDenormalizationScales1\n",
    "\n",
    "    IrradianceSH = np.concatenate([\n",
    "        AmbientVector[:, 0:1], # .x\n",
    "        SHCoefficients0Red[:],\n",
    "        SHCoefficients1Red[:],\n",
    "        AmbientVector[:, 1:2], # .y\n",
    "        SHCoefficients0Green[:],\n",
    "        SHCoefficients1Green[:],\n",
    "        AmbientVector[:, 2:3], # .z\n",
    "        SHCoefficients0Blue[:],\n",
    "        SHCoefficients1Blue[:],\n",
    "    ], axis=1)\n",
    "    return IrradianceSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b165811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BrickTextureUVs __i06676.x, __i16677.x, __i26678.x float3 0.31288, 0.43666, 0.66076\n",
    "\n",
    "# V0 _8422.x, __i06913.x, __i16915.x, __i26917.x float4 1.48535, -0.41522, 0.71059, 0.04087\n",
    "# V1 __i36919.x, __i06997.x, __i16999.x, __i27001.x float4 -0.1588, -0.80276, 0.53671, 0.2306\n",
    "# V2 __i37003.x float -0.3305\n",
    "# V0 _8423.x, __i06925.x, __i16927.x, __i26929.x float4 0.98926, -0.75354, 0.73377, 0.08955\n",
    "# V1 __i36931.x, __i07009.x, __i17011.x, __i27013.x float4 -0.16142, -0.97288, 0.60831, 0.24069\n",
    "# V2 __i37015.x float -0.27203\n",
    "# V0 _8424.x, __i06937.x, __i16939.x, __i26941.x float4 0.81299, -0.61416, 0.62958, 0.08644\n",
    "# V1 __i36943.x, __i07021.x, __i17023.x, __i27025.x float4 -0.1487, -0.83557, 0.52588, 0.21856\n",
    "# V2 __i37027.x float -0.2175\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1661b875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shader:  [[ 1.48535 -0.41522  0.71059  0.04087 -0.1588  -0.80276  0.53671  0.2306\n",
      "  -0.3305   0.98926 -0.75354  0.73377  0.08955 -0.16142 -0.97288  0.60831\n",
      "   0.24069 -0.27203  0.81299 -0.61416  0.62958  0.08644 -0.1487  -0.83557\n",
      "   0.52588  0.21856 -0.2175 ]]\n",
      "this:  [[ 1.48513877 -0.41543918  0.71072756  0.04014002 -0.15832449 -0.80275309\n",
      "   0.53758384  0.22957705 -0.33113261  0.9894107  -0.75399901  0.73368275\n",
      "   0.0887034  -0.1607023  -0.97259599  0.60830068  0.24001047 -0.27270406\n",
      "   0.81303811 -0.61448763  0.62945858  0.08567148 -0.14806844 -0.83529595\n",
      "   0.52589594  0.21794688 -0.21810566]]\n",
      "errors:  [[2.11226082e-04 2.19178445e-04 1.37558532e-04 7.29984310e-04\n",
      "  4.75512036e-04 6.91442618e-06 8.73842717e-04 1.02294865e-03\n",
      "  6.32609990e-04 1.50698414e-04 4.59014470e-04 8.72505959e-05\n",
      "  8.46600861e-04 7.17695855e-04 2.84014562e-04 9.32347010e-06\n",
      "  6.79534589e-04 6.74060019e-04 4.81107330e-05 3.27627989e-04\n",
      "  1.21424787e-04 7.68522614e-04 6.31563045e-04 2.74048831e-04\n",
      "  1.59423057e-05 6.13119608e-04 6.05660212e-04]]\n",
      "max abs error: 0.00102\n"
     ]
    }
   ],
   "source": [
    "brick_uvs = np.array([0.31288, 0.43666, 0.66076]).reshape(1, 3)\n",
    "\n",
    "shader_value = np.array([\n",
    "    1.48535, -0.41522, 0.71059, 0.04087,\n",
    "    -0.1588, -0.80276, 0.53671, 0.2306,\n",
    "    -0.3305,\n",
    "    0.98926, -0.75354, 0.73377, 0.08955,\n",
    "    -0.16142, -0.97288, 0.60831, 0.24069,\n",
    "    -0.27203,\n",
    "    0.81299, -0.61416, 0.62958, 0.08644,\n",
    "    -0.1487, -0.83557, 0.52588, 0.21856,\n",
    "    -0.2175,\n",
    "]).reshape(1, 27)\n",
    "\n",
    "this_value = GetVolumetricLightmapSH3(brick_uvs)\n",
    "\n",
    "print(\"shader: \", shader_value)\n",
    "print(\"this: \", this_value)\n",
    "print(\"errors: \", np.abs(shader_value - this_value))\n",
    "print(f\"max abs error: {np.abs(shader_value - this_value).max():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75e8dfbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1250, 27)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_sh = GetVolumetricLightmapSH3(pos_grid)\n",
    "true_sh.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bdca41",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25fdc848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class SHDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            positions_data: np.ndarray,\n",
    "            light_angles_data: np.ndarray,\n",
    "            spherical_harmonics_data: np.ndarray\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.positions_data = torch.FloatTensor(positions_data)\n",
    "        self.light_angles_data = torch.FloatTensor(light_angles_data)\n",
    "        self.spherical_harmonics_data = torch.FloatTensor(spherical_harmonics_data)\n",
    "        assert spherical_harmonics_data.shape[1] == SH_FLOAT_COUNT\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source = self.positions_data[index], self.light_angles_data[index]\n",
    "        target = self.spherical_harmonics_data[index]\n",
    "        return source, target\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.positions_data) == len(self.light_angles_data)\n",
    "        return len(self.positions_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05cd883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SHDataset(pos_grid, light_angle, true_sh)\n",
    "train_ds, test_ds = random_split(dataset, [0.8, 0.2], torch.Generator().manual_seed(42))\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "028d206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader,\n",
    "          epochs=100, lr=1e-3, patience=10, device=\"cuda\"):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "    history = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for source, target in train_loader:\n",
    "            pos, angle = source\n",
    "            pos = pos.to(device)\n",
    "            angle = angle.to(device)\n",
    "            target = target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(pos, angle)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for source, target in val_loader:\n",
    "                pos, angle = source\n",
    "                pos = pos.to(device)\n",
    "                angle = angle.to(device)\n",
    "                target = target.to(device)\n",
    "                outputs = model(pos, angle)\n",
    "                val_loss = criterion(outputs, target)\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "\n",
    "        history[\"train_loss\"].append(avg_train_loss)\n",
    "        history[\"val_loss\"].append(avg_val_loss)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "              f\"Train Loss: {avg_train_loss:.6f} \"\n",
    "              f\"Val Loss: {avg_val_loss:.6f}\")\n",
    "\n",
    "        if patience != 0: # early stopping based on validation loss\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                epochs_no_improve = 0\n",
    "                torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "                    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "                    break\n",
    "\n",
    "    if patience == 0:\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05656aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralSH()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d65167ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] Train Loss: 0.156114 Val Loss: 0.105720\n",
      "Epoch [2/1000] Train Loss: 0.125060 Val Loss: 0.090279\n",
      "Epoch [3/1000] Train Loss: 0.114332 Val Loss: 0.084796\n",
      "Epoch [4/1000] Train Loss: 0.110381 Val Loss: 0.082912\n",
      "Epoch [5/1000] Train Loss: 0.111177 Val Loss: 0.082208\n",
      "Epoch [6/1000] Train Loss: 0.109007 Val Loss: 0.081971\n",
      "Epoch [7/1000] Train Loss: 0.111021 Val Loss: 0.081776\n",
      "Epoch [8/1000] Train Loss: 0.111069 Val Loss: 0.081689\n",
      "Epoch [9/1000] Train Loss: 0.110567 Val Loss: 0.081540\n",
      "Epoch [10/1000] Train Loss: 0.110282 Val Loss: 0.081417\n",
      "Epoch [11/1000] Train Loss: 0.108967 Val Loss: 0.081323\n",
      "Epoch [12/1000] Train Loss: 0.110991 Val Loss: 0.081237\n",
      "Epoch [13/1000] Train Loss: 0.109081 Val Loss: 0.081224\n",
      "Epoch [14/1000] Train Loss: 0.109889 Val Loss: 0.081097\n",
      "Epoch [15/1000] Train Loss: 0.108821 Val Loss: 0.081064\n",
      "Epoch [16/1000] Train Loss: 0.112233 Val Loss: 0.081005\n",
      "Epoch [17/1000] Train Loss: 0.109285 Val Loss: 0.080913\n",
      "Epoch [18/1000] Train Loss: 0.107000 Val Loss: 0.080923\n",
      "Epoch [19/1000] Train Loss: 0.108336 Val Loss: 0.080754\n",
      "Epoch [20/1000] Train Loss: 0.108148 Val Loss: 0.080717\n",
      "Epoch [21/1000] Train Loss: 0.107562 Val Loss: 0.080700\n",
      "Epoch [22/1000] Train Loss: 0.107526 Val Loss: 0.080630\n",
      "Epoch [23/1000] Train Loss: 0.108781 Val Loss: 0.080618\n",
      "Epoch [24/1000] Train Loss: 0.107313 Val Loss: 0.080603\n",
      "Epoch [25/1000] Train Loss: 0.107428 Val Loss: 0.080492\n",
      "Epoch [26/1000] Train Loss: 0.107793 Val Loss: 0.080519\n",
      "Epoch [27/1000] Train Loss: 0.109429 Val Loss: 0.080445\n",
      "Epoch [28/1000] Train Loss: 0.110521 Val Loss: 0.080404\n",
      "Epoch [29/1000] Train Loss: 0.108935 Val Loss: 0.080485\n",
      "Epoch [30/1000] Train Loss: 0.107745 Val Loss: 0.080432\n",
      "Epoch [31/1000] Train Loss: 0.109581 Val Loss: 0.080339\n",
      "Epoch [32/1000] Train Loss: 0.107694 Val Loss: 0.080433\n",
      "Epoch [33/1000] Train Loss: 0.110705 Val Loss: 0.080304\n",
      "Epoch [34/1000] Train Loss: 0.106352 Val Loss: 0.080407\n",
      "Epoch [35/1000] Train Loss: 0.107304 Val Loss: 0.080313\n",
      "Epoch [36/1000] Train Loss: 0.108595 Val Loss: 0.080257\n",
      "Epoch [37/1000] Train Loss: 0.106316 Val Loss: 0.080258\n",
      "Epoch [38/1000] Train Loss: 0.107824 Val Loss: 0.080291\n",
      "Epoch [39/1000] Train Loss: 0.106468 Val Loss: 0.080332\n",
      "Epoch [40/1000] Train Loss: 0.109176 Val Loss: 0.080245\n",
      "Epoch [41/1000] Train Loss: 0.107100 Val Loss: 0.080286\n",
      "Epoch [42/1000] Train Loss: 0.106180 Val Loss: 0.080352\n",
      "Epoch [43/1000] Train Loss: 0.109631 Val Loss: 0.080240\n",
      "Epoch [44/1000] Train Loss: 0.108571 Val Loss: 0.080345\n",
      "Epoch [45/1000] Train Loss: 0.107059 Val Loss: 0.080274\n",
      "Epoch [46/1000] Train Loss: 0.107975 Val Loss: 0.080292\n",
      "Epoch [47/1000] Train Loss: 0.107504 Val Loss: 0.080239\n",
      "Epoch [48/1000] Train Loss: 0.106981 Val Loss: 0.080381\n",
      "Epoch [49/1000] Train Loss: 0.108314 Val Loss: 0.080139\n",
      "Epoch [50/1000] Train Loss: 0.106740 Val Loss: 0.080326\n",
      "Epoch [51/1000] Train Loss: 0.107147 Val Loss: 0.080224\n",
      "Epoch [52/1000] Train Loss: 0.108696 Val Loss: 0.080283\n",
      "Epoch [53/1000] Train Loss: 0.109104 Val Loss: 0.080344\n",
      "Epoch [54/1000] Train Loss: 0.108216 Val Loss: 0.080221\n",
      "Epoch [55/1000] Train Loss: 0.108734 Val Loss: 0.080132\n",
      "Epoch [56/1000] Train Loss: 0.106751 Val Loss: 0.080255\n",
      "Epoch [57/1000] Train Loss: 0.109508 Val Loss: 0.080140\n",
      "Epoch [58/1000] Train Loss: 0.107899 Val Loss: 0.080275\n",
      "Epoch [59/1000] Train Loss: 0.107710 Val Loss: 0.080206\n",
      "Epoch [60/1000] Train Loss: 0.109279 Val Loss: 0.080221\n",
      "Epoch [61/1000] Train Loss: 0.106115 Val Loss: 0.080416\n",
      "Epoch [62/1000] Train Loss: 0.106246 Val Loss: 0.080169\n",
      "Epoch [63/1000] Train Loss: 0.108578 Val Loss: 0.080280\n",
      "Epoch [64/1000] Train Loss: 0.105945 Val Loss: 0.080372\n",
      "Epoch [65/1000] Train Loss: 0.108308 Val Loss: 0.080195\n",
      "Epoch [66/1000] Train Loss: 0.106385 Val Loss: 0.080459\n",
      "Epoch [67/1000] Train Loss: 0.108487 Val Loss: 0.080207\n",
      "Epoch [68/1000] Train Loss: 0.106569 Val Loss: 0.080212\n",
      "Epoch [69/1000] Train Loss: 0.105636 Val Loss: 0.080470\n",
      "Epoch [70/1000] Train Loss: 0.108738 Val Loss: 0.080197\n",
      "Epoch [71/1000] Train Loss: 0.108090 Val Loss: 0.080267\n",
      "Epoch [72/1000] Train Loss: 0.109282 Val Loss: 0.080151\n",
      "Epoch [73/1000] Train Loss: 0.106874 Val Loss: 0.080307\n",
      "Epoch [74/1000] Train Loss: 0.107242 Val Loss: 0.080304\n",
      "Epoch [75/1000] Train Loss: 0.106134 Val Loss: 0.080299\n",
      "Epoch [76/1000] Train Loss: 0.108366 Val Loss: 0.080220\n",
      "Epoch [77/1000] Train Loss: 0.106727 Val Loss: 0.080343\n",
      "Epoch [78/1000] Train Loss: 0.106866 Val Loss: 0.080164\n",
      "Epoch [79/1000] Train Loss: 0.108348 Val Loss: 0.080343\n",
      "Epoch [80/1000] Train Loss: 0.109523 Val Loss: 0.080220\n",
      "Epoch [81/1000] Train Loss: 0.107009 Val Loss: 0.080170\n",
      "Epoch [82/1000] Train Loss: 0.109029 Val Loss: 0.080251\n",
      "Epoch [83/1000] Train Loss: 0.105893 Val Loss: 0.080314\n",
      "Epoch [84/1000] Train Loss: 0.109574 Val Loss: 0.080170\n",
      "Epoch [85/1000] Train Loss: 0.107511 Val Loss: 0.080474\n",
      "Epoch [86/1000] Train Loss: 0.106987 Val Loss: 0.080158\n",
      "Epoch [87/1000] Train Loss: 0.108272 Val Loss: 0.080208\n",
      "Epoch [88/1000] Train Loss: 0.106996 Val Loss: 0.080305\n",
      "Epoch [89/1000] Train Loss: 0.107599 Val Loss: 0.080193\n",
      "Epoch [90/1000] Train Loss: 0.105784 Val Loss: 0.080341\n",
      "Epoch [91/1000] Train Loss: 0.106161 Val Loss: 0.080231\n",
      "Epoch [92/1000] Train Loss: 0.106654 Val Loss: 0.080310\n",
      "Epoch [93/1000] Train Loss: 0.106542 Val Loss: 0.080208\n",
      "Epoch [94/1000] Train Loss: 0.109109 Val Loss: 0.080299\n",
      "Epoch [95/1000] Train Loss: 0.106277 Val Loss: 0.080231\n",
      "Epoch [96/1000] Train Loss: 0.106612 Val Loss: 0.080236\n",
      "Epoch [97/1000] Train Loss: 0.105981 Val Loss: 0.080244\n",
      "Epoch [98/1000] Train Loss: 0.107311 Val Loss: 0.080255\n",
      "Epoch [99/1000] Train Loss: 0.109145 Val Loss: 0.080227\n",
      "Epoch [100/1000] Train Loss: 0.106190 Val Loss: 0.080189\n",
      "Epoch [101/1000] Train Loss: 0.108975 Val Loss: 0.080223\n",
      "Epoch [102/1000] Train Loss: 0.106380 Val Loss: 0.080236\n",
      "Epoch [103/1000] Train Loss: 0.105880 Val Loss: 0.080426\n",
      "Epoch [104/1000] Train Loss: 0.107563 Val Loss: 0.080159\n",
      "Epoch [105/1000] Train Loss: 0.108518 Val Loss: 0.080151\n",
      "Epoch [106/1000] Train Loss: 0.106140 Val Loss: 0.080401\n",
      "Epoch [107/1000] Train Loss: 0.106221 Val Loss: 0.080202\n",
      "Epoch [108/1000] Train Loss: 0.109104 Val Loss: 0.080349\n",
      "Epoch [109/1000] Train Loss: 0.107002 Val Loss: 0.080222\n",
      "Epoch [110/1000] Train Loss: 0.109410 Val Loss: 0.080170\n",
      "Epoch [111/1000] Train Loss: 0.107445 Val Loss: 0.080402\n",
      "Epoch [112/1000] Train Loss: 0.108460 Val Loss: 0.080177\n",
      "Epoch [113/1000] Train Loss: 0.109223 Val Loss: 0.080268\n",
      "Epoch [114/1000] Train Loss: 0.110629 Val Loss: 0.080218\n",
      "Epoch [115/1000] Train Loss: 0.108414 Val Loss: 0.080287\n",
      "Epoch [116/1000] Train Loss: 0.108824 Val Loss: 0.080145\n",
      "Epoch [117/1000] Train Loss: 0.110265 Val Loss: 0.080325\n",
      "Epoch [118/1000] Train Loss: 0.106783 Val Loss: 0.080246\n",
      "Epoch [119/1000] Train Loss: 0.107822 Val Loss: 0.080306\n",
      "Epoch [120/1000] Train Loss: 0.109470 Val Loss: 0.080135\n",
      "Epoch [121/1000] Train Loss: 0.105802 Val Loss: 0.080154\n",
      "Epoch [122/1000] Train Loss: 0.108180 Val Loss: 0.080212\n",
      "Epoch [123/1000] Train Loss: 0.106829 Val Loss: 0.080180\n",
      "Epoch [124/1000] Train Loss: 0.106179 Val Loss: 0.080283\n",
      "Epoch [125/1000] Train Loss: 0.108378 Val Loss: 0.080213\n",
      "Epoch [126/1000] Train Loss: 0.105332 Val Loss: 0.080286\n",
      "Epoch [127/1000] Train Loss: 0.107386 Val Loss: 0.080183\n",
      "Epoch [128/1000] Train Loss: 0.109837 Val Loss: 0.080310\n",
      "Epoch [129/1000] Train Loss: 0.106928 Val Loss: 0.080303\n",
      "Epoch [130/1000] Train Loss: 0.107953 Val Loss: 0.080092\n",
      "Epoch [131/1000] Train Loss: 0.108436 Val Loss: 0.080329\n",
      "Epoch [132/1000] Train Loss: 0.109859 Val Loss: 0.080102\n",
      "Epoch [133/1000] Train Loss: 0.108500 Val Loss: 0.080173\n",
      "Epoch [134/1000] Train Loss: 0.105682 Val Loss: 0.080237\n",
      "Epoch [135/1000] Train Loss: 0.105678 Val Loss: 0.080005\n",
      "Epoch [136/1000] Train Loss: 0.108645 Val Loss: 0.080209\n",
      "Epoch [137/1000] Train Loss: 0.106831 Val Loss: 0.080138\n",
      "Epoch [138/1000] Train Loss: 0.108952 Val Loss: 0.080152\n",
      "Epoch [139/1000] Train Loss: 0.106548 Val Loss: 0.080185\n",
      "Epoch [140/1000] Train Loss: 0.105511 Val Loss: 0.080062\n",
      "Epoch [141/1000] Train Loss: 0.106496 Val Loss: 0.080216\n",
      "Epoch [142/1000] Train Loss: 0.106334 Val Loss: 0.080025\n",
      "Epoch [143/1000] Train Loss: 0.107507 Val Loss: 0.080076\n",
      "Epoch [144/1000] Train Loss: 0.107895 Val Loss: 0.080208\n",
      "Epoch [145/1000] Train Loss: 0.107296 Val Loss: 0.080093\n",
      "Epoch [146/1000] Train Loss: 0.110778 Val Loss: 0.080018\n",
      "Epoch [147/1000] Train Loss: 0.110069 Val Loss: 0.079981\n",
      "Epoch [148/1000] Train Loss: 0.109407 Val Loss: 0.080107\n",
      "Epoch [149/1000] Train Loss: 0.108781 Val Loss: 0.080028\n",
      "Epoch [150/1000] Train Loss: 0.105928 Val Loss: 0.080027\n",
      "Epoch [151/1000] Train Loss: 0.105878 Val Loss: 0.079944\n",
      "Epoch [152/1000] Train Loss: 0.106942 Val Loss: 0.079958\n",
      "Epoch [153/1000] Train Loss: 0.109645 Val Loss: 0.080070\n",
      "Epoch [154/1000] Train Loss: 0.110094 Val Loss: 0.080104\n",
      "Epoch [155/1000] Train Loss: 0.105687 Val Loss: 0.079932\n",
      "Epoch [156/1000] Train Loss: 0.105742 Val Loss: 0.079994\n",
      "Epoch [157/1000] Train Loss: 0.107219 Val Loss: 0.080049\n",
      "Epoch [158/1000] Train Loss: 0.105586 Val Loss: 0.079896\n",
      "Epoch [159/1000] Train Loss: 0.105573 Val Loss: 0.079824\n",
      "Epoch [160/1000] Train Loss: 0.109719 Val Loss: 0.079898\n",
      "Epoch [161/1000] Train Loss: 0.104617 Val Loss: 0.079883\n",
      "Epoch [162/1000] Train Loss: 0.105399 Val Loss: 0.079911\n",
      "Epoch [163/1000] Train Loss: 0.106551 Val Loss: 0.079837\n",
      "Epoch [164/1000] Train Loss: 0.106141 Val Loss: 0.079838\n",
      "Epoch [165/1000] Train Loss: 0.106319 Val Loss: 0.079925\n",
      "Epoch [166/1000] Train Loss: 0.104817 Val Loss: 0.079787\n",
      "Epoch [167/1000] Train Loss: 0.107266 Val Loss: 0.079766\n",
      "Epoch [168/1000] Train Loss: 0.105137 Val Loss: 0.079827\n",
      "Epoch [169/1000] Train Loss: 0.108006 Val Loss: 0.079659\n",
      "Epoch [170/1000] Train Loss: 0.107167 Val Loss: 0.079943\n",
      "Epoch [171/1000] Train Loss: 0.106577 Val Loss: 0.079748\n",
      "Epoch [172/1000] Train Loss: 0.105248 Val Loss: 0.079639\n",
      "Epoch [173/1000] Train Loss: 0.104712 Val Loss: 0.079710\n",
      "Epoch [174/1000] Train Loss: 0.106220 Val Loss: 0.079637\n",
      "Epoch [175/1000] Train Loss: 0.108222 Val Loss: 0.079611\n",
      "Epoch [176/1000] Train Loss: 0.105688 Val Loss: 0.079594\n",
      "Epoch [177/1000] Train Loss: 0.107144 Val Loss: 0.079665\n",
      "Epoch [178/1000] Train Loss: 0.104871 Val Loss: 0.079583\n",
      "Epoch [179/1000] Train Loss: 0.106444 Val Loss: 0.079600\n",
      "Epoch [180/1000] Train Loss: 0.110389 Val Loss: 0.079558\n",
      "Epoch [181/1000] Train Loss: 0.105486 Val Loss: 0.079513\n",
      "Epoch [182/1000] Train Loss: 0.107927 Val Loss: 0.079360\n",
      "Epoch [183/1000] Train Loss: 0.107513 Val Loss: 0.079716\n",
      "Epoch [184/1000] Train Loss: 0.103608 Val Loss: 0.079401\n",
      "Epoch [185/1000] Train Loss: 0.103584 Val Loss: 0.079343\n",
      "Epoch [186/1000] Train Loss: 0.106735 Val Loss: 0.079338\n",
      "Epoch [187/1000] Train Loss: 0.104422 Val Loss: 0.079401\n",
      "Epoch [188/1000] Train Loss: 0.103661 Val Loss: 0.079451\n",
      "Epoch [189/1000] Train Loss: 0.105222 Val Loss: 0.079426\n",
      "Epoch [190/1000] Train Loss: 0.108588 Val Loss: 0.079304\n",
      "Epoch [191/1000] Train Loss: 0.107223 Val Loss: 0.079192\n",
      "Epoch [192/1000] Train Loss: 0.103510 Val Loss: 0.079419\n",
      "Epoch [193/1000] Train Loss: 0.104272 Val Loss: 0.079240\n",
      "Epoch [194/1000] Train Loss: 0.105894 Val Loss: 0.079291\n",
      "Epoch [195/1000] Train Loss: 0.105249 Val Loss: 0.079216\n",
      "Epoch [196/1000] Train Loss: 0.105793 Val Loss: 0.079196\n",
      "Epoch [197/1000] Train Loss: 0.103772 Val Loss: 0.079114\n",
      "Epoch [198/1000] Train Loss: 0.106810 Val Loss: 0.079096\n",
      "Epoch [199/1000] Train Loss: 0.105271 Val Loss: 0.079142\n",
      "Epoch [200/1000] Train Loss: 0.105557 Val Loss: 0.079074\n",
      "Epoch [201/1000] Train Loss: 0.105071 Val Loss: 0.079049\n",
      "Epoch [202/1000] Train Loss: 0.104124 Val Loss: 0.079076\n",
      "Epoch [203/1000] Train Loss: 0.104221 Val Loss: 0.079044\n",
      "Epoch [204/1000] Train Loss: 0.105981 Val Loss: 0.078872\n",
      "Epoch [205/1000] Train Loss: 0.104691 Val Loss: 0.079212\n",
      "Epoch [206/1000] Train Loss: 0.103651 Val Loss: 0.078953\n",
      "Epoch [207/1000] Train Loss: 0.104137 Val Loss: 0.078883\n",
      "Epoch [208/1000] Train Loss: 0.105729 Val Loss: 0.078871\n",
      "Epoch [209/1000] Train Loss: 0.107256 Val Loss: 0.078823\n",
      "Epoch [210/1000] Train Loss: 0.105081 Val Loss: 0.078983\n",
      "Epoch [211/1000] Train Loss: 0.103487 Val Loss: 0.078770\n",
      "Epoch [212/1000] Train Loss: 0.104629 Val Loss: 0.078932\n",
      "Epoch [213/1000] Train Loss: 0.102508 Val Loss: 0.079063\n",
      "Epoch [214/1000] Train Loss: 0.106568 Val Loss: 0.078638\n",
      "Epoch [215/1000] Train Loss: 0.105551 Val Loss: 0.078780\n",
      "Epoch [216/1000] Train Loss: 0.104374 Val Loss: 0.078832\n",
      "Epoch [217/1000] Train Loss: 0.102860 Val Loss: 0.078798\n",
      "Epoch [218/1000] Train Loss: 0.105323 Val Loss: 0.078621\n",
      "Epoch [219/1000] Train Loss: 0.102969 Val Loss: 0.078842\n",
      "Epoch [220/1000] Train Loss: 0.105672 Val Loss: 0.078556\n",
      "Epoch [221/1000] Train Loss: 0.103177 Val Loss: 0.078694\n",
      "Epoch [222/1000] Train Loss: 0.102559 Val Loss: 0.078802\n",
      "Epoch [223/1000] Train Loss: 0.103487 Val Loss: 0.078745\n",
      "Epoch [224/1000] Train Loss: 0.103484 Val Loss: 0.078583\n",
      "Epoch [225/1000] Train Loss: 0.104390 Val Loss: 0.078902\n",
      "Epoch [226/1000] Train Loss: 0.105838 Val Loss: 0.078420\n",
      "Epoch [227/1000] Train Loss: 0.103011 Val Loss: 0.078827\n",
      "Epoch [228/1000] Train Loss: 0.102381 Val Loss: 0.078399\n",
      "Epoch [229/1000] Train Loss: 0.107249 Val Loss: 0.078577\n",
      "Epoch [230/1000] Train Loss: 0.101783 Val Loss: 0.078601\n",
      "Epoch [231/1000] Train Loss: 0.103221 Val Loss: 0.078505\n",
      "Epoch [232/1000] Train Loss: 0.103586 Val Loss: 0.078622\n",
      "Epoch [233/1000] Train Loss: 0.104138 Val Loss: 0.078441\n",
      "Epoch [234/1000] Train Loss: 0.103157 Val Loss: 0.078760\n",
      "Epoch [235/1000] Train Loss: 0.104830 Val Loss: 0.078403\n",
      "Epoch [236/1000] Train Loss: 0.103462 Val Loss: 0.078752\n",
      "Epoch [237/1000] Train Loss: 0.102351 Val Loss: 0.078357\n",
      "Epoch [238/1000] Train Loss: 0.103837 Val Loss: 0.078442\n",
      "Epoch [239/1000] Train Loss: 0.105488 Val Loss: 0.078501\n",
      "Epoch [240/1000] Train Loss: 0.103817 Val Loss: 0.078494\n",
      "Epoch [241/1000] Train Loss: 0.102879 Val Loss: 0.078430\n",
      "Epoch [242/1000] Train Loss: 0.102537 Val Loss: 0.078332\n",
      "Epoch [243/1000] Train Loss: 0.103032 Val Loss: 0.078627\n",
      "Epoch [244/1000] Train Loss: 0.101728 Val Loss: 0.078362\n",
      "Epoch [245/1000] Train Loss: 0.102711 Val Loss: 0.078512\n",
      "Epoch [246/1000] Train Loss: 0.103001 Val Loss: 0.078393\n",
      "Epoch [247/1000] Train Loss: 0.103163 Val Loss: 0.078439\n",
      "Epoch [248/1000] Train Loss: 0.104605 Val Loss: 0.078285\n",
      "Epoch [249/1000] Train Loss: 0.103753 Val Loss: 0.078412\n",
      "Epoch [250/1000] Train Loss: 0.101389 Val Loss: 0.078401\n",
      "Epoch [251/1000] Train Loss: 0.103764 Val Loss: 0.078324\n",
      "Epoch [252/1000] Train Loss: 0.105565 Val Loss: 0.078324\n",
      "Epoch [253/1000] Train Loss: 0.103457 Val Loss: 0.078421\n",
      "Epoch [254/1000] Train Loss: 0.101177 Val Loss: 0.078323\n",
      "Epoch [255/1000] Train Loss: 0.103026 Val Loss: 0.078210\n",
      "Epoch [256/1000] Train Loss: 0.101185 Val Loss: 0.078479\n",
      "Epoch [257/1000] Train Loss: 0.101533 Val Loss: 0.078044\n",
      "Epoch [258/1000] Train Loss: 0.104446 Val Loss: 0.078407\n",
      "Epoch [259/1000] Train Loss: 0.102325 Val Loss: 0.078160\n",
      "Epoch [260/1000] Train Loss: 0.101471 Val Loss: 0.078324\n",
      "Epoch [261/1000] Train Loss: 0.103027 Val Loss: 0.078071\n",
      "Epoch [262/1000] Train Loss: 0.101550 Val Loss: 0.078259\n",
      "Epoch [263/1000] Train Loss: 0.102650 Val Loss: 0.078295\n",
      "Epoch [264/1000] Train Loss: 0.102445 Val Loss: 0.078157\n",
      "Epoch [265/1000] Train Loss: 0.103492 Val Loss: 0.078144\n",
      "Epoch [266/1000] Train Loss: 0.104446 Val Loss: 0.078344\n",
      "Epoch [267/1000] Train Loss: 0.102465 Val Loss: 0.078025\n",
      "Epoch [268/1000] Train Loss: 0.102003 Val Loss: 0.078069\n",
      "Epoch [269/1000] Train Loss: 0.102283 Val Loss: 0.077994\n",
      "Epoch [270/1000] Train Loss: 0.103257 Val Loss: 0.077993\n",
      "Epoch [271/1000] Train Loss: 0.100489 Val Loss: 0.078055\n",
      "Epoch [272/1000] Train Loss: 0.100642 Val Loss: 0.078019\n",
      "Epoch [273/1000] Train Loss: 0.101850 Val Loss: 0.077975\n",
      "Epoch [274/1000] Train Loss: 0.100883 Val Loss: 0.077979\n",
      "Epoch [275/1000] Train Loss: 0.100741 Val Loss: 0.077991\n",
      "Epoch [276/1000] Train Loss: 0.102679 Val Loss: 0.078008\n",
      "Epoch [277/1000] Train Loss: 0.101520 Val Loss: 0.077783\n",
      "Epoch [278/1000] Train Loss: 0.100598 Val Loss: 0.078001\n",
      "Epoch [279/1000] Train Loss: 0.100514 Val Loss: 0.077900\n",
      "Epoch [280/1000] Train Loss: 0.101358 Val Loss: 0.077926\n",
      "Epoch [281/1000] Train Loss: 0.101869 Val Loss: 0.077684\n",
      "Epoch [282/1000] Train Loss: 0.100480 Val Loss: 0.078069\n",
      "Epoch [283/1000] Train Loss: 0.100279 Val Loss: 0.077883\n",
      "Epoch [284/1000] Train Loss: 0.101670 Val Loss: 0.077668\n",
      "Epoch [285/1000] Train Loss: 0.100963 Val Loss: 0.077795\n",
      "Epoch [286/1000] Train Loss: 0.100877 Val Loss: 0.077825\n",
      "Epoch [287/1000] Train Loss: 0.100419 Val Loss: 0.077767\n",
      "Epoch [288/1000] Train Loss: 0.102809 Val Loss: 0.077673\n",
      "Epoch [289/1000] Train Loss: 0.102259 Val Loss: 0.077855\n",
      "Epoch [290/1000] Train Loss: 0.102947 Val Loss: 0.077627\n",
      "Epoch [291/1000] Train Loss: 0.103806 Val Loss: 0.077734\n",
      "Epoch [292/1000] Train Loss: 0.101327 Val Loss: 0.077522\n",
      "Epoch [293/1000] Train Loss: 0.102994 Val Loss: 0.077615\n",
      "Epoch [294/1000] Train Loss: 0.100668 Val Loss: 0.077681\n",
      "Epoch [295/1000] Train Loss: 0.101327 Val Loss: 0.077391\n",
      "Epoch [296/1000] Train Loss: 0.101797 Val Loss: 0.077569\n",
      "Epoch [297/1000] Train Loss: 0.099888 Val Loss: 0.077538\n",
      "Epoch [298/1000] Train Loss: 0.100624 Val Loss: 0.077536\n",
      "Epoch [299/1000] Train Loss: 0.100152 Val Loss: 0.077616\n",
      "Epoch [300/1000] Train Loss: 0.101493 Val Loss: 0.077466\n",
      "Epoch [301/1000] Train Loss: 0.100190 Val Loss: 0.077460\n",
      "Epoch [302/1000] Train Loss: 0.100974 Val Loss: 0.077359\n",
      "Epoch [303/1000] Train Loss: 0.099922 Val Loss: 0.077534\n",
      "Epoch [304/1000] Train Loss: 0.099545 Val Loss: 0.077359\n",
      "Epoch [305/1000] Train Loss: 0.099815 Val Loss: 0.077274\n",
      "Epoch [306/1000] Train Loss: 0.100090 Val Loss: 0.077310\n",
      "Epoch [307/1000] Train Loss: 0.099740 Val Loss: 0.077249\n",
      "Epoch [308/1000] Train Loss: 0.099483 Val Loss: 0.077340\n",
      "Epoch [309/1000] Train Loss: 0.100054 Val Loss: 0.077110\n",
      "Epoch [310/1000] Train Loss: 0.100087 Val Loss: 0.077355\n",
      "Epoch [311/1000] Train Loss: 0.102727 Val Loss: 0.077068\n",
      "Epoch [312/1000] Train Loss: 0.099367 Val Loss: 0.077163\n",
      "Epoch [313/1000] Train Loss: 0.101089 Val Loss: 0.077068\n",
      "Epoch [314/1000] Train Loss: 0.100925 Val Loss: 0.076954\n",
      "Epoch [315/1000] Train Loss: 0.100566 Val Loss: 0.077294\n",
      "Epoch [316/1000] Train Loss: 0.099861 Val Loss: 0.076986\n",
      "Epoch [317/1000] Train Loss: 0.099082 Val Loss: 0.076905\n",
      "Epoch [318/1000] Train Loss: 0.099747 Val Loss: 0.077160\n",
      "Epoch [319/1000] Train Loss: 0.099531 Val Loss: 0.076947\n",
      "Epoch [320/1000] Train Loss: 0.099269 Val Loss: 0.076904\n",
      "Epoch [321/1000] Train Loss: 0.100901 Val Loss: 0.076887\n",
      "Epoch [322/1000] Train Loss: 0.100021 Val Loss: 0.077236\n",
      "Epoch [323/1000] Train Loss: 0.099056 Val Loss: 0.076739\n",
      "Epoch [324/1000] Train Loss: 0.100340 Val Loss: 0.076911\n",
      "Epoch [325/1000] Train Loss: 0.100700 Val Loss: 0.076600\n",
      "Epoch [326/1000] Train Loss: 0.100995 Val Loss: 0.076848\n",
      "Epoch [327/1000] Train Loss: 0.100463 Val Loss: 0.076641\n",
      "Epoch [328/1000] Train Loss: 0.100154 Val Loss: 0.076798\n",
      "Epoch [329/1000] Train Loss: 0.100821 Val Loss: 0.076559\n",
      "Epoch [330/1000] Train Loss: 0.100770 Val Loss: 0.076603\n",
      "Epoch [331/1000] Train Loss: 0.099563 Val Loss: 0.076577\n",
      "Epoch [332/1000] Train Loss: 0.101637 Val Loss: 0.076458\n",
      "Epoch [333/1000] Train Loss: 0.100865 Val Loss: 0.076688\n",
      "Epoch [334/1000] Train Loss: 0.099963 Val Loss: 0.076440\n",
      "Epoch [335/1000] Train Loss: 0.098279 Val Loss: 0.076448\n",
      "Epoch [336/1000] Train Loss: 0.098857 Val Loss: 0.076439\n",
      "Epoch [337/1000] Train Loss: 0.098780 Val Loss: 0.076512\n",
      "Epoch [338/1000] Train Loss: 0.098274 Val Loss: 0.076369\n",
      "Epoch [339/1000] Train Loss: 0.099254 Val Loss: 0.076176\n",
      "Epoch [340/1000] Train Loss: 0.102028 Val Loss: 0.076195\n",
      "Epoch [341/1000] Train Loss: 0.098660 Val Loss: 0.076237\n",
      "Epoch [342/1000] Train Loss: 0.099821 Val Loss: 0.076169\n",
      "Epoch [343/1000] Train Loss: 0.097904 Val Loss: 0.076233\n",
      "Epoch [344/1000] Train Loss: 0.099321 Val Loss: 0.075995\n",
      "Epoch [345/1000] Train Loss: 0.098581 Val Loss: 0.076277\n",
      "Epoch [346/1000] Train Loss: 0.098611 Val Loss: 0.075994\n",
      "Epoch [347/1000] Train Loss: 0.098492 Val Loss: 0.076064\n",
      "Epoch [348/1000] Train Loss: 0.099476 Val Loss: 0.075871\n",
      "Epoch [349/1000] Train Loss: 0.098863 Val Loss: 0.075774\n",
      "Epoch [350/1000] Train Loss: 0.098605 Val Loss: 0.076062\n",
      "Epoch [351/1000] Train Loss: 0.098520 Val Loss: 0.075805\n",
      "Epoch [352/1000] Train Loss: 0.098339 Val Loss: 0.075648\n",
      "Epoch [353/1000] Train Loss: 0.100877 Val Loss: 0.075854\n",
      "Epoch [354/1000] Train Loss: 0.098817 Val Loss: 0.075546\n",
      "Epoch [355/1000] Train Loss: 0.096390 Val Loss: 0.075791\n",
      "Epoch [356/1000] Train Loss: 0.098538 Val Loss: 0.075535\n",
      "Epoch [357/1000] Train Loss: 0.098399 Val Loss: 0.075510\n",
      "Epoch [358/1000] Train Loss: 0.098039 Val Loss: 0.075442\n",
      "Epoch [359/1000] Train Loss: 0.097382 Val Loss: 0.075517\n",
      "Epoch [360/1000] Train Loss: 0.097604 Val Loss: 0.075606\n",
      "Epoch [361/1000] Train Loss: 0.096251 Val Loss: 0.075353\n",
      "Epoch [362/1000] Train Loss: 0.097154 Val Loss: 0.075309\n",
      "Epoch [363/1000] Train Loss: 0.102311 Val Loss: 0.075277\n",
      "Epoch [364/1000] Train Loss: 0.096973 Val Loss: 0.075288\n",
      "Epoch [365/1000] Train Loss: 0.097453 Val Loss: 0.075251\n",
      "Epoch [366/1000] Train Loss: 0.098676 Val Loss: 0.075271\n",
      "Epoch [367/1000] Train Loss: 0.098223 Val Loss: 0.075081\n",
      "Epoch [368/1000] Train Loss: 0.096170 Val Loss: 0.075094\n",
      "Epoch [369/1000] Train Loss: 0.101649 Val Loss: 0.074896\n",
      "Epoch [370/1000] Train Loss: 0.098646 Val Loss: 0.075179\n",
      "Epoch [371/1000] Train Loss: 0.095400 Val Loss: 0.074954\n",
      "Epoch [372/1000] Train Loss: 0.096370 Val Loss: 0.074833\n",
      "Epoch [373/1000] Train Loss: 0.095862 Val Loss: 0.074829\n",
      "Epoch [374/1000] Train Loss: 0.098692 Val Loss: 0.074840\n",
      "Epoch [375/1000] Train Loss: 0.100671 Val Loss: 0.074653\n",
      "Epoch [376/1000] Train Loss: 0.097111 Val Loss: 0.074796\n",
      "Epoch [377/1000] Train Loss: 0.097874 Val Loss: 0.074463\n",
      "Epoch [378/1000] Train Loss: 0.096882 Val Loss: 0.074677\n",
      "Epoch [379/1000] Train Loss: 0.097772 Val Loss: 0.074583\n",
      "Epoch [380/1000] Train Loss: 0.098186 Val Loss: 0.074579\n",
      "Epoch [381/1000] Train Loss: 0.095565 Val Loss: 0.074501\n",
      "Epoch [382/1000] Train Loss: 0.098996 Val Loss: 0.074296\n",
      "Epoch [383/1000] Train Loss: 0.098362 Val Loss: 0.074456\n",
      "Epoch [384/1000] Train Loss: 0.095217 Val Loss: 0.074411\n",
      "Epoch [385/1000] Train Loss: 0.098517 Val Loss: 0.074091\n",
      "Epoch [386/1000] Train Loss: 0.096377 Val Loss: 0.074181\n",
      "Epoch [387/1000] Train Loss: 0.095099 Val Loss: 0.074219\n",
      "Epoch [388/1000] Train Loss: 0.099806 Val Loss: 0.074059\n",
      "Epoch [389/1000] Train Loss: 0.094873 Val Loss: 0.074232\n",
      "Epoch [390/1000] Train Loss: 0.095168 Val Loss: 0.073797\n",
      "Epoch [391/1000] Train Loss: 0.098728 Val Loss: 0.074001\n",
      "Epoch [392/1000] Train Loss: 0.097012 Val Loss: 0.073745\n",
      "Epoch [393/1000] Train Loss: 0.094575 Val Loss: 0.073821\n",
      "Epoch [394/1000] Train Loss: 0.094346 Val Loss: 0.073802\n",
      "Epoch [395/1000] Train Loss: 0.094276 Val Loss: 0.073607\n",
      "Epoch [396/1000] Train Loss: 0.097884 Val Loss: 0.073690\n",
      "Epoch [397/1000] Train Loss: 0.098688 Val Loss: 0.073442\n",
      "Epoch [398/1000] Train Loss: 0.094759 Val Loss: 0.073627\n",
      "Epoch [399/1000] Train Loss: 0.095111 Val Loss: 0.073525\n",
      "Epoch [400/1000] Train Loss: 0.096357 Val Loss: 0.073632\n",
      "Epoch [401/1000] Train Loss: 0.098677 Val Loss: 0.073282\n",
      "Epoch [402/1000] Train Loss: 0.094775 Val Loss: 0.073418\n",
      "Epoch [403/1000] Train Loss: 0.092882 Val Loss: 0.073189\n",
      "Epoch [404/1000] Train Loss: 0.095210 Val Loss: 0.073123\n",
      "Epoch [405/1000] Train Loss: 0.093936 Val Loss: 0.073050\n",
      "Epoch [406/1000] Train Loss: 0.095186 Val Loss: 0.073067\n",
      "Epoch [407/1000] Train Loss: 0.095064 Val Loss: 0.073007\n",
      "Epoch [408/1000] Train Loss: 0.094623 Val Loss: 0.072957\n",
      "Epoch [409/1000] Train Loss: 0.094769 Val Loss: 0.072955\n",
      "Epoch [410/1000] Train Loss: 0.095218 Val Loss: 0.072800\n",
      "Epoch [411/1000] Train Loss: 0.095485 Val Loss: 0.072873\n",
      "Epoch [412/1000] Train Loss: 0.096408 Val Loss: 0.072760\n",
      "Epoch [413/1000] Train Loss: 0.093026 Val Loss: 0.072699\n",
      "Epoch [414/1000] Train Loss: 0.094417 Val Loss: 0.072604\n",
      "Epoch [415/1000] Train Loss: 0.093305 Val Loss: 0.072552\n",
      "Epoch [416/1000] Train Loss: 0.093225 Val Loss: 0.072489\n",
      "Epoch [417/1000] Train Loss: 0.092893 Val Loss: 0.072431\n",
      "Epoch [418/1000] Train Loss: 0.091859 Val Loss: 0.072507\n",
      "Epoch [419/1000] Train Loss: 0.094011 Val Loss: 0.072391\n",
      "Epoch [420/1000] Train Loss: 0.094913 Val Loss: 0.072170\n",
      "Epoch [421/1000] Train Loss: 0.093237 Val Loss: 0.072188\n",
      "Epoch [422/1000] Train Loss: 0.092269 Val Loss: 0.072372\n",
      "Epoch [423/1000] Train Loss: 0.093262 Val Loss: 0.072077\n",
      "Epoch [424/1000] Train Loss: 0.092378 Val Loss: 0.072194\n",
      "Epoch [425/1000] Train Loss: 0.093187 Val Loss: 0.071907\n",
      "Epoch [426/1000] Train Loss: 0.092858 Val Loss: 0.072220\n",
      "Epoch [427/1000] Train Loss: 0.093421 Val Loss: 0.071862\n",
      "Epoch [428/1000] Train Loss: 0.093526 Val Loss: 0.071901\n",
      "Epoch [429/1000] Train Loss: 0.092622 Val Loss: 0.071888\n",
      "Epoch [430/1000] Train Loss: 0.092425 Val Loss: 0.071768\n",
      "Epoch [431/1000] Train Loss: 0.092259 Val Loss: 0.071839\n",
      "Epoch [432/1000] Train Loss: 0.091602 Val Loss: 0.071753\n",
      "Epoch [433/1000] Train Loss: 0.091708 Val Loss: 0.071676\n",
      "Epoch [434/1000] Train Loss: 0.093490 Val Loss: 0.071592\n",
      "Epoch [435/1000] Train Loss: 0.093323 Val Loss: 0.071657\n",
      "Epoch [436/1000] Train Loss: 0.092457 Val Loss: 0.071646\n",
      "Epoch [437/1000] Train Loss: 0.093469 Val Loss: 0.071417\n",
      "Epoch [438/1000] Train Loss: 0.095084 Val Loss: 0.071335\n",
      "Epoch [439/1000] Train Loss: 0.091632 Val Loss: 0.071262\n",
      "Epoch [440/1000] Train Loss: 0.094022 Val Loss: 0.071286\n",
      "Epoch [441/1000] Train Loss: 0.091701 Val Loss: 0.071386\n",
      "Epoch [442/1000] Train Loss: 0.091242 Val Loss: 0.071211\n",
      "Epoch [443/1000] Train Loss: 0.093933 Val Loss: 0.071288\n",
      "Epoch [444/1000] Train Loss: 0.091440 Val Loss: 0.071334\n",
      "Epoch [445/1000] Train Loss: 0.090607 Val Loss: 0.071048\n",
      "Epoch [446/1000] Train Loss: 0.091416 Val Loss: 0.071152\n",
      "Epoch [447/1000] Train Loss: 0.091671 Val Loss: 0.070903\n",
      "Epoch [448/1000] Train Loss: 0.093868 Val Loss: 0.070881\n",
      "Epoch [449/1000] Train Loss: 0.090278 Val Loss: 0.070938\n",
      "Epoch [450/1000] Train Loss: 0.091818 Val Loss: 0.070845\n",
      "Epoch [451/1000] Train Loss: 0.091453 Val Loss: 0.070871\n",
      "Epoch [452/1000] Train Loss: 0.090934 Val Loss: 0.070760\n",
      "Epoch [453/1000] Train Loss: 0.091369 Val Loss: 0.070775\n",
      "Epoch [454/1000] Train Loss: 0.094279 Val Loss: 0.070600\n",
      "Epoch [455/1000] Train Loss: 0.089334 Val Loss: 0.070790\n",
      "Epoch [456/1000] Train Loss: 0.090384 Val Loss: 0.070540\n",
      "Epoch [457/1000] Train Loss: 0.094563 Val Loss: 0.070506\n",
      "Epoch [458/1000] Train Loss: 0.089848 Val Loss: 0.070513\n",
      "Epoch [459/1000] Train Loss: 0.090737 Val Loss: 0.070512\n",
      "Epoch [460/1000] Train Loss: 0.090186 Val Loss: 0.070559\n",
      "Epoch [461/1000] Train Loss: 0.089392 Val Loss: 0.070341\n",
      "Epoch [462/1000] Train Loss: 0.090603 Val Loss: 0.070268\n",
      "Epoch [463/1000] Train Loss: 0.090094 Val Loss: 0.070289\n",
      "Epoch [464/1000] Train Loss: 0.090978 Val Loss: 0.070219\n",
      "Epoch [465/1000] Train Loss: 0.090879 Val Loss: 0.070234\n",
      "Epoch [466/1000] Train Loss: 0.088932 Val Loss: 0.070258\n",
      "Epoch [467/1000] Train Loss: 0.090220 Val Loss: 0.070241\n",
      "Epoch [468/1000] Train Loss: 0.089071 Val Loss: 0.070142\n",
      "Epoch [469/1000] Train Loss: 0.090445 Val Loss: 0.070042\n",
      "Epoch [470/1000] Train Loss: 0.090019 Val Loss: 0.070042\n",
      "Epoch [471/1000] Train Loss: 0.089965 Val Loss: 0.069857\n",
      "Epoch [472/1000] Train Loss: 0.089586 Val Loss: 0.069993\n",
      "Epoch [473/1000] Train Loss: 0.090998 Val Loss: 0.069886\n",
      "Epoch [474/1000] Train Loss: 0.090272 Val Loss: 0.069920\n",
      "Epoch [475/1000] Train Loss: 0.091838 Val Loss: 0.069810\n",
      "Epoch [476/1000] Train Loss: 0.090003 Val Loss: 0.069868\n",
      "Epoch [477/1000] Train Loss: 0.089406 Val Loss: 0.069713\n",
      "Epoch [478/1000] Train Loss: 0.089602 Val Loss: 0.069801\n",
      "Epoch [479/1000] Train Loss: 0.089561 Val Loss: 0.069593\n",
      "Epoch [480/1000] Train Loss: 0.088701 Val Loss: 0.069619\n",
      "Epoch [481/1000] Train Loss: 0.088296 Val Loss: 0.069506\n",
      "Epoch [482/1000] Train Loss: 0.089752 Val Loss: 0.069598\n",
      "Epoch [483/1000] Train Loss: 0.088031 Val Loss: 0.069566\n",
      "Epoch [484/1000] Train Loss: 0.089338 Val Loss: 0.069517\n",
      "Epoch [485/1000] Train Loss: 0.089724 Val Loss: 0.069421\n",
      "Epoch [486/1000] Train Loss: 0.087541 Val Loss: 0.069337\n",
      "Epoch [487/1000] Train Loss: 0.088868 Val Loss: 0.069324\n",
      "Epoch [488/1000] Train Loss: 0.090139 Val Loss: 0.069277\n",
      "Epoch [489/1000] Train Loss: 0.088010 Val Loss: 0.069313\n",
      "Epoch [490/1000] Train Loss: 0.090131 Val Loss: 0.069293\n",
      "Epoch [491/1000] Train Loss: 0.087836 Val Loss: 0.069217\n",
      "Epoch [492/1000] Train Loss: 0.087626 Val Loss: 0.069140\n",
      "Epoch [493/1000] Train Loss: 0.087458 Val Loss: 0.069246\n",
      "Epoch [494/1000] Train Loss: 0.088549 Val Loss: 0.069059\n",
      "Epoch [495/1000] Train Loss: 0.087263 Val Loss: 0.069118\n",
      "Epoch [496/1000] Train Loss: 0.089050 Val Loss: 0.068945\n",
      "Epoch [497/1000] Train Loss: 0.087607 Val Loss: 0.069011\n",
      "Epoch [498/1000] Train Loss: 0.086808 Val Loss: 0.068960\n",
      "Epoch [499/1000] Train Loss: 0.089324 Val Loss: 0.068886\n",
      "Epoch [500/1000] Train Loss: 0.086288 Val Loss: 0.068913\n",
      "Epoch [501/1000] Train Loss: 0.087551 Val Loss: 0.068825\n",
      "Epoch [502/1000] Train Loss: 0.086992 Val Loss: 0.068720\n",
      "Epoch [503/1000] Train Loss: 0.086863 Val Loss: 0.068792\n",
      "Epoch [504/1000] Train Loss: 0.087561 Val Loss: 0.068709\n",
      "Epoch [505/1000] Train Loss: 0.086947 Val Loss: 0.068807\n",
      "Epoch [506/1000] Train Loss: 0.088600 Val Loss: 0.068659\n",
      "Epoch [507/1000] Train Loss: 0.087740 Val Loss: 0.068597\n",
      "Epoch [508/1000] Train Loss: 0.089331 Val Loss: 0.068590\n",
      "Epoch [509/1000] Train Loss: 0.087552 Val Loss: 0.068597\n",
      "Epoch [510/1000] Train Loss: 0.087601 Val Loss: 0.068431\n",
      "Epoch [511/1000] Train Loss: 0.086492 Val Loss: 0.068517\n",
      "Epoch [512/1000] Train Loss: 0.086871 Val Loss: 0.068361\n",
      "Epoch [513/1000] Train Loss: 0.085637 Val Loss: 0.068341\n",
      "Epoch [514/1000] Train Loss: 0.085877 Val Loss: 0.068310\n",
      "Epoch [515/1000] Train Loss: 0.086294 Val Loss: 0.068370\n",
      "Epoch [516/1000] Train Loss: 0.088252 Val Loss: 0.068229\n",
      "Epoch [517/1000] Train Loss: 0.088157 Val Loss: 0.068194\n",
      "Epoch [518/1000] Train Loss: 0.085325 Val Loss: 0.068353\n",
      "Epoch [519/1000] Train Loss: 0.089506 Val Loss: 0.068128\n",
      "Epoch [520/1000] Train Loss: 0.086906 Val Loss: 0.068207\n",
      "Epoch [521/1000] Train Loss: 0.085274 Val Loss: 0.068125\n",
      "Epoch [522/1000] Train Loss: 0.086401 Val Loss: 0.068041\n",
      "Epoch [523/1000] Train Loss: 0.088205 Val Loss: 0.068006\n",
      "Epoch [524/1000] Train Loss: 0.085361 Val Loss: 0.068049\n",
      "Epoch [525/1000] Train Loss: 0.087579 Val Loss: 0.067934\n",
      "Epoch [526/1000] Train Loss: 0.086569 Val Loss: 0.067945\n",
      "Epoch [527/1000] Train Loss: 0.086018 Val Loss: 0.067903\n",
      "Epoch [528/1000] Train Loss: 0.086649 Val Loss: 0.067919\n",
      "Epoch [529/1000] Train Loss: 0.086162 Val Loss: 0.067796\n",
      "Epoch [530/1000] Train Loss: 0.085765 Val Loss: 0.067809\n",
      "Epoch [531/1000] Train Loss: 0.087753 Val Loss: 0.067743\n",
      "Epoch [532/1000] Train Loss: 0.085087 Val Loss: 0.067909\n",
      "Epoch [533/1000] Train Loss: 0.085616 Val Loss: 0.067596\n",
      "Epoch [534/1000] Train Loss: 0.085196 Val Loss: 0.067868\n",
      "Epoch [535/1000] Train Loss: 0.085330 Val Loss: 0.067473\n",
      "Epoch [536/1000] Train Loss: 0.086706 Val Loss: 0.067492\n",
      "Epoch [537/1000] Train Loss: 0.085766 Val Loss: 0.067501\n",
      "Epoch [538/1000] Train Loss: 0.085461 Val Loss: 0.067601\n",
      "Epoch [539/1000] Train Loss: 0.085587 Val Loss: 0.067574\n",
      "Epoch [540/1000] Train Loss: 0.085512 Val Loss: 0.067392\n",
      "Epoch [541/1000] Train Loss: 0.087471 Val Loss: 0.067340\n",
      "Epoch [542/1000] Train Loss: 0.085391 Val Loss: 0.067335\n",
      "Epoch [543/1000] Train Loss: 0.085850 Val Loss: 0.067440\n",
      "Epoch [544/1000] Train Loss: 0.084990 Val Loss: 0.067219\n",
      "Epoch [545/1000] Train Loss: 0.086865 Val Loss: 0.067210\n",
      "Epoch [546/1000] Train Loss: 0.086342 Val Loss: 0.067192\n",
      "Epoch [547/1000] Train Loss: 0.085506 Val Loss: 0.067120\n",
      "Epoch [548/1000] Train Loss: 0.083478 Val Loss: 0.067188\n",
      "Epoch [549/1000] Train Loss: 0.084217 Val Loss: 0.067128\n",
      "Epoch [550/1000] Train Loss: 0.085143 Val Loss: 0.067016\n",
      "Epoch [551/1000] Train Loss: 0.086277 Val Loss: 0.067123\n",
      "Epoch [552/1000] Train Loss: 0.089124 Val Loss: 0.067051\n",
      "Epoch [553/1000] Train Loss: 0.085091 Val Loss: 0.066924\n",
      "Epoch [554/1000] Train Loss: 0.084397 Val Loss: 0.066952\n",
      "Epoch [555/1000] Train Loss: 0.083778 Val Loss: 0.066901\n",
      "Epoch [556/1000] Train Loss: 0.083359 Val Loss: 0.066840\n",
      "Epoch [557/1000] Train Loss: 0.085228 Val Loss: 0.066821\n",
      "Epoch [558/1000] Train Loss: 0.084515 Val Loss: 0.066654\n",
      "Epoch [559/1000] Train Loss: 0.084892 Val Loss: 0.066924\n",
      "Epoch [560/1000] Train Loss: 0.085931 Val Loss: 0.066607\n",
      "Epoch [561/1000] Train Loss: 0.083025 Val Loss: 0.066767\n",
      "Epoch [562/1000] Train Loss: 0.084006 Val Loss: 0.066695\n",
      "Epoch [563/1000] Train Loss: 0.083653 Val Loss: 0.066627\n",
      "Epoch [564/1000] Train Loss: 0.083604 Val Loss: 0.066519\n",
      "Epoch [565/1000] Train Loss: 0.083788 Val Loss: 0.066692\n",
      "Epoch [566/1000] Train Loss: 0.084346 Val Loss: 0.066385\n",
      "Epoch [567/1000] Train Loss: 0.084589 Val Loss: 0.066421\n",
      "Epoch [568/1000] Train Loss: 0.084838 Val Loss: 0.066476\n",
      "Epoch [569/1000] Train Loss: 0.083021 Val Loss: 0.066589\n",
      "Epoch [570/1000] Train Loss: 0.083495 Val Loss: 0.066398\n",
      "Epoch [571/1000] Train Loss: 0.082803 Val Loss: 0.066452\n",
      "Epoch [572/1000] Train Loss: 0.083146 Val Loss: 0.066247\n",
      "Epoch [573/1000] Train Loss: 0.084202 Val Loss: 0.066240\n",
      "Epoch [574/1000] Train Loss: 0.082625 Val Loss: 0.066221\n",
      "Epoch [575/1000] Train Loss: 0.083771 Val Loss: 0.066302\n",
      "Epoch [576/1000] Train Loss: 0.083104 Val Loss: 0.066162\n",
      "Epoch [577/1000] Train Loss: 0.084249 Val Loss: 0.066159\n",
      "Epoch [578/1000] Train Loss: 0.084185 Val Loss: 0.066264\n",
      "Epoch [579/1000] Train Loss: 0.085016 Val Loss: 0.066086\n",
      "Epoch [580/1000] Train Loss: 0.085403 Val Loss: 0.066178\n",
      "Epoch [581/1000] Train Loss: 0.084495 Val Loss: 0.066105\n",
      "Epoch [582/1000] Train Loss: 0.081701 Val Loss: 0.066219\n",
      "Epoch [583/1000] Train Loss: 0.083492 Val Loss: 0.066081\n",
      "Epoch [584/1000] Train Loss: 0.081875 Val Loss: 0.065997\n",
      "Epoch [585/1000] Train Loss: 0.084553 Val Loss: 0.066091\n",
      "Epoch [586/1000] Train Loss: 0.081829 Val Loss: 0.065988\n",
      "Epoch [587/1000] Train Loss: 0.083811 Val Loss: 0.066144\n",
      "Epoch [588/1000] Train Loss: 0.083250 Val Loss: 0.065943\n",
      "Epoch [589/1000] Train Loss: 0.082718 Val Loss: 0.065812\n",
      "Epoch [590/1000] Train Loss: 0.085017 Val Loss: 0.065957\n",
      "Epoch [591/1000] Train Loss: 0.084235 Val Loss: 0.065878\n",
      "Epoch [592/1000] Train Loss: 0.082073 Val Loss: 0.065819\n",
      "Epoch [593/1000] Train Loss: 0.082275 Val Loss: 0.065686\n",
      "Epoch [594/1000] Train Loss: 0.083102 Val Loss: 0.065953\n",
      "Epoch [595/1000] Train Loss: 0.082107 Val Loss: 0.065762\n",
      "Epoch [596/1000] Train Loss: 0.082366 Val Loss: 0.065532\n",
      "Epoch [597/1000] Train Loss: 0.083477 Val Loss: 0.065811\n",
      "Epoch [598/1000] Train Loss: 0.082001 Val Loss: 0.065599\n",
      "Epoch [599/1000] Train Loss: 0.083229 Val Loss: 0.065856\n",
      "Epoch [600/1000] Train Loss: 0.081145 Val Loss: 0.065743\n",
      "Epoch [601/1000] Train Loss: 0.083546 Val Loss: 0.065688\n",
      "Epoch [602/1000] Train Loss: 0.081454 Val Loss: 0.065763\n",
      "Epoch [603/1000] Train Loss: 0.082219 Val Loss: 0.065541\n",
      "Epoch [604/1000] Train Loss: 0.082439 Val Loss: 0.065620\n",
      "Epoch [605/1000] Train Loss: 0.081988 Val Loss: 0.065534\n",
      "Epoch [606/1000] Train Loss: 0.080987 Val Loss: 0.065447\n",
      "Epoch [607/1000] Train Loss: 0.080947 Val Loss: 0.065746\n",
      "Epoch [608/1000] Train Loss: 0.081816 Val Loss: 0.065316\n",
      "Epoch [609/1000] Train Loss: 0.080773 Val Loss: 0.065526\n",
      "Epoch [610/1000] Train Loss: 0.083145 Val Loss: 0.065343\n",
      "Epoch [611/1000] Train Loss: 0.081054 Val Loss: 0.065443\n",
      "Epoch [612/1000] Train Loss: 0.080720 Val Loss: 0.065412\n",
      "Epoch [613/1000] Train Loss: 0.083325 Val Loss: 0.065364\n",
      "Epoch [614/1000] Train Loss: 0.081384 Val Loss: 0.065292\n",
      "Epoch [615/1000] Train Loss: 0.081997 Val Loss: 0.065467\n",
      "Epoch [616/1000] Train Loss: 0.081403 Val Loss: 0.065305\n",
      "Epoch [617/1000] Train Loss: 0.081954 Val Loss: 0.065156\n",
      "Epoch [618/1000] Train Loss: 0.081107 Val Loss: 0.065298\n",
      "Epoch [619/1000] Train Loss: 0.081884 Val Loss: 0.065342\n",
      "Epoch [620/1000] Train Loss: 0.081881 Val Loss: 0.065224\n",
      "Epoch [621/1000] Train Loss: 0.082026 Val Loss: 0.065068\n",
      "Epoch [622/1000] Train Loss: 0.081692 Val Loss: 0.065115\n",
      "Epoch [623/1000] Train Loss: 0.082909 Val Loss: 0.065228\n",
      "Epoch [624/1000] Train Loss: 0.082011 Val Loss: 0.065205\n",
      "Epoch [625/1000] Train Loss: 0.082510 Val Loss: 0.065203\n",
      "Epoch [626/1000] Train Loss: 0.080692 Val Loss: 0.065251\n",
      "Epoch [627/1000] Train Loss: 0.080301 Val Loss: 0.065160\n",
      "Epoch [628/1000] Train Loss: 0.083810 Val Loss: 0.064969\n",
      "Epoch [629/1000] Train Loss: 0.082740 Val Loss: 0.065163\n",
      "Epoch [630/1000] Train Loss: 0.080750 Val Loss: 0.065146\n",
      "Epoch [631/1000] Train Loss: 0.082213 Val Loss: 0.064956\n",
      "Epoch [632/1000] Train Loss: 0.081533 Val Loss: 0.065131\n",
      "Epoch [633/1000] Train Loss: 0.082540 Val Loss: 0.064889\n",
      "Epoch [634/1000] Train Loss: 0.080244 Val Loss: 0.065063\n",
      "Epoch [635/1000] Train Loss: 0.082075 Val Loss: 0.065035\n",
      "Epoch [636/1000] Train Loss: 0.079681 Val Loss: 0.065026\n",
      "Epoch [637/1000] Train Loss: 0.080290 Val Loss: 0.064942\n",
      "Epoch [638/1000] Train Loss: 0.081653 Val Loss: 0.064795\n",
      "Epoch [639/1000] Train Loss: 0.079866 Val Loss: 0.064839\n",
      "Epoch [640/1000] Train Loss: 0.080598 Val Loss: 0.064758\n",
      "Epoch [641/1000] Train Loss: 0.080785 Val Loss: 0.064709\n",
      "Epoch [642/1000] Train Loss: 0.080244 Val Loss: 0.064719\n",
      "Epoch [643/1000] Train Loss: 0.080194 Val Loss: 0.064784\n",
      "Epoch [644/1000] Train Loss: 0.081805 Val Loss: 0.064991\n",
      "Epoch [645/1000] Train Loss: 0.081873 Val Loss: 0.064762\n",
      "Epoch [646/1000] Train Loss: 0.079207 Val Loss: 0.064892\n",
      "Epoch [647/1000] Train Loss: 0.082781 Val Loss: 0.064654\n",
      "Epoch [648/1000] Train Loss: 0.081885 Val Loss: 0.064891\n",
      "Epoch [649/1000] Train Loss: 0.083549 Val Loss: 0.064692\n",
      "Epoch [650/1000] Train Loss: 0.079643 Val Loss: 0.064825\n",
      "Epoch [651/1000] Train Loss: 0.081609 Val Loss: 0.064592\n",
      "Epoch [652/1000] Train Loss: 0.079207 Val Loss: 0.064781\n",
      "Epoch [653/1000] Train Loss: 0.080758 Val Loss: 0.064635\n",
      "Epoch [654/1000] Train Loss: 0.079804 Val Loss: 0.064713\n",
      "Epoch [655/1000] Train Loss: 0.080158 Val Loss: 0.064526\n",
      "Epoch [656/1000] Train Loss: 0.080274 Val Loss: 0.064683\n",
      "Epoch [657/1000] Train Loss: 0.080286 Val Loss: 0.064578\n",
      "Epoch [658/1000] Train Loss: 0.081592 Val Loss: 0.064664\n",
      "Epoch [659/1000] Train Loss: 0.079497 Val Loss: 0.064735\n",
      "Epoch [660/1000] Train Loss: 0.080377 Val Loss: 0.064597\n",
      "Epoch [661/1000] Train Loss: 0.080343 Val Loss: 0.064796\n",
      "Epoch [662/1000] Train Loss: 0.079728 Val Loss: 0.064478\n",
      "Epoch [663/1000] Train Loss: 0.080417 Val Loss: 0.064559\n",
      "Epoch [664/1000] Train Loss: 0.079297 Val Loss: 0.064489\n",
      "Epoch [665/1000] Train Loss: 0.079954 Val Loss: 0.064434\n",
      "Epoch [666/1000] Train Loss: 0.079467 Val Loss: 0.064543\n",
      "Epoch [667/1000] Train Loss: 0.079605 Val Loss: 0.064367\n",
      "Epoch [668/1000] Train Loss: 0.080867 Val Loss: 0.064516\n",
      "Epoch [669/1000] Train Loss: 0.078722 Val Loss: 0.064431\n",
      "Epoch [670/1000] Train Loss: 0.079480 Val Loss: 0.064336\n",
      "Epoch [671/1000] Train Loss: 0.079349 Val Loss: 0.064553\n",
      "Epoch [672/1000] Train Loss: 0.080627 Val Loss: 0.064374\n",
      "Epoch [673/1000] Train Loss: 0.079311 Val Loss: 0.064434\n",
      "Epoch [674/1000] Train Loss: 0.079527 Val Loss: 0.064309\n",
      "Epoch [675/1000] Train Loss: 0.080236 Val Loss: 0.064402\n",
      "Epoch [676/1000] Train Loss: 0.079121 Val Loss: 0.064458\n",
      "Epoch [677/1000] Train Loss: 0.079619 Val Loss: 0.064296\n",
      "Epoch [678/1000] Train Loss: 0.081870 Val Loss: 0.064277\n",
      "Epoch [679/1000] Train Loss: 0.079932 Val Loss: 0.064338\n",
      "Epoch [680/1000] Train Loss: 0.079473 Val Loss: 0.064283\n",
      "Epoch [681/1000] Train Loss: 0.078613 Val Loss: 0.064387\n",
      "Epoch [682/1000] Train Loss: 0.081942 Val Loss: 0.064263\n",
      "Epoch [683/1000] Train Loss: 0.079301 Val Loss: 0.064388\n",
      "Epoch [684/1000] Train Loss: 0.079870 Val Loss: 0.064326\n",
      "Epoch [685/1000] Train Loss: 0.078963 Val Loss: 0.064257\n",
      "Epoch [686/1000] Train Loss: 0.080432 Val Loss: 0.064298\n",
      "Epoch [687/1000] Train Loss: 0.080141 Val Loss: 0.064260\n",
      "Epoch [688/1000] Train Loss: 0.079863 Val Loss: 0.064238\n",
      "Epoch [689/1000] Train Loss: 0.078940 Val Loss: 0.064281\n",
      "Epoch [690/1000] Train Loss: 0.078773 Val Loss: 0.064183\n",
      "Epoch [691/1000] Train Loss: 0.079478 Val Loss: 0.064259\n",
      "Epoch [692/1000] Train Loss: 0.080750 Val Loss: 0.064139\n",
      "Epoch [693/1000] Train Loss: 0.078243 Val Loss: 0.064170\n",
      "Epoch [694/1000] Train Loss: 0.080297 Val Loss: 0.064250\n",
      "Epoch [695/1000] Train Loss: 0.078829 Val Loss: 0.064252\n",
      "Epoch [696/1000] Train Loss: 0.077778 Val Loss: 0.064009\n",
      "Epoch [697/1000] Train Loss: 0.078123 Val Loss: 0.064149\n",
      "Epoch [698/1000] Train Loss: 0.079019 Val Loss: 0.064065\n",
      "Epoch [699/1000] Train Loss: 0.079700 Val Loss: 0.064082\n",
      "Epoch [700/1000] Train Loss: 0.080156 Val Loss: 0.064149\n",
      "Epoch [701/1000] Train Loss: 0.077897 Val Loss: 0.064142\n",
      "Epoch [702/1000] Train Loss: 0.077914 Val Loss: 0.064088\n",
      "Epoch [703/1000] Train Loss: 0.080226 Val Loss: 0.064005\n",
      "Epoch [704/1000] Train Loss: 0.078776 Val Loss: 0.064107\n",
      "Epoch [705/1000] Train Loss: 0.079702 Val Loss: 0.064042\n",
      "Epoch [706/1000] Train Loss: 0.077528 Val Loss: 0.064003\n",
      "Epoch [707/1000] Train Loss: 0.077686 Val Loss: 0.063925\n",
      "Epoch [708/1000] Train Loss: 0.078945 Val Loss: 0.063947\n",
      "Epoch [709/1000] Train Loss: 0.078848 Val Loss: 0.063961\n",
      "Epoch [710/1000] Train Loss: 0.078977 Val Loss: 0.064076\n",
      "Epoch [711/1000] Train Loss: 0.077641 Val Loss: 0.063938\n",
      "Epoch [712/1000] Train Loss: 0.077734 Val Loss: 0.064025\n",
      "Epoch [713/1000] Train Loss: 0.078667 Val Loss: 0.063948\n",
      "Epoch [714/1000] Train Loss: 0.078306 Val Loss: 0.063907\n",
      "Epoch [715/1000] Train Loss: 0.077764 Val Loss: 0.064034\n",
      "Epoch [716/1000] Train Loss: 0.078110 Val Loss: 0.063938\n",
      "Epoch [717/1000] Train Loss: 0.077955 Val Loss: 0.063843\n",
      "Epoch [718/1000] Train Loss: 0.077958 Val Loss: 0.063936\n",
      "Epoch [719/1000] Train Loss: 0.078526 Val Loss: 0.063953\n",
      "Epoch [720/1000] Train Loss: 0.080132 Val Loss: 0.063896\n",
      "Epoch [721/1000] Train Loss: 0.079362 Val Loss: 0.063901\n",
      "Epoch [722/1000] Train Loss: 0.078392 Val Loss: 0.063843\n",
      "Epoch [723/1000] Train Loss: 0.078470 Val Loss: 0.063962\n",
      "Epoch [724/1000] Train Loss: 0.078728 Val Loss: 0.063804\n",
      "Epoch [725/1000] Train Loss: 0.077760 Val Loss: 0.063830\n",
      "Epoch [726/1000] Train Loss: 0.078525 Val Loss: 0.063836\n",
      "Epoch [727/1000] Train Loss: 0.077928 Val Loss: 0.063800\n",
      "Epoch [728/1000] Train Loss: 0.078230 Val Loss: 0.063922\n",
      "Epoch [729/1000] Train Loss: 0.078632 Val Loss: 0.063754\n",
      "Epoch [730/1000] Train Loss: 0.079539 Val Loss: 0.063819\n",
      "Epoch [731/1000] Train Loss: 0.078074 Val Loss: 0.063842\n",
      "Epoch [732/1000] Train Loss: 0.078226 Val Loss: 0.063761\n",
      "Epoch [733/1000] Train Loss: 0.077711 Val Loss: 0.063826\n",
      "Epoch [734/1000] Train Loss: 0.077037 Val Loss: 0.063744\n",
      "Epoch [735/1000] Train Loss: 0.078609 Val Loss: 0.063770\n",
      "Epoch [736/1000] Train Loss: 0.076790 Val Loss: 0.063827\n",
      "Epoch [737/1000] Train Loss: 0.078230 Val Loss: 0.063745\n",
      "Epoch [738/1000] Train Loss: 0.077377 Val Loss: 0.063645\n",
      "Epoch [739/1000] Train Loss: 0.078520 Val Loss: 0.063781\n",
      "Epoch [740/1000] Train Loss: 0.078654 Val Loss: 0.063655\n",
      "Epoch [741/1000] Train Loss: 0.079156 Val Loss: 0.063700\n",
      "Epoch [742/1000] Train Loss: 0.080455 Val Loss: 0.063658\n",
      "Epoch [743/1000] Train Loss: 0.078127 Val Loss: 0.063825\n",
      "Epoch [744/1000] Train Loss: 0.077967 Val Loss: 0.063754\n",
      "Epoch [745/1000] Train Loss: 0.077093 Val Loss: 0.063691\n",
      "Epoch [746/1000] Train Loss: 0.077936 Val Loss: 0.063694\n",
      "Epoch [747/1000] Train Loss: 0.077601 Val Loss: 0.063614\n",
      "Epoch [748/1000] Train Loss: 0.076808 Val Loss: 0.063591\n",
      "Epoch [749/1000] Train Loss: 0.076417 Val Loss: 0.063607\n",
      "Epoch [750/1000] Train Loss: 0.077265 Val Loss: 0.063770\n",
      "Epoch [751/1000] Train Loss: 0.078103 Val Loss: 0.063701\n",
      "Epoch [752/1000] Train Loss: 0.077480 Val Loss: 0.063661\n",
      "Epoch [753/1000] Train Loss: 0.077594 Val Loss: 0.063502\n",
      "Epoch [754/1000] Train Loss: 0.077778 Val Loss: 0.063643\n",
      "Epoch [755/1000] Train Loss: 0.076700 Val Loss: 0.063566\n",
      "Epoch [756/1000] Train Loss: 0.079086 Val Loss: 0.063557\n",
      "Epoch [757/1000] Train Loss: 0.076550 Val Loss: 0.063725\n",
      "Epoch [758/1000] Train Loss: 0.077102 Val Loss: 0.063524\n",
      "Epoch [759/1000] Train Loss: 0.077170 Val Loss: 0.063619\n",
      "Epoch [760/1000] Train Loss: 0.076685 Val Loss: 0.063700\n",
      "Epoch [761/1000] Train Loss: 0.078175 Val Loss: 0.063603\n",
      "Epoch [762/1000] Train Loss: 0.077305 Val Loss: 0.063426\n",
      "Epoch [763/1000] Train Loss: 0.077644 Val Loss: 0.063555\n",
      "Epoch [764/1000] Train Loss: 0.076467 Val Loss: 0.063635\n",
      "Epoch [765/1000] Train Loss: 0.076933 Val Loss: 0.063562\n",
      "Epoch [766/1000] Train Loss: 0.078235 Val Loss: 0.063533\n",
      "Epoch [767/1000] Train Loss: 0.076559 Val Loss: 0.063616\n",
      "Epoch [768/1000] Train Loss: 0.077821 Val Loss: 0.063625\n",
      "Epoch [769/1000] Train Loss: 0.075987 Val Loss: 0.063522\n",
      "Epoch [770/1000] Train Loss: 0.076782 Val Loss: 0.063447\n",
      "Epoch [771/1000] Train Loss: 0.078545 Val Loss: 0.063491\n",
      "Epoch [772/1000] Train Loss: 0.076984 Val Loss: 0.063593\n",
      "Epoch [773/1000] Train Loss: 0.076555 Val Loss: 0.063558\n",
      "Epoch [774/1000] Train Loss: 0.079028 Val Loss: 0.063503\n",
      "Epoch [775/1000] Train Loss: 0.075893 Val Loss: 0.063614\n",
      "Epoch [776/1000] Train Loss: 0.077907 Val Loss: 0.063447\n",
      "Epoch [777/1000] Train Loss: 0.076167 Val Loss: 0.063565\n",
      "Epoch [778/1000] Train Loss: 0.076013 Val Loss: 0.063543\n",
      "Epoch [779/1000] Train Loss: 0.076310 Val Loss: 0.063557\n",
      "Epoch [780/1000] Train Loss: 0.076148 Val Loss: 0.063408\n",
      "Epoch [781/1000] Train Loss: 0.076061 Val Loss: 0.063296\n",
      "Epoch [782/1000] Train Loss: 0.079834 Val Loss: 0.063377\n",
      "Epoch [783/1000] Train Loss: 0.076739 Val Loss: 0.063525\n",
      "Epoch [784/1000] Train Loss: 0.077612 Val Loss: 0.063362\n",
      "Epoch [785/1000] Train Loss: 0.076582 Val Loss: 0.063659\n",
      "Epoch [786/1000] Train Loss: 0.076723 Val Loss: 0.063390\n",
      "Epoch [787/1000] Train Loss: 0.076513 Val Loss: 0.063499\n",
      "Epoch [788/1000] Train Loss: 0.076150 Val Loss: 0.063418\n",
      "Epoch [789/1000] Train Loss: 0.076338 Val Loss: 0.063328\n",
      "Epoch [790/1000] Train Loss: 0.077688 Val Loss: 0.063427\n",
      "Epoch [791/1000] Train Loss: 0.077175 Val Loss: 0.063471\n",
      "Epoch [792/1000] Train Loss: 0.077762 Val Loss: 0.063248\n",
      "Epoch [793/1000] Train Loss: 0.076912 Val Loss: 0.063340\n",
      "Epoch [794/1000] Train Loss: 0.077404 Val Loss: 0.063454\n",
      "Epoch [795/1000] Train Loss: 0.075951 Val Loss: 0.063316\n",
      "Epoch [796/1000] Train Loss: 0.076461 Val Loss: 0.063335\n",
      "Epoch [797/1000] Train Loss: 0.075820 Val Loss: 0.063332\n",
      "Epoch [798/1000] Train Loss: 0.076178 Val Loss: 0.063336\n",
      "Epoch [799/1000] Train Loss: 0.079354 Val Loss: 0.063409\n",
      "Epoch [800/1000] Train Loss: 0.076999 Val Loss: 0.063334\n",
      "Epoch [801/1000] Train Loss: 0.077082 Val Loss: 0.063251\n",
      "Epoch [802/1000] Train Loss: 0.075460 Val Loss: 0.063347\n",
      "Epoch [803/1000] Train Loss: 0.076001 Val Loss: 0.063348\n",
      "Epoch [804/1000] Train Loss: 0.077713 Val Loss: 0.063362\n",
      "Epoch [805/1000] Train Loss: 0.076449 Val Loss: 0.063274\n",
      "Epoch [806/1000] Train Loss: 0.075452 Val Loss: 0.063354\n",
      "Epoch [807/1000] Train Loss: 0.075797 Val Loss: 0.063364\n",
      "Epoch [808/1000] Train Loss: 0.078412 Val Loss: 0.063169\n",
      "Epoch [809/1000] Train Loss: 0.075632 Val Loss: 0.063196\n",
      "Epoch [810/1000] Train Loss: 0.075639 Val Loss: 0.063335\n",
      "Epoch [811/1000] Train Loss: 0.075839 Val Loss: 0.063181\n",
      "Epoch [812/1000] Train Loss: 0.076342 Val Loss: 0.063151\n",
      "Epoch [813/1000] Train Loss: 0.076442 Val Loss: 0.063178\n",
      "Epoch [814/1000] Train Loss: 0.077916 Val Loss: 0.063122\n",
      "Epoch [815/1000] Train Loss: 0.075947 Val Loss: 0.063339\n",
      "Epoch [816/1000] Train Loss: 0.075978 Val Loss: 0.063122\n",
      "Epoch [817/1000] Train Loss: 0.076052 Val Loss: 0.063151\n",
      "Epoch [818/1000] Train Loss: 0.076005 Val Loss: 0.063169\n",
      "Epoch [819/1000] Train Loss: 0.075986 Val Loss: 0.063305\n",
      "Epoch [820/1000] Train Loss: 0.076619 Val Loss: 0.063253\n",
      "Epoch [821/1000] Train Loss: 0.076705 Val Loss: 0.063013\n",
      "Epoch [822/1000] Train Loss: 0.075247 Val Loss: 0.063178\n",
      "Epoch [823/1000] Train Loss: 0.074780 Val Loss: 0.063141\n",
      "Epoch [824/1000] Train Loss: 0.075856 Val Loss: 0.063265\n",
      "Epoch [825/1000] Train Loss: 0.076354 Val Loss: 0.063221\n",
      "Epoch [826/1000] Train Loss: 0.076135 Val Loss: 0.063121\n",
      "Epoch [827/1000] Train Loss: 0.074604 Val Loss: 0.063182\n",
      "Epoch [828/1000] Train Loss: 0.075354 Val Loss: 0.063151\n",
      "Epoch [829/1000] Train Loss: 0.074957 Val Loss: 0.063095\n",
      "Epoch [830/1000] Train Loss: 0.075377 Val Loss: 0.063189\n",
      "Epoch [831/1000] Train Loss: 0.074766 Val Loss: 0.063067\n",
      "Epoch [832/1000] Train Loss: 0.075884 Val Loss: 0.063024\n",
      "Epoch [833/1000] Train Loss: 0.074575 Val Loss: 0.063004\n",
      "Epoch [834/1000] Train Loss: 0.075514 Val Loss: 0.063111\n",
      "Epoch [835/1000] Train Loss: 0.074799 Val Loss: 0.063033\n",
      "Epoch [836/1000] Train Loss: 0.074836 Val Loss: 0.063052\n",
      "Epoch [837/1000] Train Loss: 0.074837 Val Loss: 0.063151\n",
      "Epoch [838/1000] Train Loss: 0.075717 Val Loss: 0.062971\n",
      "Epoch [839/1000] Train Loss: 0.075907 Val Loss: 0.063087\n",
      "Epoch [840/1000] Train Loss: 0.075966 Val Loss: 0.063021\n",
      "Epoch [841/1000] Train Loss: 0.076280 Val Loss: 0.063024\n",
      "Epoch [842/1000] Train Loss: 0.074376 Val Loss: 0.063094\n",
      "Epoch [843/1000] Train Loss: 0.075277 Val Loss: 0.062943\n",
      "Epoch [844/1000] Train Loss: 0.075144 Val Loss: 0.063056\n",
      "Epoch [845/1000] Train Loss: 0.074615 Val Loss: 0.062902\n",
      "Epoch [846/1000] Train Loss: 0.077599 Val Loss: 0.063084\n",
      "Epoch [847/1000] Train Loss: 0.076509 Val Loss: 0.062935\n",
      "Epoch [848/1000] Train Loss: 0.074426 Val Loss: 0.062999\n",
      "Epoch [849/1000] Train Loss: 0.074982 Val Loss: 0.062891\n",
      "Epoch [850/1000] Train Loss: 0.074816 Val Loss: 0.062924\n",
      "Epoch [851/1000] Train Loss: 0.078668 Val Loss: 0.063000\n",
      "Epoch [852/1000] Train Loss: 0.075742 Val Loss: 0.063038\n",
      "Epoch [853/1000] Train Loss: 0.075407 Val Loss: 0.062839\n",
      "Epoch [854/1000] Train Loss: 0.076945 Val Loss: 0.062902\n",
      "Epoch [855/1000] Train Loss: 0.075420 Val Loss: 0.062820\n",
      "Epoch [856/1000] Train Loss: 0.075661 Val Loss: 0.063081\n",
      "Epoch [857/1000] Train Loss: 0.075345 Val Loss: 0.062844\n",
      "Epoch [858/1000] Train Loss: 0.076948 Val Loss: 0.062968\n",
      "Epoch [859/1000] Train Loss: 0.074147 Val Loss: 0.063009\n",
      "Epoch [860/1000] Train Loss: 0.073838 Val Loss: 0.062844\n",
      "Epoch [861/1000] Train Loss: 0.075264 Val Loss: 0.062875\n",
      "Epoch [862/1000] Train Loss: 0.074490 Val Loss: 0.062822\n",
      "Epoch [863/1000] Train Loss: 0.075294 Val Loss: 0.062733\n",
      "Epoch [864/1000] Train Loss: 0.075372 Val Loss: 0.062909\n",
      "Epoch [865/1000] Train Loss: 0.075873 Val Loss: 0.062856\n",
      "Epoch [866/1000] Train Loss: 0.074640 Val Loss: 0.062865\n",
      "Epoch [867/1000] Train Loss: 0.073622 Val Loss: 0.062741\n",
      "Epoch [868/1000] Train Loss: 0.075781 Val Loss: 0.062807\n",
      "Epoch [869/1000] Train Loss: 0.074074 Val Loss: 0.062831\n",
      "Epoch [870/1000] Train Loss: 0.073964 Val Loss: 0.062990\n",
      "Epoch [871/1000] Train Loss: 0.075036 Val Loss: 0.062693\n",
      "Epoch [872/1000] Train Loss: 0.075157 Val Loss: 0.062775\n",
      "Epoch [873/1000] Train Loss: 0.076460 Val Loss: 0.062650\n",
      "Epoch [874/1000] Train Loss: 0.075216 Val Loss: 0.062718\n",
      "Epoch [875/1000] Train Loss: 0.075157 Val Loss: 0.062782\n",
      "Epoch [876/1000] Train Loss: 0.075450 Val Loss: 0.062885\n",
      "Epoch [877/1000] Train Loss: 0.073723 Val Loss: 0.062732\n",
      "Epoch [878/1000] Train Loss: 0.073951 Val Loss: 0.062740\n",
      "Epoch [879/1000] Train Loss: 0.073692 Val Loss: 0.062835\n",
      "Epoch [880/1000] Train Loss: 0.074847 Val Loss: 0.062728\n",
      "Epoch [881/1000] Train Loss: 0.075916 Val Loss: 0.062530\n",
      "Epoch [882/1000] Train Loss: 0.075942 Val Loss: 0.062691\n",
      "Epoch [883/1000] Train Loss: 0.075134 Val Loss: 0.062506\n",
      "Epoch [884/1000] Train Loss: 0.074277 Val Loss: 0.062707\n",
      "Epoch [885/1000] Train Loss: 0.073682 Val Loss: 0.062615\n",
      "Epoch [886/1000] Train Loss: 0.075892 Val Loss: 0.062687\n",
      "Epoch [887/1000] Train Loss: 0.073994 Val Loss: 0.062700\n",
      "Epoch [888/1000] Train Loss: 0.073852 Val Loss: 0.062662\n",
      "Epoch [889/1000] Train Loss: 0.074754 Val Loss: 0.062763\n",
      "Epoch [890/1000] Train Loss: 0.073401 Val Loss: 0.062602\n",
      "Epoch [891/1000] Train Loss: 0.074749 Val Loss: 0.062612\n",
      "Epoch [892/1000] Train Loss: 0.074465 Val Loss: 0.062631\n",
      "Epoch [893/1000] Train Loss: 0.074027 Val Loss: 0.062566\n",
      "Epoch [894/1000] Train Loss: 0.073722 Val Loss: 0.062501\n",
      "Epoch [895/1000] Train Loss: 0.075364 Val Loss: 0.062595\n",
      "Epoch [896/1000] Train Loss: 0.074207 Val Loss: 0.062502\n",
      "Epoch [897/1000] Train Loss: 0.073234 Val Loss: 0.062547\n",
      "Epoch [898/1000] Train Loss: 0.074224 Val Loss: 0.062468\n",
      "Epoch [899/1000] Train Loss: 0.074828 Val Loss: 0.062550\n",
      "Epoch [900/1000] Train Loss: 0.073564 Val Loss: 0.062688\n",
      "Epoch [901/1000] Train Loss: 0.073516 Val Loss: 0.062574\n",
      "Epoch [902/1000] Train Loss: 0.074319 Val Loss: 0.062622\n",
      "Epoch [903/1000] Train Loss: 0.074867 Val Loss: 0.062565\n",
      "Epoch [904/1000] Train Loss: 0.073387 Val Loss: 0.062445\n",
      "Epoch [905/1000] Train Loss: 0.075554 Val Loss: 0.062440\n",
      "Epoch [906/1000] Train Loss: 0.074890 Val Loss: 0.062517\n",
      "Epoch [907/1000] Train Loss: 0.073417 Val Loss: 0.062692\n",
      "Epoch [908/1000] Train Loss: 0.075493 Val Loss: 0.062493\n",
      "Epoch [909/1000] Train Loss: 0.074426 Val Loss: 0.062635\n",
      "Epoch [910/1000] Train Loss: 0.074178 Val Loss: 0.062519\n",
      "Epoch [911/1000] Train Loss: 0.075171 Val Loss: 0.062503\n",
      "Epoch [912/1000] Train Loss: 0.073208 Val Loss: 0.062519\n",
      "Epoch [913/1000] Train Loss: 0.072877 Val Loss: 0.062404\n",
      "Epoch [914/1000] Train Loss: 0.073719 Val Loss: 0.062438\n",
      "Epoch [915/1000] Train Loss: 0.074741 Val Loss: 0.062425\n",
      "Epoch [916/1000] Train Loss: 0.076377 Val Loss: 0.062480\n",
      "Epoch [917/1000] Train Loss: 0.074375 Val Loss: 0.062536\n",
      "Epoch [918/1000] Train Loss: 0.076015 Val Loss: 0.062462\n",
      "Epoch [919/1000] Train Loss: 0.073988 Val Loss: 0.062511\n",
      "Epoch [920/1000] Train Loss: 0.073490 Val Loss: 0.062802\n",
      "Epoch [921/1000] Train Loss: 0.075184 Val Loss: 0.062483\n",
      "Epoch [922/1000] Train Loss: 0.073686 Val Loss: 0.062378\n",
      "Epoch [923/1000] Train Loss: 0.075427 Val Loss: 0.062453\n",
      "Epoch [924/1000] Train Loss: 0.074401 Val Loss: 0.062734\n",
      "Epoch [925/1000] Train Loss: 0.072901 Val Loss: 0.062455\n",
      "Epoch [926/1000] Train Loss: 0.072658 Val Loss: 0.062414\n",
      "Epoch [927/1000] Train Loss: 0.074500 Val Loss: 0.062414\n",
      "Epoch [928/1000] Train Loss: 0.072557 Val Loss: 0.062485\n",
      "Epoch [929/1000] Train Loss: 0.073324 Val Loss: 0.062402\n",
      "Epoch [930/1000] Train Loss: 0.072562 Val Loss: 0.062478\n",
      "Epoch [931/1000] Train Loss: 0.073487 Val Loss: 0.062473\n",
      "Epoch [932/1000] Train Loss: 0.074171 Val Loss: 0.062149\n",
      "Epoch [933/1000] Train Loss: 0.074470 Val Loss: 0.062404\n",
      "Epoch [934/1000] Train Loss: 0.072772 Val Loss: 0.062525\n",
      "Epoch [935/1000] Train Loss: 0.073871 Val Loss: 0.062354\n",
      "Epoch [936/1000] Train Loss: 0.073128 Val Loss: 0.062442\n",
      "Epoch [937/1000] Train Loss: 0.073005 Val Loss: 0.062560\n",
      "Epoch [938/1000] Train Loss: 0.073827 Val Loss: 0.062300\n",
      "Epoch [939/1000] Train Loss: 0.072880 Val Loss: 0.062407\n",
      "Epoch [940/1000] Train Loss: 0.073915 Val Loss: 0.062310\n",
      "Epoch [941/1000] Train Loss: 0.072774 Val Loss: 0.062375\n",
      "Epoch [942/1000] Train Loss: 0.073776 Val Loss: 0.062281\n",
      "Epoch [943/1000] Train Loss: 0.073955 Val Loss: 0.062416\n",
      "Epoch [944/1000] Train Loss: 0.071995 Val Loss: 0.062386\n",
      "Epoch [945/1000] Train Loss: 0.072794 Val Loss: 0.062144\n",
      "Epoch [946/1000] Train Loss: 0.074793 Val Loss: 0.062234\n",
      "Epoch [947/1000] Train Loss: 0.074092 Val Loss: 0.062341\n",
      "Epoch [948/1000] Train Loss: 0.072473 Val Loss: 0.062138\n",
      "Epoch [949/1000] Train Loss: 0.073339 Val Loss: 0.062259\n",
      "Epoch [950/1000] Train Loss: 0.075059 Val Loss: 0.062270\n",
      "Epoch [951/1000] Train Loss: 0.072412 Val Loss: 0.062266\n",
      "Epoch [952/1000] Train Loss: 0.072335 Val Loss: 0.062296\n",
      "Epoch [953/1000] Train Loss: 0.073449 Val Loss: 0.062083\n",
      "Epoch [954/1000] Train Loss: 0.074948 Val Loss: 0.062423\n",
      "Epoch [955/1000] Train Loss: 0.075717 Val Loss: 0.062227\n",
      "Epoch [956/1000] Train Loss: 0.073397 Val Loss: 0.062341\n",
      "Epoch [957/1000] Train Loss: 0.072439 Val Loss: 0.062146\n",
      "Epoch [958/1000] Train Loss: 0.075557 Val Loss: 0.062192\n",
      "Epoch [959/1000] Train Loss: 0.075113 Val Loss: 0.062170\n",
      "Epoch [960/1000] Train Loss: 0.072874 Val Loss: 0.062099\n",
      "Epoch [961/1000] Train Loss: 0.072996 Val Loss: 0.062211\n",
      "Epoch [962/1000] Train Loss: 0.072087 Val Loss: 0.062159\n",
      "Epoch [963/1000] Train Loss: 0.074183 Val Loss: 0.062023\n",
      "Epoch [964/1000] Train Loss: 0.072305 Val Loss: 0.062197\n",
      "Epoch [965/1000] Train Loss: 0.072410 Val Loss: 0.062013\n",
      "Epoch [966/1000] Train Loss: 0.075513 Val Loss: 0.061991\n",
      "Epoch [967/1000] Train Loss: 0.072574 Val Loss: 0.062276\n",
      "Epoch [968/1000] Train Loss: 0.073999 Val Loss: 0.062180\n",
      "Epoch [969/1000] Train Loss: 0.072775 Val Loss: 0.062086\n",
      "Epoch [970/1000] Train Loss: 0.073091 Val Loss: 0.061902\n",
      "Epoch [971/1000] Train Loss: 0.073683 Val Loss: 0.062117\n",
      "Epoch [972/1000] Train Loss: 0.073238 Val Loss: 0.061977\n",
      "Epoch [973/1000] Train Loss: 0.072709 Val Loss: 0.062226\n",
      "Epoch [974/1000] Train Loss: 0.072440 Val Loss: 0.062134\n",
      "Epoch [975/1000] Train Loss: 0.072233 Val Loss: 0.062023\n",
      "Epoch [976/1000] Train Loss: 0.072112 Val Loss: 0.062201\n",
      "Epoch [977/1000] Train Loss: 0.072115 Val Loss: 0.062014\n",
      "Epoch [978/1000] Train Loss: 0.072434 Val Loss: 0.062004\n",
      "Epoch [979/1000] Train Loss: 0.071715 Val Loss: 0.062049\n",
      "Epoch [980/1000] Train Loss: 0.071983 Val Loss: 0.062114\n",
      "Epoch [981/1000] Train Loss: 0.071817 Val Loss: 0.062030\n",
      "Epoch [982/1000] Train Loss: 0.072658 Val Loss: 0.061852\n",
      "Epoch [983/1000] Train Loss: 0.071686 Val Loss: 0.061908\n",
      "Epoch [984/1000] Train Loss: 0.071542 Val Loss: 0.062019\n",
      "Epoch [985/1000] Train Loss: 0.073256 Val Loss: 0.061934\n",
      "Epoch [986/1000] Train Loss: 0.072882 Val Loss: 0.061894\n",
      "Epoch [987/1000] Train Loss: 0.071905 Val Loss: 0.062091\n",
      "Epoch [988/1000] Train Loss: 0.073335 Val Loss: 0.061906\n",
      "Epoch [989/1000] Train Loss: 0.073443 Val Loss: 0.061960\n",
      "Epoch [990/1000] Train Loss: 0.071626 Val Loss: 0.061903\n",
      "Epoch [991/1000] Train Loss: 0.071576 Val Loss: 0.061909\n",
      "Epoch [992/1000] Train Loss: 0.073255 Val Loss: 0.061977\n",
      "Epoch [993/1000] Train Loss: 0.073559 Val Loss: 0.062082\n",
      "Epoch [994/1000] Train Loss: 0.072237 Val Loss: 0.061872\n",
      "Epoch [995/1000] Train Loss: 0.071338 Val Loss: 0.061877\n",
      "Epoch [996/1000] Train Loss: 0.072393 Val Loss: 0.061919\n",
      "Epoch [997/1000] Train Loss: 0.072120 Val Loss: 0.061773\n",
      "Epoch [998/1000] Train Loss: 0.072212 Val Loss: 0.062022\n",
      "Epoch [999/1000] Train Loss: 0.072556 Val Loss: 0.061908\n",
      "Epoch [1000/1000] Train Loss: 0.074020 Val Loss: 0.061830\n"
     ]
    }
   ],
   "source": [
    "model, history = train(model, train_dl, test_dl, epochs=1000, patience=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7630277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neural-lightmaps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
