{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4db5a8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39777b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable constants\n",
    "L_POS = 2\n",
    "L_ANGLE = 3\n",
    "\n",
    "MLP_HIDDEN_LAYER_WIDTH = 27\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10000\n",
    "PATIENCE = 0\n",
    "\n",
    "# fixed constants\n",
    "SH_FLOAT_COUNT = 27\n",
    "\n",
    "PROBES_DIM_X = 50\n",
    "PROBES_DIM_Y = 5\n",
    "PROBES_DIM_Z = 5\n",
    "PROBES_COUNT = PROBES_DIM_X * PROBES_DIM_Y * PROBES_DIM_Z\n",
    "\n",
    "ENCODED_POS_DIM = 3 * L_POS * 2\n",
    "ENCODED_ANGLE_DIM = 1 * L_ANGLE * 2\n",
    "\n",
    "INPUT_DIM = PROBES_COUNT + ENCODED_POS_DIM + ENCODED_ANGLE_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28963ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralSH(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralSH, self).__init__()\n",
    "        self.probe_features = nn.Parameter(torch.rand(PROBES_COUNT), requires_grad=True)\n",
    "\n",
    "        self.hidden_layer = nn.Linear(INPUT_DIM, MLP_HIDDEN_LAYER_WIDTH)\n",
    "        self.output_layer = nn.Linear(MLP_HIDDEN_LAYER_WIDTH, SH_FLOAT_COUNT)\n",
    "\n",
    "    def trigonometric_encoding(self, x: torch.Tensor, L: int):\n",
    "        assert x.ndim == 2\n",
    "        y = []\n",
    "        for i in range(L):\n",
    "            s = torch.sin(2**i * torch.pi * x)\n",
    "            c = torch.cos(2**i * torch.pi * x)\n",
    "            y.append(s)\n",
    "            y.append(c)\n",
    "        y = torch.cat(y, dim=1)\n",
    "        return y\n",
    "\n",
    "    def forward(self, pos, angle):\n",
    "        assert pos.ndim == 2\n",
    "        assert angle.ndim == 2\n",
    "\n",
    "        pos = pos.view(-1, 3)\n",
    "        angle = angle.view(-1, 1)\n",
    "\n",
    "        assert pos.shape[0] == angle.shape[0]\n",
    "        batch_size = pos.shape[0]\n",
    "\n",
    "        pos_enc = self.trigonometric_encoding(pos, L=L_POS)\n",
    "        angle_enc = self.trigonometric_encoding(angle, L=L_ANGLE)\n",
    "\n",
    "        x = torch.cat([pos_enc, angle_enc, self.probe_features.repeat(batch_size, 1)], dim=1)\n",
    "        x = self.hidden_layer(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "805dd4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1250, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx, yy, zz = np.meshgrid(\n",
    "    np.linspace(0.0 + 1/(2*PROBES_DIM_X), 1.0 - 1/(2*PROBES_DIM_X), PROBES_DIM_X),\n",
    "    np.linspace(0.0 + 1/(2*PROBES_DIM_Y), 1.0 - 1/(2*PROBES_DIM_Y), PROBES_DIM_Y),\n",
    "    np.linspace(0.0 + 1/(2*PROBES_DIM_Z), 1.0 - 1/(2*PROBES_DIM_Z), PROBES_DIM_Z),\n",
    "    indexing=\"ij\"\n",
    ")\n",
    "pos_grid = np.stack([xx, yy, zz], axis=-1).reshape(-1, 3, order=\"F\") # ensures texture-like (x=fastest, y=middle, z=slowest) ordering\n",
    "pos_grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cd4ce85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01, 0.1 , 0.1 ],\n",
       "       [0.03, 0.1 , 0.1 ],\n",
       "       [0.05, 0.1 , 0.1 ],\n",
       "       ...,\n",
       "       [0.95, 0.9 , 0.9 ],\n",
       "       [0.97, 0.9 , 0.9 ],\n",
       "       [0.99, 0.9 , 0.9 ]], shape=(1250, 3))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2181dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1250, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "light_angle = np.zeros((pos_grid.shape[0], 1), dtype=float)\n",
    "light_angle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9faf1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, fields\n",
    "from pathlib import Path\n",
    "\n",
    "from texture_sampler import Texture, load_texture_by_name, sample_uv\n",
    "\n",
    "DATA_DIR = Path(\"LightmapsData/\")\n",
    "\n",
    "@dataclass\n",
    "class SHTextures:\n",
    "    AmbientVector: Texture\n",
    "\n",
    "    SHCoefficients0Red: Texture\n",
    "    SHCoefficients0Green: Texture\n",
    "    SHCoefficients0Blue: Texture\n",
    "\n",
    "    SHCoefficients1Red: Texture\n",
    "    SHCoefficients1Green: Texture\n",
    "    SHCoefficients1Blue: Texture\n",
    "\n",
    "def LoadTextures():\n",
    "    return SHTextures(\n",
    "        AmbientVector=load_texture_by_name(DATA_DIR, \"AmbientVector\"),\n",
    "\n",
    "        SHCoefficients0Red=load_texture_by_name(DATA_DIR, \"SHCoefficients_0\"),\n",
    "        SHCoefficients0Green=load_texture_by_name(DATA_DIR, \"SHCoefficients_2\"),\n",
    "        SHCoefficients0Blue=load_texture_by_name(DATA_DIR, \"SHCoefficients_4\"),\n",
    "\n",
    "        SHCoefficients1Red=load_texture_by_name(DATA_DIR, \"SHCoefficients_1\"),\n",
    "        SHCoefficients1Green=load_texture_by_name(DATA_DIR, \"SHCoefficients_3\"),\n",
    "        SHCoefficients1Blue=load_texture_by_name(DATA_DIR, \"SHCoefficients_5\"),\n",
    "    )\n",
    "\n",
    "def Texture3DSample(tex: Texture, uvw: np.ndarray, method: str = \"nearest\"):\n",
    "    uvw = uvw.reshape(-1, 3)\n",
    "    _, _, _, channels = tex.data.shape\n",
    "    results = []\n",
    "    for p in uvw:\n",
    "        color = sample_uv(tex, p[0], p[1], p[2], method=method)\n",
    "        color = color.reshape(-1, channels)\n",
    "        results.append(color)\n",
    "    results = np.concatenate(results, axis=0)\n",
    "    return results\n",
    "\n",
    "def GetRawSH3(BrickTextureUVs: np.ndarray):\n",
    "    tex = LoadTextures()\n",
    "\n",
    "    AmbientVector = Texture3DSample(tex.AmbientVector, BrickTextureUVs)\n",
    "\n",
    "    SHCoefficients0Red = Texture3DSample(tex.SHCoefficients0Red, BrickTextureUVs)\n",
    "    SHCoefficients0Green = Texture3DSample(tex.SHCoefficients0Green, BrickTextureUVs)\n",
    "    SHCoefficients0Blue = Texture3DSample(tex.SHCoefficients0Blue, BrickTextureUVs)\n",
    "\n",
    "    SHCoefficients1Red = Texture3DSample(tex.SHCoefficients1Red, BrickTextureUVs)\n",
    "    SHCoefficients1Green = Texture3DSample(tex.SHCoefficients1Green, BrickTextureUVs)\n",
    "    SHCoefficients1Blue = Texture3DSample(tex.SHCoefficients1Blue, BrickTextureUVs)\n",
    "\n",
    "    IrradianceSH = np.concatenate([\n",
    "        AmbientVector[:, 0:1], # .x\n",
    "        SHCoefficients0Red[:],\n",
    "        SHCoefficients1Red[:],\n",
    "        AmbientVector[:, 1:2], # .y\n",
    "        SHCoefficients0Green[:],\n",
    "        SHCoefficients1Green[:],\n",
    "        AmbientVector[:, 2:3], # .z\n",
    "        SHCoefficients0Blue[:],\n",
    "        SHCoefficients1Blue[:],\n",
    "    ], axis=1)\n",
    "\n",
    "    return IrradianceSH\n",
    "\n",
    "def SH3ToTex(sh: np.ndarray):\n",
    "    assert sh.ndim == 2\n",
    "    assert sh.shape[1] == 27\n",
    "\n",
    "    textures = LoadTextures()\n",
    "\n",
    "    textures.AmbientVector.data = np.concatenate([sh[:, 0:1], sh[:, 9:10], sh[:, 18:19]], axis=1)\n",
    "    textures.SHCoefficients0Red.data = sh[:, 1:5]\n",
    "    textures.SHCoefficients1Red.data = sh[:, 5:9]\n",
    "    textures.SHCoefficients0Green.data = sh[:, 10:14]\n",
    "    textures.SHCoefficients1Green.data = sh[:, 14:18]\n",
    "    textures.SHCoefficients0Blue.data = sh[:, 19:23]\n",
    "    textures.SHCoefficients1Blue.data = sh[:, 23:27]\n",
    "\n",
    "    for field in fields(SHTextures):\n",
    "        tex = getattr(textures, field.name)\n",
    "        # this assumes correct texture-like ordering of tex.data,\n",
    "        # where width changes the fastest, then height and depth is the slowest\n",
    "        tex.data = tex.data.reshape(\n",
    "            tex.meta.depth, tex.meta.height, tex.meta.width, -1\n",
    "        )\n",
    "\n",
    "    return textures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64b82db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1250, 27)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_sh = GetRawSH3(pos_grid)\n",
    "true_sh.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bdca41",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25fdc848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class SHDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            positions_data: np.ndarray,\n",
    "            light_angles_data: np.ndarray,\n",
    "            spherical_harmonics_data: np.ndarray\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.positions_data = torch.FloatTensor(positions_data)\n",
    "        self.light_angles_data = torch.FloatTensor(light_angles_data)\n",
    "        self.spherical_harmonics_data = torch.FloatTensor(spherical_harmonics_data)\n",
    "        assert spherical_harmonics_data.shape[1] == SH_FLOAT_COUNT\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source = self.positions_data[index], self.light_angles_data[index]\n",
    "        target = self.spherical_harmonics_data[index]\n",
    "        return source, target\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.positions_data) == len(self.light_angles_data)\n",
    "        return len(self.positions_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05cd883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SHDataset(pos_grid, light_angle, true_sh)\n",
    "# train_ds, test_ds = random_split(dataset, [1.0, 0.0], torch.Generator().manual_seed(42))\n",
    "\n",
    "# train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# test_dl = DataLoader(test_ds, batch_size=len(test_ds))\n",
    "\n",
    "train_ds = dataset\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "028d206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader,\n",
    "          epochs=100, lr=1e-3, patience=10, device=\"cuda\"):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "    history = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for source, target in train_loader:\n",
    "            pos, angle = source\n",
    "            pos = pos.to(device)\n",
    "            angle = angle.to(device)\n",
    "            target = target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(pos, angle)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "\n",
    "        # Validation\n",
    "        avg_val_loss = np.nan\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            with torch.no_grad():\n",
    "                for source, target in val_loader:\n",
    "                    pos, angle = source\n",
    "                    pos = pos.to(device)\n",
    "                    angle = angle.to(device)\n",
    "                    target = target.to(device)\n",
    "                    outputs = model(pos, angle)\n",
    "                    val_loss = criterion(outputs, target)\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "            avg_val_loss = np.mean(val_losses)\n",
    "\n",
    "        history[\"train_loss\"].append(avg_train_loss)\n",
    "        history[\"val_loss\"].append(avg_val_loss)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "              f\"Train Loss: {avg_train_loss:.6f} \"\n",
    "              f\"Val Loss: {avg_val_loss:.6f}\")\n",
    "\n",
    "        if (patience != 0) and val_loader: # early stopping based on validation loss\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                epochs_no_improve = 0\n",
    "                torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "                    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "                    break\n",
    "\n",
    "    if (patience == 0) or (val_loader is None): # save model every epoch\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05656aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralSH()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d65167ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10000] Train Loss: 0.098176 Val Loss: nan\n",
      "Epoch [2/10000] Train Loss: 0.050480 Val Loss: nan\n",
      "Epoch [3/10000] Train Loss: 0.034986 Val Loss: nan\n",
      "Epoch [4/10000] Train Loss: 0.029523 Val Loss: nan\n",
      "Epoch [5/10000] Train Loss: 0.028344 Val Loss: nan\n",
      "Epoch [6/10000] Train Loss: 0.027849 Val Loss: nan\n",
      "Epoch [7/10000] Train Loss: 0.027716 Val Loss: nan\n",
      "Epoch [8/10000] Train Loss: 0.027304 Val Loss: nan\n",
      "Epoch [9/10000] Train Loss: 0.027501 Val Loss: nan\n",
      "Epoch [10/10000] Train Loss: 0.027504 Val Loss: nan\n",
      "Epoch [11/10000] Train Loss: 0.027670 Val Loss: nan\n",
      "Epoch [12/10000] Train Loss: 0.027379 Val Loss: nan\n",
      "Epoch [13/10000] Train Loss: 0.027664 Val Loss: nan\n",
      "Epoch [14/10000] Train Loss: 0.027495 Val Loss: nan\n",
      "Epoch [15/10000] Train Loss: 0.027437 Val Loss: nan\n",
      "Epoch [16/10000] Train Loss: 0.027332 Val Loss: nan\n",
      "Epoch [17/10000] Train Loss: 0.027756 Val Loss: nan\n",
      "Epoch [18/10000] Train Loss: 0.027556 Val Loss: nan\n",
      "Epoch [19/10000] Train Loss: 0.027450 Val Loss: nan\n",
      "Epoch [20/10000] Train Loss: 0.027580 Val Loss: nan\n",
      "Epoch [21/10000] Train Loss: 0.027642 Val Loss: nan\n",
      "Epoch [22/10000] Train Loss: 0.027172 Val Loss: nan\n",
      "Epoch [23/10000] Train Loss: 0.027389 Val Loss: nan\n",
      "Epoch [24/10000] Train Loss: 0.027436 Val Loss: nan\n",
      "Epoch [25/10000] Train Loss: 0.027875 Val Loss: nan\n",
      "Epoch [26/10000] Train Loss: 0.027709 Val Loss: nan\n",
      "Epoch [27/10000] Train Loss: 0.027102 Val Loss: nan\n",
      "Epoch [28/10000] Train Loss: 0.027100 Val Loss: nan\n",
      "Epoch [29/10000] Train Loss: 0.027109 Val Loss: nan\n",
      "Epoch [30/10000] Train Loss: 0.027111 Val Loss: nan\n",
      "Epoch [31/10000] Train Loss: 0.027347 Val Loss: nan\n",
      "Epoch [32/10000] Train Loss: 0.027650 Val Loss: nan\n",
      "Epoch [33/10000] Train Loss: 0.027124 Val Loss: nan\n",
      "Epoch [34/10000] Train Loss: 0.026859 Val Loss: nan\n",
      "Epoch [35/10000] Train Loss: 0.026934 Val Loss: nan\n",
      "Epoch [36/10000] Train Loss: 0.027034 Val Loss: nan\n",
      "Epoch [37/10000] Train Loss: 0.026601 Val Loss: nan\n",
      "Epoch [38/10000] Train Loss: 0.027028 Val Loss: nan\n",
      "Epoch [39/10000] Train Loss: 0.026881 Val Loss: nan\n",
      "Epoch [40/10000] Train Loss: 0.027247 Val Loss: nan\n",
      "Epoch [41/10000] Train Loss: 0.026847 Val Loss: nan\n",
      "Epoch [42/10000] Train Loss: 0.026507 Val Loss: nan\n",
      "Epoch [43/10000] Train Loss: 0.026441 Val Loss: nan\n",
      "Epoch [44/10000] Train Loss: 0.026466 Val Loss: nan\n",
      "Epoch [45/10000] Train Loss: 0.026659 Val Loss: nan\n",
      "Epoch [46/10000] Train Loss: 0.026352 Val Loss: nan\n",
      "Epoch [47/10000] Train Loss: 0.026463 Val Loss: nan\n",
      "Epoch [48/10000] Train Loss: 0.026729 Val Loss: nan\n",
      "Epoch [49/10000] Train Loss: 0.026521 Val Loss: nan\n",
      "Epoch [50/10000] Train Loss: 0.026417 Val Loss: nan\n",
      "Epoch [51/10000] Train Loss: 0.026406 Val Loss: nan\n",
      "Epoch [52/10000] Train Loss: 0.026663 Val Loss: nan\n",
      "Epoch [53/10000] Train Loss: 0.026241 Val Loss: nan\n",
      "Epoch [54/10000] Train Loss: 0.026158 Val Loss: nan\n",
      "Epoch [55/10000] Train Loss: 0.026446 Val Loss: nan\n",
      "Epoch [56/10000] Train Loss: 0.026694 Val Loss: nan\n",
      "Epoch [57/10000] Train Loss: 0.026383 Val Loss: nan\n",
      "Epoch [58/10000] Train Loss: 0.026630 Val Loss: nan\n",
      "Epoch [59/10000] Train Loss: 0.026298 Val Loss: nan\n",
      "Epoch [60/10000] Train Loss: 0.026363 Val Loss: nan\n",
      "Epoch [61/10000] Train Loss: 0.026499 Val Loss: nan\n",
      "Epoch [62/10000] Train Loss: 0.026551 Val Loss: nan\n",
      "Epoch [63/10000] Train Loss: 0.026092 Val Loss: nan\n",
      "Epoch [64/10000] Train Loss: 0.026224 Val Loss: nan\n",
      "Epoch [65/10000] Train Loss: 0.026126 Val Loss: nan\n",
      "Epoch [66/10000] Train Loss: 0.026599 Val Loss: nan\n",
      "Epoch [67/10000] Train Loss: 0.026226 Val Loss: nan\n",
      "Epoch [68/10000] Train Loss: 0.026441 Val Loss: nan\n",
      "Epoch [69/10000] Train Loss: 0.026362 Val Loss: nan\n",
      "Epoch [70/10000] Train Loss: 0.026313 Val Loss: nan\n",
      "Epoch [71/10000] Train Loss: 0.026171 Val Loss: nan\n",
      "Epoch [72/10000] Train Loss: 0.026041 Val Loss: nan\n",
      "Epoch [73/10000] Train Loss: 0.026773 Val Loss: nan\n",
      "Epoch [74/10000] Train Loss: 0.026563 Val Loss: nan\n",
      "Epoch [75/10000] Train Loss: 0.026591 Val Loss: nan\n",
      "Epoch [76/10000] Train Loss: 0.026044 Val Loss: nan\n",
      "Epoch [77/10000] Train Loss: 0.025996 Val Loss: nan\n",
      "Epoch [78/10000] Train Loss: 0.026432 Val Loss: nan\n",
      "Epoch [79/10000] Train Loss: 0.025917 Val Loss: nan\n",
      "Epoch [80/10000] Train Loss: 0.026287 Val Loss: nan\n",
      "Epoch [81/10000] Train Loss: 0.025849 Val Loss: nan\n",
      "Epoch [82/10000] Train Loss: 0.026022 Val Loss: nan\n",
      "Epoch [83/10000] Train Loss: 0.026355 Val Loss: nan\n",
      "Epoch [84/10000] Train Loss: 0.025977 Val Loss: nan\n",
      "Epoch [85/10000] Train Loss: 0.026026 Val Loss: nan\n",
      "Epoch [86/10000] Train Loss: 0.026136 Val Loss: nan\n",
      "Epoch [87/10000] Train Loss: 0.026288 Val Loss: nan\n",
      "Epoch [88/10000] Train Loss: 0.025865 Val Loss: nan\n",
      "Epoch [89/10000] Train Loss: 0.026037 Val Loss: nan\n",
      "Epoch [90/10000] Train Loss: 0.026301 Val Loss: nan\n",
      "Epoch [91/10000] Train Loss: 0.026058 Val Loss: nan\n",
      "Epoch [92/10000] Train Loss: 0.025864 Val Loss: nan\n",
      "Epoch [93/10000] Train Loss: 0.025906 Val Loss: nan\n",
      "Epoch [94/10000] Train Loss: 0.026015 Val Loss: nan\n",
      "Epoch [95/10000] Train Loss: 0.025737 Val Loss: nan\n",
      "Epoch [96/10000] Train Loss: 0.026222 Val Loss: nan\n",
      "Epoch [97/10000] Train Loss: 0.026201 Val Loss: nan\n",
      "Epoch [98/10000] Train Loss: 0.025914 Val Loss: nan\n",
      "Epoch [99/10000] Train Loss: 0.025905 Val Loss: nan\n",
      "Epoch [100/10000] Train Loss: 0.026059 Val Loss: nan\n",
      "Epoch [101/10000] Train Loss: 0.026433 Val Loss: nan\n",
      "Epoch [102/10000] Train Loss: 0.026160 Val Loss: nan\n",
      "Epoch [103/10000] Train Loss: 0.025982 Val Loss: nan\n",
      "Epoch [104/10000] Train Loss: 0.026572 Val Loss: nan\n",
      "Epoch [105/10000] Train Loss: 0.025673 Val Loss: nan\n",
      "Epoch [106/10000] Train Loss: 0.025879 Val Loss: nan\n",
      "Epoch [107/10000] Train Loss: 0.025749 Val Loss: nan\n",
      "Epoch [108/10000] Train Loss: 0.025653 Val Loss: nan\n",
      "Epoch [109/10000] Train Loss: 0.025507 Val Loss: nan\n",
      "Epoch [110/10000] Train Loss: 0.025958 Val Loss: nan\n",
      "Epoch [111/10000] Train Loss: 0.025902 Val Loss: nan\n",
      "Epoch [112/10000] Train Loss: 0.025769 Val Loss: nan\n",
      "Epoch [113/10000] Train Loss: 0.025549 Val Loss: nan\n",
      "Epoch [114/10000] Train Loss: 0.025874 Val Loss: nan\n",
      "Epoch [115/10000] Train Loss: 0.025785 Val Loss: nan\n",
      "Epoch [116/10000] Train Loss: 0.025763 Val Loss: nan\n",
      "Epoch [117/10000] Train Loss: 0.025521 Val Loss: nan\n",
      "Epoch [118/10000] Train Loss: 0.025366 Val Loss: nan\n",
      "Epoch [119/10000] Train Loss: 0.025684 Val Loss: nan\n",
      "Epoch [120/10000] Train Loss: 0.025406 Val Loss: nan\n",
      "Epoch [121/10000] Train Loss: 0.025483 Val Loss: nan\n",
      "Epoch [122/10000] Train Loss: 0.025404 Val Loss: nan\n",
      "Epoch [123/10000] Train Loss: 0.025496 Val Loss: nan\n",
      "Epoch [124/10000] Train Loss: 0.025863 Val Loss: nan\n",
      "Epoch [125/10000] Train Loss: 0.025395 Val Loss: nan\n",
      "Epoch [126/10000] Train Loss: 0.025329 Val Loss: nan\n",
      "Epoch [127/10000] Train Loss: 0.025207 Val Loss: nan\n",
      "Epoch [128/10000] Train Loss: 0.025319 Val Loss: nan\n",
      "Epoch [129/10000] Train Loss: 0.025054 Val Loss: nan\n",
      "Epoch [130/10000] Train Loss: 0.025287 Val Loss: nan\n",
      "Epoch [131/10000] Train Loss: 0.025079 Val Loss: nan\n",
      "Epoch [132/10000] Train Loss: 0.025500 Val Loss: nan\n",
      "Epoch [133/10000] Train Loss: 0.024977 Val Loss: nan\n",
      "Epoch [134/10000] Train Loss: 0.025456 Val Loss: nan\n",
      "Epoch [135/10000] Train Loss: 0.025195 Val Loss: nan\n",
      "Epoch [136/10000] Train Loss: 0.025285 Val Loss: nan\n",
      "Epoch [137/10000] Train Loss: 0.025204 Val Loss: nan\n",
      "Epoch [138/10000] Train Loss: 0.025208 Val Loss: nan\n",
      "Epoch [139/10000] Train Loss: 0.024836 Val Loss: nan\n",
      "Epoch [140/10000] Train Loss: 0.024875 Val Loss: nan\n",
      "Epoch [141/10000] Train Loss: 0.024823 Val Loss: nan\n",
      "Epoch [142/10000] Train Loss: 0.025103 Val Loss: nan\n",
      "Epoch [143/10000] Train Loss: 0.024687 Val Loss: nan\n",
      "Epoch [144/10000] Train Loss: 0.024983 Val Loss: nan\n",
      "Epoch [145/10000] Train Loss: 0.024857 Val Loss: nan\n",
      "Epoch [146/10000] Train Loss: 0.025173 Val Loss: nan\n",
      "Epoch [147/10000] Train Loss: 0.024640 Val Loss: nan\n",
      "Epoch [148/10000] Train Loss: 0.024765 Val Loss: nan\n",
      "Epoch [149/10000] Train Loss: 0.024836 Val Loss: nan\n",
      "Epoch [150/10000] Train Loss: 0.024398 Val Loss: nan\n",
      "Epoch [151/10000] Train Loss: 0.024543 Val Loss: nan\n",
      "Epoch [152/10000] Train Loss: 0.024705 Val Loss: nan\n",
      "Epoch [153/10000] Train Loss: 0.024624 Val Loss: nan\n",
      "Epoch [154/10000] Train Loss: 0.024996 Val Loss: nan\n",
      "Epoch [155/10000] Train Loss: 0.024879 Val Loss: nan\n",
      "Epoch [156/10000] Train Loss: 0.024423 Val Loss: nan\n",
      "Epoch [157/10000] Train Loss: 0.024238 Val Loss: nan\n",
      "Epoch [158/10000] Train Loss: 0.024458 Val Loss: nan\n",
      "Epoch [159/10000] Train Loss: 0.024433 Val Loss: nan\n",
      "Epoch [160/10000] Train Loss: 0.024382 Val Loss: nan\n",
      "Epoch [161/10000] Train Loss: 0.025174 Val Loss: nan\n",
      "Epoch [162/10000] Train Loss: 0.024428 Val Loss: nan\n",
      "Epoch [163/10000] Train Loss: 0.024217 Val Loss: nan\n",
      "Epoch [164/10000] Train Loss: 0.024194 Val Loss: nan\n",
      "Epoch [165/10000] Train Loss: 0.024118 Val Loss: nan\n",
      "Epoch [166/10000] Train Loss: 0.024555 Val Loss: nan\n",
      "Epoch [167/10000] Train Loss: 0.024333 Val Loss: nan\n",
      "Epoch [168/10000] Train Loss: 0.024144 Val Loss: nan\n",
      "Epoch [169/10000] Train Loss: 0.024277 Val Loss: nan\n",
      "Epoch [170/10000] Train Loss: 0.024484 Val Loss: nan\n",
      "Epoch [171/10000] Train Loss: 0.024254 Val Loss: nan\n",
      "Epoch [172/10000] Train Loss: 0.024316 Val Loss: nan\n",
      "Epoch [173/10000] Train Loss: 0.024501 Val Loss: nan\n",
      "Epoch [174/10000] Train Loss: 0.024119 Val Loss: nan\n",
      "Epoch [175/10000] Train Loss: 0.023936 Val Loss: nan\n",
      "Epoch [176/10000] Train Loss: 0.024460 Val Loss: nan\n",
      "Epoch [177/10000] Train Loss: 0.024205 Val Loss: nan\n",
      "Epoch [178/10000] Train Loss: 0.024200 Val Loss: nan\n",
      "Epoch [179/10000] Train Loss: 0.024100 Val Loss: nan\n",
      "Epoch [180/10000] Train Loss: 0.023929 Val Loss: nan\n",
      "Epoch [181/10000] Train Loss: 0.024384 Val Loss: nan\n",
      "Epoch [182/10000] Train Loss: 0.024278 Val Loss: nan\n",
      "Epoch [183/10000] Train Loss: 0.024114 Val Loss: nan\n",
      "Epoch [184/10000] Train Loss: 0.024422 Val Loss: nan\n",
      "Epoch [185/10000] Train Loss: 0.023888 Val Loss: nan\n",
      "Epoch [186/10000] Train Loss: 0.023809 Val Loss: nan\n",
      "Epoch [187/10000] Train Loss: 0.023921 Val Loss: nan\n",
      "Epoch [188/10000] Train Loss: 0.024670 Val Loss: nan\n",
      "Epoch [189/10000] Train Loss: 0.024462 Val Loss: nan\n",
      "Epoch [190/10000] Train Loss: 0.023968 Val Loss: nan\n",
      "Epoch [191/10000] Train Loss: 0.024474 Val Loss: nan\n",
      "Epoch [192/10000] Train Loss: 0.024065 Val Loss: nan\n",
      "Epoch [193/10000] Train Loss: 0.023964 Val Loss: nan\n",
      "Epoch [194/10000] Train Loss: 0.024031 Val Loss: nan\n",
      "Epoch [195/10000] Train Loss: 0.024177 Val Loss: nan\n",
      "Epoch [196/10000] Train Loss: 0.023927 Val Loss: nan\n",
      "Epoch [197/10000] Train Loss: 0.023819 Val Loss: nan\n",
      "Epoch [198/10000] Train Loss: 0.023971 Val Loss: nan\n",
      "Epoch [199/10000] Train Loss: 0.023879 Val Loss: nan\n",
      "Epoch [200/10000] Train Loss: 0.024306 Val Loss: nan\n",
      "Epoch [201/10000] Train Loss: 0.024191 Val Loss: nan\n",
      "Epoch [202/10000] Train Loss: 0.024299 Val Loss: nan\n",
      "Epoch [203/10000] Train Loss: 0.023699 Val Loss: nan\n",
      "Epoch [204/10000] Train Loss: 0.024089 Val Loss: nan\n",
      "Epoch [205/10000] Train Loss: 0.023809 Val Loss: nan\n",
      "Epoch [206/10000] Train Loss: 0.023765 Val Loss: nan\n",
      "Epoch [207/10000] Train Loss: 0.023867 Val Loss: nan\n",
      "Epoch [208/10000] Train Loss: 0.023652 Val Loss: nan\n",
      "Epoch [209/10000] Train Loss: 0.023738 Val Loss: nan\n",
      "Epoch [210/10000] Train Loss: 0.023760 Val Loss: nan\n",
      "Epoch [211/10000] Train Loss: 0.023645 Val Loss: nan\n",
      "Epoch [212/10000] Train Loss: 0.023893 Val Loss: nan\n",
      "Epoch [213/10000] Train Loss: 0.024038 Val Loss: nan\n",
      "Epoch [214/10000] Train Loss: 0.023792 Val Loss: nan\n",
      "Epoch [215/10000] Train Loss: 0.023840 Val Loss: nan\n",
      "Epoch [216/10000] Train Loss: 0.023815 Val Loss: nan\n",
      "Epoch [217/10000] Train Loss: 0.023716 Val Loss: nan\n",
      "Epoch [218/10000] Train Loss: 0.023774 Val Loss: nan\n",
      "Epoch [219/10000] Train Loss: 0.023716 Val Loss: nan\n",
      "Epoch [220/10000] Train Loss: 0.023950 Val Loss: nan\n",
      "Epoch [221/10000] Train Loss: 0.023608 Val Loss: nan\n",
      "Epoch [222/10000] Train Loss: 0.023844 Val Loss: nan\n",
      "Epoch [223/10000] Train Loss: 0.023840 Val Loss: nan\n",
      "Epoch [224/10000] Train Loss: 0.023710 Val Loss: nan\n",
      "Epoch [225/10000] Train Loss: 0.024054 Val Loss: nan\n",
      "Epoch [226/10000] Train Loss: 0.023605 Val Loss: nan\n",
      "Epoch [227/10000] Train Loss: 0.023766 Val Loss: nan\n",
      "Epoch [228/10000] Train Loss: 0.023765 Val Loss: nan\n",
      "Epoch [229/10000] Train Loss: 0.023696 Val Loss: nan\n",
      "Epoch [230/10000] Train Loss: 0.023663 Val Loss: nan\n",
      "Epoch [231/10000] Train Loss: 0.023505 Val Loss: nan\n",
      "Epoch [232/10000] Train Loss: 0.023533 Val Loss: nan\n",
      "Epoch [233/10000] Train Loss: 0.023834 Val Loss: nan\n",
      "Epoch [234/10000] Train Loss: 0.023365 Val Loss: nan\n",
      "Epoch [235/10000] Train Loss: 0.023866 Val Loss: nan\n",
      "Epoch [236/10000] Train Loss: 0.023641 Val Loss: nan\n",
      "Epoch [237/10000] Train Loss: 0.023493 Val Loss: nan\n",
      "Epoch [238/10000] Train Loss: 0.023462 Val Loss: nan\n",
      "Epoch [239/10000] Train Loss: 0.023854 Val Loss: nan\n",
      "Epoch [240/10000] Train Loss: 0.023420 Val Loss: nan\n",
      "Epoch [241/10000] Train Loss: 0.023343 Val Loss: nan\n",
      "Epoch [242/10000] Train Loss: 0.023532 Val Loss: nan\n",
      "Epoch [243/10000] Train Loss: 0.023711 Val Loss: nan\n",
      "Epoch [244/10000] Train Loss: 0.023670 Val Loss: nan\n",
      "Epoch [245/10000] Train Loss: 0.023438 Val Loss: nan\n",
      "Epoch [246/10000] Train Loss: 0.023498 Val Loss: nan\n",
      "Epoch [247/10000] Train Loss: 0.023418 Val Loss: nan\n",
      "Epoch [248/10000] Train Loss: 0.023485 Val Loss: nan\n",
      "Epoch [249/10000] Train Loss: 0.023446 Val Loss: nan\n",
      "Epoch [250/10000] Train Loss: 0.023622 Val Loss: nan\n",
      "Epoch [251/10000] Train Loss: 0.023703 Val Loss: nan\n",
      "Epoch [252/10000] Train Loss: 0.023323 Val Loss: nan\n",
      "Epoch [253/10000] Train Loss: 0.023181 Val Loss: nan\n",
      "Epoch [254/10000] Train Loss: 0.023164 Val Loss: nan\n",
      "Epoch [255/10000] Train Loss: 0.023328 Val Loss: nan\n",
      "Epoch [256/10000] Train Loss: 0.023420 Val Loss: nan\n",
      "Epoch [257/10000] Train Loss: 0.023331 Val Loss: nan\n",
      "Epoch [258/10000] Train Loss: 0.023265 Val Loss: nan\n",
      "Epoch [259/10000] Train Loss: 0.023375 Val Loss: nan\n",
      "Epoch [260/10000] Train Loss: 0.023468 Val Loss: nan\n",
      "Epoch [261/10000] Train Loss: 0.023488 Val Loss: nan\n",
      "Epoch [262/10000] Train Loss: 0.023310 Val Loss: nan\n",
      "Epoch [263/10000] Train Loss: 0.023040 Val Loss: nan\n",
      "Epoch [264/10000] Train Loss: 0.023223 Val Loss: nan\n",
      "Epoch [265/10000] Train Loss: 0.023563 Val Loss: nan\n",
      "Epoch [266/10000] Train Loss: 0.023305 Val Loss: nan\n",
      "Epoch [267/10000] Train Loss: 0.023425 Val Loss: nan\n",
      "Epoch [268/10000] Train Loss: 0.023479 Val Loss: nan\n",
      "Epoch [269/10000] Train Loss: 0.023096 Val Loss: nan\n",
      "Epoch [270/10000] Train Loss: 0.023490 Val Loss: nan\n",
      "Epoch [271/10000] Train Loss: 0.023437 Val Loss: nan\n",
      "Epoch [272/10000] Train Loss: 0.022989 Val Loss: nan\n",
      "Epoch [273/10000] Train Loss: 0.023469 Val Loss: nan\n",
      "Epoch [274/10000] Train Loss: 0.023428 Val Loss: nan\n",
      "Epoch [275/10000] Train Loss: 0.023317 Val Loss: nan\n",
      "Epoch [276/10000] Train Loss: 0.023069 Val Loss: nan\n",
      "Epoch [277/10000] Train Loss: 0.022976 Val Loss: nan\n",
      "Epoch [278/10000] Train Loss: 0.023214 Val Loss: nan\n",
      "Epoch [279/10000] Train Loss: 0.023047 Val Loss: nan\n",
      "Epoch [280/10000] Train Loss: 0.023237 Val Loss: nan\n",
      "Epoch [281/10000] Train Loss: 0.023161 Val Loss: nan\n",
      "Epoch [282/10000] Train Loss: 0.023918 Val Loss: nan\n",
      "Epoch [283/10000] Train Loss: 0.023113 Val Loss: nan\n",
      "Epoch [284/10000] Train Loss: 0.023012 Val Loss: nan\n",
      "Epoch [285/10000] Train Loss: 0.023101 Val Loss: nan\n",
      "Epoch [286/10000] Train Loss: 0.023249 Val Loss: nan\n",
      "Epoch [287/10000] Train Loss: 0.022924 Val Loss: nan\n",
      "Epoch [288/10000] Train Loss: 0.023344 Val Loss: nan\n",
      "Epoch [289/10000] Train Loss: 0.022944 Val Loss: nan\n",
      "Epoch [290/10000] Train Loss: 0.023102 Val Loss: nan\n",
      "Epoch [291/10000] Train Loss: 0.023405 Val Loss: nan\n",
      "Epoch [292/10000] Train Loss: 0.023486 Val Loss: nan\n",
      "Epoch [293/10000] Train Loss: 0.023197 Val Loss: nan\n",
      "Epoch [294/10000] Train Loss: 0.023319 Val Loss: nan\n",
      "Epoch [295/10000] Train Loss: 0.023070 Val Loss: nan\n",
      "Epoch [296/10000] Train Loss: 0.023339 Val Loss: nan\n",
      "Epoch [297/10000] Train Loss: 0.022732 Val Loss: nan\n",
      "Epoch [298/10000] Train Loss: 0.022886 Val Loss: nan\n",
      "Epoch [299/10000] Train Loss: 0.023437 Val Loss: nan\n",
      "Epoch [300/10000] Train Loss: 0.022898 Val Loss: nan\n",
      "Epoch [301/10000] Train Loss: 0.022913 Val Loss: nan\n",
      "Epoch [302/10000] Train Loss: 0.023074 Val Loss: nan\n",
      "Epoch [303/10000] Train Loss: 0.023107 Val Loss: nan\n",
      "Epoch [304/10000] Train Loss: 0.023443 Val Loss: nan\n",
      "Epoch [305/10000] Train Loss: 0.022944 Val Loss: nan\n",
      "Epoch [306/10000] Train Loss: 0.023038 Val Loss: nan\n",
      "Epoch [307/10000] Train Loss: 0.022954 Val Loss: nan\n",
      "Epoch [308/10000] Train Loss: 0.022851 Val Loss: nan\n",
      "Epoch [309/10000] Train Loss: 0.022801 Val Loss: nan\n",
      "Epoch [310/10000] Train Loss: 0.022949 Val Loss: nan\n",
      "Epoch [311/10000] Train Loss: 0.023108 Val Loss: nan\n",
      "Epoch [312/10000] Train Loss: 0.022901 Val Loss: nan\n",
      "Epoch [313/10000] Train Loss: 0.023112 Val Loss: nan\n",
      "Epoch [314/10000] Train Loss: 0.023091 Val Loss: nan\n",
      "Epoch [315/10000] Train Loss: 0.023008 Val Loss: nan\n",
      "Epoch [316/10000] Train Loss: 0.023070 Val Loss: nan\n",
      "Epoch [317/10000] Train Loss: 0.023138 Val Loss: nan\n",
      "Epoch [318/10000] Train Loss: 0.022720 Val Loss: nan\n",
      "Epoch [319/10000] Train Loss: 0.022790 Val Loss: nan\n",
      "Epoch [320/10000] Train Loss: 0.023361 Val Loss: nan\n",
      "Epoch [321/10000] Train Loss: 0.023152 Val Loss: nan\n",
      "Epoch [322/10000] Train Loss: 0.022747 Val Loss: nan\n",
      "Epoch [323/10000] Train Loss: 0.023236 Val Loss: nan\n",
      "Epoch [324/10000] Train Loss: 0.022806 Val Loss: nan\n",
      "Epoch [325/10000] Train Loss: 0.023014 Val Loss: nan\n",
      "Epoch [326/10000] Train Loss: 0.022531 Val Loss: nan\n",
      "Epoch [327/10000] Train Loss: 0.022927 Val Loss: nan\n",
      "Epoch [328/10000] Train Loss: 0.022792 Val Loss: nan\n",
      "Epoch [329/10000] Train Loss: 0.022977 Val Loss: nan\n",
      "Epoch [330/10000] Train Loss: 0.022766 Val Loss: nan\n",
      "Epoch [331/10000] Train Loss: 0.022658 Val Loss: nan\n",
      "Epoch [332/10000] Train Loss: 0.022696 Val Loss: nan\n",
      "Epoch [333/10000] Train Loss: 0.023273 Val Loss: nan\n",
      "Epoch [334/10000] Train Loss: 0.022740 Val Loss: nan\n",
      "Epoch [335/10000] Train Loss: 0.022956 Val Loss: nan\n",
      "Epoch [336/10000] Train Loss: 0.023381 Val Loss: nan\n",
      "Epoch [337/10000] Train Loss: 0.022968 Val Loss: nan\n",
      "Epoch [338/10000] Train Loss: 0.022440 Val Loss: nan\n",
      "Epoch [339/10000] Train Loss: 0.022696 Val Loss: nan\n",
      "Epoch [340/10000] Train Loss: 0.022692 Val Loss: nan\n",
      "Epoch [341/10000] Train Loss: 0.022678 Val Loss: nan\n",
      "Epoch [342/10000] Train Loss: 0.022523 Val Loss: nan\n",
      "Epoch [343/10000] Train Loss: 0.022858 Val Loss: nan\n",
      "Epoch [344/10000] Train Loss: 0.022875 Val Loss: nan\n",
      "Epoch [345/10000] Train Loss: 0.022414 Val Loss: nan\n",
      "Epoch [346/10000] Train Loss: 0.022675 Val Loss: nan\n",
      "Epoch [347/10000] Train Loss: 0.022574 Val Loss: nan\n",
      "Epoch [348/10000] Train Loss: 0.023337 Val Loss: nan\n",
      "Epoch [349/10000] Train Loss: 0.023024 Val Loss: nan\n",
      "Epoch [350/10000] Train Loss: 0.022880 Val Loss: nan\n",
      "Epoch [351/10000] Train Loss: 0.022540 Val Loss: nan\n",
      "Epoch [352/10000] Train Loss: 0.022936 Val Loss: nan\n",
      "Epoch [353/10000] Train Loss: 0.022687 Val Loss: nan\n",
      "Epoch [354/10000] Train Loss: 0.022611 Val Loss: nan\n",
      "Epoch [355/10000] Train Loss: 0.022554 Val Loss: nan\n",
      "Epoch [356/10000] Train Loss: 0.022605 Val Loss: nan\n",
      "Epoch [357/10000] Train Loss: 0.022469 Val Loss: nan\n",
      "Epoch [358/10000] Train Loss: 0.022382 Val Loss: nan\n",
      "Epoch [359/10000] Train Loss: 0.022970 Val Loss: nan\n",
      "Epoch [360/10000] Train Loss: 0.022932 Val Loss: nan\n",
      "Epoch [361/10000] Train Loss: 0.022371 Val Loss: nan\n",
      "Epoch [362/10000] Train Loss: 0.022377 Val Loss: nan\n",
      "Epoch [363/10000] Train Loss: 0.022632 Val Loss: nan\n",
      "Epoch [364/10000] Train Loss: 0.022750 Val Loss: nan\n",
      "Epoch [365/10000] Train Loss: 0.022434 Val Loss: nan\n",
      "Epoch [366/10000] Train Loss: 0.023185 Val Loss: nan\n",
      "Epoch [367/10000] Train Loss: 0.022530 Val Loss: nan\n",
      "Epoch [368/10000] Train Loss: 0.022540 Val Loss: nan\n",
      "Epoch [369/10000] Train Loss: 0.022583 Val Loss: nan\n",
      "Epoch [370/10000] Train Loss: 0.022648 Val Loss: nan\n",
      "Epoch [371/10000] Train Loss: 0.022696 Val Loss: nan\n",
      "Epoch [372/10000] Train Loss: 0.022348 Val Loss: nan\n",
      "Epoch [373/10000] Train Loss: 0.023065 Val Loss: nan\n",
      "Epoch [374/10000] Train Loss: 0.022761 Val Loss: nan\n",
      "Epoch [375/10000] Train Loss: 0.022543 Val Loss: nan\n",
      "Epoch [376/10000] Train Loss: 0.022284 Val Loss: nan\n",
      "Epoch [377/10000] Train Loss: 0.022882 Val Loss: nan\n",
      "Epoch [378/10000] Train Loss: 0.022361 Val Loss: nan\n",
      "Epoch [379/10000] Train Loss: 0.022497 Val Loss: nan\n",
      "Epoch [380/10000] Train Loss: 0.022216 Val Loss: nan\n",
      "Epoch [381/10000] Train Loss: 0.022277 Val Loss: nan\n",
      "Epoch [382/10000] Train Loss: 0.022394 Val Loss: nan\n",
      "Epoch [383/10000] Train Loss: 0.022362 Val Loss: nan\n",
      "Epoch [384/10000] Train Loss: 0.022333 Val Loss: nan\n",
      "Epoch [385/10000] Train Loss: 0.022183 Val Loss: nan\n",
      "Epoch [386/10000] Train Loss: 0.022174 Val Loss: nan\n",
      "Epoch [387/10000] Train Loss: 0.022599 Val Loss: nan\n",
      "Epoch [388/10000] Train Loss: 0.022548 Val Loss: nan\n",
      "Epoch [389/10000] Train Loss: 0.022581 Val Loss: nan\n",
      "Epoch [390/10000] Train Loss: 0.022182 Val Loss: nan\n",
      "Epoch [391/10000] Train Loss: 0.022306 Val Loss: nan\n",
      "Epoch [392/10000] Train Loss: 0.022178 Val Loss: nan\n",
      "Epoch [393/10000] Train Loss: 0.021966 Val Loss: nan\n",
      "Epoch [394/10000] Train Loss: 0.022475 Val Loss: nan\n",
      "Epoch [395/10000] Train Loss: 0.022629 Val Loss: nan\n",
      "Epoch [396/10000] Train Loss: 0.022558 Val Loss: nan\n",
      "Epoch [397/10000] Train Loss: 0.022366 Val Loss: nan\n",
      "Epoch [398/10000] Train Loss: 0.022143 Val Loss: nan\n",
      "Epoch [399/10000] Train Loss: 0.022418 Val Loss: nan\n",
      "Epoch [400/10000] Train Loss: 0.022071 Val Loss: nan\n",
      "Epoch [401/10000] Train Loss: 0.022373 Val Loss: nan\n",
      "Epoch [402/10000] Train Loss: 0.022171 Val Loss: nan\n",
      "Epoch [403/10000] Train Loss: 0.022496 Val Loss: nan\n",
      "Epoch [404/10000] Train Loss: 0.022073 Val Loss: nan\n",
      "Epoch [405/10000] Train Loss: 0.022074 Val Loss: nan\n",
      "Epoch [406/10000] Train Loss: 0.022261 Val Loss: nan\n",
      "Epoch [407/10000] Train Loss: 0.022438 Val Loss: nan\n",
      "Epoch [408/10000] Train Loss: 0.021827 Val Loss: nan\n",
      "Epoch [409/10000] Train Loss: 0.021965 Val Loss: nan\n",
      "Epoch [410/10000] Train Loss: 0.022069 Val Loss: nan\n",
      "Epoch [411/10000] Train Loss: 0.021928 Val Loss: nan\n",
      "Epoch [412/10000] Train Loss: 0.021874 Val Loss: nan\n",
      "Epoch [413/10000] Train Loss: 0.021964 Val Loss: nan\n",
      "Epoch [414/10000] Train Loss: 0.021999 Val Loss: nan\n",
      "Epoch [415/10000] Train Loss: 0.022137 Val Loss: nan\n",
      "Epoch [416/10000] Train Loss: 0.022020 Val Loss: nan\n",
      "Epoch [417/10000] Train Loss: 0.021980 Val Loss: nan\n",
      "Epoch [418/10000] Train Loss: 0.022293 Val Loss: nan\n",
      "Epoch [419/10000] Train Loss: 0.022447 Val Loss: nan\n",
      "Epoch [420/10000] Train Loss: 0.022000 Val Loss: nan\n",
      "Epoch [421/10000] Train Loss: 0.021926 Val Loss: nan\n",
      "Epoch [422/10000] Train Loss: 0.022202 Val Loss: nan\n",
      "Epoch [423/10000] Train Loss: 0.021862 Val Loss: nan\n",
      "Epoch [424/10000] Train Loss: 0.021882 Val Loss: nan\n",
      "Epoch [425/10000] Train Loss: 0.022245 Val Loss: nan\n",
      "Epoch [426/10000] Train Loss: 0.022053 Val Loss: nan\n",
      "Epoch [427/10000] Train Loss: 0.021805 Val Loss: nan\n",
      "Epoch [428/10000] Train Loss: 0.021739 Val Loss: nan\n",
      "Epoch [429/10000] Train Loss: 0.021846 Val Loss: nan\n",
      "Epoch [430/10000] Train Loss: 0.021800 Val Loss: nan\n",
      "Epoch [431/10000] Train Loss: 0.021689 Val Loss: nan\n",
      "Epoch [432/10000] Train Loss: 0.021715 Val Loss: nan\n",
      "Epoch [433/10000] Train Loss: 0.021717 Val Loss: nan\n",
      "Epoch [434/10000] Train Loss: 0.021704 Val Loss: nan\n",
      "Epoch [435/10000] Train Loss: 0.021711 Val Loss: nan\n",
      "Epoch [436/10000] Train Loss: 0.021640 Val Loss: nan\n",
      "Epoch [437/10000] Train Loss: 0.021889 Val Loss: nan\n",
      "Epoch [438/10000] Train Loss: 0.021715 Val Loss: nan\n",
      "Epoch [439/10000] Train Loss: 0.021652 Val Loss: nan\n",
      "Epoch [440/10000] Train Loss: 0.021697 Val Loss: nan\n",
      "Epoch [441/10000] Train Loss: 0.021511 Val Loss: nan\n",
      "Epoch [442/10000] Train Loss: 0.021477 Val Loss: nan\n",
      "Epoch [443/10000] Train Loss: 0.021944 Val Loss: nan\n",
      "Epoch [444/10000] Train Loss: 0.021626 Val Loss: nan\n",
      "Epoch [445/10000] Train Loss: 0.021394 Val Loss: nan\n",
      "Epoch [446/10000] Train Loss: 0.022299 Val Loss: nan\n",
      "Epoch [447/10000] Train Loss: 0.021545 Val Loss: nan\n",
      "Epoch [448/10000] Train Loss: 0.022135 Val Loss: nan\n",
      "Epoch [449/10000] Train Loss: 0.021503 Val Loss: nan\n",
      "Epoch [450/10000] Train Loss: 0.021658 Val Loss: nan\n",
      "Epoch [451/10000] Train Loss: 0.021460 Val Loss: nan\n",
      "Epoch [452/10000] Train Loss: 0.021507 Val Loss: nan\n",
      "Epoch [453/10000] Train Loss: 0.021423 Val Loss: nan\n",
      "Epoch [454/10000] Train Loss: 0.021943 Val Loss: nan\n",
      "Epoch [455/10000] Train Loss: 0.021664 Val Loss: nan\n",
      "Epoch [456/10000] Train Loss: 0.021348 Val Loss: nan\n",
      "Epoch [457/10000] Train Loss: 0.021330 Val Loss: nan\n",
      "Epoch [458/10000] Train Loss: 0.021628 Val Loss: nan\n",
      "Epoch [459/10000] Train Loss: 0.021783 Val Loss: nan\n",
      "Epoch [460/10000] Train Loss: 0.021588 Val Loss: nan\n",
      "Epoch [461/10000] Train Loss: 0.021483 Val Loss: nan\n",
      "Epoch [462/10000] Train Loss: 0.021899 Val Loss: nan\n",
      "Epoch [463/10000] Train Loss: 0.021788 Val Loss: nan\n",
      "Epoch [464/10000] Train Loss: 0.021193 Val Loss: nan\n",
      "Epoch [465/10000] Train Loss: 0.021322 Val Loss: nan\n",
      "Epoch [466/10000] Train Loss: 0.021268 Val Loss: nan\n",
      "Epoch [467/10000] Train Loss: 0.021049 Val Loss: nan\n",
      "Epoch [468/10000] Train Loss: 0.021792 Val Loss: nan\n",
      "Epoch [469/10000] Train Loss: 0.021349 Val Loss: nan\n",
      "Epoch [470/10000] Train Loss: 0.022060 Val Loss: nan\n",
      "Epoch [471/10000] Train Loss: 0.021379 Val Loss: nan\n",
      "Epoch [472/10000] Train Loss: 0.021361 Val Loss: nan\n",
      "Epoch [473/10000] Train Loss: 0.021212 Val Loss: nan\n",
      "Epoch [474/10000] Train Loss: 0.020933 Val Loss: nan\n",
      "Epoch [475/10000] Train Loss: 0.021341 Val Loss: nan\n",
      "Epoch [476/10000] Train Loss: 0.021334 Val Loss: nan\n",
      "Epoch [477/10000] Train Loss: 0.021461 Val Loss: nan\n",
      "Epoch [478/10000] Train Loss: 0.021333 Val Loss: nan\n",
      "Epoch [479/10000] Train Loss: 0.021352 Val Loss: nan\n",
      "Epoch [480/10000] Train Loss: 0.021205 Val Loss: nan\n",
      "Epoch [481/10000] Train Loss: 0.021557 Val Loss: nan\n",
      "Epoch [482/10000] Train Loss: 0.021282 Val Loss: nan\n",
      "Epoch [483/10000] Train Loss: 0.021087 Val Loss: nan\n",
      "Epoch [484/10000] Train Loss: 0.021463 Val Loss: nan\n",
      "Epoch [485/10000] Train Loss: 0.021235 Val Loss: nan\n",
      "Epoch [486/10000] Train Loss: 0.021041 Val Loss: nan\n",
      "Epoch [487/10000] Train Loss: 0.021080 Val Loss: nan\n",
      "Epoch [488/10000] Train Loss: 0.020837 Val Loss: nan\n",
      "Epoch [489/10000] Train Loss: 0.021150 Val Loss: nan\n",
      "Epoch [490/10000] Train Loss: 0.020798 Val Loss: nan\n",
      "Epoch [491/10000] Train Loss: 0.020915 Val Loss: nan\n",
      "Epoch [492/10000] Train Loss: 0.020807 Val Loss: nan\n",
      "Epoch [493/10000] Train Loss: 0.020759 Val Loss: nan\n",
      "Epoch [494/10000] Train Loss: 0.020877 Val Loss: nan\n",
      "Epoch [495/10000] Train Loss: 0.020767 Val Loss: nan\n",
      "Epoch [496/10000] Train Loss: 0.020689 Val Loss: nan\n",
      "Epoch [497/10000] Train Loss: 0.020757 Val Loss: nan\n",
      "Epoch [498/10000] Train Loss: 0.021206 Val Loss: nan\n",
      "Epoch [499/10000] Train Loss: 0.020928 Val Loss: nan\n",
      "Epoch [500/10000] Train Loss: 0.021188 Val Loss: nan\n",
      "Epoch [501/10000] Train Loss: 0.020941 Val Loss: nan\n",
      "Epoch [502/10000] Train Loss: 0.021279 Val Loss: nan\n",
      "Epoch [503/10000] Train Loss: 0.020686 Val Loss: nan\n",
      "Epoch [504/10000] Train Loss: 0.020644 Val Loss: nan\n",
      "Epoch [505/10000] Train Loss: 0.021015 Val Loss: nan\n",
      "Epoch [506/10000] Train Loss: 0.021374 Val Loss: nan\n",
      "Epoch [507/10000] Train Loss: 0.020846 Val Loss: nan\n",
      "Epoch [508/10000] Train Loss: 0.020690 Val Loss: nan\n",
      "Epoch [509/10000] Train Loss: 0.020836 Val Loss: nan\n",
      "Epoch [510/10000] Train Loss: 0.020474 Val Loss: nan\n",
      "Epoch [511/10000] Train Loss: 0.020522 Val Loss: nan\n",
      "Epoch [512/10000] Train Loss: 0.020971 Val Loss: nan\n",
      "Epoch [513/10000] Train Loss: 0.020886 Val Loss: nan\n",
      "Epoch [514/10000] Train Loss: 0.020665 Val Loss: nan\n",
      "Epoch [515/10000] Train Loss: 0.020538 Val Loss: nan\n",
      "Epoch [516/10000] Train Loss: 0.020454 Val Loss: nan\n",
      "Epoch [517/10000] Train Loss: 0.021230 Val Loss: nan\n",
      "Epoch [518/10000] Train Loss: 0.021080 Val Loss: nan\n",
      "Epoch [519/10000] Train Loss: 0.020991 Val Loss: nan\n",
      "Epoch [520/10000] Train Loss: 0.020598 Val Loss: nan\n",
      "Epoch [521/10000] Train Loss: 0.020277 Val Loss: nan\n",
      "Epoch [522/10000] Train Loss: 0.020293 Val Loss: nan\n",
      "Epoch [523/10000] Train Loss: 0.020738 Val Loss: nan\n",
      "Epoch [524/10000] Train Loss: 0.020459 Val Loss: nan\n",
      "Epoch [525/10000] Train Loss: 0.020687 Val Loss: nan\n",
      "Epoch [526/10000] Train Loss: 0.020256 Val Loss: nan\n",
      "Epoch [527/10000] Train Loss: 0.020817 Val Loss: nan\n",
      "Epoch [528/10000] Train Loss: 0.020418 Val Loss: nan\n",
      "Epoch [529/10000] Train Loss: 0.020425 Val Loss: nan\n",
      "Epoch [530/10000] Train Loss: 0.020614 Val Loss: nan\n",
      "Epoch [531/10000] Train Loss: 0.020431 Val Loss: nan\n",
      "Epoch [532/10000] Train Loss: 0.020628 Val Loss: nan\n",
      "Epoch [533/10000] Train Loss: 0.020371 Val Loss: nan\n",
      "Epoch [534/10000] Train Loss: 0.020718 Val Loss: nan\n",
      "Epoch [535/10000] Train Loss: 0.020614 Val Loss: nan\n",
      "Epoch [536/10000] Train Loss: 0.020300 Val Loss: nan\n",
      "Epoch [537/10000] Train Loss: 0.020600 Val Loss: nan\n",
      "Epoch [538/10000] Train Loss: 0.020599 Val Loss: nan\n",
      "Epoch [539/10000] Train Loss: 0.020432 Val Loss: nan\n",
      "Epoch [540/10000] Train Loss: 0.020219 Val Loss: nan\n",
      "Epoch [541/10000] Train Loss: 0.020459 Val Loss: nan\n",
      "Epoch [542/10000] Train Loss: 0.020105 Val Loss: nan\n",
      "Epoch [543/10000] Train Loss: 0.020071 Val Loss: nan\n",
      "Epoch [544/10000] Train Loss: 0.020162 Val Loss: nan\n",
      "Epoch [545/10000] Train Loss: 0.020052 Val Loss: nan\n",
      "Epoch [546/10000] Train Loss: 0.020245 Val Loss: nan\n",
      "Epoch [547/10000] Train Loss: 0.020451 Val Loss: nan\n",
      "Epoch [548/10000] Train Loss: 0.020310 Val Loss: nan\n",
      "Epoch [549/10000] Train Loss: 0.020266 Val Loss: nan\n",
      "Epoch [550/10000] Train Loss: 0.020480 Val Loss: nan\n",
      "Epoch [551/10000] Train Loss: 0.020703 Val Loss: nan\n",
      "Epoch [552/10000] Train Loss: 0.019991 Val Loss: nan\n",
      "Epoch [553/10000] Train Loss: 0.020188 Val Loss: nan\n",
      "Epoch [554/10000] Train Loss: 0.019953 Val Loss: nan\n",
      "Epoch [555/10000] Train Loss: 0.020462 Val Loss: nan\n",
      "Epoch [556/10000] Train Loss: 0.020381 Val Loss: nan\n",
      "Epoch [557/10000] Train Loss: 0.020047 Val Loss: nan\n",
      "Epoch [558/10000] Train Loss: 0.020191 Val Loss: nan\n",
      "Epoch [559/10000] Train Loss: 0.020183 Val Loss: nan\n",
      "Epoch [560/10000] Train Loss: 0.020280 Val Loss: nan\n",
      "Epoch [561/10000] Train Loss: 0.020317 Val Loss: nan\n",
      "Epoch [562/10000] Train Loss: 0.020709 Val Loss: nan\n",
      "Epoch [563/10000] Train Loss: 0.020339 Val Loss: nan\n",
      "Epoch [564/10000] Train Loss: 0.020040 Val Loss: nan\n",
      "Epoch [565/10000] Train Loss: 0.019914 Val Loss: nan\n",
      "Epoch [566/10000] Train Loss: 0.019872 Val Loss: nan\n",
      "Epoch [567/10000] Train Loss: 0.020056 Val Loss: nan\n",
      "Epoch [568/10000] Train Loss: 0.020205 Val Loss: nan\n",
      "Epoch [569/10000] Train Loss: 0.020030 Val Loss: nan\n",
      "Epoch [570/10000] Train Loss: 0.019996 Val Loss: nan\n",
      "Epoch [571/10000] Train Loss: 0.020237 Val Loss: nan\n",
      "Epoch [572/10000] Train Loss: 0.019845 Val Loss: nan\n",
      "Epoch [573/10000] Train Loss: 0.019793 Val Loss: nan\n",
      "Epoch [574/10000] Train Loss: 0.020000 Val Loss: nan\n",
      "Epoch [575/10000] Train Loss: 0.019676 Val Loss: nan\n",
      "Epoch [576/10000] Train Loss: 0.019854 Val Loss: nan\n",
      "Epoch [577/10000] Train Loss: 0.019893 Val Loss: nan\n",
      "Epoch [578/10000] Train Loss: 0.020009 Val Loss: nan\n",
      "Epoch [579/10000] Train Loss: 0.019825 Val Loss: nan\n",
      "Epoch [580/10000] Train Loss: 0.020104 Val Loss: nan\n",
      "Epoch [581/10000] Train Loss: 0.020398 Val Loss: nan\n",
      "Epoch [582/10000] Train Loss: 0.020010 Val Loss: nan\n",
      "Epoch [583/10000] Train Loss: 0.019926 Val Loss: nan\n",
      "Epoch [584/10000] Train Loss: 0.019872 Val Loss: nan\n",
      "Epoch [585/10000] Train Loss: 0.020271 Val Loss: nan\n",
      "Epoch [586/10000] Train Loss: 0.019712 Val Loss: nan\n",
      "Epoch [587/10000] Train Loss: 0.020202 Val Loss: nan\n",
      "Epoch [588/10000] Train Loss: 0.020269 Val Loss: nan\n",
      "Epoch [589/10000] Train Loss: 0.020305 Val Loss: nan\n",
      "Epoch [590/10000] Train Loss: 0.019993 Val Loss: nan\n",
      "Epoch [591/10000] Train Loss: 0.020201 Val Loss: nan\n",
      "Epoch [592/10000] Train Loss: 0.019687 Val Loss: nan\n",
      "Epoch [593/10000] Train Loss: 0.019516 Val Loss: nan\n",
      "Epoch [594/10000] Train Loss: 0.020044 Val Loss: nan\n",
      "Epoch [595/10000] Train Loss: 0.020101 Val Loss: nan\n",
      "Epoch [596/10000] Train Loss: 0.019705 Val Loss: nan\n",
      "Epoch [597/10000] Train Loss: 0.019441 Val Loss: nan\n",
      "Epoch [598/10000] Train Loss: 0.019836 Val Loss: nan\n",
      "Epoch [599/10000] Train Loss: 0.019839 Val Loss: nan\n",
      "Epoch [600/10000] Train Loss: 0.019614 Val Loss: nan\n",
      "Epoch [601/10000] Train Loss: 0.019787 Val Loss: nan\n",
      "Epoch [602/10000] Train Loss: 0.019749 Val Loss: nan\n",
      "Epoch [603/10000] Train Loss: 0.019801 Val Loss: nan\n",
      "Epoch [604/10000] Train Loss: 0.019829 Val Loss: nan\n",
      "Epoch [605/10000] Train Loss: 0.019720 Val Loss: nan\n",
      "Epoch [606/10000] Train Loss: 0.020064 Val Loss: nan\n",
      "Epoch [607/10000] Train Loss: 0.019436 Val Loss: nan\n",
      "Epoch [608/10000] Train Loss: 0.019475 Val Loss: nan\n",
      "Epoch [609/10000] Train Loss: 0.019716 Val Loss: nan\n",
      "Epoch [610/10000] Train Loss: 0.019917 Val Loss: nan\n",
      "Epoch [611/10000] Train Loss: 0.019349 Val Loss: nan\n",
      "Epoch [612/10000] Train Loss: 0.019535 Val Loss: nan\n",
      "Epoch [613/10000] Train Loss: 0.019603 Val Loss: nan\n",
      "Epoch [614/10000] Train Loss: 0.019678 Val Loss: nan\n",
      "Epoch [615/10000] Train Loss: 0.019690 Val Loss: nan\n",
      "Epoch [616/10000] Train Loss: 0.019520 Val Loss: nan\n",
      "Epoch [617/10000] Train Loss: 0.019624 Val Loss: nan\n",
      "Epoch [618/10000] Train Loss: 0.019565 Val Loss: nan\n",
      "Epoch [619/10000] Train Loss: 0.019813 Val Loss: nan\n",
      "Epoch [620/10000] Train Loss: 0.019578 Val Loss: nan\n",
      "Epoch [621/10000] Train Loss: 0.019661 Val Loss: nan\n",
      "Epoch [622/10000] Train Loss: 0.019308 Val Loss: nan\n",
      "Epoch [623/10000] Train Loss: 0.019476 Val Loss: nan\n",
      "Epoch [624/10000] Train Loss: 0.019223 Val Loss: nan\n",
      "Epoch [625/10000] Train Loss: 0.019201 Val Loss: nan\n",
      "Epoch [626/10000] Train Loss: 0.019691 Val Loss: nan\n",
      "Epoch [627/10000] Train Loss: 0.019817 Val Loss: nan\n",
      "Epoch [628/10000] Train Loss: 0.019400 Val Loss: nan\n",
      "Epoch [629/10000] Train Loss: 0.019915 Val Loss: nan\n",
      "Epoch [630/10000] Train Loss: 0.019252 Val Loss: nan\n",
      "Epoch [631/10000] Train Loss: 0.019684 Val Loss: nan\n",
      "Epoch [632/10000] Train Loss: 0.019637 Val Loss: nan\n",
      "Epoch [633/10000] Train Loss: 0.019572 Val Loss: nan\n",
      "Epoch [634/10000] Train Loss: 0.019842 Val Loss: nan\n",
      "Epoch [635/10000] Train Loss: 0.019435 Val Loss: nan\n",
      "Epoch [636/10000] Train Loss: 0.019591 Val Loss: nan\n",
      "Epoch [637/10000] Train Loss: 0.019256 Val Loss: nan\n",
      "Epoch [638/10000] Train Loss: 0.019448 Val Loss: nan\n",
      "Epoch [639/10000] Train Loss: 0.019275 Val Loss: nan\n",
      "Epoch [640/10000] Train Loss: 0.019395 Val Loss: nan\n",
      "Epoch [641/10000] Train Loss: 0.019251 Val Loss: nan\n",
      "Epoch [642/10000] Train Loss: 0.019689 Val Loss: nan\n",
      "Epoch [643/10000] Train Loss: 0.019294 Val Loss: nan\n",
      "Epoch [644/10000] Train Loss: 0.019254 Val Loss: nan\n",
      "Epoch [645/10000] Train Loss: 0.019252 Val Loss: nan\n",
      "Epoch [646/10000] Train Loss: 0.019094 Val Loss: nan\n",
      "Epoch [647/10000] Train Loss: 0.019340 Val Loss: nan\n",
      "Epoch [648/10000] Train Loss: 0.019272 Val Loss: nan\n",
      "Epoch [649/10000] Train Loss: 0.019310 Val Loss: nan\n",
      "Epoch [650/10000] Train Loss: 0.019164 Val Loss: nan\n",
      "Epoch [651/10000] Train Loss: 0.019362 Val Loss: nan\n",
      "Epoch [652/10000] Train Loss: 0.019192 Val Loss: nan\n",
      "Epoch [653/10000] Train Loss: 0.019549 Val Loss: nan\n",
      "Epoch [654/10000] Train Loss: 0.019049 Val Loss: nan\n",
      "Epoch [655/10000] Train Loss: 0.019083 Val Loss: nan\n",
      "Epoch [656/10000] Train Loss: 0.019262 Val Loss: nan\n",
      "Epoch [657/10000] Train Loss: 0.019311 Val Loss: nan\n",
      "Epoch [658/10000] Train Loss: 0.019115 Val Loss: nan\n",
      "Epoch [659/10000] Train Loss: 0.019445 Val Loss: nan\n",
      "Epoch [660/10000] Train Loss: 0.019193 Val Loss: nan\n",
      "Epoch [661/10000] Train Loss: 0.019292 Val Loss: nan\n",
      "Epoch [662/10000] Train Loss: 0.019260 Val Loss: nan\n",
      "Epoch [663/10000] Train Loss: 0.018995 Val Loss: nan\n",
      "Epoch [664/10000] Train Loss: 0.018863 Val Loss: nan\n",
      "Epoch [665/10000] Train Loss: 0.018889 Val Loss: nan\n",
      "Epoch [666/10000] Train Loss: 0.019218 Val Loss: nan\n",
      "Epoch [667/10000] Train Loss: 0.019295 Val Loss: nan\n",
      "Epoch [668/10000] Train Loss: 0.019128 Val Loss: nan\n",
      "Epoch [669/10000] Train Loss: 0.019428 Val Loss: nan\n",
      "Epoch [670/10000] Train Loss: 0.018969 Val Loss: nan\n",
      "Epoch [671/10000] Train Loss: 0.018968 Val Loss: nan\n",
      "Epoch [672/10000] Train Loss: 0.019165 Val Loss: nan\n",
      "Epoch [673/10000] Train Loss: 0.019007 Val Loss: nan\n",
      "Epoch [674/10000] Train Loss: 0.019385 Val Loss: nan\n",
      "Epoch [675/10000] Train Loss: 0.019253 Val Loss: nan\n",
      "Epoch [676/10000] Train Loss: 0.018893 Val Loss: nan\n",
      "Epoch [677/10000] Train Loss: 0.019062 Val Loss: nan\n",
      "Epoch [678/10000] Train Loss: 0.018921 Val Loss: nan\n",
      "Epoch [679/10000] Train Loss: 0.019079 Val Loss: nan\n",
      "Epoch [680/10000] Train Loss: 0.018956 Val Loss: nan\n",
      "Epoch [681/10000] Train Loss: 0.019396 Val Loss: nan\n",
      "Epoch [682/10000] Train Loss: 0.018938 Val Loss: nan\n",
      "Epoch [683/10000] Train Loss: 0.019006 Val Loss: nan\n",
      "Epoch [684/10000] Train Loss: 0.019053 Val Loss: nan\n",
      "Epoch [685/10000] Train Loss: 0.019019 Val Loss: nan\n",
      "Epoch [686/10000] Train Loss: 0.019150 Val Loss: nan\n",
      "Epoch [687/10000] Train Loss: 0.018853 Val Loss: nan\n",
      "Epoch [688/10000] Train Loss: 0.019122 Val Loss: nan\n",
      "Epoch [689/10000] Train Loss: 0.018817 Val Loss: nan\n",
      "Epoch [690/10000] Train Loss: 0.018794 Val Loss: nan\n",
      "Epoch [691/10000] Train Loss: 0.018704 Val Loss: nan\n",
      "Epoch [692/10000] Train Loss: 0.018625 Val Loss: nan\n",
      "Epoch [693/10000] Train Loss: 0.018759 Val Loss: nan\n",
      "Epoch [694/10000] Train Loss: 0.018851 Val Loss: nan\n",
      "Epoch [695/10000] Train Loss: 0.019030 Val Loss: nan\n",
      "Epoch [696/10000] Train Loss: 0.018930 Val Loss: nan\n",
      "Epoch [697/10000] Train Loss: 0.018580 Val Loss: nan\n",
      "Epoch [698/10000] Train Loss: 0.018805 Val Loss: nan\n",
      "Epoch [699/10000] Train Loss: 0.018576 Val Loss: nan\n",
      "Epoch [700/10000] Train Loss: 0.019023 Val Loss: nan\n",
      "Epoch [701/10000] Train Loss: 0.018868 Val Loss: nan\n",
      "Epoch [702/10000] Train Loss: 0.018895 Val Loss: nan\n",
      "Epoch [703/10000] Train Loss: 0.018852 Val Loss: nan\n",
      "Epoch [704/10000] Train Loss: 0.018759 Val Loss: nan\n",
      "Epoch [705/10000] Train Loss: 0.018651 Val Loss: nan\n",
      "Epoch [706/10000] Train Loss: 0.018617 Val Loss: nan\n",
      "Epoch [707/10000] Train Loss: 0.018519 Val Loss: nan\n",
      "Epoch [708/10000] Train Loss: 0.018491 Val Loss: nan\n",
      "Epoch [709/10000] Train Loss: 0.018859 Val Loss: nan\n",
      "Epoch [710/10000] Train Loss: 0.018805 Val Loss: nan\n",
      "Epoch [711/10000] Train Loss: 0.018794 Val Loss: nan\n",
      "Epoch [712/10000] Train Loss: 0.019022 Val Loss: nan\n",
      "Epoch [713/10000] Train Loss: 0.018466 Val Loss: nan\n",
      "Epoch [714/10000] Train Loss: 0.018558 Val Loss: nan\n",
      "Epoch [715/10000] Train Loss: 0.018535 Val Loss: nan\n",
      "Epoch [716/10000] Train Loss: 0.018838 Val Loss: nan\n",
      "Epoch [717/10000] Train Loss: 0.018663 Val Loss: nan\n",
      "Epoch [718/10000] Train Loss: 0.019016 Val Loss: nan\n",
      "Epoch [719/10000] Train Loss: 0.018619 Val Loss: nan\n",
      "Epoch [720/10000] Train Loss: 0.018552 Val Loss: nan\n",
      "Epoch [721/10000] Train Loss: 0.018455 Val Loss: nan\n",
      "Epoch [722/10000] Train Loss: 0.018856 Val Loss: nan\n",
      "Epoch [723/10000] Train Loss: 0.018786 Val Loss: nan\n",
      "Epoch [724/10000] Train Loss: 0.018592 Val Loss: nan\n",
      "Epoch [725/10000] Train Loss: 0.018582 Val Loss: nan\n",
      "Epoch [726/10000] Train Loss: 0.018522 Val Loss: nan\n",
      "Epoch [727/10000] Train Loss: 0.018703 Val Loss: nan\n",
      "Epoch [728/10000] Train Loss: 0.018769 Val Loss: nan\n",
      "Epoch [729/10000] Train Loss: 0.018712 Val Loss: nan\n",
      "Epoch [730/10000] Train Loss: 0.019112 Val Loss: nan\n",
      "Epoch [731/10000] Train Loss: 0.018479 Val Loss: nan\n",
      "Epoch [732/10000] Train Loss: 0.018397 Val Loss: nan\n",
      "Epoch [733/10000] Train Loss: 0.018541 Val Loss: nan\n",
      "Epoch [734/10000] Train Loss: 0.018862 Val Loss: nan\n",
      "Epoch [735/10000] Train Loss: 0.018723 Val Loss: nan\n",
      "Epoch [736/10000] Train Loss: 0.018778 Val Loss: nan\n",
      "Epoch [737/10000] Train Loss: 0.018662 Val Loss: nan\n",
      "Epoch [738/10000] Train Loss: 0.018802 Val Loss: nan\n",
      "Epoch [739/10000] Train Loss: 0.018529 Val Loss: nan\n",
      "Epoch [740/10000] Train Loss: 0.018483 Val Loss: nan\n",
      "Epoch [741/10000] Train Loss: 0.018320 Val Loss: nan\n",
      "Epoch [742/10000] Train Loss: 0.018352 Val Loss: nan\n",
      "Epoch [743/10000] Train Loss: 0.018244 Val Loss: nan\n",
      "Epoch [744/10000] Train Loss: 0.018399 Val Loss: nan\n",
      "Epoch [745/10000] Train Loss: 0.018539 Val Loss: nan\n",
      "Epoch [746/10000] Train Loss: 0.018390 Val Loss: nan\n",
      "Epoch [747/10000] Train Loss: 0.018338 Val Loss: nan\n",
      "Epoch [748/10000] Train Loss: 0.018336 Val Loss: nan\n",
      "Epoch [749/10000] Train Loss: 0.018621 Val Loss: nan\n",
      "Epoch [750/10000] Train Loss: 0.018431 Val Loss: nan\n",
      "Epoch [751/10000] Train Loss: 0.018398 Val Loss: nan\n",
      "Epoch [752/10000] Train Loss: 0.018078 Val Loss: nan\n",
      "Epoch [753/10000] Train Loss: 0.018182 Val Loss: nan\n",
      "Epoch [754/10000] Train Loss: 0.018226 Val Loss: nan\n",
      "Epoch [755/10000] Train Loss: 0.018402 Val Loss: nan\n",
      "Epoch [756/10000] Train Loss: 0.018182 Val Loss: nan\n",
      "Epoch [757/10000] Train Loss: 0.018371 Val Loss: nan\n",
      "Epoch [758/10000] Train Loss: 0.018537 Val Loss: nan\n",
      "Epoch [759/10000] Train Loss: 0.018241 Val Loss: nan\n",
      "Epoch [760/10000] Train Loss: 0.018420 Val Loss: nan\n",
      "Epoch [761/10000] Train Loss: 0.018400 Val Loss: nan\n",
      "Epoch [762/10000] Train Loss: 0.018437 Val Loss: nan\n",
      "Epoch [763/10000] Train Loss: 0.018338 Val Loss: nan\n",
      "Epoch [764/10000] Train Loss: 0.018283 Val Loss: nan\n",
      "Epoch [765/10000] Train Loss: 0.018427 Val Loss: nan\n",
      "Epoch [766/10000] Train Loss: 0.018099 Val Loss: nan\n",
      "Epoch [767/10000] Train Loss: 0.018105 Val Loss: nan\n",
      "Epoch [768/10000] Train Loss: 0.018136 Val Loss: nan\n",
      "Epoch [769/10000] Train Loss: 0.018263 Val Loss: nan\n",
      "Epoch [770/10000] Train Loss: 0.018077 Val Loss: nan\n",
      "Epoch [771/10000] Train Loss: 0.018001 Val Loss: nan\n",
      "Epoch [772/10000] Train Loss: 0.018425 Val Loss: nan\n",
      "Epoch [773/10000] Train Loss: 0.018442 Val Loss: nan\n",
      "Epoch [774/10000] Train Loss: 0.018037 Val Loss: nan\n",
      "Epoch [775/10000] Train Loss: 0.018007 Val Loss: nan\n",
      "Epoch [776/10000] Train Loss: 0.018486 Val Loss: nan\n",
      "Epoch [777/10000] Train Loss: 0.018426 Val Loss: nan\n",
      "Epoch [778/10000] Train Loss: 0.018150 Val Loss: nan\n",
      "Epoch [779/10000] Train Loss: 0.017855 Val Loss: nan\n",
      "Epoch [780/10000] Train Loss: 0.018570 Val Loss: nan\n",
      "Epoch [781/10000] Train Loss: 0.018543 Val Loss: nan\n",
      "Epoch [782/10000] Train Loss: 0.017815 Val Loss: nan\n",
      "Epoch [783/10000] Train Loss: 0.017989 Val Loss: nan\n",
      "Epoch [784/10000] Train Loss: 0.017832 Val Loss: nan\n",
      "Epoch [785/10000] Train Loss: 0.018044 Val Loss: nan\n",
      "Epoch [786/10000] Train Loss: 0.018183 Val Loss: nan\n",
      "Epoch [787/10000] Train Loss: 0.018338 Val Loss: nan\n",
      "Epoch [788/10000] Train Loss: 0.017894 Val Loss: nan\n",
      "Epoch [789/10000] Train Loss: 0.017955 Val Loss: nan\n",
      "Epoch [790/10000] Train Loss: 0.017986 Val Loss: nan\n",
      "Epoch [791/10000] Train Loss: 0.018006 Val Loss: nan\n",
      "Epoch [792/10000] Train Loss: 0.017993 Val Loss: nan\n",
      "Epoch [793/10000] Train Loss: 0.018289 Val Loss: nan\n",
      "Epoch [794/10000] Train Loss: 0.017844 Val Loss: nan\n",
      "Epoch [795/10000] Train Loss: 0.017976 Val Loss: nan\n",
      "Epoch [796/10000] Train Loss: 0.018213 Val Loss: nan\n",
      "Epoch [797/10000] Train Loss: 0.017832 Val Loss: nan\n",
      "Epoch [798/10000] Train Loss: 0.017796 Val Loss: nan\n",
      "Epoch [799/10000] Train Loss: 0.018058 Val Loss: nan\n",
      "Epoch [800/10000] Train Loss: 0.017839 Val Loss: nan\n",
      "Epoch [801/10000] Train Loss: 0.017693 Val Loss: nan\n",
      "Epoch [802/10000] Train Loss: 0.018404 Val Loss: nan\n",
      "Epoch [803/10000] Train Loss: 0.018396 Val Loss: nan\n",
      "Epoch [804/10000] Train Loss: 0.017857 Val Loss: nan\n",
      "Epoch [805/10000] Train Loss: 0.018022 Val Loss: nan\n",
      "Epoch [806/10000] Train Loss: 0.017816 Val Loss: nan\n",
      "Epoch [807/10000] Train Loss: 0.017878 Val Loss: nan\n",
      "Epoch [808/10000] Train Loss: 0.018089 Val Loss: nan\n",
      "Epoch [809/10000] Train Loss: 0.017810 Val Loss: nan\n",
      "Epoch [810/10000] Train Loss: 0.017917 Val Loss: nan\n",
      "Epoch [811/10000] Train Loss: 0.017529 Val Loss: nan\n",
      "Epoch [812/10000] Train Loss: 0.017918 Val Loss: nan\n",
      "Epoch [813/10000] Train Loss: 0.017722 Val Loss: nan\n",
      "Epoch [814/10000] Train Loss: 0.017627 Val Loss: nan\n",
      "Epoch [815/10000] Train Loss: 0.018002 Val Loss: nan\n",
      "Epoch [816/10000] Train Loss: 0.017690 Val Loss: nan\n",
      "Epoch [817/10000] Train Loss: 0.017669 Val Loss: nan\n",
      "Epoch [818/10000] Train Loss: 0.017979 Val Loss: nan\n",
      "Epoch [819/10000] Train Loss: 0.017784 Val Loss: nan\n",
      "Epoch [820/10000] Train Loss: 0.018030 Val Loss: nan\n",
      "Epoch [821/10000] Train Loss: 0.017714 Val Loss: nan\n",
      "Epoch [822/10000] Train Loss: 0.018148 Val Loss: nan\n",
      "Epoch [823/10000] Train Loss: 0.017633 Val Loss: nan\n",
      "Epoch [824/10000] Train Loss: 0.017783 Val Loss: nan\n",
      "Epoch [825/10000] Train Loss: 0.018262 Val Loss: nan\n",
      "Epoch [826/10000] Train Loss: 0.018249 Val Loss: nan\n",
      "Epoch [827/10000] Train Loss: 0.018035 Val Loss: nan\n",
      "Epoch [828/10000] Train Loss: 0.017640 Val Loss: nan\n",
      "Epoch [829/10000] Train Loss: 0.017707 Val Loss: nan\n",
      "Epoch [830/10000] Train Loss: 0.017584 Val Loss: nan\n",
      "Epoch [831/10000] Train Loss: 0.017791 Val Loss: nan\n",
      "Epoch [832/10000] Train Loss: 0.017817 Val Loss: nan\n",
      "Epoch [833/10000] Train Loss: 0.017753 Val Loss: nan\n",
      "Epoch [834/10000] Train Loss: 0.017463 Val Loss: nan\n",
      "Epoch [835/10000] Train Loss: 0.017438 Val Loss: nan\n",
      "Epoch [836/10000] Train Loss: 0.017722 Val Loss: nan\n",
      "Epoch [837/10000] Train Loss: 0.017439 Val Loss: nan\n",
      "Epoch [838/10000] Train Loss: 0.017729 Val Loss: nan\n",
      "Epoch [839/10000] Train Loss: 0.017345 Val Loss: nan\n",
      "Epoch [840/10000] Train Loss: 0.017690 Val Loss: nan\n",
      "Epoch [841/10000] Train Loss: 0.017769 Val Loss: nan\n",
      "Epoch [842/10000] Train Loss: 0.017748 Val Loss: nan\n",
      "Epoch [843/10000] Train Loss: 0.017553 Val Loss: nan\n",
      "Epoch [844/10000] Train Loss: 0.017498 Val Loss: nan\n",
      "Epoch [845/10000] Train Loss: 0.017468 Val Loss: nan\n",
      "Epoch [846/10000] Train Loss: 0.017363 Val Loss: nan\n",
      "Epoch [847/10000] Train Loss: 0.017446 Val Loss: nan\n",
      "Epoch [848/10000] Train Loss: 0.017781 Val Loss: nan\n",
      "Epoch [849/10000] Train Loss: 0.017265 Val Loss: nan\n",
      "Epoch [850/10000] Train Loss: 0.017560 Val Loss: nan\n",
      "Epoch [851/10000] Train Loss: 0.017256 Val Loss: nan\n",
      "Epoch [852/10000] Train Loss: 0.017377 Val Loss: nan\n",
      "Epoch [853/10000] Train Loss: 0.017551 Val Loss: nan\n",
      "Epoch [854/10000] Train Loss: 0.017221 Val Loss: nan\n",
      "Epoch [855/10000] Train Loss: 0.017611 Val Loss: nan\n",
      "Epoch [856/10000] Train Loss: 0.017487 Val Loss: nan\n",
      "Epoch [857/10000] Train Loss: 0.017494 Val Loss: nan\n",
      "Epoch [858/10000] Train Loss: 0.017396 Val Loss: nan\n",
      "Epoch [859/10000] Train Loss: 0.017326 Val Loss: nan\n",
      "Epoch [860/10000] Train Loss: 0.017959 Val Loss: nan\n",
      "Epoch [861/10000] Train Loss: 0.017316 Val Loss: nan\n",
      "Epoch [862/10000] Train Loss: 0.017626 Val Loss: nan\n",
      "Epoch [863/10000] Train Loss: 0.017140 Val Loss: nan\n",
      "Epoch [864/10000] Train Loss: 0.017109 Val Loss: nan\n",
      "Epoch [865/10000] Train Loss: 0.017351 Val Loss: nan\n",
      "Epoch [866/10000] Train Loss: 0.017331 Val Loss: nan\n",
      "Epoch [867/10000] Train Loss: 0.017185 Val Loss: nan\n",
      "Epoch [868/10000] Train Loss: 0.017530 Val Loss: nan\n",
      "Epoch [869/10000] Train Loss: 0.017368 Val Loss: nan\n",
      "Epoch [870/10000] Train Loss: 0.017803 Val Loss: nan\n",
      "Epoch [871/10000] Train Loss: 0.017276 Val Loss: nan\n",
      "Epoch [872/10000] Train Loss: 0.018010 Val Loss: nan\n",
      "Epoch [873/10000] Train Loss: 0.017350 Val Loss: nan\n",
      "Epoch [874/10000] Train Loss: 0.017225 Val Loss: nan\n",
      "Epoch [875/10000] Train Loss: 0.017056 Val Loss: nan\n",
      "Epoch [876/10000] Train Loss: 0.017544 Val Loss: nan\n",
      "Epoch [877/10000] Train Loss: 0.017508 Val Loss: nan\n",
      "Epoch [878/10000] Train Loss: 0.017066 Val Loss: nan\n",
      "Epoch [879/10000] Train Loss: 0.017251 Val Loss: nan\n",
      "Epoch [880/10000] Train Loss: 0.017194 Val Loss: nan\n",
      "Epoch [881/10000] Train Loss: 0.016985 Val Loss: nan\n",
      "Epoch [882/10000] Train Loss: 0.017227 Val Loss: nan\n",
      "Epoch [883/10000] Train Loss: 0.017187 Val Loss: nan\n",
      "Epoch [884/10000] Train Loss: 0.017397 Val Loss: nan\n",
      "Epoch [885/10000] Train Loss: 0.016902 Val Loss: nan\n",
      "Epoch [886/10000] Train Loss: 0.016919 Val Loss: nan\n",
      "Epoch [887/10000] Train Loss: 0.017205 Val Loss: nan\n",
      "Epoch [888/10000] Train Loss: 0.017116 Val Loss: nan\n",
      "Epoch [889/10000] Train Loss: 0.017146 Val Loss: nan\n",
      "Epoch [890/10000] Train Loss: 0.018133 Val Loss: nan\n",
      "Epoch [891/10000] Train Loss: 0.017180 Val Loss: nan\n",
      "Epoch [892/10000] Train Loss: 0.017278 Val Loss: nan\n",
      "Epoch [893/10000] Train Loss: 0.017301 Val Loss: nan\n",
      "Epoch [894/10000] Train Loss: 0.017403 Val Loss: nan\n",
      "Epoch [895/10000] Train Loss: 0.017274 Val Loss: nan\n",
      "Epoch [896/10000] Train Loss: 0.017241 Val Loss: nan\n",
      "Epoch [897/10000] Train Loss: 0.016904 Val Loss: nan\n",
      "Epoch [898/10000] Train Loss: 0.017135 Val Loss: nan\n",
      "Epoch [899/10000] Train Loss: 0.016916 Val Loss: nan\n",
      "Epoch [900/10000] Train Loss: 0.016860 Val Loss: nan\n",
      "Epoch [901/10000] Train Loss: 0.016710 Val Loss: nan\n",
      "Epoch [902/10000] Train Loss: 0.017031 Val Loss: nan\n",
      "Epoch [903/10000] Train Loss: 0.016887 Val Loss: nan\n",
      "Epoch [904/10000] Train Loss: 0.017097 Val Loss: nan\n",
      "Epoch [905/10000] Train Loss: 0.017223 Val Loss: nan\n",
      "Epoch [906/10000] Train Loss: 0.016885 Val Loss: nan\n",
      "Epoch [907/10000] Train Loss: 0.016976 Val Loss: nan\n",
      "Epoch [908/10000] Train Loss: 0.017083 Val Loss: nan\n",
      "Epoch [909/10000] Train Loss: 0.016950 Val Loss: nan\n",
      "Epoch [910/10000] Train Loss: 0.016723 Val Loss: nan\n",
      "Epoch [911/10000] Train Loss: 0.017108 Val Loss: nan\n",
      "Epoch [912/10000] Train Loss: 0.017154 Val Loss: nan\n",
      "Epoch [913/10000] Train Loss: 0.017528 Val Loss: nan\n",
      "Epoch [914/10000] Train Loss: 0.016883 Val Loss: nan\n",
      "Epoch [915/10000] Train Loss: 0.016943 Val Loss: nan\n",
      "Epoch [916/10000] Train Loss: 0.017132 Val Loss: nan\n",
      "Epoch [917/10000] Train Loss: 0.016711 Val Loss: nan\n",
      "Epoch [918/10000] Train Loss: 0.017131 Val Loss: nan\n",
      "Epoch [919/10000] Train Loss: 0.016838 Val Loss: nan\n",
      "Epoch [920/10000] Train Loss: 0.016670 Val Loss: nan\n",
      "Epoch [921/10000] Train Loss: 0.016869 Val Loss: nan\n",
      "Epoch [922/10000] Train Loss: 0.016746 Val Loss: nan\n",
      "Epoch [923/10000] Train Loss: 0.016804 Val Loss: nan\n",
      "Epoch [924/10000] Train Loss: 0.016826 Val Loss: nan\n",
      "Epoch [925/10000] Train Loss: 0.016855 Val Loss: nan\n",
      "Epoch [926/10000] Train Loss: 0.016552 Val Loss: nan\n",
      "Epoch [927/10000] Train Loss: 0.016626 Val Loss: nan\n",
      "Epoch [928/10000] Train Loss: 0.016590 Val Loss: nan\n",
      "Epoch [929/10000] Train Loss: 0.016823 Val Loss: nan\n",
      "Epoch [930/10000] Train Loss: 0.017091 Val Loss: nan\n",
      "Epoch [931/10000] Train Loss: 0.016785 Val Loss: nan\n",
      "Epoch [932/10000] Train Loss: 0.016813 Val Loss: nan\n",
      "Epoch [933/10000] Train Loss: 0.016742 Val Loss: nan\n",
      "Epoch [934/10000] Train Loss: 0.016692 Val Loss: nan\n",
      "Epoch [935/10000] Train Loss: 0.016495 Val Loss: nan\n",
      "Epoch [936/10000] Train Loss: 0.016539 Val Loss: nan\n",
      "Epoch [937/10000] Train Loss: 0.016445 Val Loss: nan\n",
      "Epoch [938/10000] Train Loss: 0.016543 Val Loss: nan\n",
      "Epoch [939/10000] Train Loss: 0.016468 Val Loss: nan\n",
      "Epoch [940/10000] Train Loss: 0.016759 Val Loss: nan\n",
      "Epoch [941/10000] Train Loss: 0.016691 Val Loss: nan\n",
      "Epoch [942/10000] Train Loss: 0.017003 Val Loss: nan\n",
      "Epoch [943/10000] Train Loss: 0.016610 Val Loss: nan\n",
      "Epoch [944/10000] Train Loss: 0.016604 Val Loss: nan\n",
      "Epoch [945/10000] Train Loss: 0.016597 Val Loss: nan\n",
      "Epoch [946/10000] Train Loss: 0.016822 Val Loss: nan\n",
      "Epoch [947/10000] Train Loss: 0.016748 Val Loss: nan\n",
      "Epoch [948/10000] Train Loss: 0.016767 Val Loss: nan\n",
      "Epoch [949/10000] Train Loss: 0.016538 Val Loss: nan\n",
      "Epoch [950/10000] Train Loss: 0.016394 Val Loss: nan\n",
      "Epoch [951/10000] Train Loss: 0.016830 Val Loss: nan\n",
      "Epoch [952/10000] Train Loss: 0.016479 Val Loss: nan\n",
      "Epoch [953/10000] Train Loss: 0.016616 Val Loss: nan\n",
      "Epoch [954/10000] Train Loss: 0.016579 Val Loss: nan\n",
      "Epoch [955/10000] Train Loss: 0.016354 Val Loss: nan\n",
      "Epoch [956/10000] Train Loss: 0.016277 Val Loss: nan\n",
      "Epoch [957/10000] Train Loss: 0.016437 Val Loss: nan\n",
      "Epoch [958/10000] Train Loss: 0.016480 Val Loss: nan\n",
      "Epoch [959/10000] Train Loss: 0.016565 Val Loss: nan\n",
      "Epoch [960/10000] Train Loss: 0.016604 Val Loss: nan\n",
      "Epoch [961/10000] Train Loss: 0.016653 Val Loss: nan\n",
      "Epoch [962/10000] Train Loss: 0.016346 Val Loss: nan\n",
      "Epoch [963/10000] Train Loss: 0.016642 Val Loss: nan\n",
      "Epoch [964/10000] Train Loss: 0.016403 Val Loss: nan\n",
      "Epoch [965/10000] Train Loss: 0.016847 Val Loss: nan\n",
      "Epoch [966/10000] Train Loss: 0.016696 Val Loss: nan\n",
      "Epoch [967/10000] Train Loss: 0.016886 Val Loss: nan\n",
      "Epoch [968/10000] Train Loss: 0.016415 Val Loss: nan\n",
      "Epoch [969/10000] Train Loss: 0.016537 Val Loss: nan\n",
      "Epoch [970/10000] Train Loss: 0.016819 Val Loss: nan\n",
      "Epoch [971/10000] Train Loss: 0.016189 Val Loss: nan\n",
      "Epoch [972/10000] Train Loss: 0.016461 Val Loss: nan\n",
      "Epoch [973/10000] Train Loss: 0.016249 Val Loss: nan\n",
      "Epoch [974/10000] Train Loss: 0.016350 Val Loss: nan\n",
      "Epoch [975/10000] Train Loss: 0.016342 Val Loss: nan\n",
      "Epoch [976/10000] Train Loss: 0.016780 Val Loss: nan\n",
      "Epoch [977/10000] Train Loss: 0.016396 Val Loss: nan\n",
      "Epoch [978/10000] Train Loss: 0.016201 Val Loss: nan\n",
      "Epoch [979/10000] Train Loss: 0.016602 Val Loss: nan\n",
      "Epoch [980/10000] Train Loss: 0.016379 Val Loss: nan\n",
      "Epoch [981/10000] Train Loss: 0.016025 Val Loss: nan\n",
      "Epoch [982/10000] Train Loss: 0.016038 Val Loss: nan\n",
      "Epoch [983/10000] Train Loss: 0.016396 Val Loss: nan\n",
      "Epoch [984/10000] Train Loss: 0.016276 Val Loss: nan\n",
      "Epoch [985/10000] Train Loss: 0.016560 Val Loss: nan\n",
      "Epoch [986/10000] Train Loss: 0.016094 Val Loss: nan\n",
      "Epoch [987/10000] Train Loss: 0.016060 Val Loss: nan\n",
      "Epoch [988/10000] Train Loss: 0.016227 Val Loss: nan\n",
      "Epoch [989/10000] Train Loss: 0.016189 Val Loss: nan\n",
      "Epoch [990/10000] Train Loss: 0.016112 Val Loss: nan\n",
      "Epoch [991/10000] Train Loss: 0.016231 Val Loss: nan\n",
      "Epoch [992/10000] Train Loss: 0.016130 Val Loss: nan\n",
      "Epoch [993/10000] Train Loss: 0.016152 Val Loss: nan\n",
      "Epoch [994/10000] Train Loss: 0.016120 Val Loss: nan\n",
      "Epoch [995/10000] Train Loss: 0.016127 Val Loss: nan\n",
      "Epoch [996/10000] Train Loss: 0.015936 Val Loss: nan\n",
      "Epoch [997/10000] Train Loss: 0.016183 Val Loss: nan\n",
      "Epoch [998/10000] Train Loss: 0.015959 Val Loss: nan\n",
      "Epoch [999/10000] Train Loss: 0.016296 Val Loss: nan\n",
      "Epoch [1000/10000] Train Loss: 0.016419 Val Loss: nan\n",
      "Epoch [1001/10000] Train Loss: 0.016106 Val Loss: nan\n",
      "Epoch [1002/10000] Train Loss: 0.015984 Val Loss: nan\n",
      "Epoch [1003/10000] Train Loss: 0.015838 Val Loss: nan\n",
      "Epoch [1004/10000] Train Loss: 0.016320 Val Loss: nan\n",
      "Epoch [1005/10000] Train Loss: 0.016364 Val Loss: nan\n",
      "Epoch [1006/10000] Train Loss: 0.016135 Val Loss: nan\n",
      "Epoch [1007/10000] Train Loss: 0.016016 Val Loss: nan\n",
      "Epoch [1008/10000] Train Loss: 0.016077 Val Loss: nan\n",
      "Epoch [1009/10000] Train Loss: 0.015886 Val Loss: nan\n",
      "Epoch [1010/10000] Train Loss: 0.015988 Val Loss: nan\n",
      "Epoch [1011/10000] Train Loss: 0.015810 Val Loss: nan\n",
      "Epoch [1012/10000] Train Loss: 0.015855 Val Loss: nan\n",
      "Epoch [1013/10000] Train Loss: 0.015986 Val Loss: nan\n",
      "Epoch [1014/10000] Train Loss: 0.015776 Val Loss: nan\n",
      "Epoch [1015/10000] Train Loss: 0.015900 Val Loss: nan\n",
      "Epoch [1016/10000] Train Loss: 0.015800 Val Loss: nan\n",
      "Epoch [1017/10000] Train Loss: 0.015756 Val Loss: nan\n",
      "Epoch [1018/10000] Train Loss: 0.015921 Val Loss: nan\n",
      "Epoch [1019/10000] Train Loss: 0.016304 Val Loss: nan\n",
      "Epoch [1020/10000] Train Loss: 0.015845 Val Loss: nan\n",
      "Epoch [1021/10000] Train Loss: 0.015803 Val Loss: nan\n",
      "Epoch [1022/10000] Train Loss: 0.015996 Val Loss: nan\n",
      "Epoch [1023/10000] Train Loss: 0.015869 Val Loss: nan\n",
      "Epoch [1024/10000] Train Loss: 0.015890 Val Loss: nan\n",
      "Epoch [1025/10000] Train Loss: 0.016025 Val Loss: nan\n",
      "Epoch [1026/10000] Train Loss: 0.015947 Val Loss: nan\n",
      "Epoch [1027/10000] Train Loss: 0.015982 Val Loss: nan\n",
      "Epoch [1028/10000] Train Loss: 0.015967 Val Loss: nan\n",
      "Epoch [1029/10000] Train Loss: 0.015964 Val Loss: nan\n",
      "Epoch [1030/10000] Train Loss: 0.015784 Val Loss: nan\n",
      "Epoch [1031/10000] Train Loss: 0.015894 Val Loss: nan\n",
      "Epoch [1032/10000] Train Loss: 0.016339 Val Loss: nan\n",
      "Epoch [1033/10000] Train Loss: 0.015904 Val Loss: nan\n",
      "Epoch [1034/10000] Train Loss: 0.015952 Val Loss: nan\n",
      "Epoch [1035/10000] Train Loss: 0.016138 Val Loss: nan\n",
      "Epoch [1036/10000] Train Loss: 0.015626 Val Loss: nan\n",
      "Epoch [1037/10000] Train Loss: 0.015758 Val Loss: nan\n",
      "Epoch [1038/10000] Train Loss: 0.015753 Val Loss: nan\n",
      "Epoch [1039/10000] Train Loss: 0.015514 Val Loss: nan\n",
      "Epoch [1040/10000] Train Loss: 0.015883 Val Loss: nan\n",
      "Epoch [1041/10000] Train Loss: 0.015486 Val Loss: nan\n",
      "Epoch [1042/10000] Train Loss: 0.016016 Val Loss: nan\n",
      "Epoch [1043/10000] Train Loss: 0.016101 Val Loss: nan\n",
      "Epoch [1044/10000] Train Loss: 0.015559 Val Loss: nan\n",
      "Epoch [1045/10000] Train Loss: 0.015579 Val Loss: nan\n",
      "Epoch [1046/10000] Train Loss: 0.015583 Val Loss: nan\n",
      "Epoch [1047/10000] Train Loss: 0.015801 Val Loss: nan\n",
      "Epoch [1048/10000] Train Loss: 0.015887 Val Loss: nan\n",
      "Epoch [1049/10000] Train Loss: 0.015606 Val Loss: nan\n",
      "Epoch [1050/10000] Train Loss: 0.015965 Val Loss: nan\n",
      "Epoch [1051/10000] Train Loss: 0.016105 Val Loss: nan\n",
      "Epoch [1052/10000] Train Loss: 0.015714 Val Loss: nan\n",
      "Epoch [1053/10000] Train Loss: 0.015848 Val Loss: nan\n",
      "Epoch [1054/10000] Train Loss: 0.015781 Val Loss: nan\n",
      "Epoch [1055/10000] Train Loss: 0.015620 Val Loss: nan\n",
      "Epoch [1056/10000] Train Loss: 0.015976 Val Loss: nan\n",
      "Epoch [1057/10000] Train Loss: 0.015815 Val Loss: nan\n",
      "Epoch [1058/10000] Train Loss: 0.015405 Val Loss: nan\n",
      "Epoch [1059/10000] Train Loss: 0.015631 Val Loss: nan\n",
      "Epoch [1060/10000] Train Loss: 0.015585 Val Loss: nan\n",
      "Epoch [1061/10000] Train Loss: 0.015665 Val Loss: nan\n",
      "Epoch [1062/10000] Train Loss: 0.015355 Val Loss: nan\n",
      "Epoch [1063/10000] Train Loss: 0.015669 Val Loss: nan\n",
      "Epoch [1064/10000] Train Loss: 0.015428 Val Loss: nan\n",
      "Epoch [1065/10000] Train Loss: 0.015714 Val Loss: nan\n",
      "Epoch [1066/10000] Train Loss: 0.015596 Val Loss: nan\n",
      "Epoch [1067/10000] Train Loss: 0.015454 Val Loss: nan\n",
      "Epoch [1068/10000] Train Loss: 0.015292 Val Loss: nan\n",
      "Epoch [1069/10000] Train Loss: 0.015456 Val Loss: nan\n",
      "Epoch [1070/10000] Train Loss: 0.015768 Val Loss: nan\n",
      "Epoch [1071/10000] Train Loss: 0.015454 Val Loss: nan\n",
      "Epoch [1072/10000] Train Loss: 0.015583 Val Loss: nan\n",
      "Epoch [1073/10000] Train Loss: 0.015173 Val Loss: nan\n",
      "Epoch [1074/10000] Train Loss: 0.015297 Val Loss: nan\n",
      "Epoch [1075/10000] Train Loss: 0.015168 Val Loss: nan\n",
      "Epoch [1076/10000] Train Loss: 0.015586 Val Loss: nan\n",
      "Epoch [1077/10000] Train Loss: 0.015822 Val Loss: nan\n",
      "Epoch [1078/10000] Train Loss: 0.015310 Val Loss: nan\n",
      "Epoch [1079/10000] Train Loss: 0.015526 Val Loss: nan\n",
      "Epoch [1080/10000] Train Loss: 0.015480 Val Loss: nan\n",
      "Epoch [1081/10000] Train Loss: 0.015557 Val Loss: nan\n",
      "Epoch [1082/10000] Train Loss: 0.015492 Val Loss: nan\n",
      "Epoch [1083/10000] Train Loss: 0.015301 Val Loss: nan\n",
      "Epoch [1084/10000] Train Loss: 0.015575 Val Loss: nan\n",
      "Epoch [1085/10000] Train Loss: 0.015268 Val Loss: nan\n",
      "Epoch [1086/10000] Train Loss: 0.015289 Val Loss: nan\n",
      "Epoch [1087/10000] Train Loss: 0.015582 Val Loss: nan\n",
      "Epoch [1088/10000] Train Loss: 0.015484 Val Loss: nan\n",
      "Epoch [1089/10000] Train Loss: 0.015747 Val Loss: nan\n",
      "Epoch [1090/10000] Train Loss: 0.015465 Val Loss: nan\n",
      "Epoch [1091/10000] Train Loss: 0.015143 Val Loss: nan\n",
      "Epoch [1092/10000] Train Loss: 0.015315 Val Loss: nan\n",
      "Epoch [1093/10000] Train Loss: 0.015333 Val Loss: nan\n",
      "Epoch [1094/10000] Train Loss: 0.014985 Val Loss: nan\n",
      "Epoch [1095/10000] Train Loss: 0.015370 Val Loss: nan\n",
      "Epoch [1096/10000] Train Loss: 0.015173 Val Loss: nan\n",
      "Epoch [1097/10000] Train Loss: 0.015111 Val Loss: nan\n",
      "Epoch [1098/10000] Train Loss: 0.015203 Val Loss: nan\n",
      "Epoch [1099/10000] Train Loss: 0.015597 Val Loss: nan\n",
      "Epoch [1100/10000] Train Loss: 0.015264 Val Loss: nan\n",
      "Epoch [1101/10000] Train Loss: 0.015419 Val Loss: nan\n",
      "Epoch [1102/10000] Train Loss: 0.015294 Val Loss: nan\n",
      "Epoch [1103/10000] Train Loss: 0.015162 Val Loss: nan\n",
      "Epoch [1104/10000] Train Loss: 0.015245 Val Loss: nan\n",
      "Epoch [1105/10000] Train Loss: 0.015321 Val Loss: nan\n",
      "Epoch [1106/10000] Train Loss: 0.015099 Val Loss: nan\n",
      "Epoch [1107/10000] Train Loss: 0.015407 Val Loss: nan\n",
      "Epoch [1108/10000] Train Loss: 0.014964 Val Loss: nan\n",
      "Epoch [1109/10000] Train Loss: 0.015101 Val Loss: nan\n",
      "Epoch [1110/10000] Train Loss: 0.015414 Val Loss: nan\n",
      "Epoch [1111/10000] Train Loss: 0.015269 Val Loss: nan\n",
      "Epoch [1112/10000] Train Loss: 0.015009 Val Loss: nan\n",
      "Epoch [1113/10000] Train Loss: 0.014844 Val Loss: nan\n",
      "Epoch [1114/10000] Train Loss: 0.014892 Val Loss: nan\n",
      "Epoch [1115/10000] Train Loss: 0.015310 Val Loss: nan\n",
      "Epoch [1116/10000] Train Loss: 0.015394 Val Loss: nan\n",
      "Epoch [1117/10000] Train Loss: 0.014850 Val Loss: nan\n",
      "Epoch [1118/10000] Train Loss: 0.014866 Val Loss: nan\n",
      "Epoch [1119/10000] Train Loss: 0.015104 Val Loss: nan\n",
      "Epoch [1120/10000] Train Loss: 0.015020 Val Loss: nan\n",
      "Epoch [1121/10000] Train Loss: 0.015354 Val Loss: nan\n",
      "Epoch [1122/10000] Train Loss: 0.015061 Val Loss: nan\n",
      "Epoch [1123/10000] Train Loss: 0.015192 Val Loss: nan\n",
      "Epoch [1124/10000] Train Loss: 0.014843 Val Loss: nan\n",
      "Epoch [1125/10000] Train Loss: 0.015122 Val Loss: nan\n",
      "Epoch [1126/10000] Train Loss: 0.014744 Val Loss: nan\n",
      "Epoch [1127/10000] Train Loss: 0.014939 Val Loss: nan\n",
      "Epoch [1128/10000] Train Loss: 0.014796 Val Loss: nan\n",
      "Epoch [1129/10000] Train Loss: 0.014732 Val Loss: nan\n",
      "Epoch [1130/10000] Train Loss: 0.014883 Val Loss: nan\n",
      "Epoch [1131/10000] Train Loss: 0.014964 Val Loss: nan\n",
      "Epoch [1132/10000] Train Loss: 0.015141 Val Loss: nan\n",
      "Epoch [1133/10000] Train Loss: 0.015293 Val Loss: nan\n",
      "Epoch [1134/10000] Train Loss: 0.015057 Val Loss: nan\n",
      "Epoch [1135/10000] Train Loss: 0.014686 Val Loss: nan\n",
      "Epoch [1136/10000] Train Loss: 0.015295 Val Loss: nan\n",
      "Epoch [1137/10000] Train Loss: 0.015146 Val Loss: nan\n",
      "Epoch [1138/10000] Train Loss: 0.014824 Val Loss: nan\n",
      "Epoch [1139/10000] Train Loss: 0.014888 Val Loss: nan\n",
      "Epoch [1140/10000] Train Loss: 0.014733 Val Loss: nan\n",
      "Epoch [1141/10000] Train Loss: 0.014856 Val Loss: nan\n",
      "Epoch [1142/10000] Train Loss: 0.014830 Val Loss: nan\n",
      "Epoch [1143/10000] Train Loss: 0.014956 Val Loss: nan\n",
      "Epoch [1144/10000] Train Loss: 0.014766 Val Loss: nan\n",
      "Epoch [1145/10000] Train Loss: 0.014905 Val Loss: nan\n",
      "Epoch [1146/10000] Train Loss: 0.014837 Val Loss: nan\n",
      "Epoch [1147/10000] Train Loss: 0.014933 Val Loss: nan\n",
      "Epoch [1148/10000] Train Loss: 0.015051 Val Loss: nan\n",
      "Epoch [1149/10000] Train Loss: 0.015160 Val Loss: nan\n",
      "Epoch [1150/10000] Train Loss: 0.014974 Val Loss: nan\n",
      "Epoch [1151/10000] Train Loss: 0.014891 Val Loss: nan\n",
      "Epoch [1152/10000] Train Loss: 0.014615 Val Loss: nan\n",
      "Epoch [1153/10000] Train Loss: 0.014847 Val Loss: nan\n",
      "Epoch [1154/10000] Train Loss: 0.014649 Val Loss: nan\n",
      "Epoch [1155/10000] Train Loss: 0.015305 Val Loss: nan\n",
      "Epoch [1156/10000] Train Loss: 0.014757 Val Loss: nan\n",
      "Epoch [1157/10000] Train Loss: 0.014910 Val Loss: nan\n",
      "Epoch [1158/10000] Train Loss: 0.014592 Val Loss: nan\n",
      "Epoch [1159/10000] Train Loss: 0.014639 Val Loss: nan\n",
      "Epoch [1160/10000] Train Loss: 0.014551 Val Loss: nan\n",
      "Epoch [1161/10000] Train Loss: 0.014523 Val Loss: nan\n",
      "Epoch [1162/10000] Train Loss: 0.014761 Val Loss: nan\n",
      "Epoch [1163/10000] Train Loss: 0.014520 Val Loss: nan\n",
      "Epoch [1164/10000] Train Loss: 0.014884 Val Loss: nan\n",
      "Epoch [1165/10000] Train Loss: 0.014554 Val Loss: nan\n",
      "Epoch [1166/10000] Train Loss: 0.015080 Val Loss: nan\n",
      "Epoch [1167/10000] Train Loss: 0.014712 Val Loss: nan\n",
      "Epoch [1168/10000] Train Loss: 0.014691 Val Loss: nan\n",
      "Epoch [1169/10000] Train Loss: 0.014963 Val Loss: nan\n",
      "Epoch [1170/10000] Train Loss: 0.014724 Val Loss: nan\n",
      "Epoch [1171/10000] Train Loss: 0.014392 Val Loss: nan\n",
      "Epoch [1172/10000] Train Loss: 0.014515 Val Loss: nan\n",
      "Epoch [1173/10000] Train Loss: 0.014645 Val Loss: nan\n",
      "Epoch [1174/10000] Train Loss: 0.014820 Val Loss: nan\n",
      "Epoch [1175/10000] Train Loss: 0.014704 Val Loss: nan\n",
      "Epoch [1176/10000] Train Loss: 0.014420 Val Loss: nan\n",
      "Epoch [1177/10000] Train Loss: 0.014444 Val Loss: nan\n",
      "Epoch [1178/10000] Train Loss: 0.014635 Val Loss: nan\n",
      "Epoch [1179/10000] Train Loss: 0.014582 Val Loss: nan\n",
      "Epoch [1180/10000] Train Loss: 0.014951 Val Loss: nan\n",
      "Epoch [1181/10000] Train Loss: 0.015019 Val Loss: nan\n",
      "Epoch [1182/10000] Train Loss: 0.014370 Val Loss: nan\n",
      "Epoch [1183/10000] Train Loss: 0.014555 Val Loss: nan\n",
      "Epoch [1184/10000] Train Loss: 0.014465 Val Loss: nan\n",
      "Epoch [1185/10000] Train Loss: 0.014791 Val Loss: nan\n",
      "Epoch [1186/10000] Train Loss: 0.014419 Val Loss: nan\n",
      "Epoch [1187/10000] Train Loss: 0.014654 Val Loss: nan\n",
      "Epoch [1188/10000] Train Loss: 0.014450 Val Loss: nan\n",
      "Epoch [1189/10000] Train Loss: 0.014305 Val Loss: nan\n",
      "Epoch [1190/10000] Train Loss: 0.014370 Val Loss: nan\n",
      "Epoch [1191/10000] Train Loss: 0.014370 Val Loss: nan\n",
      "Epoch [1192/10000] Train Loss: 0.014605 Val Loss: nan\n",
      "Epoch [1193/10000] Train Loss: 0.014521 Val Loss: nan\n",
      "Epoch [1194/10000] Train Loss: 0.014536 Val Loss: nan\n",
      "Epoch [1195/10000] Train Loss: 0.014377 Val Loss: nan\n",
      "Epoch [1196/10000] Train Loss: 0.014696 Val Loss: nan\n",
      "Epoch [1197/10000] Train Loss: 0.014314 Val Loss: nan\n",
      "Epoch [1198/10000] Train Loss: 0.014283 Val Loss: nan\n",
      "Epoch [1199/10000] Train Loss: 0.014518 Val Loss: nan\n",
      "Epoch [1200/10000] Train Loss: 0.014213 Val Loss: nan\n",
      "Epoch [1201/10000] Train Loss: 0.014462 Val Loss: nan\n",
      "Epoch [1202/10000] Train Loss: 0.014267 Val Loss: nan\n",
      "Epoch [1203/10000] Train Loss: 0.014339 Val Loss: nan\n",
      "Epoch [1204/10000] Train Loss: 0.014439 Val Loss: nan\n",
      "Epoch [1205/10000] Train Loss: 0.014380 Val Loss: nan\n",
      "Epoch [1206/10000] Train Loss: 0.014312 Val Loss: nan\n",
      "Epoch [1207/10000] Train Loss: 0.014345 Val Loss: nan\n",
      "Epoch [1208/10000] Train Loss: 0.014218 Val Loss: nan\n",
      "Epoch [1209/10000] Train Loss: 0.014537 Val Loss: nan\n",
      "Epoch [1210/10000] Train Loss: 0.014693 Val Loss: nan\n",
      "Epoch [1211/10000] Train Loss: 0.014580 Val Loss: nan\n",
      "Epoch [1212/10000] Train Loss: 0.014383 Val Loss: nan\n",
      "Epoch [1213/10000] Train Loss: 0.014189 Val Loss: nan\n",
      "Epoch [1214/10000] Train Loss: 0.014048 Val Loss: nan\n",
      "Epoch [1215/10000] Train Loss: 0.014472 Val Loss: nan\n",
      "Epoch [1216/10000] Train Loss: 0.014479 Val Loss: nan\n",
      "Epoch [1217/10000] Train Loss: 0.014221 Val Loss: nan\n",
      "Epoch [1218/10000] Train Loss: 0.014347 Val Loss: nan\n",
      "Epoch [1219/10000] Train Loss: 0.014197 Val Loss: nan\n",
      "Epoch [1220/10000] Train Loss: 0.014284 Val Loss: nan\n",
      "Epoch [1221/10000] Train Loss: 0.014385 Val Loss: nan\n",
      "Epoch [1222/10000] Train Loss: 0.014224 Val Loss: nan\n",
      "Epoch [1223/10000] Train Loss: 0.014391 Val Loss: nan\n",
      "Epoch [1224/10000] Train Loss: 0.014243 Val Loss: nan\n",
      "Epoch [1225/10000] Train Loss: 0.014242 Val Loss: nan\n",
      "Epoch [1226/10000] Train Loss: 0.014207 Val Loss: nan\n",
      "Epoch [1227/10000] Train Loss: 0.014478 Val Loss: nan\n",
      "Epoch [1228/10000] Train Loss: 0.014194 Val Loss: nan\n",
      "Epoch [1229/10000] Train Loss: 0.014078 Val Loss: nan\n",
      "Epoch [1230/10000] Train Loss: 0.014536 Val Loss: nan\n",
      "Epoch [1231/10000] Train Loss: 0.014150 Val Loss: nan\n",
      "Epoch [1232/10000] Train Loss: 0.014071 Val Loss: nan\n",
      "Epoch [1233/10000] Train Loss: 0.014454 Val Loss: nan\n",
      "Epoch [1234/10000] Train Loss: 0.014502 Val Loss: nan\n",
      "Epoch [1235/10000] Train Loss: 0.014452 Val Loss: nan\n",
      "Epoch [1236/10000] Train Loss: 0.014050 Val Loss: nan\n",
      "Epoch [1237/10000] Train Loss: 0.014051 Val Loss: nan\n",
      "Epoch [1238/10000] Train Loss: 0.014153 Val Loss: nan\n",
      "Epoch [1239/10000] Train Loss: 0.014255 Val Loss: nan\n",
      "Epoch [1240/10000] Train Loss: 0.014267 Val Loss: nan\n",
      "Epoch [1241/10000] Train Loss: 0.014138 Val Loss: nan\n",
      "Epoch [1242/10000] Train Loss: 0.014498 Val Loss: nan\n",
      "Epoch [1243/10000] Train Loss: 0.014037 Val Loss: nan\n",
      "Epoch [1244/10000] Train Loss: 0.014148 Val Loss: nan\n",
      "Epoch [1245/10000] Train Loss: 0.014042 Val Loss: nan\n",
      "Epoch [1246/10000] Train Loss: 0.013847 Val Loss: nan\n",
      "Epoch [1247/10000] Train Loss: 0.014121 Val Loss: nan\n",
      "Epoch [1248/10000] Train Loss: 0.014179 Val Loss: nan\n",
      "Epoch [1249/10000] Train Loss: 0.013959 Val Loss: nan\n",
      "Epoch [1250/10000] Train Loss: 0.013935 Val Loss: nan\n",
      "Epoch [1251/10000] Train Loss: 0.013989 Val Loss: nan\n",
      "Epoch [1252/10000] Train Loss: 0.014138 Val Loss: nan\n",
      "Epoch [1253/10000] Train Loss: 0.013934 Val Loss: nan\n",
      "Epoch [1254/10000] Train Loss: 0.014059 Val Loss: nan\n",
      "Epoch [1255/10000] Train Loss: 0.014281 Val Loss: nan\n",
      "Epoch [1256/10000] Train Loss: 0.014241 Val Loss: nan\n",
      "Epoch [1257/10000] Train Loss: 0.013816 Val Loss: nan\n",
      "Epoch [1258/10000] Train Loss: 0.014536 Val Loss: nan\n",
      "Epoch [1259/10000] Train Loss: 0.014049 Val Loss: nan\n",
      "Epoch [1260/10000] Train Loss: 0.013956 Val Loss: nan\n",
      "Epoch [1261/10000] Train Loss: 0.014062 Val Loss: nan\n",
      "Epoch [1262/10000] Train Loss: 0.014302 Val Loss: nan\n",
      "Epoch [1263/10000] Train Loss: 0.013893 Val Loss: nan\n",
      "Epoch [1264/10000] Train Loss: 0.013779 Val Loss: nan\n",
      "Epoch [1265/10000] Train Loss: 0.014096 Val Loss: nan\n",
      "Epoch [1266/10000] Train Loss: 0.013853 Val Loss: nan\n",
      "Epoch [1267/10000] Train Loss: 0.014043 Val Loss: nan\n",
      "Epoch [1268/10000] Train Loss: 0.013766 Val Loss: nan\n",
      "Epoch [1269/10000] Train Loss: 0.013923 Val Loss: nan\n",
      "Epoch [1270/10000] Train Loss: 0.013756 Val Loss: nan\n",
      "Epoch [1271/10000] Train Loss: 0.014096 Val Loss: nan\n",
      "Epoch [1272/10000] Train Loss: 0.014042 Val Loss: nan\n",
      "Epoch [1273/10000] Train Loss: 0.014049 Val Loss: nan\n",
      "Epoch [1274/10000] Train Loss: 0.014166 Val Loss: nan\n",
      "Epoch [1275/10000] Train Loss: 0.014085 Val Loss: nan\n",
      "Epoch [1276/10000] Train Loss: 0.013974 Val Loss: nan\n",
      "Epoch [1277/10000] Train Loss: 0.013707 Val Loss: nan\n",
      "Epoch [1278/10000] Train Loss: 0.014296 Val Loss: nan\n",
      "Epoch [1279/10000] Train Loss: 0.013690 Val Loss: nan\n",
      "Epoch [1280/10000] Train Loss: 0.013689 Val Loss: nan\n",
      "Epoch [1281/10000] Train Loss: 0.013761 Val Loss: nan\n",
      "Epoch [1282/10000] Train Loss: 0.013977 Val Loss: nan\n",
      "Epoch [1283/10000] Train Loss: 0.013668 Val Loss: nan\n",
      "Epoch [1284/10000] Train Loss: 0.013653 Val Loss: nan\n",
      "Epoch [1285/10000] Train Loss: 0.013926 Val Loss: nan\n",
      "Epoch [1286/10000] Train Loss: 0.013666 Val Loss: nan\n",
      "Epoch [1287/10000] Train Loss: 0.013602 Val Loss: nan\n",
      "Epoch [1288/10000] Train Loss: 0.013662 Val Loss: nan\n",
      "Epoch [1289/10000] Train Loss: 0.013689 Val Loss: nan\n",
      "Epoch [1290/10000] Train Loss: 0.014161 Val Loss: nan\n",
      "Epoch [1291/10000] Train Loss: 0.013790 Val Loss: nan\n",
      "Epoch [1292/10000] Train Loss: 0.013778 Val Loss: nan\n",
      "Epoch [1293/10000] Train Loss: 0.013804 Val Loss: nan\n",
      "Epoch [1294/10000] Train Loss: 0.013748 Val Loss: nan\n",
      "Epoch [1295/10000] Train Loss: 0.013792 Val Loss: nan\n",
      "Epoch [1296/10000] Train Loss: 0.013733 Val Loss: nan\n",
      "Epoch [1297/10000] Train Loss: 0.013858 Val Loss: nan\n",
      "Epoch [1298/10000] Train Loss: 0.013780 Val Loss: nan\n",
      "Epoch [1299/10000] Train Loss: 0.013878 Val Loss: nan\n",
      "Epoch [1300/10000] Train Loss: 0.013809 Val Loss: nan\n",
      "Epoch [1301/10000] Train Loss: 0.013614 Val Loss: nan\n",
      "Epoch [1302/10000] Train Loss: 0.013614 Val Loss: nan\n",
      "Epoch [1303/10000] Train Loss: 0.013620 Val Loss: nan\n",
      "Epoch [1304/10000] Train Loss: 0.013960 Val Loss: nan\n",
      "Epoch [1305/10000] Train Loss: 0.013710 Val Loss: nan\n",
      "Epoch [1306/10000] Train Loss: 0.013543 Val Loss: nan\n",
      "Epoch [1307/10000] Train Loss: 0.014036 Val Loss: nan\n",
      "Epoch [1308/10000] Train Loss: 0.013623 Val Loss: nan\n",
      "Epoch [1309/10000] Train Loss: 0.013683 Val Loss: nan\n",
      "Epoch [1310/10000] Train Loss: 0.013846 Val Loss: nan\n",
      "Epoch [1311/10000] Train Loss: 0.013522 Val Loss: nan\n",
      "Epoch [1312/10000] Train Loss: 0.013790 Val Loss: nan\n",
      "Epoch [1313/10000] Train Loss: 0.013543 Val Loss: nan\n",
      "Epoch [1314/10000] Train Loss: 0.013746 Val Loss: nan\n",
      "Epoch [1315/10000] Train Loss: 0.013782 Val Loss: nan\n",
      "Epoch [1316/10000] Train Loss: 0.013714 Val Loss: nan\n",
      "Epoch [1317/10000] Train Loss: 0.013576 Val Loss: nan\n",
      "Epoch [1318/10000] Train Loss: 0.013673 Val Loss: nan\n",
      "Epoch [1319/10000] Train Loss: 0.013493 Val Loss: nan\n",
      "Epoch [1320/10000] Train Loss: 0.013621 Val Loss: nan\n",
      "Epoch [1321/10000] Train Loss: 0.013435 Val Loss: nan\n",
      "Epoch [1322/10000] Train Loss: 0.013761 Val Loss: nan\n",
      "Epoch [1323/10000] Train Loss: 0.013650 Val Loss: nan\n",
      "Epoch [1324/10000] Train Loss: 0.013806 Val Loss: nan\n",
      "Epoch [1325/10000] Train Loss: 0.013607 Val Loss: nan\n",
      "Epoch [1326/10000] Train Loss: 0.013491 Val Loss: nan\n",
      "Epoch [1327/10000] Train Loss: 0.013634 Val Loss: nan\n",
      "Epoch [1328/10000] Train Loss: 0.013657 Val Loss: nan\n",
      "Epoch [1329/10000] Train Loss: 0.013596 Val Loss: nan\n",
      "Epoch [1330/10000] Train Loss: 0.013661 Val Loss: nan\n",
      "Epoch [1331/10000] Train Loss: 0.013497 Val Loss: nan\n",
      "Epoch [1332/10000] Train Loss: 0.013833 Val Loss: nan\n",
      "Epoch [1333/10000] Train Loss: 0.013650 Val Loss: nan\n",
      "Epoch [1334/10000] Train Loss: 0.013599 Val Loss: nan\n",
      "Epoch [1335/10000] Train Loss: 0.013612 Val Loss: nan\n",
      "Epoch [1336/10000] Train Loss: 0.013628 Val Loss: nan\n",
      "Epoch [1337/10000] Train Loss: 0.013486 Val Loss: nan\n",
      "Epoch [1338/10000] Train Loss: 0.013482 Val Loss: nan\n",
      "Epoch [1339/10000] Train Loss: 0.013441 Val Loss: nan\n",
      "Epoch [1340/10000] Train Loss: 0.013594 Val Loss: nan\n",
      "Epoch [1341/10000] Train Loss: 0.013463 Val Loss: nan\n",
      "Epoch [1342/10000] Train Loss: 0.013436 Val Loss: nan\n",
      "Epoch [1343/10000] Train Loss: 0.013417 Val Loss: nan\n",
      "Epoch [1344/10000] Train Loss: 0.013255 Val Loss: nan\n",
      "Epoch [1345/10000] Train Loss: 0.013631 Val Loss: nan\n",
      "Epoch [1346/10000] Train Loss: 0.013525 Val Loss: nan\n",
      "Epoch [1347/10000] Train Loss: 0.013404 Val Loss: nan\n",
      "Epoch [1348/10000] Train Loss: 0.013241 Val Loss: nan\n",
      "Epoch [1349/10000] Train Loss: 0.013533 Val Loss: nan\n",
      "Epoch [1350/10000] Train Loss: 0.013360 Val Loss: nan\n",
      "Epoch [1351/10000] Train Loss: 0.013426 Val Loss: nan\n",
      "Epoch [1352/10000] Train Loss: 0.013485 Val Loss: nan\n",
      "Epoch [1353/10000] Train Loss: 0.013690 Val Loss: nan\n",
      "Epoch [1354/10000] Train Loss: 0.013413 Val Loss: nan\n",
      "Epoch [1355/10000] Train Loss: 0.013672 Val Loss: nan\n",
      "Epoch [1356/10000] Train Loss: 0.013552 Val Loss: nan\n",
      "Epoch [1357/10000] Train Loss: 0.013365 Val Loss: nan\n",
      "Epoch [1358/10000] Train Loss: 0.013322 Val Loss: nan\n",
      "Epoch [1359/10000] Train Loss: 0.013380 Val Loss: nan\n",
      "Epoch [1360/10000] Train Loss: 0.013661 Val Loss: nan\n",
      "Epoch [1361/10000] Train Loss: 0.013492 Val Loss: nan\n",
      "Epoch [1362/10000] Train Loss: 0.013374 Val Loss: nan\n",
      "Epoch [1363/10000] Train Loss: 0.013383 Val Loss: nan\n",
      "Epoch [1364/10000] Train Loss: 0.013277 Val Loss: nan\n",
      "Epoch [1365/10000] Train Loss: 0.013504 Val Loss: nan\n",
      "Epoch [1366/10000] Train Loss: 0.013279 Val Loss: nan\n",
      "Epoch [1367/10000] Train Loss: 0.013410 Val Loss: nan\n",
      "Epoch [1368/10000] Train Loss: 0.013130 Val Loss: nan\n",
      "Epoch [1369/10000] Train Loss: 0.013619 Val Loss: nan\n",
      "Epoch [1370/10000] Train Loss: 0.013077 Val Loss: nan\n",
      "Epoch [1371/10000] Train Loss: 0.013343 Val Loss: nan\n",
      "Epoch [1372/10000] Train Loss: 0.013413 Val Loss: nan\n",
      "Epoch [1373/10000] Train Loss: 0.013315 Val Loss: nan\n",
      "Epoch [1374/10000] Train Loss: 0.013653 Val Loss: nan\n",
      "Epoch [1375/10000] Train Loss: 0.013502 Val Loss: nan\n",
      "Epoch [1376/10000] Train Loss: 0.013446 Val Loss: nan\n",
      "Epoch [1377/10000] Train Loss: 0.013087 Val Loss: nan\n",
      "Epoch [1378/10000] Train Loss: 0.013177 Val Loss: nan\n",
      "Epoch [1379/10000] Train Loss: 0.013067 Val Loss: nan\n",
      "Epoch [1380/10000] Train Loss: 0.013301 Val Loss: nan\n",
      "Epoch [1381/10000] Train Loss: 0.013220 Val Loss: nan\n",
      "Epoch [1382/10000] Train Loss: 0.013074 Val Loss: nan\n",
      "Epoch [1383/10000] Train Loss: 0.013245 Val Loss: nan\n",
      "Epoch [1384/10000] Train Loss: 0.013611 Val Loss: nan\n",
      "Epoch [1385/10000] Train Loss: 0.013283 Val Loss: nan\n",
      "Epoch [1386/10000] Train Loss: 0.013081 Val Loss: nan\n",
      "Epoch [1387/10000] Train Loss: 0.013130 Val Loss: nan\n",
      "Epoch [1388/10000] Train Loss: 0.013112 Val Loss: nan\n",
      "Epoch [1389/10000] Train Loss: 0.013020 Val Loss: nan\n",
      "Epoch [1390/10000] Train Loss: 0.013028 Val Loss: nan\n",
      "Epoch [1391/10000] Train Loss: 0.013383 Val Loss: nan\n",
      "Epoch [1392/10000] Train Loss: 0.013357 Val Loss: nan\n",
      "Epoch [1393/10000] Train Loss: 0.013142 Val Loss: nan\n",
      "Epoch [1394/10000] Train Loss: 0.013155 Val Loss: nan\n",
      "Epoch [1395/10000] Train Loss: 0.013342 Val Loss: nan\n",
      "Epoch [1396/10000] Train Loss: 0.013331 Val Loss: nan\n",
      "Epoch [1397/10000] Train Loss: 0.013180 Val Loss: nan\n",
      "Epoch [1398/10000] Train Loss: 0.013176 Val Loss: nan\n",
      "Epoch [1399/10000] Train Loss: 0.012954 Val Loss: nan\n",
      "Epoch [1400/10000] Train Loss: 0.013020 Val Loss: nan\n",
      "Epoch [1401/10000] Train Loss: 0.013135 Val Loss: nan\n",
      "Epoch [1402/10000] Train Loss: 0.013104 Val Loss: nan\n",
      "Epoch [1403/10000] Train Loss: 0.013174 Val Loss: nan\n",
      "Epoch [1404/10000] Train Loss: 0.013387 Val Loss: nan\n",
      "Epoch [1405/10000] Train Loss: 0.013090 Val Loss: nan\n",
      "Epoch [1406/10000] Train Loss: 0.013192 Val Loss: nan\n",
      "Epoch [1407/10000] Train Loss: 0.013114 Val Loss: nan\n",
      "Epoch [1408/10000] Train Loss: 0.012972 Val Loss: nan\n",
      "Epoch [1409/10000] Train Loss: 0.013115 Val Loss: nan\n",
      "Epoch [1410/10000] Train Loss: 0.013203 Val Loss: nan\n",
      "Epoch [1411/10000] Train Loss: 0.013001 Val Loss: nan\n",
      "Epoch [1412/10000] Train Loss: 0.013188 Val Loss: nan\n",
      "Epoch [1413/10000] Train Loss: 0.013143 Val Loss: nan\n",
      "Epoch [1414/10000] Train Loss: 0.013073 Val Loss: nan\n",
      "Epoch [1415/10000] Train Loss: 0.013445 Val Loss: nan\n",
      "Epoch [1416/10000] Train Loss: 0.013166 Val Loss: nan\n",
      "Epoch [1417/10000] Train Loss: 0.012868 Val Loss: nan\n",
      "Epoch [1418/10000] Train Loss: 0.013182 Val Loss: nan\n",
      "Epoch [1419/10000] Train Loss: 0.012978 Val Loss: nan\n",
      "Epoch [1420/10000] Train Loss: 0.012950 Val Loss: nan\n",
      "Epoch [1421/10000] Train Loss: 0.012916 Val Loss: nan\n",
      "Epoch [1422/10000] Train Loss: 0.013075 Val Loss: nan\n",
      "Epoch [1423/10000] Train Loss: 0.012839 Val Loss: nan\n",
      "Epoch [1424/10000] Train Loss: 0.012941 Val Loss: nan\n",
      "Epoch [1425/10000] Train Loss: 0.012764 Val Loss: nan\n",
      "Epoch [1426/10000] Train Loss: 0.012828 Val Loss: nan\n",
      "Epoch [1427/10000] Train Loss: 0.013152 Val Loss: nan\n",
      "Epoch [1428/10000] Train Loss: 0.012806 Val Loss: nan\n",
      "Epoch [1429/10000] Train Loss: 0.013333 Val Loss: nan\n",
      "Epoch [1430/10000] Train Loss: 0.013195 Val Loss: nan\n",
      "Epoch [1431/10000] Train Loss: 0.013090 Val Loss: nan\n",
      "Epoch [1432/10000] Train Loss: 0.013139 Val Loss: nan\n",
      "Epoch [1433/10000] Train Loss: 0.012969 Val Loss: nan\n",
      "Epoch [1434/10000] Train Loss: 0.012752 Val Loss: nan\n",
      "Epoch [1435/10000] Train Loss: 0.013006 Val Loss: nan\n",
      "Epoch [1436/10000] Train Loss: 0.012703 Val Loss: nan\n",
      "Epoch [1437/10000] Train Loss: 0.012907 Val Loss: nan\n",
      "Epoch [1438/10000] Train Loss: 0.012934 Val Loss: nan\n",
      "Epoch [1439/10000] Train Loss: 0.012959 Val Loss: nan\n",
      "Epoch [1440/10000] Train Loss: 0.012796 Val Loss: nan\n",
      "Epoch [1441/10000] Train Loss: 0.012925 Val Loss: nan\n",
      "Epoch [1442/10000] Train Loss: 0.012855 Val Loss: nan\n",
      "Epoch [1443/10000] Train Loss: 0.012692 Val Loss: nan\n",
      "Epoch [1444/10000] Train Loss: 0.012775 Val Loss: nan\n",
      "Epoch [1445/10000] Train Loss: 0.012881 Val Loss: nan\n",
      "Epoch [1446/10000] Train Loss: 0.013047 Val Loss: nan\n",
      "Epoch [1447/10000] Train Loss: 0.012996 Val Loss: nan\n",
      "Epoch [1448/10000] Train Loss: 0.013001 Val Loss: nan\n",
      "Epoch [1449/10000] Train Loss: 0.012672 Val Loss: nan\n",
      "Epoch [1450/10000] Train Loss: 0.012669 Val Loss: nan\n",
      "Epoch [1451/10000] Train Loss: 0.012980 Val Loss: nan\n",
      "Epoch [1452/10000] Train Loss: 0.012986 Val Loss: nan\n",
      "Epoch [1453/10000] Train Loss: 0.012896 Val Loss: nan\n",
      "Epoch [1454/10000] Train Loss: 0.012751 Val Loss: nan\n",
      "Epoch [1455/10000] Train Loss: 0.012897 Val Loss: nan\n",
      "Epoch [1456/10000] Train Loss: 0.012734 Val Loss: nan\n",
      "Epoch [1457/10000] Train Loss: 0.012917 Val Loss: nan\n",
      "Epoch [1458/10000] Train Loss: 0.012829 Val Loss: nan\n",
      "Epoch [1459/10000] Train Loss: 0.012742 Val Loss: nan\n",
      "Epoch [1460/10000] Train Loss: 0.012777 Val Loss: nan\n",
      "Epoch [1461/10000] Train Loss: 0.012740 Val Loss: nan\n",
      "Epoch [1462/10000] Train Loss: 0.012788 Val Loss: nan\n",
      "Epoch [1463/10000] Train Loss: 0.012664 Val Loss: nan\n",
      "Epoch [1464/10000] Train Loss: 0.012650 Val Loss: nan\n",
      "Epoch [1465/10000] Train Loss: 0.012735 Val Loss: nan\n",
      "Epoch [1466/10000] Train Loss: 0.012686 Val Loss: nan\n",
      "Epoch [1467/10000] Train Loss: 0.012667 Val Loss: nan\n",
      "Epoch [1468/10000] Train Loss: 0.012660 Val Loss: nan\n",
      "Epoch [1469/10000] Train Loss: 0.012794 Val Loss: nan\n",
      "Epoch [1470/10000] Train Loss: 0.012622 Val Loss: nan\n",
      "Epoch [1471/10000] Train Loss: 0.012573 Val Loss: nan\n",
      "Epoch [1472/10000] Train Loss: 0.012631 Val Loss: nan\n",
      "Epoch [1473/10000] Train Loss: 0.012673 Val Loss: nan\n",
      "Epoch [1474/10000] Train Loss: 0.012512 Val Loss: nan\n",
      "Epoch [1475/10000] Train Loss: 0.012760 Val Loss: nan\n",
      "Epoch [1476/10000] Train Loss: 0.012959 Val Loss: nan\n",
      "Epoch [1477/10000] Train Loss: 0.012638 Val Loss: nan\n",
      "Epoch [1478/10000] Train Loss: 0.012564 Val Loss: nan\n",
      "Epoch [1479/10000] Train Loss: 0.012632 Val Loss: nan\n",
      "Epoch [1480/10000] Train Loss: 0.012811 Val Loss: nan\n",
      "Epoch [1481/10000] Train Loss: 0.012722 Val Loss: nan\n",
      "Epoch [1482/10000] Train Loss: 0.012804 Val Loss: nan\n",
      "Epoch [1483/10000] Train Loss: 0.012708 Val Loss: nan\n",
      "Epoch [1484/10000] Train Loss: 0.012522 Val Loss: nan\n",
      "Epoch [1485/10000] Train Loss: 0.012573 Val Loss: nan\n",
      "Epoch [1486/10000] Train Loss: 0.012670 Val Loss: nan\n",
      "Epoch [1487/10000] Train Loss: 0.012673 Val Loss: nan\n",
      "Epoch [1488/10000] Train Loss: 0.012670 Val Loss: nan\n",
      "Epoch [1489/10000] Train Loss: 0.012561 Val Loss: nan\n",
      "Epoch [1490/10000] Train Loss: 0.012481 Val Loss: nan\n",
      "Epoch [1491/10000] Train Loss: 0.012650 Val Loss: nan\n",
      "Epoch [1492/10000] Train Loss: 0.012571 Val Loss: nan\n",
      "Epoch [1493/10000] Train Loss: 0.012578 Val Loss: nan\n",
      "Epoch [1494/10000] Train Loss: 0.012735 Val Loss: nan\n",
      "Epoch [1495/10000] Train Loss: 0.012480 Val Loss: nan\n",
      "Epoch [1496/10000] Train Loss: 0.012892 Val Loss: nan\n",
      "Epoch [1497/10000] Train Loss: 0.012929 Val Loss: nan\n",
      "Epoch [1498/10000] Train Loss: 0.012534 Val Loss: nan\n",
      "Epoch [1499/10000] Train Loss: 0.012631 Val Loss: nan\n",
      "Epoch [1500/10000] Train Loss: 0.012457 Val Loss: nan\n",
      "Epoch [1501/10000] Train Loss: 0.012580 Val Loss: nan\n",
      "Epoch [1502/10000] Train Loss: 0.012395 Val Loss: nan\n",
      "Epoch [1503/10000] Train Loss: 0.012713 Val Loss: nan\n",
      "Epoch [1504/10000] Train Loss: 0.012515 Val Loss: nan\n",
      "Epoch [1505/10000] Train Loss: 0.012907 Val Loss: nan\n",
      "Epoch [1506/10000] Train Loss: 0.012623 Val Loss: nan\n",
      "Epoch [1507/10000] Train Loss: 0.012601 Val Loss: nan\n",
      "Epoch [1508/10000] Train Loss: 0.012726 Val Loss: nan\n",
      "Epoch [1509/10000] Train Loss: 0.012493 Val Loss: nan\n",
      "Epoch [1510/10000] Train Loss: 0.012585 Val Loss: nan\n",
      "Epoch [1511/10000] Train Loss: 0.012471 Val Loss: nan\n",
      "Epoch [1512/10000] Train Loss: 0.012440 Val Loss: nan\n",
      "Epoch [1513/10000] Train Loss: 0.012616 Val Loss: nan\n",
      "Epoch [1514/10000] Train Loss: 0.012512 Val Loss: nan\n",
      "Epoch [1515/10000] Train Loss: 0.012466 Val Loss: nan\n",
      "Epoch [1516/10000] Train Loss: 0.012340 Val Loss: nan\n",
      "Epoch [1517/10000] Train Loss: 0.012482 Val Loss: nan\n",
      "Epoch [1518/10000] Train Loss: 0.012217 Val Loss: nan\n",
      "Epoch [1519/10000] Train Loss: 0.012457 Val Loss: nan\n",
      "Epoch [1520/10000] Train Loss: 0.012391 Val Loss: nan\n",
      "Epoch [1521/10000] Train Loss: 0.012813 Val Loss: nan\n",
      "Epoch [1522/10000] Train Loss: 0.012455 Val Loss: nan\n",
      "Epoch [1523/10000] Train Loss: 0.012273 Val Loss: nan\n",
      "Epoch [1524/10000] Train Loss: 0.012240 Val Loss: nan\n",
      "Epoch [1525/10000] Train Loss: 0.012697 Val Loss: nan\n",
      "Epoch [1526/10000] Train Loss: 0.012464 Val Loss: nan\n",
      "Epoch [1527/10000] Train Loss: 0.012220 Val Loss: nan\n",
      "Epoch [1528/10000] Train Loss: 0.012272 Val Loss: nan\n",
      "Epoch [1529/10000] Train Loss: 0.012243 Val Loss: nan\n",
      "Epoch [1530/10000] Train Loss: 0.012463 Val Loss: nan\n",
      "Epoch [1531/10000] Train Loss: 0.012297 Val Loss: nan\n",
      "Epoch [1532/10000] Train Loss: 0.012598 Val Loss: nan\n",
      "Epoch [1533/10000] Train Loss: 0.012338 Val Loss: nan\n",
      "Epoch [1534/10000] Train Loss: 0.012357 Val Loss: nan\n",
      "Epoch [1535/10000] Train Loss: 0.012530 Val Loss: nan\n",
      "Epoch [1536/10000] Train Loss: 0.012190 Val Loss: nan\n",
      "Epoch [1537/10000] Train Loss: 0.012370 Val Loss: nan\n",
      "Epoch [1538/10000] Train Loss: 0.012221 Val Loss: nan\n",
      "Epoch [1539/10000] Train Loss: 0.012502 Val Loss: nan\n",
      "Epoch [1540/10000] Train Loss: 0.012241 Val Loss: nan\n",
      "Epoch [1541/10000] Train Loss: 0.012292 Val Loss: nan\n",
      "Epoch [1542/10000] Train Loss: 0.012382 Val Loss: nan\n",
      "Epoch [1543/10000] Train Loss: 0.012279 Val Loss: nan\n",
      "Epoch [1544/10000] Train Loss: 0.012139 Val Loss: nan\n",
      "Epoch [1545/10000] Train Loss: 0.012199 Val Loss: nan\n",
      "Epoch [1546/10000] Train Loss: 0.012255 Val Loss: nan\n",
      "Epoch [1547/10000] Train Loss: 0.012319 Val Loss: nan\n",
      "Epoch [1548/10000] Train Loss: 0.012184 Val Loss: nan\n",
      "Epoch [1549/10000] Train Loss: 0.012375 Val Loss: nan\n",
      "Epoch [1550/10000] Train Loss: 0.012220 Val Loss: nan\n",
      "Epoch [1551/10000] Train Loss: 0.012516 Val Loss: nan\n",
      "Epoch [1552/10000] Train Loss: 0.012147 Val Loss: nan\n",
      "Epoch [1553/10000] Train Loss: 0.012044 Val Loss: nan\n",
      "Epoch [1554/10000] Train Loss: 0.012170 Val Loss: nan\n",
      "Epoch [1555/10000] Train Loss: 0.012075 Val Loss: nan\n",
      "Epoch [1556/10000] Train Loss: 0.012085 Val Loss: nan\n",
      "Epoch [1557/10000] Train Loss: 0.012301 Val Loss: nan\n",
      "Epoch [1558/10000] Train Loss: 0.012348 Val Loss: nan\n",
      "Epoch [1559/10000] Train Loss: 0.012552 Val Loss: nan\n",
      "Epoch [1560/10000] Train Loss: 0.012775 Val Loss: nan\n",
      "Epoch [1561/10000] Train Loss: 0.012078 Val Loss: nan\n",
      "Epoch [1562/10000] Train Loss: 0.012301 Val Loss: nan\n",
      "Epoch [1563/10000] Train Loss: 0.012188 Val Loss: nan\n",
      "Epoch [1564/10000] Train Loss: 0.012224 Val Loss: nan\n",
      "Epoch [1565/10000] Train Loss: 0.012019 Val Loss: nan\n",
      "Epoch [1566/10000] Train Loss: 0.011936 Val Loss: nan\n",
      "Epoch [1567/10000] Train Loss: 0.011997 Val Loss: nan\n",
      "Epoch [1568/10000] Train Loss: 0.012543 Val Loss: nan\n",
      "Epoch [1569/10000] Train Loss: 0.012186 Val Loss: nan\n",
      "Epoch [1570/10000] Train Loss: 0.012010 Val Loss: nan\n",
      "Epoch [1571/10000] Train Loss: 0.012269 Val Loss: nan\n",
      "Epoch [1572/10000] Train Loss: 0.012096 Val Loss: nan\n",
      "Epoch [1573/10000] Train Loss: 0.012022 Val Loss: nan\n",
      "Epoch [1574/10000] Train Loss: 0.012471 Val Loss: nan\n",
      "Epoch [1575/10000] Train Loss: 0.012043 Val Loss: nan\n",
      "Epoch [1576/10000] Train Loss: 0.012154 Val Loss: nan\n",
      "Epoch [1577/10000] Train Loss: 0.012023 Val Loss: nan\n",
      "Epoch [1578/10000] Train Loss: 0.012208 Val Loss: nan\n",
      "Epoch [1579/10000] Train Loss: 0.012459 Val Loss: nan\n",
      "Epoch [1580/10000] Train Loss: 0.012360 Val Loss: nan\n",
      "Epoch [1581/10000] Train Loss: 0.012214 Val Loss: nan\n",
      "Epoch [1582/10000] Train Loss: 0.012295 Val Loss: nan\n",
      "Epoch [1583/10000] Train Loss: 0.012217 Val Loss: nan\n",
      "Epoch [1584/10000] Train Loss: 0.012131 Val Loss: nan\n",
      "Epoch [1585/10000] Train Loss: 0.011894 Val Loss: nan\n",
      "Epoch [1586/10000] Train Loss: 0.012039 Val Loss: nan\n",
      "Epoch [1587/10000] Train Loss: 0.011911 Val Loss: nan\n",
      "Epoch [1588/10000] Train Loss: 0.012040 Val Loss: nan\n",
      "Epoch [1589/10000] Train Loss: 0.012067 Val Loss: nan\n",
      "Epoch [1590/10000] Train Loss: 0.011947 Val Loss: nan\n",
      "Epoch [1591/10000] Train Loss: 0.012216 Val Loss: nan\n",
      "Epoch [1592/10000] Train Loss: 0.011959 Val Loss: nan\n",
      "Epoch [1593/10000] Train Loss: 0.012038 Val Loss: nan\n",
      "Epoch [1594/10000] Train Loss: 0.012446 Val Loss: nan\n",
      "Epoch [1595/10000] Train Loss: 0.012376 Val Loss: nan\n",
      "Epoch [1596/10000] Train Loss: 0.011999 Val Loss: nan\n",
      "Epoch [1597/10000] Train Loss: 0.012179 Val Loss: nan\n",
      "Epoch [1598/10000] Train Loss: 0.011931 Val Loss: nan\n",
      "Epoch [1599/10000] Train Loss: 0.011899 Val Loss: nan\n",
      "Epoch [1600/10000] Train Loss: 0.011848 Val Loss: nan\n",
      "Epoch [1601/10000] Train Loss: 0.012094 Val Loss: nan\n",
      "Epoch [1602/10000] Train Loss: 0.011968 Val Loss: nan\n",
      "Epoch [1603/10000] Train Loss: 0.012001 Val Loss: nan\n",
      "Epoch [1604/10000] Train Loss: 0.012063 Val Loss: nan\n",
      "Epoch [1605/10000] Train Loss: 0.011889 Val Loss: nan\n",
      "Epoch [1606/10000] Train Loss: 0.011941 Val Loss: nan\n",
      "Epoch [1607/10000] Train Loss: 0.011911 Val Loss: nan\n",
      "Epoch [1608/10000] Train Loss: 0.011898 Val Loss: nan\n",
      "Epoch [1609/10000] Train Loss: 0.012002 Val Loss: nan\n",
      "Epoch [1610/10000] Train Loss: 0.011995 Val Loss: nan\n",
      "Epoch [1611/10000] Train Loss: 0.012001 Val Loss: nan\n",
      "Epoch [1612/10000] Train Loss: 0.011928 Val Loss: nan\n",
      "Epoch [1613/10000] Train Loss: 0.012158 Val Loss: nan\n",
      "Epoch [1614/10000] Train Loss: 0.012453 Val Loss: nan\n",
      "Epoch [1615/10000] Train Loss: 0.012468 Val Loss: nan\n",
      "Epoch [1616/10000] Train Loss: 0.012026 Val Loss: nan\n",
      "Epoch [1617/10000] Train Loss: 0.011750 Val Loss: nan\n",
      "Epoch [1618/10000] Train Loss: 0.011857 Val Loss: nan\n",
      "Epoch [1619/10000] Train Loss: 0.011938 Val Loss: nan\n",
      "Epoch [1620/10000] Train Loss: 0.011723 Val Loss: nan\n",
      "Epoch [1621/10000] Train Loss: 0.011819 Val Loss: nan\n",
      "Epoch [1622/10000] Train Loss: 0.011733 Val Loss: nan\n",
      "Epoch [1623/10000] Train Loss: 0.011845 Val Loss: nan\n",
      "Epoch [1624/10000] Train Loss: 0.011817 Val Loss: nan\n",
      "Epoch [1625/10000] Train Loss: 0.011952 Val Loss: nan\n",
      "Epoch [1626/10000] Train Loss: 0.011901 Val Loss: nan\n",
      "Epoch [1627/10000] Train Loss: 0.011961 Val Loss: nan\n",
      "Epoch [1628/10000] Train Loss: 0.011742 Val Loss: nan\n",
      "Epoch [1629/10000] Train Loss: 0.011871 Val Loss: nan\n",
      "Epoch [1630/10000] Train Loss: 0.011861 Val Loss: nan\n",
      "Epoch [1631/10000] Train Loss: 0.012025 Val Loss: nan\n",
      "Epoch [1632/10000] Train Loss: 0.012015 Val Loss: nan\n",
      "Epoch [1633/10000] Train Loss: 0.011889 Val Loss: nan\n",
      "Epoch [1634/10000] Train Loss: 0.011966 Val Loss: nan\n",
      "Epoch [1635/10000] Train Loss: 0.011758 Val Loss: nan\n",
      "Epoch [1636/10000] Train Loss: 0.011892 Val Loss: nan\n",
      "Epoch [1637/10000] Train Loss: 0.011817 Val Loss: nan\n",
      "Epoch [1638/10000] Train Loss: 0.011652 Val Loss: nan\n",
      "Epoch [1639/10000] Train Loss: 0.011785 Val Loss: nan\n",
      "Epoch [1640/10000] Train Loss: 0.011819 Val Loss: nan\n",
      "Epoch [1641/10000] Train Loss: 0.011836 Val Loss: nan\n",
      "Epoch [1642/10000] Train Loss: 0.011728 Val Loss: nan\n",
      "Epoch [1643/10000] Train Loss: 0.011860 Val Loss: nan\n",
      "Epoch [1644/10000] Train Loss: 0.011537 Val Loss: nan\n",
      "Epoch [1645/10000] Train Loss: 0.011717 Val Loss: nan\n",
      "Epoch [1646/10000] Train Loss: 0.011664 Val Loss: nan\n",
      "Epoch [1647/10000] Train Loss: 0.011668 Val Loss: nan\n",
      "Epoch [1648/10000] Train Loss: 0.011958 Val Loss: nan\n",
      "Epoch [1649/10000] Train Loss: 0.012088 Val Loss: nan\n",
      "Epoch [1650/10000] Train Loss: 0.011574 Val Loss: nan\n",
      "Epoch [1651/10000] Train Loss: 0.011914 Val Loss: nan\n",
      "Epoch [1652/10000] Train Loss: 0.011752 Val Loss: nan\n",
      "Epoch [1653/10000] Train Loss: 0.011806 Val Loss: nan\n",
      "Epoch [1654/10000] Train Loss: 0.011612 Val Loss: nan\n",
      "Epoch [1655/10000] Train Loss: 0.011866 Val Loss: nan\n",
      "Epoch [1656/10000] Train Loss: 0.011835 Val Loss: nan\n",
      "Epoch [1657/10000] Train Loss: 0.011614 Val Loss: nan\n",
      "Epoch [1658/10000] Train Loss: 0.011780 Val Loss: nan\n",
      "Epoch [1659/10000] Train Loss: 0.011630 Val Loss: nan\n",
      "Epoch [1660/10000] Train Loss: 0.011666 Val Loss: nan\n",
      "Epoch [1661/10000] Train Loss: 0.011902 Val Loss: nan\n",
      "Epoch [1662/10000] Train Loss: 0.011657 Val Loss: nan\n",
      "Epoch [1663/10000] Train Loss: 0.011846 Val Loss: nan\n",
      "Epoch [1664/10000] Train Loss: 0.011589 Val Loss: nan\n",
      "Epoch [1665/10000] Train Loss: 0.011515 Val Loss: nan\n",
      "Epoch [1666/10000] Train Loss: 0.011495 Val Loss: nan\n",
      "Epoch [1667/10000] Train Loss: 0.011930 Val Loss: nan\n",
      "Epoch [1668/10000] Train Loss: 0.011607 Val Loss: nan\n",
      "Epoch [1669/10000] Train Loss: 0.011746 Val Loss: nan\n",
      "Epoch [1670/10000] Train Loss: 0.011539 Val Loss: nan\n",
      "Epoch [1671/10000] Train Loss: 0.011729 Val Loss: nan\n",
      "Epoch [1672/10000] Train Loss: 0.011691 Val Loss: nan\n",
      "Epoch [1673/10000] Train Loss: 0.011571 Val Loss: nan\n",
      "Epoch [1674/10000] Train Loss: 0.012040 Val Loss: nan\n",
      "Epoch [1675/10000] Train Loss: 0.011764 Val Loss: nan\n",
      "Epoch [1676/10000] Train Loss: 0.011996 Val Loss: nan\n",
      "Epoch [1677/10000] Train Loss: 0.011527 Val Loss: nan\n",
      "Epoch [1678/10000] Train Loss: 0.011610 Val Loss: nan\n",
      "Epoch [1679/10000] Train Loss: 0.011660 Val Loss: nan\n",
      "Epoch [1680/10000] Train Loss: 0.011684 Val Loss: nan\n",
      "Epoch [1681/10000] Train Loss: 0.011571 Val Loss: nan\n",
      "Epoch [1682/10000] Train Loss: 0.011648 Val Loss: nan\n",
      "Epoch [1683/10000] Train Loss: 0.011802 Val Loss: nan\n",
      "Epoch [1684/10000] Train Loss: 0.011679 Val Loss: nan\n",
      "Epoch [1685/10000] Train Loss: 0.011394 Val Loss: nan\n",
      "Epoch [1686/10000] Train Loss: 0.011866 Val Loss: nan\n",
      "Epoch [1687/10000] Train Loss: 0.011594 Val Loss: nan\n",
      "Epoch [1688/10000] Train Loss: 0.011667 Val Loss: nan\n",
      "Epoch [1689/10000] Train Loss: 0.011576 Val Loss: nan\n",
      "Epoch [1690/10000] Train Loss: 0.011598 Val Loss: nan\n",
      "Epoch [1691/10000] Train Loss: 0.011336 Val Loss: nan\n",
      "Epoch [1692/10000] Train Loss: 0.011455 Val Loss: nan\n",
      "Epoch [1693/10000] Train Loss: 0.011407 Val Loss: nan\n",
      "Epoch [1694/10000] Train Loss: 0.011335 Val Loss: nan\n",
      "Epoch [1695/10000] Train Loss: 0.011548 Val Loss: nan\n",
      "Epoch [1696/10000] Train Loss: 0.011456 Val Loss: nan\n",
      "Epoch [1697/10000] Train Loss: 0.011794 Val Loss: nan\n",
      "Epoch [1698/10000] Train Loss: 0.011451 Val Loss: nan\n",
      "Epoch [1699/10000] Train Loss: 0.011578 Val Loss: nan\n",
      "Epoch [1700/10000] Train Loss: 0.011707 Val Loss: nan\n",
      "Epoch [1701/10000] Train Loss: 0.011404 Val Loss: nan\n",
      "Epoch [1702/10000] Train Loss: 0.011416 Val Loss: nan\n",
      "Epoch [1703/10000] Train Loss: 0.011591 Val Loss: nan\n",
      "Epoch [1704/10000] Train Loss: 0.011700 Val Loss: nan\n",
      "Epoch [1705/10000] Train Loss: 0.011498 Val Loss: nan\n",
      "Epoch [1706/10000] Train Loss: 0.011491 Val Loss: nan\n",
      "Epoch [1707/10000] Train Loss: 0.011504 Val Loss: nan\n",
      "Epoch [1708/10000] Train Loss: 0.011484 Val Loss: nan\n",
      "Epoch [1709/10000] Train Loss: 0.011331 Val Loss: nan\n",
      "Epoch [1710/10000] Train Loss: 0.011454 Val Loss: nan\n",
      "Epoch [1711/10000] Train Loss: 0.011265 Val Loss: nan\n",
      "Epoch [1712/10000] Train Loss: 0.011521 Val Loss: nan\n",
      "Epoch [1713/10000] Train Loss: 0.011504 Val Loss: nan\n",
      "Epoch [1714/10000] Train Loss: 0.011333 Val Loss: nan\n",
      "Epoch [1715/10000] Train Loss: 0.011323 Val Loss: nan\n",
      "Epoch [1716/10000] Train Loss: 0.011541 Val Loss: nan\n",
      "Epoch [1717/10000] Train Loss: 0.011603 Val Loss: nan\n",
      "Epoch [1718/10000] Train Loss: 0.011465 Val Loss: nan\n",
      "Epoch [1719/10000] Train Loss: 0.011511 Val Loss: nan\n",
      "Epoch [1720/10000] Train Loss: 0.011584 Val Loss: nan\n",
      "Epoch [1721/10000] Train Loss: 0.011400 Val Loss: nan\n",
      "Epoch [1722/10000] Train Loss: 0.011633 Val Loss: nan\n",
      "Epoch [1723/10000] Train Loss: 0.011319 Val Loss: nan\n",
      "Epoch [1724/10000] Train Loss: 0.011539 Val Loss: nan\n",
      "Epoch [1725/10000] Train Loss: 0.011436 Val Loss: nan\n",
      "Epoch [1726/10000] Train Loss: 0.011768 Val Loss: nan\n",
      "Epoch [1727/10000] Train Loss: 0.011763 Val Loss: nan\n",
      "Epoch [1728/10000] Train Loss: 0.011401 Val Loss: nan\n",
      "Epoch [1729/10000] Train Loss: 0.011443 Val Loss: nan\n",
      "Epoch [1730/10000] Train Loss: 0.011472 Val Loss: nan\n",
      "Epoch [1731/10000] Train Loss: 0.011262 Val Loss: nan\n",
      "Epoch [1732/10000] Train Loss: 0.011342 Val Loss: nan\n",
      "Epoch [1733/10000] Train Loss: 0.011298 Val Loss: nan\n",
      "Epoch [1734/10000] Train Loss: 0.011527 Val Loss: nan\n",
      "Epoch [1735/10000] Train Loss: 0.011358 Val Loss: nan\n",
      "Epoch [1736/10000] Train Loss: 0.011342 Val Loss: nan\n",
      "Epoch [1737/10000] Train Loss: 0.011755 Val Loss: nan\n",
      "Epoch [1738/10000] Train Loss: 0.011646 Val Loss: nan\n",
      "Epoch [1739/10000] Train Loss: 0.011362 Val Loss: nan\n",
      "Epoch [1740/10000] Train Loss: 0.011377 Val Loss: nan\n",
      "Epoch [1741/10000] Train Loss: 0.011134 Val Loss: nan\n",
      "Epoch [1742/10000] Train Loss: 0.011407 Val Loss: nan\n",
      "Epoch [1743/10000] Train Loss: 0.011658 Val Loss: nan\n",
      "Epoch [1744/10000] Train Loss: 0.011201 Val Loss: nan\n",
      "Epoch [1745/10000] Train Loss: 0.011229 Val Loss: nan\n",
      "Epoch [1746/10000] Train Loss: 0.011227 Val Loss: nan\n",
      "Epoch [1747/10000] Train Loss: 0.011394 Val Loss: nan\n",
      "Epoch [1748/10000] Train Loss: 0.011483 Val Loss: nan\n",
      "Epoch [1749/10000] Train Loss: 0.011326 Val Loss: nan\n",
      "Epoch [1750/10000] Train Loss: 0.011199 Val Loss: nan\n",
      "Epoch [1751/10000] Train Loss: 0.011136 Val Loss: nan\n",
      "Epoch [1752/10000] Train Loss: 0.011279 Val Loss: nan\n",
      "Epoch [1753/10000] Train Loss: 0.011254 Val Loss: nan\n",
      "Epoch [1754/10000] Train Loss: 0.011280 Val Loss: nan\n",
      "Epoch [1755/10000] Train Loss: 0.011462 Val Loss: nan\n",
      "Epoch [1756/10000] Train Loss: 0.011153 Val Loss: nan\n",
      "Epoch [1757/10000] Train Loss: 0.011134 Val Loss: nan\n",
      "Epoch [1758/10000] Train Loss: 0.011475 Val Loss: nan\n",
      "Epoch [1759/10000] Train Loss: 0.011518 Val Loss: nan\n",
      "Epoch [1760/10000] Train Loss: 0.011102 Val Loss: nan\n",
      "Epoch [1761/10000] Train Loss: 0.011107 Val Loss: nan\n",
      "Epoch [1762/10000] Train Loss: 0.011467 Val Loss: nan\n",
      "Epoch [1763/10000] Train Loss: 0.011112 Val Loss: nan\n",
      "Epoch [1764/10000] Train Loss: 0.011343 Val Loss: nan\n",
      "Epoch [1765/10000] Train Loss: 0.011417 Val Loss: nan\n",
      "Epoch [1766/10000] Train Loss: 0.011442 Val Loss: nan\n",
      "Epoch [1767/10000] Train Loss: 0.011224 Val Loss: nan\n",
      "Epoch [1768/10000] Train Loss: 0.011580 Val Loss: nan\n",
      "Epoch [1769/10000] Train Loss: 0.011187 Val Loss: nan\n",
      "Epoch [1770/10000] Train Loss: 0.011091 Val Loss: nan\n",
      "Epoch [1771/10000] Train Loss: 0.011270 Val Loss: nan\n",
      "Epoch [1772/10000] Train Loss: 0.011064 Val Loss: nan\n",
      "Epoch [1773/10000] Train Loss: 0.011033 Val Loss: nan\n",
      "Epoch [1774/10000] Train Loss: 0.011046 Val Loss: nan\n",
      "Epoch [1775/10000] Train Loss: 0.011340 Val Loss: nan\n",
      "Epoch [1776/10000] Train Loss: 0.011155 Val Loss: nan\n",
      "Epoch [1777/10000] Train Loss: 0.011063 Val Loss: nan\n",
      "Epoch [1778/10000] Train Loss: 0.011088 Val Loss: nan\n",
      "Epoch [1779/10000] Train Loss: 0.011283 Val Loss: nan\n",
      "Epoch [1780/10000] Train Loss: 0.011048 Val Loss: nan\n",
      "Epoch [1781/10000] Train Loss: 0.011326 Val Loss: nan\n",
      "Epoch [1782/10000] Train Loss: 0.011280 Val Loss: nan\n",
      "Epoch [1783/10000] Train Loss: 0.011050 Val Loss: nan\n",
      "Epoch [1784/10000] Train Loss: 0.011054 Val Loss: nan\n",
      "Epoch [1785/10000] Train Loss: 0.011070 Val Loss: nan\n",
      "Epoch [1786/10000] Train Loss: 0.011034 Val Loss: nan\n",
      "Epoch [1787/10000] Train Loss: 0.011021 Val Loss: nan\n",
      "Epoch [1788/10000] Train Loss: 0.011017 Val Loss: nan\n",
      "Epoch [1789/10000] Train Loss: 0.011145 Val Loss: nan\n",
      "Epoch [1790/10000] Train Loss: 0.011034 Val Loss: nan\n",
      "Epoch [1791/10000] Train Loss: 0.011232 Val Loss: nan\n",
      "Epoch [1792/10000] Train Loss: 0.011228 Val Loss: nan\n",
      "Epoch [1793/10000] Train Loss: 0.011180 Val Loss: nan\n",
      "Epoch [1794/10000] Train Loss: 0.011307 Val Loss: nan\n",
      "Epoch [1795/10000] Train Loss: 0.011102 Val Loss: nan\n",
      "Epoch [1796/10000] Train Loss: 0.011070 Val Loss: nan\n",
      "Epoch [1797/10000] Train Loss: 0.010965 Val Loss: nan\n",
      "Epoch [1798/10000] Train Loss: 0.011120 Val Loss: nan\n",
      "Epoch [1799/10000] Train Loss: 0.011373 Val Loss: nan\n",
      "Epoch [1800/10000] Train Loss: 0.011204 Val Loss: nan\n",
      "Epoch [1801/10000] Train Loss: 0.011407 Val Loss: nan\n",
      "Epoch [1802/10000] Train Loss: 0.011186 Val Loss: nan\n",
      "Epoch [1803/10000] Train Loss: 0.011213 Val Loss: nan\n",
      "Epoch [1804/10000] Train Loss: 0.011119 Val Loss: nan\n",
      "Epoch [1805/10000] Train Loss: 0.010948 Val Loss: nan\n",
      "Epoch [1806/10000] Train Loss: 0.010983 Val Loss: nan\n",
      "Epoch [1807/10000] Train Loss: 0.010990 Val Loss: nan\n",
      "Epoch [1808/10000] Train Loss: 0.011005 Val Loss: nan\n",
      "Epoch [1809/10000] Train Loss: 0.010940 Val Loss: nan\n",
      "Epoch [1810/10000] Train Loss: 0.011004 Val Loss: nan\n",
      "Epoch [1811/10000] Train Loss: 0.011143 Val Loss: nan\n",
      "Epoch [1812/10000] Train Loss: 0.011182 Val Loss: nan\n",
      "Epoch [1813/10000] Train Loss: 0.010966 Val Loss: nan\n",
      "Epoch [1814/10000] Train Loss: 0.010930 Val Loss: nan\n",
      "Epoch [1815/10000] Train Loss: 0.010920 Val Loss: nan\n",
      "Epoch [1816/10000] Train Loss: 0.011106 Val Loss: nan\n",
      "Epoch [1817/10000] Train Loss: 0.011174 Val Loss: nan\n",
      "Epoch [1818/10000] Train Loss: 0.011205 Val Loss: nan\n",
      "Epoch [1819/10000] Train Loss: 0.011106 Val Loss: nan\n",
      "Epoch [1820/10000] Train Loss: 0.011043 Val Loss: nan\n",
      "Epoch [1821/10000] Train Loss: 0.011214 Val Loss: nan\n",
      "Epoch [1822/10000] Train Loss: 0.010870 Val Loss: nan\n",
      "Epoch [1823/10000] Train Loss: 0.011036 Val Loss: nan\n",
      "Epoch [1824/10000] Train Loss: 0.011088 Val Loss: nan\n",
      "Epoch [1825/10000] Train Loss: 0.010879 Val Loss: nan\n",
      "Epoch [1826/10000] Train Loss: 0.011001 Val Loss: nan\n",
      "Epoch [1827/10000] Train Loss: 0.010948 Val Loss: nan\n",
      "Epoch [1828/10000] Train Loss: 0.010958 Val Loss: nan\n",
      "Epoch [1829/10000] Train Loss: 0.011658 Val Loss: nan\n",
      "Epoch [1830/10000] Train Loss: 0.011467 Val Loss: nan\n",
      "Epoch [1831/10000] Train Loss: 0.011029 Val Loss: nan\n",
      "Epoch [1832/10000] Train Loss: 0.011249 Val Loss: nan\n",
      "Epoch [1833/10000] Train Loss: 0.011185 Val Loss: nan\n",
      "Epoch [1834/10000] Train Loss: 0.010997 Val Loss: nan\n",
      "Epoch [1835/10000] Train Loss: 0.011031 Val Loss: nan\n",
      "Epoch [1836/10000] Train Loss: 0.010917 Val Loss: nan\n",
      "Epoch [1837/10000] Train Loss: 0.010985 Val Loss: nan\n",
      "Epoch [1838/10000] Train Loss: 0.011183 Val Loss: nan\n",
      "Epoch [1839/10000] Train Loss: 0.011297 Val Loss: nan\n",
      "Epoch [1840/10000] Train Loss: 0.011016 Val Loss: nan\n",
      "Epoch [1841/10000] Train Loss: 0.010837 Val Loss: nan\n",
      "Epoch [1842/10000] Train Loss: 0.010977 Val Loss: nan\n",
      "Epoch [1843/10000] Train Loss: 0.010832 Val Loss: nan\n",
      "Epoch [1844/10000] Train Loss: 0.011028 Val Loss: nan\n",
      "Epoch [1845/10000] Train Loss: 0.010936 Val Loss: nan\n",
      "Epoch [1846/10000] Train Loss: 0.010872 Val Loss: nan\n",
      "Epoch [1847/10000] Train Loss: 0.010902 Val Loss: nan\n",
      "Epoch [1848/10000] Train Loss: 0.011159 Val Loss: nan\n",
      "Epoch [1849/10000] Train Loss: 0.010877 Val Loss: nan\n",
      "Epoch [1850/10000] Train Loss: 0.010962 Val Loss: nan\n",
      "Epoch [1851/10000] Train Loss: 0.010823 Val Loss: nan\n",
      "Epoch [1852/10000] Train Loss: 0.010844 Val Loss: nan\n",
      "Epoch [1853/10000] Train Loss: 0.010798 Val Loss: nan\n",
      "Epoch [1854/10000] Train Loss: 0.011013 Val Loss: nan\n",
      "Epoch [1855/10000] Train Loss: 0.011105 Val Loss: nan\n",
      "Epoch [1856/10000] Train Loss: 0.011115 Val Loss: nan\n",
      "Epoch [1857/10000] Train Loss: 0.010800 Val Loss: nan\n",
      "Epoch [1858/10000] Train Loss: 0.010804 Val Loss: nan\n",
      "Epoch [1859/10000] Train Loss: 0.011038 Val Loss: nan\n",
      "Epoch [1860/10000] Train Loss: 0.010825 Val Loss: nan\n",
      "Epoch [1861/10000] Train Loss: 0.010754 Val Loss: nan\n",
      "Epoch [1862/10000] Train Loss: 0.010802 Val Loss: nan\n",
      "Epoch [1863/10000] Train Loss: 0.010859 Val Loss: nan\n",
      "Epoch [1864/10000] Train Loss: 0.010970 Val Loss: nan\n",
      "Epoch [1865/10000] Train Loss: 0.010805 Val Loss: nan\n",
      "Epoch [1866/10000] Train Loss: 0.010810 Val Loss: nan\n",
      "Epoch [1867/10000] Train Loss: 0.010749 Val Loss: nan\n",
      "Epoch [1868/10000] Train Loss: 0.010803 Val Loss: nan\n",
      "Epoch [1869/10000] Train Loss: 0.010811 Val Loss: nan\n",
      "Epoch [1870/10000] Train Loss: 0.010922 Val Loss: nan\n",
      "Epoch [1871/10000] Train Loss: 0.010954 Val Loss: nan\n",
      "Epoch [1872/10000] Train Loss: 0.010755 Val Loss: nan\n",
      "Epoch [1873/10000] Train Loss: 0.010965 Val Loss: nan\n",
      "Epoch [1874/10000] Train Loss: 0.010913 Val Loss: nan\n",
      "Epoch [1875/10000] Train Loss: 0.010887 Val Loss: nan\n",
      "Epoch [1876/10000] Train Loss: 0.010711 Val Loss: nan\n",
      "Epoch [1877/10000] Train Loss: 0.010709 Val Loss: nan\n",
      "Epoch [1878/10000] Train Loss: 0.010982 Val Loss: nan\n",
      "Epoch [1879/10000] Train Loss: 0.010871 Val Loss: nan\n",
      "Epoch [1880/10000] Train Loss: 0.010976 Val Loss: nan\n",
      "Epoch [1881/10000] Train Loss: 0.010820 Val Loss: nan\n",
      "Epoch [1882/10000] Train Loss: 0.010923 Val Loss: nan\n",
      "Epoch [1883/10000] Train Loss: 0.010976 Val Loss: nan\n",
      "Epoch [1884/10000] Train Loss: 0.011032 Val Loss: nan\n",
      "Epoch [1885/10000] Train Loss: 0.010710 Val Loss: nan\n",
      "Epoch [1886/10000] Train Loss: 0.010943 Val Loss: nan\n",
      "Epoch [1887/10000] Train Loss: 0.010794 Val Loss: nan\n",
      "Epoch [1888/10000] Train Loss: 0.011074 Val Loss: nan\n",
      "Epoch [1889/10000] Train Loss: 0.010861 Val Loss: nan\n",
      "Epoch [1890/10000] Train Loss: 0.010793 Val Loss: nan\n",
      "Epoch [1891/10000] Train Loss: 0.011258 Val Loss: nan\n",
      "Epoch [1892/10000] Train Loss: 0.011110 Val Loss: nan\n",
      "Epoch [1893/10000] Train Loss: 0.010684 Val Loss: nan\n",
      "Epoch [1894/10000] Train Loss: 0.011005 Val Loss: nan\n",
      "Epoch [1895/10000] Train Loss: 0.010991 Val Loss: nan\n",
      "Epoch [1896/10000] Train Loss: 0.010837 Val Loss: nan\n",
      "Epoch [1897/10000] Train Loss: 0.010948 Val Loss: nan\n",
      "Epoch [1898/10000] Train Loss: 0.010896 Val Loss: nan\n",
      "Epoch [1899/10000] Train Loss: 0.010764 Val Loss: nan\n",
      "Epoch [1900/10000] Train Loss: 0.010961 Val Loss: nan\n",
      "Epoch [1901/10000] Train Loss: 0.010749 Val Loss: nan\n",
      "Epoch [1902/10000] Train Loss: 0.010830 Val Loss: nan\n",
      "Epoch [1903/10000] Train Loss: 0.010773 Val Loss: nan\n",
      "Epoch [1904/10000] Train Loss: 0.010725 Val Loss: nan\n",
      "Epoch [1905/10000] Train Loss: 0.010892 Val Loss: nan\n",
      "Epoch [1906/10000] Train Loss: 0.010624 Val Loss: nan\n",
      "Epoch [1907/10000] Train Loss: 0.010669 Val Loss: nan\n",
      "Epoch [1908/10000] Train Loss: 0.010693 Val Loss: nan\n",
      "Epoch [1909/10000] Train Loss: 0.010737 Val Loss: nan\n",
      "Epoch [1910/10000] Train Loss: 0.010857 Val Loss: nan\n",
      "Epoch [1911/10000] Train Loss: 0.010679 Val Loss: nan\n",
      "Epoch [1912/10000] Train Loss: 0.010644 Val Loss: nan\n",
      "Epoch [1913/10000] Train Loss: 0.010609 Val Loss: nan\n",
      "Epoch [1914/10000] Train Loss: 0.010624 Val Loss: nan\n",
      "Epoch [1915/10000] Train Loss: 0.010721 Val Loss: nan\n",
      "Epoch [1916/10000] Train Loss: 0.010800 Val Loss: nan\n",
      "Epoch [1917/10000] Train Loss: 0.010871 Val Loss: nan\n",
      "Epoch [1918/10000] Train Loss: 0.010716 Val Loss: nan\n",
      "Epoch [1919/10000] Train Loss: 0.010582 Val Loss: nan\n",
      "Epoch [1920/10000] Train Loss: 0.010749 Val Loss: nan\n",
      "Epoch [1921/10000] Train Loss: 0.010606 Val Loss: nan\n",
      "Epoch [1922/10000] Train Loss: 0.010929 Val Loss: nan\n",
      "Epoch [1923/10000] Train Loss: 0.010768 Val Loss: nan\n",
      "Epoch [1924/10000] Train Loss: 0.010558 Val Loss: nan\n",
      "Epoch [1925/10000] Train Loss: 0.010946 Val Loss: nan\n",
      "Epoch [1926/10000] Train Loss: 0.010652 Val Loss: nan\n",
      "Epoch [1927/10000] Train Loss: 0.010553 Val Loss: nan\n",
      "Epoch [1928/10000] Train Loss: 0.010605 Val Loss: nan\n",
      "Epoch [1929/10000] Train Loss: 0.010632 Val Loss: nan\n",
      "Epoch [1930/10000] Train Loss: 0.010737 Val Loss: nan\n",
      "Epoch [1931/10000] Train Loss: 0.010855 Val Loss: nan\n",
      "Epoch [1932/10000] Train Loss: 0.010586 Val Loss: nan\n",
      "Epoch [1933/10000] Train Loss: 0.010902 Val Loss: nan\n",
      "Epoch [1934/10000] Train Loss: 0.010927 Val Loss: nan\n",
      "Epoch [1935/10000] Train Loss: 0.010511 Val Loss: nan\n",
      "Epoch [1936/10000] Train Loss: 0.010696 Val Loss: nan\n",
      "Epoch [1937/10000] Train Loss: 0.010694 Val Loss: nan\n",
      "Epoch [1938/10000] Train Loss: 0.010644 Val Loss: nan\n",
      "Epoch [1939/10000] Train Loss: 0.010513 Val Loss: nan\n",
      "Epoch [1940/10000] Train Loss: 0.010834 Val Loss: nan\n",
      "Epoch [1941/10000] Train Loss: 0.010549 Val Loss: nan\n",
      "Epoch [1942/10000] Train Loss: 0.010864 Val Loss: nan\n",
      "Epoch [1943/10000] Train Loss: 0.010569 Val Loss: nan\n",
      "Epoch [1944/10000] Train Loss: 0.010912 Val Loss: nan\n",
      "Epoch [1945/10000] Train Loss: 0.010853 Val Loss: nan\n",
      "Epoch [1946/10000] Train Loss: 0.010508 Val Loss: nan\n",
      "Epoch [1947/10000] Train Loss: 0.010498 Val Loss: nan\n",
      "Epoch [1948/10000] Train Loss: 0.010594 Val Loss: nan\n",
      "Epoch [1949/10000] Train Loss: 0.010654 Val Loss: nan\n",
      "Epoch [1950/10000] Train Loss: 0.010623 Val Loss: nan\n",
      "Epoch [1951/10000] Train Loss: 0.010624 Val Loss: nan\n",
      "Epoch [1952/10000] Train Loss: 0.010635 Val Loss: nan\n",
      "Epoch [1953/10000] Train Loss: 0.010603 Val Loss: nan\n",
      "Epoch [1954/10000] Train Loss: 0.010769 Val Loss: nan\n",
      "Epoch [1955/10000] Train Loss: 0.010489 Val Loss: nan\n",
      "Epoch [1956/10000] Train Loss: 0.010890 Val Loss: nan\n",
      "Epoch [1957/10000] Train Loss: 0.010621 Val Loss: nan\n",
      "Epoch [1958/10000] Train Loss: 0.010546 Val Loss: nan\n",
      "Epoch [1959/10000] Train Loss: 0.010622 Val Loss: nan\n",
      "Epoch [1960/10000] Train Loss: 0.010600 Val Loss: nan\n",
      "Epoch [1961/10000] Train Loss: 0.010488 Val Loss: nan\n",
      "Epoch [1962/10000] Train Loss: 0.010910 Val Loss: nan\n",
      "Epoch [1963/10000] Train Loss: 0.010579 Val Loss: nan\n",
      "Epoch [1964/10000] Train Loss: 0.010643 Val Loss: nan\n",
      "Epoch [1965/10000] Train Loss: 0.010541 Val Loss: nan\n",
      "Epoch [1966/10000] Train Loss: 0.010592 Val Loss: nan\n",
      "Epoch [1967/10000] Train Loss: 0.010590 Val Loss: nan\n",
      "Epoch [1968/10000] Train Loss: 0.010551 Val Loss: nan\n",
      "Epoch [1969/10000] Train Loss: 0.010555 Val Loss: nan\n",
      "Epoch [1970/10000] Train Loss: 0.010817 Val Loss: nan\n",
      "Epoch [1971/10000] Train Loss: 0.010580 Val Loss: nan\n",
      "Epoch [1972/10000] Train Loss: 0.010553 Val Loss: nan\n",
      "Epoch [1973/10000] Train Loss: 0.010440 Val Loss: nan\n",
      "Epoch [1974/10000] Train Loss: 0.010845 Val Loss: nan\n",
      "Epoch [1975/10000] Train Loss: 0.010649 Val Loss: nan\n",
      "Epoch [1976/10000] Train Loss: 0.010453 Val Loss: nan\n",
      "Epoch [1977/10000] Train Loss: 0.010419 Val Loss: nan\n",
      "Epoch [1978/10000] Train Loss: 0.010496 Val Loss: nan\n",
      "Epoch [1979/10000] Train Loss: 0.010790 Val Loss: nan\n",
      "Epoch [1980/10000] Train Loss: 0.010378 Val Loss: nan\n",
      "Epoch [1981/10000] Train Loss: 0.010636 Val Loss: nan\n",
      "Epoch [1982/10000] Train Loss: 0.010538 Val Loss: nan\n",
      "Epoch [1983/10000] Train Loss: 0.010399 Val Loss: nan\n",
      "Epoch [1984/10000] Train Loss: 0.010673 Val Loss: nan\n",
      "Epoch [1985/10000] Train Loss: 0.011027 Val Loss: nan\n",
      "Epoch [1986/10000] Train Loss: 0.010822 Val Loss: nan\n",
      "Epoch [1987/10000] Train Loss: 0.010422 Val Loss: nan\n",
      "Epoch [1988/10000] Train Loss: 0.010630 Val Loss: nan\n",
      "Epoch [1989/10000] Train Loss: 0.010559 Val Loss: nan\n",
      "Epoch [1990/10000] Train Loss: 0.010773 Val Loss: nan\n",
      "Epoch [1991/10000] Train Loss: 0.010581 Val Loss: nan\n",
      "Epoch [1992/10000] Train Loss: 0.010416 Val Loss: nan\n",
      "Epoch [1993/10000] Train Loss: 0.010485 Val Loss: nan\n",
      "Epoch [1994/10000] Train Loss: 0.010462 Val Loss: nan\n",
      "Epoch [1995/10000] Train Loss: 0.010351 Val Loss: nan\n",
      "Epoch [1996/10000] Train Loss: 0.010511 Val Loss: nan\n",
      "Epoch [1997/10000] Train Loss: 0.010445 Val Loss: nan\n",
      "Epoch [1998/10000] Train Loss: 0.010579 Val Loss: nan\n",
      "Epoch [1999/10000] Train Loss: 0.010421 Val Loss: nan\n",
      "Epoch [2000/10000] Train Loss: 0.010363 Val Loss: nan\n",
      "Epoch [2001/10000] Train Loss: 0.010821 Val Loss: nan\n",
      "Epoch [2002/10000] Train Loss: 0.010506 Val Loss: nan\n",
      "Epoch [2003/10000] Train Loss: 0.010694 Val Loss: nan\n",
      "Epoch [2004/10000] Train Loss: 0.010602 Val Loss: nan\n",
      "Epoch [2005/10000] Train Loss: 0.010678 Val Loss: nan\n",
      "Epoch [2006/10000] Train Loss: 0.010339 Val Loss: nan\n",
      "Epoch [2007/10000] Train Loss: 0.010351 Val Loss: nan\n",
      "Epoch [2008/10000] Train Loss: 0.010388 Val Loss: nan\n",
      "Epoch [2009/10000] Train Loss: 0.010948 Val Loss: nan\n",
      "Epoch [2010/10000] Train Loss: 0.010401 Val Loss: nan\n",
      "Epoch [2011/10000] Train Loss: 0.010458 Val Loss: nan\n",
      "Epoch [2012/10000] Train Loss: 0.010678 Val Loss: nan\n",
      "Epoch [2013/10000] Train Loss: 0.010640 Val Loss: nan\n",
      "Epoch [2014/10000] Train Loss: 0.010331 Val Loss: nan\n",
      "Epoch [2015/10000] Train Loss: 0.010470 Val Loss: nan\n",
      "Epoch [2016/10000] Train Loss: 0.010534 Val Loss: nan\n",
      "Epoch [2017/10000] Train Loss: 0.010542 Val Loss: nan\n",
      "Epoch [2018/10000] Train Loss: 0.010308 Val Loss: nan\n",
      "Epoch [2019/10000] Train Loss: 0.010246 Val Loss: nan\n",
      "Epoch [2020/10000] Train Loss: 0.010298 Val Loss: nan\n",
      "Epoch [2021/10000] Train Loss: 0.010417 Val Loss: nan\n",
      "Epoch [2022/10000] Train Loss: 0.010445 Val Loss: nan\n",
      "Epoch [2023/10000] Train Loss: 0.010315 Val Loss: nan\n",
      "Epoch [2024/10000] Train Loss: 0.010257 Val Loss: nan\n",
      "Epoch [2025/10000] Train Loss: 0.010624 Val Loss: nan\n",
      "Epoch [2026/10000] Train Loss: 0.010702 Val Loss: nan\n",
      "Epoch [2027/10000] Train Loss: 0.010346 Val Loss: nan\n",
      "Epoch [2028/10000] Train Loss: 0.010380 Val Loss: nan\n",
      "Epoch [2029/10000] Train Loss: 0.010461 Val Loss: nan\n",
      "Epoch [2030/10000] Train Loss: 0.010452 Val Loss: nan\n",
      "Epoch [2031/10000] Train Loss: 0.010436 Val Loss: nan\n",
      "Epoch [2032/10000] Train Loss: 0.010291 Val Loss: nan\n",
      "Epoch [2033/10000] Train Loss: 0.010408 Val Loss: nan\n",
      "Epoch [2034/10000] Train Loss: 0.010252 Val Loss: nan\n",
      "Epoch [2035/10000] Train Loss: 0.010254 Val Loss: nan\n",
      "Epoch [2036/10000] Train Loss: 0.010393 Val Loss: nan\n",
      "Epoch [2037/10000] Train Loss: 0.010486 Val Loss: nan\n",
      "Epoch [2038/10000] Train Loss: 0.010362 Val Loss: nan\n",
      "Epoch [2039/10000] Train Loss: 0.010344 Val Loss: nan\n",
      "Epoch [2040/10000] Train Loss: 0.010401 Val Loss: nan\n",
      "Epoch [2041/10000] Train Loss: 0.010458 Val Loss: nan\n",
      "Epoch [2042/10000] Train Loss: 0.010333 Val Loss: nan\n",
      "Epoch [2043/10000] Train Loss: 0.010631 Val Loss: nan\n",
      "Epoch [2044/10000] Train Loss: 0.010581 Val Loss: nan\n",
      "Epoch [2045/10000] Train Loss: 0.010311 Val Loss: nan\n",
      "Epoch [2046/10000] Train Loss: 0.010187 Val Loss: nan\n",
      "Epoch [2047/10000] Train Loss: 0.010572 Val Loss: nan\n",
      "Epoch [2048/10000] Train Loss: 0.010419 Val Loss: nan\n",
      "Epoch [2049/10000] Train Loss: 0.010430 Val Loss: nan\n",
      "Epoch [2050/10000] Train Loss: 0.010172 Val Loss: nan\n",
      "Epoch [2051/10000] Train Loss: 0.010334 Val Loss: nan\n",
      "Epoch [2052/10000] Train Loss: 0.010515 Val Loss: nan\n",
      "Epoch [2053/10000] Train Loss: 0.010209 Val Loss: nan\n",
      "Epoch [2054/10000] Train Loss: 0.010305 Val Loss: nan\n",
      "Epoch [2055/10000] Train Loss: 0.010242 Val Loss: nan\n",
      "Epoch [2056/10000] Train Loss: 0.010300 Val Loss: nan\n",
      "Epoch [2057/10000] Train Loss: 0.010221 Val Loss: nan\n",
      "Epoch [2058/10000] Train Loss: 0.010321 Val Loss: nan\n",
      "Epoch [2059/10000] Train Loss: 0.010367 Val Loss: nan\n",
      "Epoch [2060/10000] Train Loss: 0.010416 Val Loss: nan\n",
      "Epoch [2061/10000] Train Loss: 0.010507 Val Loss: nan\n",
      "Epoch [2062/10000] Train Loss: 0.010148 Val Loss: nan\n",
      "Epoch [2063/10000] Train Loss: 0.010229 Val Loss: nan\n",
      "Epoch [2064/10000] Train Loss: 0.010577 Val Loss: nan\n",
      "Epoch [2065/10000] Train Loss: 0.010363 Val Loss: nan\n",
      "Epoch [2066/10000] Train Loss: 0.010149 Val Loss: nan\n",
      "Epoch [2067/10000] Train Loss: 0.010280 Val Loss: nan\n",
      "Epoch [2068/10000] Train Loss: 0.010527 Val Loss: nan\n",
      "Epoch [2069/10000] Train Loss: 0.010359 Val Loss: nan\n",
      "Epoch [2070/10000] Train Loss: 0.010396 Val Loss: nan\n",
      "Epoch [2071/10000] Train Loss: 0.010404 Val Loss: nan\n",
      "Epoch [2072/10000] Train Loss: 0.010196 Val Loss: nan\n",
      "Epoch [2073/10000] Train Loss: 0.010199 Val Loss: nan\n",
      "Epoch [2074/10000] Train Loss: 0.010659 Val Loss: nan\n",
      "Epoch [2075/10000] Train Loss: 0.010445 Val Loss: nan\n",
      "Epoch [2076/10000] Train Loss: 0.010262 Val Loss: nan\n",
      "Epoch [2077/10000] Train Loss: 0.010254 Val Loss: nan\n",
      "Epoch [2078/10000] Train Loss: 0.010145 Val Loss: nan\n",
      "Epoch [2079/10000] Train Loss: 0.010108 Val Loss: nan\n",
      "Epoch [2080/10000] Train Loss: 0.010490 Val Loss: nan\n",
      "Epoch [2081/10000] Train Loss: 0.010274 Val Loss: nan\n",
      "Epoch [2082/10000] Train Loss: 0.010291 Val Loss: nan\n",
      "Epoch [2083/10000] Train Loss: 0.010259 Val Loss: nan\n",
      "Epoch [2084/10000] Train Loss: 0.010391 Val Loss: nan\n",
      "Epoch [2085/10000] Train Loss: 0.010236 Val Loss: nan\n",
      "Epoch [2086/10000] Train Loss: 0.010276 Val Loss: nan\n",
      "Epoch [2087/10000] Train Loss: 0.010180 Val Loss: nan\n",
      "Epoch [2088/10000] Train Loss: 0.010640 Val Loss: nan\n",
      "Epoch [2089/10000] Train Loss: 0.010318 Val Loss: nan\n",
      "Epoch [2090/10000] Train Loss: 0.010223 Val Loss: nan\n",
      "Epoch [2091/10000] Train Loss: 0.010186 Val Loss: nan\n",
      "Epoch [2092/10000] Train Loss: 0.010428 Val Loss: nan\n",
      "Epoch [2093/10000] Train Loss: 0.010234 Val Loss: nan\n",
      "Epoch [2094/10000] Train Loss: 0.010399 Val Loss: nan\n",
      "Epoch [2095/10000] Train Loss: 0.010640 Val Loss: nan\n",
      "Epoch [2096/10000] Train Loss: 0.010067 Val Loss: nan\n",
      "Epoch [2097/10000] Train Loss: 0.010090 Val Loss: nan\n",
      "Epoch [2098/10000] Train Loss: 0.010145 Val Loss: nan\n",
      "Epoch [2099/10000] Train Loss: 0.010335 Val Loss: nan\n",
      "Epoch [2100/10000] Train Loss: 0.010335 Val Loss: nan\n",
      "Epoch [2101/10000] Train Loss: 0.010199 Val Loss: nan\n",
      "Epoch [2102/10000] Train Loss: 0.010226 Val Loss: nan\n",
      "Epoch [2103/10000] Train Loss: 0.010229 Val Loss: nan\n",
      "Epoch [2104/10000] Train Loss: 0.010108 Val Loss: nan\n",
      "Epoch [2105/10000] Train Loss: 0.010512 Val Loss: nan\n",
      "Epoch [2106/10000] Train Loss: 0.010310 Val Loss: nan\n",
      "Epoch [2107/10000] Train Loss: 0.010266 Val Loss: nan\n",
      "Epoch [2108/10000] Train Loss: 0.010241 Val Loss: nan\n",
      "Epoch [2109/10000] Train Loss: 0.010341 Val Loss: nan\n",
      "Epoch [2110/10000] Train Loss: 0.010145 Val Loss: nan\n",
      "Epoch [2111/10000] Train Loss: 0.010040 Val Loss: nan\n",
      "Epoch [2112/10000] Train Loss: 0.010319 Val Loss: nan\n",
      "Epoch [2113/10000] Train Loss: 0.010039 Val Loss: nan\n",
      "Epoch [2114/10000] Train Loss: 0.010137 Val Loss: nan\n",
      "Epoch [2115/10000] Train Loss: 0.010121 Val Loss: nan\n",
      "Epoch [2116/10000] Train Loss: 0.010171 Val Loss: nan\n",
      "Epoch [2117/10000] Train Loss: 0.010109 Val Loss: nan\n",
      "Epoch [2118/10000] Train Loss: 0.010245 Val Loss: nan\n",
      "Epoch [2119/10000] Train Loss: 0.010487 Val Loss: nan\n",
      "Epoch [2120/10000] Train Loss: 0.010145 Val Loss: nan\n",
      "Epoch [2121/10000] Train Loss: 0.010350 Val Loss: nan\n",
      "Epoch [2122/10000] Train Loss: 0.010235 Val Loss: nan\n",
      "Epoch [2123/10000] Train Loss: 0.010302 Val Loss: nan\n",
      "Epoch [2124/10000] Train Loss: 0.010569 Val Loss: nan\n",
      "Epoch [2125/10000] Train Loss: 0.010317 Val Loss: nan\n",
      "Epoch [2126/10000] Train Loss: 0.010175 Val Loss: nan\n",
      "Epoch [2127/10000] Train Loss: 0.010121 Val Loss: nan\n",
      "Epoch [2128/10000] Train Loss: 0.010099 Val Loss: nan\n",
      "Epoch [2129/10000] Train Loss: 0.010357 Val Loss: nan\n",
      "Epoch [2130/10000] Train Loss: 0.010240 Val Loss: nan\n",
      "Epoch [2131/10000] Train Loss: 0.010104 Val Loss: nan\n",
      "Epoch [2132/10000] Train Loss: 0.010348 Val Loss: nan\n",
      "Epoch [2133/10000] Train Loss: 0.010049 Val Loss: nan\n",
      "Epoch [2134/10000] Train Loss: 0.010195 Val Loss: nan\n",
      "Epoch [2135/10000] Train Loss: 0.010049 Val Loss: nan\n",
      "Epoch [2136/10000] Train Loss: 0.010232 Val Loss: nan\n",
      "Epoch [2137/10000] Train Loss: 0.010365 Val Loss: nan\n",
      "Epoch [2138/10000] Train Loss: 0.010134 Val Loss: nan\n",
      "Epoch [2139/10000] Train Loss: 0.010010 Val Loss: nan\n",
      "Epoch [2140/10000] Train Loss: 0.010110 Val Loss: nan\n",
      "Epoch [2141/10000] Train Loss: 0.010329 Val Loss: nan\n",
      "Epoch [2142/10000] Train Loss: 0.010143 Val Loss: nan\n",
      "Epoch [2143/10000] Train Loss: 0.010287 Val Loss: nan\n",
      "Epoch [2144/10000] Train Loss: 0.010301 Val Loss: nan\n",
      "Epoch [2145/10000] Train Loss: 0.010046 Val Loss: nan\n",
      "Epoch [2146/10000] Train Loss: 0.010005 Val Loss: nan\n",
      "Epoch [2147/10000] Train Loss: 0.010188 Val Loss: nan\n",
      "Epoch [2148/10000] Train Loss: 0.010201 Val Loss: nan\n",
      "Epoch [2149/10000] Train Loss: 0.009961 Val Loss: nan\n",
      "Epoch [2150/10000] Train Loss: 0.010020 Val Loss: nan\n",
      "Epoch [2151/10000] Train Loss: 0.010159 Val Loss: nan\n",
      "Epoch [2152/10000] Train Loss: 0.010055 Val Loss: nan\n",
      "Epoch [2153/10000] Train Loss: 0.010043 Val Loss: nan\n",
      "Epoch [2154/10000] Train Loss: 0.010038 Val Loss: nan\n",
      "Epoch [2155/10000] Train Loss: 0.010258 Val Loss: nan\n",
      "Epoch [2156/10000] Train Loss: 0.009996 Val Loss: nan\n",
      "Epoch [2157/10000] Train Loss: 0.010374 Val Loss: nan\n",
      "Epoch [2158/10000] Train Loss: 0.010117 Val Loss: nan\n",
      "Epoch [2159/10000] Train Loss: 0.009901 Val Loss: nan\n",
      "Epoch [2160/10000] Train Loss: 0.009970 Val Loss: nan\n",
      "Epoch [2161/10000] Train Loss: 0.009958 Val Loss: nan\n",
      "Epoch [2162/10000] Train Loss: 0.009935 Val Loss: nan\n",
      "Epoch [2163/10000] Train Loss: 0.010093 Val Loss: nan\n",
      "Epoch [2164/10000] Train Loss: 0.010071 Val Loss: nan\n",
      "Epoch [2165/10000] Train Loss: 0.009953 Val Loss: nan\n",
      "Epoch [2166/10000] Train Loss: 0.010026 Val Loss: nan\n",
      "Epoch [2167/10000] Train Loss: 0.009954 Val Loss: nan\n",
      "Epoch [2168/10000] Train Loss: 0.009977 Val Loss: nan\n",
      "Epoch [2169/10000] Train Loss: 0.010158 Val Loss: nan\n",
      "Epoch [2170/10000] Train Loss: 0.010144 Val Loss: nan\n",
      "Epoch [2171/10000] Train Loss: 0.010041 Val Loss: nan\n",
      "Epoch [2172/10000] Train Loss: 0.010167 Val Loss: nan\n",
      "Epoch [2173/10000] Train Loss: 0.009979 Val Loss: nan\n",
      "Epoch [2174/10000] Train Loss: 0.009909 Val Loss: nan\n",
      "Epoch [2175/10000] Train Loss: 0.009861 Val Loss: nan\n",
      "Epoch [2176/10000] Train Loss: 0.010058 Val Loss: nan\n",
      "Epoch [2177/10000] Train Loss: 0.009922 Val Loss: nan\n",
      "Epoch [2178/10000] Train Loss: 0.009964 Val Loss: nan\n",
      "Epoch [2179/10000] Train Loss: 0.010270 Val Loss: nan\n",
      "Epoch [2180/10000] Train Loss: 0.010110 Val Loss: nan\n",
      "Epoch [2181/10000] Train Loss: 0.009912 Val Loss: nan\n",
      "Epoch [2182/10000] Train Loss: 0.010120 Val Loss: nan\n",
      "Epoch [2183/10000] Train Loss: 0.010182 Val Loss: nan\n",
      "Epoch [2184/10000] Train Loss: 0.010092 Val Loss: nan\n",
      "Epoch [2185/10000] Train Loss: 0.010246 Val Loss: nan\n",
      "Epoch [2186/10000] Train Loss: 0.010100 Val Loss: nan\n",
      "Epoch [2187/10000] Train Loss: 0.009967 Val Loss: nan\n",
      "Epoch [2188/10000] Train Loss: 0.009957 Val Loss: nan\n",
      "Epoch [2189/10000] Train Loss: 0.010186 Val Loss: nan\n",
      "Epoch [2190/10000] Train Loss: 0.010250 Val Loss: nan\n",
      "Epoch [2191/10000] Train Loss: 0.010209 Val Loss: nan\n",
      "Epoch [2192/10000] Train Loss: 0.009909 Val Loss: nan\n",
      "Epoch [2193/10000] Train Loss: 0.009881 Val Loss: nan\n",
      "Epoch [2194/10000] Train Loss: 0.010461 Val Loss: nan\n",
      "Epoch [2195/10000] Train Loss: 0.009919 Val Loss: nan\n",
      "Epoch [2196/10000] Train Loss: 0.010104 Val Loss: nan\n",
      "Epoch [2197/10000] Train Loss: 0.009885 Val Loss: nan\n",
      "Epoch [2198/10000] Train Loss: 0.010036 Val Loss: nan\n",
      "Epoch [2199/10000] Train Loss: 0.010005 Val Loss: nan\n",
      "Epoch [2200/10000] Train Loss: 0.010051 Val Loss: nan\n",
      "Epoch [2201/10000] Train Loss: 0.009918 Val Loss: nan\n",
      "Epoch [2202/10000] Train Loss: 0.009939 Val Loss: nan\n",
      "Epoch [2203/10000] Train Loss: 0.009875 Val Loss: nan\n",
      "Epoch [2204/10000] Train Loss: 0.010012 Val Loss: nan\n",
      "Epoch [2205/10000] Train Loss: 0.010008 Val Loss: nan\n",
      "Epoch [2206/10000] Train Loss: 0.010043 Val Loss: nan\n",
      "Epoch [2207/10000] Train Loss: 0.010157 Val Loss: nan\n",
      "Epoch [2208/10000] Train Loss: 0.009877 Val Loss: nan\n",
      "Epoch [2209/10000] Train Loss: 0.010030 Val Loss: nan\n",
      "Epoch [2210/10000] Train Loss: 0.009808 Val Loss: nan\n",
      "Epoch [2211/10000] Train Loss: 0.009857 Val Loss: nan\n",
      "Epoch [2212/10000] Train Loss: 0.009973 Val Loss: nan\n",
      "Epoch [2213/10000] Train Loss: 0.009885 Val Loss: nan\n",
      "Epoch [2214/10000] Train Loss: 0.009829 Val Loss: nan\n",
      "Epoch [2215/10000] Train Loss: 0.010276 Val Loss: nan\n",
      "Epoch [2216/10000] Train Loss: 0.009899 Val Loss: nan\n",
      "Epoch [2217/10000] Train Loss: 0.009932 Val Loss: nan\n",
      "Epoch [2218/10000] Train Loss: 0.009816 Val Loss: nan\n",
      "Epoch [2219/10000] Train Loss: 0.010161 Val Loss: nan\n",
      "Epoch [2220/10000] Train Loss: 0.009790 Val Loss: nan\n",
      "Epoch [2221/10000] Train Loss: 0.009833 Val Loss: nan\n",
      "Epoch [2222/10000] Train Loss: 0.009935 Val Loss: nan\n",
      "Epoch [2223/10000] Train Loss: 0.009838 Val Loss: nan\n",
      "Epoch [2224/10000] Train Loss: 0.009840 Val Loss: nan\n",
      "Epoch [2225/10000] Train Loss: 0.009890 Val Loss: nan\n",
      "Epoch [2226/10000] Train Loss: 0.009916 Val Loss: nan\n",
      "Epoch [2227/10000] Train Loss: 0.009853 Val Loss: nan\n",
      "Epoch [2228/10000] Train Loss: 0.009943 Val Loss: nan\n",
      "Epoch [2229/10000] Train Loss: 0.009950 Val Loss: nan\n",
      "Epoch [2230/10000] Train Loss: 0.009839 Val Loss: nan\n",
      "Epoch [2231/10000] Train Loss: 0.009919 Val Loss: nan\n",
      "Epoch [2232/10000] Train Loss: 0.010075 Val Loss: nan\n",
      "Epoch [2233/10000] Train Loss: 0.009827 Val Loss: nan\n",
      "Epoch [2234/10000] Train Loss: 0.010076 Val Loss: nan\n",
      "Epoch [2235/10000] Train Loss: 0.010100 Val Loss: nan\n",
      "Epoch [2236/10000] Train Loss: 0.009817 Val Loss: nan\n",
      "Epoch [2237/10000] Train Loss: 0.009808 Val Loss: nan\n",
      "Epoch [2238/10000] Train Loss: 0.010100 Val Loss: nan\n",
      "Epoch [2239/10000] Train Loss: 0.010137 Val Loss: nan\n",
      "Epoch [2240/10000] Train Loss: 0.010024 Val Loss: nan\n",
      "Epoch [2241/10000] Train Loss: 0.009770 Val Loss: nan\n",
      "Epoch [2242/10000] Train Loss: 0.010189 Val Loss: nan\n",
      "Epoch [2243/10000] Train Loss: 0.010191 Val Loss: nan\n",
      "Epoch [2244/10000] Train Loss: 0.010191 Val Loss: nan\n",
      "Epoch [2245/10000] Train Loss: 0.009976 Val Loss: nan\n",
      "Epoch [2246/10000] Train Loss: 0.009736 Val Loss: nan\n",
      "Epoch [2247/10000] Train Loss: 0.009897 Val Loss: nan\n",
      "Epoch [2248/10000] Train Loss: 0.009783 Val Loss: nan\n",
      "Epoch [2249/10000] Train Loss: 0.009919 Val Loss: nan\n",
      "Epoch [2250/10000] Train Loss: 0.010189 Val Loss: nan\n",
      "Epoch [2251/10000] Train Loss: 0.009943 Val Loss: nan\n",
      "Epoch [2252/10000] Train Loss: 0.009968 Val Loss: nan\n",
      "Epoch [2253/10000] Train Loss: 0.009940 Val Loss: nan\n",
      "Epoch [2254/10000] Train Loss: 0.010078 Val Loss: nan\n",
      "Epoch [2255/10000] Train Loss: 0.010030 Val Loss: nan\n",
      "Epoch [2256/10000] Train Loss: 0.009810 Val Loss: nan\n",
      "Epoch [2257/10000] Train Loss: 0.009776 Val Loss: nan\n",
      "Epoch [2258/10000] Train Loss: 0.009859 Val Loss: nan\n",
      "Epoch [2259/10000] Train Loss: 0.009910 Val Loss: nan\n",
      "Epoch [2260/10000] Train Loss: 0.010115 Val Loss: nan\n",
      "Epoch [2261/10000] Train Loss: 0.010043 Val Loss: nan\n",
      "Epoch [2262/10000] Train Loss: 0.009834 Val Loss: nan\n",
      "Epoch [2263/10000] Train Loss: 0.009926 Val Loss: nan\n",
      "Epoch [2264/10000] Train Loss: 0.009922 Val Loss: nan\n",
      "Epoch [2265/10000] Train Loss: 0.009860 Val Loss: nan\n",
      "Epoch [2266/10000] Train Loss: 0.009875 Val Loss: nan\n",
      "Epoch [2267/10000] Train Loss: 0.009727 Val Loss: nan\n",
      "Epoch [2268/10000] Train Loss: 0.010046 Val Loss: nan\n",
      "Epoch [2269/10000] Train Loss: 0.009701 Val Loss: nan\n",
      "Epoch [2270/10000] Train Loss: 0.010018 Val Loss: nan\n",
      "Epoch [2271/10000] Train Loss: 0.009721 Val Loss: nan\n",
      "Epoch [2272/10000] Train Loss: 0.009842 Val Loss: nan\n",
      "Epoch [2273/10000] Train Loss: 0.009856 Val Loss: nan\n",
      "Epoch [2274/10000] Train Loss: 0.009706 Val Loss: nan\n",
      "Epoch [2275/10000] Train Loss: 0.009704 Val Loss: nan\n",
      "Epoch [2276/10000] Train Loss: 0.009835 Val Loss: nan\n",
      "Epoch [2277/10000] Train Loss: 0.009884 Val Loss: nan\n",
      "Epoch [2278/10000] Train Loss: 0.009692 Val Loss: nan\n",
      "Epoch [2279/10000] Train Loss: 0.009661 Val Loss: nan\n",
      "Epoch [2280/10000] Train Loss: 0.009804 Val Loss: nan\n",
      "Epoch [2281/10000] Train Loss: 0.009904 Val Loss: nan\n",
      "Epoch [2282/10000] Train Loss: 0.009913 Val Loss: nan\n",
      "Epoch [2283/10000] Train Loss: 0.009815 Val Loss: nan\n",
      "Epoch [2284/10000] Train Loss: 0.010006 Val Loss: nan\n",
      "Epoch [2285/10000] Train Loss: 0.009748 Val Loss: nan\n",
      "Epoch [2286/10000] Train Loss: 0.009678 Val Loss: nan\n",
      "Epoch [2287/10000] Train Loss: 0.009812 Val Loss: nan\n",
      "Epoch [2288/10000] Train Loss: 0.009659 Val Loss: nan\n",
      "Epoch [2289/10000] Train Loss: 0.009736 Val Loss: nan\n",
      "Epoch [2290/10000] Train Loss: 0.009857 Val Loss: nan\n",
      "Epoch [2291/10000] Train Loss: 0.009714 Val Loss: nan\n",
      "Epoch [2292/10000] Train Loss: 0.009725 Val Loss: nan\n",
      "Epoch [2293/10000] Train Loss: 0.009745 Val Loss: nan\n",
      "Epoch [2294/10000] Train Loss: 0.010010 Val Loss: nan\n",
      "Epoch [2295/10000] Train Loss: 0.009825 Val Loss: nan\n",
      "Epoch [2296/10000] Train Loss: 0.009795 Val Loss: nan\n",
      "Epoch [2297/10000] Train Loss: 0.009630 Val Loss: nan\n",
      "Epoch [2298/10000] Train Loss: 0.009841 Val Loss: nan\n",
      "Epoch [2299/10000] Train Loss: 0.009993 Val Loss: nan\n",
      "Epoch [2300/10000] Train Loss: 0.009786 Val Loss: nan\n",
      "Epoch [2301/10000] Train Loss: 0.010029 Val Loss: nan\n",
      "Epoch [2302/10000] Train Loss: 0.010186 Val Loss: nan\n",
      "Epoch [2303/10000] Train Loss: 0.009825 Val Loss: nan\n",
      "Epoch [2304/10000] Train Loss: 0.009913 Val Loss: nan\n",
      "Epoch [2305/10000] Train Loss: 0.010029 Val Loss: nan\n",
      "Epoch [2306/10000] Train Loss: 0.009737 Val Loss: nan\n",
      "Epoch [2307/10000] Train Loss: 0.009693 Val Loss: nan\n",
      "Epoch [2308/10000] Train Loss: 0.009688 Val Loss: nan\n",
      "Epoch [2309/10000] Train Loss: 0.009645 Val Loss: nan\n",
      "Epoch [2310/10000] Train Loss: 0.009969 Val Loss: nan\n",
      "Epoch [2311/10000] Train Loss: 0.009723 Val Loss: nan\n",
      "Epoch [2312/10000] Train Loss: 0.009661 Val Loss: nan\n",
      "Epoch [2313/10000] Train Loss: 0.009755 Val Loss: nan\n",
      "Epoch [2314/10000] Train Loss: 0.009810 Val Loss: nan\n",
      "Epoch [2315/10000] Train Loss: 0.009774 Val Loss: nan\n",
      "Epoch [2316/10000] Train Loss: 0.009656 Val Loss: nan\n",
      "Epoch [2317/10000] Train Loss: 0.009644 Val Loss: nan\n",
      "Epoch [2318/10000] Train Loss: 0.009617 Val Loss: nan\n",
      "Epoch [2319/10000] Train Loss: 0.009653 Val Loss: nan\n",
      "Epoch [2320/10000] Train Loss: 0.010073 Val Loss: nan\n",
      "Epoch [2321/10000] Train Loss: 0.009896 Val Loss: nan\n",
      "Epoch [2322/10000] Train Loss: 0.009847 Val Loss: nan\n",
      "Epoch [2323/10000] Train Loss: 0.009714 Val Loss: nan\n",
      "Epoch [2324/10000] Train Loss: 0.009789 Val Loss: nan\n",
      "Epoch [2325/10000] Train Loss: 0.009671 Val Loss: nan\n",
      "Epoch [2326/10000] Train Loss: 0.009845 Val Loss: nan\n",
      "Epoch [2327/10000] Train Loss: 0.009596 Val Loss: nan\n",
      "Epoch [2328/10000] Train Loss: 0.009730 Val Loss: nan\n",
      "Epoch [2329/10000] Train Loss: 0.009979 Val Loss: nan\n",
      "Epoch [2330/10000] Train Loss: 0.009895 Val Loss: nan\n",
      "Epoch [2331/10000] Train Loss: 0.009635 Val Loss: nan\n",
      "Epoch [2332/10000] Train Loss: 0.009583 Val Loss: nan\n",
      "Epoch [2333/10000] Train Loss: 0.009888 Val Loss: nan\n",
      "Epoch [2334/10000] Train Loss: 0.009628 Val Loss: nan\n",
      "Epoch [2335/10000] Train Loss: 0.009593 Val Loss: nan\n",
      "Epoch [2336/10000] Train Loss: 0.009601 Val Loss: nan\n",
      "Epoch [2337/10000] Train Loss: 0.009911 Val Loss: nan\n",
      "Epoch [2338/10000] Train Loss: 0.009893 Val Loss: nan\n",
      "Epoch [2339/10000] Train Loss: 0.010150 Val Loss: nan\n",
      "Epoch [2340/10000] Train Loss: 0.009959 Val Loss: nan\n",
      "Epoch [2341/10000] Train Loss: 0.009566 Val Loss: nan\n",
      "Epoch [2342/10000] Train Loss: 0.009781 Val Loss: nan\n",
      "Epoch [2343/10000] Train Loss: 0.009578 Val Loss: nan\n",
      "Epoch [2344/10000] Train Loss: 0.009761 Val Loss: nan\n",
      "Epoch [2345/10000] Train Loss: 0.009751 Val Loss: nan\n",
      "Epoch [2346/10000] Train Loss: 0.009842 Val Loss: nan\n",
      "Epoch [2347/10000] Train Loss: 0.009615 Val Loss: nan\n",
      "Epoch [2348/10000] Train Loss: 0.009656 Val Loss: nan\n",
      "Epoch [2349/10000] Train Loss: 0.009604 Val Loss: nan\n",
      "Epoch [2350/10000] Train Loss: 0.009568 Val Loss: nan\n",
      "Epoch [2351/10000] Train Loss: 0.009691 Val Loss: nan\n",
      "Epoch [2352/10000] Train Loss: 0.009577 Val Loss: nan\n",
      "Epoch [2353/10000] Train Loss: 0.009852 Val Loss: nan\n",
      "Epoch [2354/10000] Train Loss: 0.009746 Val Loss: nan\n",
      "Epoch [2355/10000] Train Loss: 0.009599 Val Loss: nan\n",
      "Epoch [2356/10000] Train Loss: 0.009601 Val Loss: nan\n",
      "Epoch [2357/10000] Train Loss: 0.009739 Val Loss: nan\n",
      "Epoch [2358/10000] Train Loss: 0.009884 Val Loss: nan\n",
      "Epoch [2359/10000] Train Loss: 0.009858 Val Loss: nan\n",
      "Epoch [2360/10000] Train Loss: 0.009900 Val Loss: nan\n",
      "Epoch [2361/10000] Train Loss: 0.009702 Val Loss: nan\n",
      "Epoch [2362/10000] Train Loss: 0.009702 Val Loss: nan\n",
      "Epoch [2363/10000] Train Loss: 0.009583 Val Loss: nan\n",
      "Epoch [2364/10000] Train Loss: 0.009797 Val Loss: nan\n",
      "Epoch [2365/10000] Train Loss: 0.009804 Val Loss: nan\n",
      "Epoch [2366/10000] Train Loss: 0.009695 Val Loss: nan\n",
      "Epoch [2367/10000] Train Loss: 0.009551 Val Loss: nan\n",
      "Epoch [2368/10000] Train Loss: 0.009496 Val Loss: nan\n",
      "Epoch [2369/10000] Train Loss: 0.009649 Val Loss: nan\n",
      "Epoch [2370/10000] Train Loss: 0.009588 Val Loss: nan\n",
      "Epoch [2371/10000] Train Loss: 0.009842 Val Loss: nan\n",
      "Epoch [2372/10000] Train Loss: 0.009628 Val Loss: nan\n",
      "Epoch [2373/10000] Train Loss: 0.009648 Val Loss: nan\n",
      "Epoch [2374/10000] Train Loss: 0.009784 Val Loss: nan\n",
      "Epoch [2375/10000] Train Loss: 0.009489 Val Loss: nan\n",
      "Epoch [2376/10000] Train Loss: 0.009544 Val Loss: nan\n",
      "Epoch [2377/10000] Train Loss: 0.009572 Val Loss: nan\n",
      "Epoch [2378/10000] Train Loss: 0.009754 Val Loss: nan\n",
      "Epoch [2379/10000] Train Loss: 0.009583 Val Loss: nan\n",
      "Epoch [2380/10000] Train Loss: 0.009652 Val Loss: nan\n",
      "Epoch [2381/10000] Train Loss: 0.009702 Val Loss: nan\n",
      "Epoch [2382/10000] Train Loss: 0.009570 Val Loss: nan\n",
      "Epoch [2383/10000] Train Loss: 0.009717 Val Loss: nan\n",
      "Epoch [2384/10000] Train Loss: 0.009740 Val Loss: nan\n",
      "Epoch [2385/10000] Train Loss: 0.009505 Val Loss: nan\n",
      "Epoch [2386/10000] Train Loss: 0.009755 Val Loss: nan\n",
      "Epoch [2387/10000] Train Loss: 0.009492 Val Loss: nan\n",
      "Epoch [2388/10000] Train Loss: 0.009880 Val Loss: nan\n",
      "Epoch [2389/10000] Train Loss: 0.009638 Val Loss: nan\n",
      "Epoch [2390/10000] Train Loss: 0.009663 Val Loss: nan\n",
      "Epoch [2391/10000] Train Loss: 0.009691 Val Loss: nan\n",
      "Epoch [2392/10000] Train Loss: 0.009538 Val Loss: nan\n",
      "Epoch [2393/10000] Train Loss: 0.009701 Val Loss: nan\n",
      "Epoch [2394/10000] Train Loss: 0.009822 Val Loss: nan\n",
      "Epoch [2395/10000] Train Loss: 0.009712 Val Loss: nan\n",
      "Epoch [2396/10000] Train Loss: 0.009771 Val Loss: nan\n",
      "Epoch [2397/10000] Train Loss: 0.009414 Val Loss: nan\n",
      "Epoch [2398/10000] Train Loss: 0.009682 Val Loss: nan\n",
      "Epoch [2399/10000] Train Loss: 0.009786 Val Loss: nan\n",
      "Epoch [2400/10000] Train Loss: 0.009442 Val Loss: nan\n",
      "Epoch [2401/10000] Train Loss: 0.009642 Val Loss: nan\n",
      "Epoch [2402/10000] Train Loss: 0.009587 Val Loss: nan\n",
      "Epoch [2403/10000] Train Loss: 0.009684 Val Loss: nan\n",
      "Epoch [2404/10000] Train Loss: 0.009513 Val Loss: nan\n",
      "Epoch [2405/10000] Train Loss: 0.009441 Val Loss: nan\n",
      "Epoch [2406/10000] Train Loss: 0.009458 Val Loss: nan\n",
      "Epoch [2407/10000] Train Loss: 0.009927 Val Loss: nan\n",
      "Epoch [2408/10000] Train Loss: 0.009476 Val Loss: nan\n",
      "Epoch [2409/10000] Train Loss: 0.009453 Val Loss: nan\n",
      "Epoch [2410/10000] Train Loss: 0.009630 Val Loss: nan\n",
      "Epoch [2411/10000] Train Loss: 0.009572 Val Loss: nan\n",
      "Epoch [2412/10000] Train Loss: 0.009579 Val Loss: nan\n",
      "Epoch [2413/10000] Train Loss: 0.009461 Val Loss: nan\n",
      "Epoch [2414/10000] Train Loss: 0.009547 Val Loss: nan\n",
      "Epoch [2415/10000] Train Loss: 0.009622 Val Loss: nan\n",
      "Epoch [2416/10000] Train Loss: 0.009722 Val Loss: nan\n",
      "Epoch [2417/10000] Train Loss: 0.009737 Val Loss: nan\n",
      "Epoch [2418/10000] Train Loss: 0.009558 Val Loss: nan\n",
      "Epoch [2419/10000] Train Loss: 0.009898 Val Loss: nan\n",
      "Epoch [2420/10000] Train Loss: 0.009550 Val Loss: nan\n",
      "Epoch [2421/10000] Train Loss: 0.009610 Val Loss: nan\n",
      "Epoch [2422/10000] Train Loss: 0.009477 Val Loss: nan\n",
      "Epoch [2423/10000] Train Loss: 0.009406 Val Loss: nan\n",
      "Epoch [2424/10000] Train Loss: 0.009646 Val Loss: nan\n",
      "Epoch [2425/10000] Train Loss: 0.009792 Val Loss: nan\n",
      "Epoch [2426/10000] Train Loss: 0.009652 Val Loss: nan\n",
      "Epoch [2427/10000] Train Loss: 0.009406 Val Loss: nan\n",
      "Epoch [2428/10000] Train Loss: 0.009431 Val Loss: nan\n",
      "Epoch [2429/10000] Train Loss: 0.009408 Val Loss: nan\n",
      "Epoch [2430/10000] Train Loss: 0.009376 Val Loss: nan\n",
      "Epoch [2431/10000] Train Loss: 0.009428 Val Loss: nan\n",
      "Epoch [2432/10000] Train Loss: 0.009428 Val Loss: nan\n",
      "Epoch [2433/10000] Train Loss: 0.009358 Val Loss: nan\n",
      "Epoch [2434/10000] Train Loss: 0.009399 Val Loss: nan\n",
      "Epoch [2435/10000] Train Loss: 0.009540 Val Loss: nan\n",
      "Epoch [2436/10000] Train Loss: 0.009417 Val Loss: nan\n",
      "Epoch [2437/10000] Train Loss: 0.009399 Val Loss: nan\n",
      "Epoch [2438/10000] Train Loss: 0.009578 Val Loss: nan\n",
      "Epoch [2439/10000] Train Loss: 0.009514 Val Loss: nan\n",
      "Epoch [2440/10000] Train Loss: 0.009445 Val Loss: nan\n",
      "Epoch [2441/10000] Train Loss: 0.009413 Val Loss: nan\n",
      "Epoch [2442/10000] Train Loss: 0.009391 Val Loss: nan\n",
      "Epoch [2443/10000] Train Loss: 0.009373 Val Loss: nan\n",
      "Epoch [2444/10000] Train Loss: 0.009530 Val Loss: nan\n",
      "Epoch [2445/10000] Train Loss: 0.009411 Val Loss: nan\n",
      "Epoch [2446/10000] Train Loss: 0.009398 Val Loss: nan\n",
      "Epoch [2447/10000] Train Loss: 0.009513 Val Loss: nan\n",
      "Epoch [2448/10000] Train Loss: 0.009480 Val Loss: nan\n",
      "Epoch [2449/10000] Train Loss: 0.009361 Val Loss: nan\n",
      "Epoch [2450/10000] Train Loss: 0.009596 Val Loss: nan\n",
      "Epoch [2451/10000] Train Loss: 0.009531 Val Loss: nan\n",
      "Epoch [2452/10000] Train Loss: 0.009360 Val Loss: nan\n",
      "Epoch [2453/10000] Train Loss: 0.009457 Val Loss: nan\n",
      "Epoch [2454/10000] Train Loss: 0.009537 Val Loss: nan\n",
      "Epoch [2455/10000] Train Loss: 0.009713 Val Loss: nan\n",
      "Epoch [2456/10000] Train Loss: 0.009472 Val Loss: nan\n",
      "Epoch [2457/10000] Train Loss: 0.009382 Val Loss: nan\n",
      "Epoch [2458/10000] Train Loss: 0.009342 Val Loss: nan\n",
      "Epoch [2459/10000] Train Loss: 0.009505 Val Loss: nan\n",
      "Epoch [2460/10000] Train Loss: 0.009450 Val Loss: nan\n",
      "Epoch [2461/10000] Train Loss: 0.009493 Val Loss: nan\n",
      "Epoch [2462/10000] Train Loss: 0.009514 Val Loss: nan\n",
      "Epoch [2463/10000] Train Loss: 0.009329 Val Loss: nan\n",
      "Epoch [2464/10000] Train Loss: 0.009363 Val Loss: nan\n",
      "Epoch [2465/10000] Train Loss: 0.009548 Val Loss: nan\n",
      "Epoch [2466/10000] Train Loss: 0.009658 Val Loss: nan\n",
      "Epoch [2467/10000] Train Loss: 0.009390 Val Loss: nan\n",
      "Epoch [2468/10000] Train Loss: 0.009778 Val Loss: nan\n",
      "Epoch [2469/10000] Train Loss: 0.009610 Val Loss: nan\n",
      "Epoch [2470/10000] Train Loss: 0.009647 Val Loss: nan\n",
      "Epoch [2471/10000] Train Loss: 0.009837 Val Loss: nan\n",
      "Epoch [2472/10000] Train Loss: 0.009759 Val Loss: nan\n",
      "Epoch [2473/10000] Train Loss: 0.009672 Val Loss: nan\n",
      "Epoch [2474/10000] Train Loss: 0.009473 Val Loss: nan\n",
      "Epoch [2475/10000] Train Loss: 0.009611 Val Loss: nan\n",
      "Epoch [2476/10000] Train Loss: 0.009321 Val Loss: nan\n",
      "Epoch [2477/10000] Train Loss: 0.009352 Val Loss: nan\n",
      "Epoch [2478/10000] Train Loss: 0.009482 Val Loss: nan\n",
      "Epoch [2479/10000] Train Loss: 0.009511 Val Loss: nan\n",
      "Epoch [2480/10000] Train Loss: 0.009455 Val Loss: nan\n",
      "Epoch [2481/10000] Train Loss: 0.009396 Val Loss: nan\n",
      "Epoch [2482/10000] Train Loss: 0.009461 Val Loss: nan\n",
      "Epoch [2483/10000] Train Loss: 0.009436 Val Loss: nan\n",
      "Epoch [2484/10000] Train Loss: 0.009477 Val Loss: nan\n",
      "Epoch [2485/10000] Train Loss: 0.009313 Val Loss: nan\n",
      "Epoch [2486/10000] Train Loss: 0.009283 Val Loss: nan\n",
      "Epoch [2487/10000] Train Loss: 0.009386 Val Loss: nan\n",
      "Epoch [2488/10000] Train Loss: 0.009446 Val Loss: nan\n",
      "Epoch [2489/10000] Train Loss: 0.009569 Val Loss: nan\n",
      "Epoch [2490/10000] Train Loss: 0.009627 Val Loss: nan\n",
      "Epoch [2491/10000] Train Loss: 0.009333 Val Loss: nan\n",
      "Epoch [2492/10000] Train Loss: 0.009362 Val Loss: nan\n",
      "Epoch [2493/10000] Train Loss: 0.009291 Val Loss: nan\n",
      "Epoch [2494/10000] Train Loss: 0.009420 Val Loss: nan\n",
      "Epoch [2495/10000] Train Loss: 0.009294 Val Loss: nan\n",
      "Epoch [2496/10000] Train Loss: 0.009424 Val Loss: nan\n",
      "Epoch [2497/10000] Train Loss: 0.009385 Val Loss: nan\n",
      "Epoch [2498/10000] Train Loss: 0.009395 Val Loss: nan\n",
      "Epoch [2499/10000] Train Loss: 0.009448 Val Loss: nan\n",
      "Epoch [2500/10000] Train Loss: 0.009494 Val Loss: nan\n",
      "Epoch [2501/10000] Train Loss: 0.009476 Val Loss: nan\n",
      "Epoch [2502/10000] Train Loss: 0.009344 Val Loss: nan\n",
      "Epoch [2503/10000] Train Loss: 0.009424 Val Loss: nan\n",
      "Epoch [2504/10000] Train Loss: 0.009397 Val Loss: nan\n",
      "Epoch [2505/10000] Train Loss: 0.009628 Val Loss: nan\n",
      "Epoch [2506/10000] Train Loss: 0.009504 Val Loss: nan\n",
      "Epoch [2507/10000] Train Loss: 0.009413 Val Loss: nan\n",
      "Epoch [2508/10000] Train Loss: 0.009253 Val Loss: nan\n",
      "Epoch [2509/10000] Train Loss: 0.009264 Val Loss: nan\n",
      "Epoch [2510/10000] Train Loss: 0.009302 Val Loss: nan\n",
      "Epoch [2511/10000] Train Loss: 0.009269 Val Loss: nan\n",
      "Epoch [2512/10000] Train Loss: 0.009455 Val Loss: nan\n",
      "Epoch [2513/10000] Train Loss: 0.009356 Val Loss: nan\n",
      "Epoch [2514/10000] Train Loss: 0.009450 Val Loss: nan\n",
      "Epoch [2515/10000] Train Loss: 0.009389 Val Loss: nan\n",
      "Epoch [2516/10000] Train Loss: 0.009439 Val Loss: nan\n",
      "Epoch [2517/10000] Train Loss: 0.009511 Val Loss: nan\n",
      "Epoch [2518/10000] Train Loss: 0.009267 Val Loss: nan\n",
      "Epoch [2519/10000] Train Loss: 0.009339 Val Loss: nan\n",
      "Epoch [2520/10000] Train Loss: 0.009732 Val Loss: nan\n",
      "Epoch [2521/10000] Train Loss: 0.009381 Val Loss: nan\n",
      "Epoch [2522/10000] Train Loss: 0.009337 Val Loss: nan\n",
      "Epoch [2523/10000] Train Loss: 0.009273 Val Loss: nan\n",
      "Epoch [2524/10000] Train Loss: 0.009369 Val Loss: nan\n",
      "Epoch [2525/10000] Train Loss: 0.009218 Val Loss: nan\n",
      "Epoch [2526/10000] Train Loss: 0.009421 Val Loss: nan\n",
      "Epoch [2527/10000] Train Loss: 0.009274 Val Loss: nan\n",
      "Epoch [2528/10000] Train Loss: 0.009421 Val Loss: nan\n",
      "Epoch [2529/10000] Train Loss: 0.009589 Val Loss: nan\n",
      "Epoch [2530/10000] Train Loss: 0.009537 Val Loss: nan\n",
      "Epoch [2531/10000] Train Loss: 0.009507 Val Loss: nan\n",
      "Epoch [2532/10000] Train Loss: 0.009427 Val Loss: nan\n",
      "Epoch [2533/10000] Train Loss: 0.009309 Val Loss: nan\n",
      "Epoch [2534/10000] Train Loss: 0.009323 Val Loss: nan\n",
      "Epoch [2535/10000] Train Loss: 0.009263 Val Loss: nan\n",
      "Epoch [2536/10000] Train Loss: 0.009318 Val Loss: nan\n",
      "Epoch [2537/10000] Train Loss: 0.009413 Val Loss: nan\n",
      "Epoch [2538/10000] Train Loss: 0.009376 Val Loss: nan\n",
      "Epoch [2539/10000] Train Loss: 0.009246 Val Loss: nan\n",
      "Epoch [2540/10000] Train Loss: 0.009315 Val Loss: nan\n",
      "Epoch [2541/10000] Train Loss: 0.009682 Val Loss: nan\n",
      "Epoch [2542/10000] Train Loss: 0.009339 Val Loss: nan\n",
      "Epoch [2543/10000] Train Loss: 0.009381 Val Loss: nan\n",
      "Epoch [2544/10000] Train Loss: 0.009409 Val Loss: nan\n",
      "Epoch [2545/10000] Train Loss: 0.009476 Val Loss: nan\n",
      "Epoch [2546/10000] Train Loss: 0.009551 Val Loss: nan\n",
      "Epoch [2547/10000] Train Loss: 0.009369 Val Loss: nan\n",
      "Epoch [2548/10000] Train Loss: 0.009215 Val Loss: nan\n",
      "Epoch [2549/10000] Train Loss: 0.009201 Val Loss: nan\n",
      "Epoch [2550/10000] Train Loss: 0.009427 Val Loss: nan\n",
      "Epoch [2551/10000] Train Loss: 0.009476 Val Loss: nan\n",
      "Epoch [2552/10000] Train Loss: 0.009413 Val Loss: nan\n",
      "Epoch [2553/10000] Train Loss: 0.009189 Val Loss: nan\n",
      "Epoch [2554/10000] Train Loss: 0.009346 Val Loss: nan\n",
      "Epoch [2555/10000] Train Loss: 0.009499 Val Loss: nan\n",
      "Epoch [2556/10000] Train Loss: 0.009391 Val Loss: nan\n",
      "Epoch [2557/10000] Train Loss: 0.009262 Val Loss: nan\n",
      "Epoch [2558/10000] Train Loss: 0.009250 Val Loss: nan\n",
      "Epoch [2559/10000] Train Loss: 0.009288 Val Loss: nan\n",
      "Epoch [2560/10000] Train Loss: 0.009220 Val Loss: nan\n",
      "Epoch [2561/10000] Train Loss: 0.009234 Val Loss: nan\n",
      "Epoch [2562/10000] Train Loss: 0.009174 Val Loss: nan\n",
      "Epoch [2563/10000] Train Loss: 0.009147 Val Loss: nan\n",
      "Epoch [2564/10000] Train Loss: 0.009201 Val Loss: nan\n",
      "Epoch [2565/10000] Train Loss: 0.009620 Val Loss: nan\n",
      "Epoch [2566/10000] Train Loss: 0.009375 Val Loss: nan\n",
      "Epoch [2567/10000] Train Loss: 0.009215 Val Loss: nan\n",
      "Epoch [2568/10000] Train Loss: 0.009544 Val Loss: nan\n",
      "Epoch [2569/10000] Train Loss: 0.009241 Val Loss: nan\n",
      "Epoch [2570/10000] Train Loss: 0.009277 Val Loss: nan\n",
      "Epoch [2571/10000] Train Loss: 0.009189 Val Loss: nan\n",
      "Epoch [2572/10000] Train Loss: 0.009483 Val Loss: nan\n",
      "Epoch [2573/10000] Train Loss: 0.009381 Val Loss: nan\n",
      "Epoch [2574/10000] Train Loss: 0.009348 Val Loss: nan\n",
      "Epoch [2575/10000] Train Loss: 0.009326 Val Loss: nan\n",
      "Epoch [2576/10000] Train Loss: 0.009138 Val Loss: nan\n",
      "Epoch [2577/10000] Train Loss: 0.009289 Val Loss: nan\n",
      "Epoch [2578/10000] Train Loss: 0.009332 Val Loss: nan\n",
      "Epoch [2579/10000] Train Loss: 0.009298 Val Loss: nan\n",
      "Epoch [2580/10000] Train Loss: 0.009320 Val Loss: nan\n",
      "Epoch [2581/10000] Train Loss: 0.009135 Val Loss: nan\n",
      "Epoch [2582/10000] Train Loss: 0.009180 Val Loss: nan\n",
      "Epoch [2583/10000] Train Loss: 0.009149 Val Loss: nan\n",
      "Epoch [2584/10000] Train Loss: 0.009364 Val Loss: nan\n",
      "Epoch [2585/10000] Train Loss: 0.009270 Val Loss: nan\n",
      "Epoch [2586/10000] Train Loss: 0.009139 Val Loss: nan\n",
      "Epoch [2587/10000] Train Loss: 0.009219 Val Loss: nan\n",
      "Epoch [2588/10000] Train Loss: 0.009617 Val Loss: nan\n",
      "Epoch [2589/10000] Train Loss: 0.009444 Val Loss: nan\n",
      "Epoch [2590/10000] Train Loss: 0.009189 Val Loss: nan\n",
      "Epoch [2591/10000] Train Loss: 0.009122 Val Loss: nan\n",
      "Epoch [2592/10000] Train Loss: 0.009228 Val Loss: nan\n",
      "Epoch [2593/10000] Train Loss: 0.009122 Val Loss: nan\n",
      "Epoch [2594/10000] Train Loss: 0.009276 Val Loss: nan\n",
      "Epoch [2595/10000] Train Loss: 0.009246 Val Loss: nan\n",
      "Epoch [2596/10000] Train Loss: 0.009053 Val Loss: nan\n",
      "Epoch [2597/10000] Train Loss: 0.009294 Val Loss: nan\n",
      "Epoch [2598/10000] Train Loss: 0.009249 Val Loss: nan\n",
      "Epoch [2599/10000] Train Loss: 0.009117 Val Loss: nan\n",
      "Epoch [2600/10000] Train Loss: 0.009131 Val Loss: nan\n",
      "Epoch [2601/10000] Train Loss: 0.009102 Val Loss: nan\n",
      "Epoch [2602/10000] Train Loss: 0.009229 Val Loss: nan\n",
      "Epoch [2603/10000] Train Loss: 0.009267 Val Loss: nan\n",
      "Epoch [2604/10000] Train Loss: 0.009229 Val Loss: nan\n",
      "Epoch [2605/10000] Train Loss: 0.009087 Val Loss: nan\n",
      "Epoch [2606/10000] Train Loss: 0.009081 Val Loss: nan\n",
      "Epoch [2607/10000] Train Loss: 0.009279 Val Loss: nan\n",
      "Epoch [2608/10000] Train Loss: 0.009053 Val Loss: nan\n",
      "Epoch [2609/10000] Train Loss: 0.009067 Val Loss: nan\n",
      "Epoch [2610/10000] Train Loss: 0.009318 Val Loss: nan\n",
      "Epoch [2611/10000] Train Loss: 0.009173 Val Loss: nan\n",
      "Epoch [2612/10000] Train Loss: 0.009297 Val Loss: nan\n",
      "Epoch [2613/10000] Train Loss: 0.009186 Val Loss: nan\n",
      "Epoch [2614/10000] Train Loss: 0.009287 Val Loss: nan\n",
      "Epoch [2615/10000] Train Loss: 0.009540 Val Loss: nan\n",
      "Epoch [2616/10000] Train Loss: 0.009387 Val Loss: nan\n",
      "Epoch [2617/10000] Train Loss: 0.009308 Val Loss: nan\n",
      "Epoch [2618/10000] Train Loss: 0.009499 Val Loss: nan\n",
      "Epoch [2619/10000] Train Loss: 0.009458 Val Loss: nan\n",
      "Epoch [2620/10000] Train Loss: 0.009178 Val Loss: nan\n",
      "Epoch [2621/10000] Train Loss: 0.009208 Val Loss: nan\n",
      "Epoch [2622/10000] Train Loss: 0.009126 Val Loss: nan\n",
      "Epoch [2623/10000] Train Loss: 0.009386 Val Loss: nan\n",
      "Epoch [2624/10000] Train Loss: 0.009101 Val Loss: nan\n",
      "Epoch [2625/10000] Train Loss: 0.009066 Val Loss: nan\n",
      "Epoch [2626/10000] Train Loss: 0.009113 Val Loss: nan\n",
      "Epoch [2627/10000] Train Loss: 0.009234 Val Loss: nan\n",
      "Epoch [2628/10000] Train Loss: 0.009079 Val Loss: nan\n",
      "Epoch [2629/10000] Train Loss: 0.009219 Val Loss: nan\n",
      "Epoch [2630/10000] Train Loss: 0.009073 Val Loss: nan\n",
      "Epoch [2631/10000] Train Loss: 0.009049 Val Loss: nan\n",
      "Epoch [2632/10000] Train Loss: 0.009174 Val Loss: nan\n",
      "Epoch [2633/10000] Train Loss: 0.009483 Val Loss: nan\n",
      "Epoch [2634/10000] Train Loss: 0.009200 Val Loss: nan\n",
      "Epoch [2635/10000] Train Loss: 0.009199 Val Loss: nan\n",
      "Epoch [2636/10000] Train Loss: 0.009498 Val Loss: nan\n",
      "Epoch [2637/10000] Train Loss: 0.009300 Val Loss: nan\n",
      "Epoch [2638/10000] Train Loss: 0.009449 Val Loss: nan\n",
      "Epoch [2639/10000] Train Loss: 0.009340 Val Loss: nan\n",
      "Epoch [2640/10000] Train Loss: 0.009238 Val Loss: nan\n",
      "Epoch [2641/10000] Train Loss: 0.009098 Val Loss: nan\n",
      "Epoch [2642/10000] Train Loss: 0.009223 Val Loss: nan\n",
      "Epoch [2643/10000] Train Loss: 0.009019 Val Loss: nan\n",
      "Epoch [2644/10000] Train Loss: 0.009055 Val Loss: nan\n",
      "Epoch [2645/10000] Train Loss: 0.009189 Val Loss: nan\n",
      "Epoch [2646/10000] Train Loss: 0.009171 Val Loss: nan\n",
      "Epoch [2647/10000] Train Loss: 0.009037 Val Loss: nan\n",
      "Epoch [2648/10000] Train Loss: 0.009014 Val Loss: nan\n",
      "Epoch [2649/10000] Train Loss: 0.009165 Val Loss: nan\n",
      "Epoch [2650/10000] Train Loss: 0.009222 Val Loss: nan\n",
      "Epoch [2651/10000] Train Loss: 0.009263 Val Loss: nan\n",
      "Epoch [2652/10000] Train Loss: 0.009065 Val Loss: nan\n",
      "Epoch [2653/10000] Train Loss: 0.009068 Val Loss: nan\n",
      "Epoch [2654/10000] Train Loss: 0.009197 Val Loss: nan\n",
      "Epoch [2655/10000] Train Loss: 0.009232 Val Loss: nan\n",
      "Epoch [2656/10000] Train Loss: 0.009121 Val Loss: nan\n",
      "Epoch [2657/10000] Train Loss: 0.009127 Val Loss: nan\n",
      "Epoch [2658/10000] Train Loss: 0.009219 Val Loss: nan\n",
      "Epoch [2659/10000] Train Loss: 0.009203 Val Loss: nan\n",
      "Epoch [2660/10000] Train Loss: 0.009146 Val Loss: nan\n",
      "Epoch [2661/10000] Train Loss: 0.009015 Val Loss: nan\n",
      "Epoch [2662/10000] Train Loss: 0.009158 Val Loss: nan\n",
      "Epoch [2663/10000] Train Loss: 0.009258 Val Loss: nan\n",
      "Epoch [2664/10000] Train Loss: 0.009337 Val Loss: nan\n",
      "Epoch [2665/10000] Train Loss: 0.009294 Val Loss: nan\n",
      "Epoch [2666/10000] Train Loss: 0.009080 Val Loss: nan\n",
      "Epoch [2667/10000] Train Loss: 0.009204 Val Loss: nan\n",
      "Epoch [2668/10000] Train Loss: 0.009145 Val Loss: nan\n",
      "Epoch [2669/10000] Train Loss: 0.009285 Val Loss: nan\n",
      "Epoch [2670/10000] Train Loss: 0.009355 Val Loss: nan\n",
      "Epoch [2671/10000] Train Loss: 0.009133 Val Loss: nan\n",
      "Epoch [2672/10000] Train Loss: 0.009017 Val Loss: nan\n",
      "Epoch [2673/10000] Train Loss: 0.008978 Val Loss: nan\n",
      "Epoch [2674/10000] Train Loss: 0.008993 Val Loss: nan\n",
      "Epoch [2675/10000] Train Loss: 0.009180 Val Loss: nan\n",
      "Epoch [2676/10000] Train Loss: 0.009120 Val Loss: nan\n",
      "Epoch [2677/10000] Train Loss: 0.008983 Val Loss: nan\n",
      "Epoch [2678/10000] Train Loss: 0.009145 Val Loss: nan\n",
      "Epoch [2679/10000] Train Loss: 0.009219 Val Loss: nan\n",
      "Epoch [2680/10000] Train Loss: 0.009019 Val Loss: nan\n",
      "Epoch [2681/10000] Train Loss: 0.009176 Val Loss: nan\n",
      "Epoch [2682/10000] Train Loss: 0.009049 Val Loss: nan\n",
      "Epoch [2683/10000] Train Loss: 0.009028 Val Loss: nan\n",
      "Epoch [2684/10000] Train Loss: 0.009168 Val Loss: nan\n",
      "Epoch [2685/10000] Train Loss: 0.009019 Val Loss: nan\n",
      "Epoch [2686/10000] Train Loss: 0.009288 Val Loss: nan\n",
      "Epoch [2687/10000] Train Loss: 0.009097 Val Loss: nan\n",
      "Epoch [2688/10000] Train Loss: 0.009069 Val Loss: nan\n",
      "Epoch [2689/10000] Train Loss: 0.009109 Val Loss: nan\n",
      "Epoch [2690/10000] Train Loss: 0.008993 Val Loss: nan\n",
      "Epoch [2691/10000] Train Loss: 0.009187 Val Loss: nan\n",
      "Epoch [2692/10000] Train Loss: 0.009026 Val Loss: nan\n",
      "Epoch [2693/10000] Train Loss: 0.009006 Val Loss: nan\n",
      "Epoch [2694/10000] Train Loss: 0.009117 Val Loss: nan\n",
      "Epoch [2695/10000] Train Loss: 0.009126 Val Loss: nan\n",
      "Epoch [2696/10000] Train Loss: 0.009084 Val Loss: nan\n",
      "Epoch [2697/10000] Train Loss: 0.009088 Val Loss: nan\n",
      "Epoch [2698/10000] Train Loss: 0.008977 Val Loss: nan\n",
      "Epoch [2699/10000] Train Loss: 0.008907 Val Loss: nan\n",
      "Epoch [2700/10000] Train Loss: 0.009087 Val Loss: nan\n",
      "Epoch [2701/10000] Train Loss: 0.009014 Val Loss: nan\n",
      "Epoch [2702/10000] Train Loss: 0.009033 Val Loss: nan\n",
      "Epoch [2703/10000] Train Loss: 0.009066 Val Loss: nan\n",
      "Epoch [2704/10000] Train Loss: 0.008947 Val Loss: nan\n",
      "Epoch [2705/10000] Train Loss: 0.008924 Val Loss: nan\n",
      "Epoch [2706/10000] Train Loss: 0.009037 Val Loss: nan\n",
      "Epoch [2707/10000] Train Loss: 0.008889 Val Loss: nan\n",
      "Epoch [2708/10000] Train Loss: 0.009047 Val Loss: nan\n",
      "Epoch [2709/10000] Train Loss: 0.009202 Val Loss: nan\n",
      "Epoch [2710/10000] Train Loss: 0.008901 Val Loss: nan\n",
      "Epoch [2711/10000] Train Loss: 0.009067 Val Loss: nan\n",
      "Epoch [2712/10000] Train Loss: 0.009234 Val Loss: nan\n",
      "Epoch [2713/10000] Train Loss: 0.009138 Val Loss: nan\n",
      "Epoch [2714/10000] Train Loss: 0.008932 Val Loss: nan\n",
      "Epoch [2715/10000] Train Loss: 0.008937 Val Loss: nan\n",
      "Epoch [2716/10000] Train Loss: 0.008924 Val Loss: nan\n",
      "Epoch [2717/10000] Train Loss: 0.009131 Val Loss: nan\n",
      "Epoch [2718/10000] Train Loss: 0.009142 Val Loss: nan\n",
      "Epoch [2719/10000] Train Loss: 0.008927 Val Loss: nan\n",
      "Epoch [2720/10000] Train Loss: 0.008907 Val Loss: nan\n",
      "Epoch [2721/10000] Train Loss: 0.009013 Val Loss: nan\n",
      "Epoch [2722/10000] Train Loss: 0.009028 Val Loss: nan\n",
      "Epoch [2723/10000] Train Loss: 0.008981 Val Loss: nan\n",
      "Epoch [2724/10000] Train Loss: 0.009040 Val Loss: nan\n",
      "Epoch [2725/10000] Train Loss: 0.009129 Val Loss: nan\n",
      "Epoch [2726/10000] Train Loss: 0.008933 Val Loss: nan\n",
      "Epoch [2727/10000] Train Loss: 0.008871 Val Loss: nan\n",
      "Epoch [2728/10000] Train Loss: 0.009015 Val Loss: nan\n",
      "Epoch [2729/10000] Train Loss: 0.009097 Val Loss: nan\n",
      "Epoch [2730/10000] Train Loss: 0.008961 Val Loss: nan\n",
      "Epoch [2731/10000] Train Loss: 0.008890 Val Loss: nan\n",
      "Epoch [2732/10000] Train Loss: 0.009019 Val Loss: nan\n",
      "Epoch [2733/10000] Train Loss: 0.009199 Val Loss: nan\n",
      "Epoch [2734/10000] Train Loss: 0.008983 Val Loss: nan\n",
      "Epoch [2735/10000] Train Loss: 0.008999 Val Loss: nan\n",
      "Epoch [2736/10000] Train Loss: 0.008928 Val Loss: nan\n",
      "Epoch [2737/10000] Train Loss: 0.008870 Val Loss: nan\n",
      "Epoch [2738/10000] Train Loss: 0.009058 Val Loss: nan\n",
      "Epoch [2739/10000] Train Loss: 0.009176 Val Loss: nan\n",
      "Epoch [2740/10000] Train Loss: 0.009409 Val Loss: nan\n",
      "Epoch [2741/10000] Train Loss: 0.009008 Val Loss: nan\n",
      "Epoch [2742/10000] Train Loss: 0.008949 Val Loss: nan\n",
      "Epoch [2743/10000] Train Loss: 0.009111 Val Loss: nan\n",
      "Epoch [2744/10000] Train Loss: 0.009204 Val Loss: nan\n",
      "Epoch [2745/10000] Train Loss: 0.009017 Val Loss: nan\n",
      "Epoch [2746/10000] Train Loss: 0.008836 Val Loss: nan\n",
      "Epoch [2747/10000] Train Loss: 0.009134 Val Loss: nan\n",
      "Epoch [2748/10000] Train Loss: 0.009062 Val Loss: nan\n",
      "Epoch [2749/10000] Train Loss: 0.008898 Val Loss: nan\n",
      "Epoch [2750/10000] Train Loss: 0.008907 Val Loss: nan\n",
      "Epoch [2751/10000] Train Loss: 0.009045 Val Loss: nan\n",
      "Epoch [2752/10000] Train Loss: 0.008894 Val Loss: nan\n",
      "Epoch [2753/10000] Train Loss: 0.009373 Val Loss: nan\n",
      "Epoch [2754/10000] Train Loss: 0.008983 Val Loss: nan\n",
      "Epoch [2755/10000] Train Loss: 0.008968 Val Loss: nan\n",
      "Epoch [2756/10000] Train Loss: 0.009051 Val Loss: nan\n",
      "Epoch [2757/10000] Train Loss: 0.008916 Val Loss: nan\n",
      "Epoch [2758/10000] Train Loss: 0.008877 Val Loss: nan\n",
      "Epoch [2759/10000] Train Loss: 0.009235 Val Loss: nan\n",
      "Epoch [2760/10000] Train Loss: 0.009080 Val Loss: nan\n",
      "Epoch [2761/10000] Train Loss: 0.009291 Val Loss: nan\n",
      "Epoch [2762/10000] Train Loss: 0.009056 Val Loss: nan\n",
      "Epoch [2763/10000] Train Loss: 0.008969 Val Loss: nan\n",
      "Epoch [2764/10000] Train Loss: 0.009283 Val Loss: nan\n",
      "Epoch [2765/10000] Train Loss: 0.009309 Val Loss: nan\n",
      "Epoch [2766/10000] Train Loss: 0.009021 Val Loss: nan\n",
      "Epoch [2767/10000] Train Loss: 0.009062 Val Loss: nan\n",
      "Epoch [2768/10000] Train Loss: 0.008874 Val Loss: nan\n",
      "Epoch [2769/10000] Train Loss: 0.008940 Val Loss: nan\n",
      "Epoch [2770/10000] Train Loss: 0.008888 Val Loss: nan\n",
      "Epoch [2771/10000] Train Loss: 0.008988 Val Loss: nan\n",
      "Epoch [2772/10000] Train Loss: 0.009035 Val Loss: nan\n",
      "Epoch [2773/10000] Train Loss: 0.008824 Val Loss: nan\n",
      "Epoch [2774/10000] Train Loss: 0.009159 Val Loss: nan\n",
      "Epoch [2775/10000] Train Loss: 0.008827 Val Loss: nan\n",
      "Epoch [2776/10000] Train Loss: 0.008779 Val Loss: nan\n",
      "Epoch [2777/10000] Train Loss: 0.008933 Val Loss: nan\n",
      "Epoch [2778/10000] Train Loss: 0.008796 Val Loss: nan\n",
      "Epoch [2779/10000] Train Loss: 0.009203 Val Loss: nan\n",
      "Epoch [2780/10000] Train Loss: 0.008923 Val Loss: nan\n",
      "Epoch [2781/10000] Train Loss: 0.009088 Val Loss: nan\n",
      "Epoch [2782/10000] Train Loss: 0.009074 Val Loss: nan\n",
      "Epoch [2783/10000] Train Loss: 0.009009 Val Loss: nan\n",
      "Epoch [2784/10000] Train Loss: 0.008904 Val Loss: nan\n",
      "Epoch [2785/10000] Train Loss: 0.009016 Val Loss: nan\n",
      "Epoch [2786/10000] Train Loss: 0.008885 Val Loss: nan\n",
      "Epoch [2787/10000] Train Loss: 0.008903 Val Loss: nan\n",
      "Epoch [2788/10000] Train Loss: 0.009145 Val Loss: nan\n",
      "Epoch [2789/10000] Train Loss: 0.008811 Val Loss: nan\n",
      "Epoch [2790/10000] Train Loss: 0.008856 Val Loss: nan\n",
      "Epoch [2791/10000] Train Loss: 0.008786 Val Loss: nan\n",
      "Epoch [2792/10000] Train Loss: 0.008994 Val Loss: nan\n",
      "Epoch [2793/10000] Train Loss: 0.009075 Val Loss: nan\n",
      "Epoch [2794/10000] Train Loss: 0.008781 Val Loss: nan\n",
      "Epoch [2795/10000] Train Loss: 0.008763 Val Loss: nan\n",
      "Epoch [2796/10000] Train Loss: 0.008966 Val Loss: nan\n",
      "Epoch [2797/10000] Train Loss: 0.008949 Val Loss: nan\n",
      "Epoch [2798/10000] Train Loss: 0.008779 Val Loss: nan\n",
      "Epoch [2799/10000] Train Loss: 0.008978 Val Loss: nan\n",
      "Epoch [2800/10000] Train Loss: 0.008769 Val Loss: nan\n",
      "Epoch [2801/10000] Train Loss: 0.008781 Val Loss: nan\n",
      "Epoch [2802/10000] Train Loss: 0.008886 Val Loss: nan\n",
      "Epoch [2803/10000] Train Loss: 0.009044 Val Loss: nan\n",
      "Epoch [2804/10000] Train Loss: 0.008780 Val Loss: nan\n",
      "Epoch [2805/10000] Train Loss: 0.008776 Val Loss: nan\n",
      "Epoch [2806/10000] Train Loss: 0.008945 Val Loss: nan\n",
      "Epoch [2807/10000] Train Loss: 0.008816 Val Loss: nan\n",
      "Epoch [2808/10000] Train Loss: 0.008892 Val Loss: nan\n",
      "Epoch [2809/10000] Train Loss: 0.008828 Val Loss: nan\n",
      "Epoch [2810/10000] Train Loss: 0.008826 Val Loss: nan\n",
      "Epoch [2811/10000] Train Loss: 0.008821 Val Loss: nan\n",
      "Epoch [2812/10000] Train Loss: 0.008934 Val Loss: nan\n",
      "Epoch [2813/10000] Train Loss: 0.008949 Val Loss: nan\n",
      "Epoch [2814/10000] Train Loss: 0.008757 Val Loss: nan\n",
      "Epoch [2815/10000] Train Loss: 0.008812 Val Loss: nan\n",
      "Epoch [2816/10000] Train Loss: 0.009012 Val Loss: nan\n",
      "Epoch [2817/10000] Train Loss: 0.008900 Val Loss: nan\n",
      "Epoch [2818/10000] Train Loss: 0.008995 Val Loss: nan\n",
      "Epoch [2819/10000] Train Loss: 0.009243 Val Loss: nan\n",
      "Epoch [2820/10000] Train Loss: 0.008908 Val Loss: nan\n",
      "Epoch [2821/10000] Train Loss: 0.008781 Val Loss: nan\n",
      "Epoch [2822/10000] Train Loss: 0.008910 Val Loss: nan\n",
      "Epoch [2823/10000] Train Loss: 0.008744 Val Loss: nan\n",
      "Epoch [2824/10000] Train Loss: 0.008957 Val Loss: nan\n",
      "Epoch [2825/10000] Train Loss: 0.008762 Val Loss: nan\n",
      "Epoch [2826/10000] Train Loss: 0.008913 Val Loss: nan\n",
      "Epoch [2827/10000] Train Loss: 0.008868 Val Loss: nan\n",
      "Epoch [2828/10000] Train Loss: 0.008942 Val Loss: nan\n",
      "Epoch [2829/10000] Train Loss: 0.008893 Val Loss: nan\n",
      "Epoch [2830/10000] Train Loss: 0.008840 Val Loss: nan\n",
      "Epoch [2831/10000] Train Loss: 0.008941 Val Loss: nan\n",
      "Epoch [2832/10000] Train Loss: 0.008922 Val Loss: nan\n",
      "Epoch [2833/10000] Train Loss: 0.009046 Val Loss: nan\n",
      "Epoch [2834/10000] Train Loss: 0.008891 Val Loss: nan\n",
      "Epoch [2835/10000] Train Loss: 0.008797 Val Loss: nan\n",
      "Epoch [2836/10000] Train Loss: 0.008760 Val Loss: nan\n",
      "Epoch [2837/10000] Train Loss: 0.009019 Val Loss: nan\n",
      "Epoch [2838/10000] Train Loss: 0.009052 Val Loss: nan\n",
      "Epoch [2839/10000] Train Loss: 0.008935 Val Loss: nan\n",
      "Epoch [2840/10000] Train Loss: 0.008900 Val Loss: nan\n",
      "Epoch [2841/10000] Train Loss: 0.009045 Val Loss: nan\n",
      "Epoch [2842/10000] Train Loss: 0.009006 Val Loss: nan\n",
      "Epoch [2843/10000] Train Loss: 0.008783 Val Loss: nan\n",
      "Epoch [2844/10000] Train Loss: 0.008893 Val Loss: nan\n",
      "Epoch [2845/10000] Train Loss: 0.009041 Val Loss: nan\n",
      "Epoch [2846/10000] Train Loss: 0.008848 Val Loss: nan\n",
      "Epoch [2847/10000] Train Loss: 0.008915 Val Loss: nan\n",
      "Epoch [2848/10000] Train Loss: 0.008730 Val Loss: nan\n",
      "Epoch [2849/10000] Train Loss: 0.008723 Val Loss: nan\n",
      "Epoch [2850/10000] Train Loss: 0.008867 Val Loss: nan\n",
      "Epoch [2851/10000] Train Loss: 0.008721 Val Loss: nan\n",
      "Epoch [2852/10000] Train Loss: 0.008725 Val Loss: nan\n",
      "Epoch [2853/10000] Train Loss: 0.008859 Val Loss: nan\n",
      "Epoch [2854/10000] Train Loss: 0.008863 Val Loss: nan\n",
      "Epoch [2855/10000] Train Loss: 0.008728 Val Loss: nan\n",
      "Epoch [2856/10000] Train Loss: 0.008833 Val Loss: nan\n",
      "Epoch [2857/10000] Train Loss: 0.008960 Val Loss: nan\n",
      "Epoch [2858/10000] Train Loss: 0.008689 Val Loss: nan\n",
      "Epoch [2859/10000] Train Loss: 0.008716 Val Loss: nan\n",
      "Epoch [2860/10000] Train Loss: 0.008889 Val Loss: nan\n",
      "Epoch [2861/10000] Train Loss: 0.009055 Val Loss: nan\n",
      "Epoch [2862/10000] Train Loss: 0.009234 Val Loss: nan\n",
      "Epoch [2863/10000] Train Loss: 0.008842 Val Loss: nan\n",
      "Epoch [2864/10000] Train Loss: 0.008842 Val Loss: nan\n",
      "Epoch [2865/10000] Train Loss: 0.008835 Val Loss: nan\n",
      "Epoch [2866/10000] Train Loss: 0.008893 Val Loss: nan\n",
      "Epoch [2867/10000] Train Loss: 0.008951 Val Loss: nan\n",
      "Epoch [2868/10000] Train Loss: 0.008849 Val Loss: nan\n",
      "Epoch [2869/10000] Train Loss: 0.008897 Val Loss: nan\n",
      "Epoch [2870/10000] Train Loss: 0.008875 Val Loss: nan\n",
      "Epoch [2871/10000] Train Loss: 0.008824 Val Loss: nan\n",
      "Epoch [2872/10000] Train Loss: 0.008989 Val Loss: nan\n",
      "Epoch [2873/10000] Train Loss: 0.008848 Val Loss: nan\n",
      "Epoch [2874/10000] Train Loss: 0.008683 Val Loss: nan\n",
      "Epoch [2875/10000] Train Loss: 0.008830 Val Loss: nan\n",
      "Epoch [2876/10000] Train Loss: 0.008790 Val Loss: nan\n",
      "Epoch [2877/10000] Train Loss: 0.008742 Val Loss: nan\n",
      "Epoch [2878/10000] Train Loss: 0.008723 Val Loss: nan\n",
      "Epoch [2879/10000] Train Loss: 0.008672 Val Loss: nan\n",
      "Epoch [2880/10000] Train Loss: 0.008789 Val Loss: nan\n",
      "Epoch [2881/10000] Train Loss: 0.008875 Val Loss: nan\n",
      "Epoch [2882/10000] Train Loss: 0.008816 Val Loss: nan\n",
      "Epoch [2883/10000] Train Loss: 0.009135 Val Loss: nan\n",
      "Epoch [2884/10000] Train Loss: 0.009059 Val Loss: nan\n",
      "Epoch [2885/10000] Train Loss: 0.008709 Val Loss: nan\n",
      "Epoch [2886/10000] Train Loss: 0.008762 Val Loss: nan\n",
      "Epoch [2887/10000] Train Loss: 0.008762 Val Loss: nan\n",
      "Epoch [2888/10000] Train Loss: 0.008757 Val Loss: nan\n",
      "Epoch [2889/10000] Train Loss: 0.008666 Val Loss: nan\n",
      "Epoch [2890/10000] Train Loss: 0.008847 Val Loss: nan\n",
      "Epoch [2891/10000] Train Loss: 0.008801 Val Loss: nan\n",
      "Epoch [2892/10000] Train Loss: 0.008939 Val Loss: nan\n",
      "Epoch [2893/10000] Train Loss: 0.008726 Val Loss: nan\n",
      "Epoch [2894/10000] Train Loss: 0.008728 Val Loss: nan\n",
      "Epoch [2895/10000] Train Loss: 0.008688 Val Loss: nan\n",
      "Epoch [2896/10000] Train Loss: 0.008756 Val Loss: nan\n",
      "Epoch [2897/10000] Train Loss: 0.009268 Val Loss: nan\n",
      "Epoch [2898/10000] Train Loss: 0.008853 Val Loss: nan\n",
      "Epoch [2899/10000] Train Loss: 0.009196 Val Loss: nan\n",
      "Epoch [2900/10000] Train Loss: 0.008926 Val Loss: nan\n",
      "Epoch [2901/10000] Train Loss: 0.009003 Val Loss: nan\n",
      "Epoch [2902/10000] Train Loss: 0.009033 Val Loss: nan\n",
      "Epoch [2903/10000] Train Loss: 0.008677 Val Loss: nan\n",
      "Epoch [2904/10000] Train Loss: 0.008809 Val Loss: nan\n",
      "Epoch [2905/10000] Train Loss: 0.008703 Val Loss: nan\n",
      "Epoch [2906/10000] Train Loss: 0.009009 Val Loss: nan\n",
      "Epoch [2907/10000] Train Loss: 0.008818 Val Loss: nan\n",
      "Epoch [2908/10000] Train Loss: 0.008908 Val Loss: nan\n",
      "Epoch [2909/10000] Train Loss: 0.008837 Val Loss: nan\n",
      "Epoch [2910/10000] Train Loss: 0.008751 Val Loss: nan\n",
      "Epoch [2911/10000] Train Loss: 0.008757 Val Loss: nan\n",
      "Epoch [2912/10000] Train Loss: 0.008997 Val Loss: nan\n",
      "Epoch [2913/10000] Train Loss: 0.008984 Val Loss: nan\n",
      "Epoch [2914/10000] Train Loss: 0.008716 Val Loss: nan\n",
      "Epoch [2915/10000] Train Loss: 0.008740 Val Loss: nan\n",
      "Epoch [2916/10000] Train Loss: 0.008652 Val Loss: nan\n",
      "Epoch [2917/10000] Train Loss: 0.008907 Val Loss: nan\n",
      "Epoch [2918/10000] Train Loss: 0.008716 Val Loss: nan\n",
      "Epoch [2919/10000] Train Loss: 0.008621 Val Loss: nan\n",
      "Epoch [2920/10000] Train Loss: 0.008789 Val Loss: nan\n",
      "Epoch [2921/10000] Train Loss: 0.008634 Val Loss: nan\n",
      "Epoch [2922/10000] Train Loss: 0.008664 Val Loss: nan\n",
      "Epoch [2923/10000] Train Loss: 0.008695 Val Loss: nan\n",
      "Epoch [2924/10000] Train Loss: 0.008667 Val Loss: nan\n",
      "Epoch [2925/10000] Train Loss: 0.008688 Val Loss: nan\n",
      "Epoch [2926/10000] Train Loss: 0.008623 Val Loss: nan\n",
      "Epoch [2927/10000] Train Loss: 0.008735 Val Loss: nan\n",
      "Epoch [2928/10000] Train Loss: 0.008794 Val Loss: nan\n",
      "Epoch [2929/10000] Train Loss: 0.008851 Val Loss: nan\n",
      "Epoch [2930/10000] Train Loss: 0.008649 Val Loss: nan\n",
      "Epoch [2931/10000] Train Loss: 0.008878 Val Loss: nan\n",
      "Epoch [2932/10000] Train Loss: 0.008909 Val Loss: nan\n",
      "Epoch [2933/10000] Train Loss: 0.008692 Val Loss: nan\n",
      "Epoch [2934/10000] Train Loss: 0.008780 Val Loss: nan\n",
      "Epoch [2935/10000] Train Loss: 0.008704 Val Loss: nan\n",
      "Epoch [2936/10000] Train Loss: 0.008894 Val Loss: nan\n",
      "Epoch [2937/10000] Train Loss: 0.008705 Val Loss: nan\n",
      "Epoch [2938/10000] Train Loss: 0.008616 Val Loss: nan\n",
      "Epoch [2939/10000] Train Loss: 0.008660 Val Loss: nan\n",
      "Epoch [2940/10000] Train Loss: 0.008606 Val Loss: nan\n",
      "Epoch [2941/10000] Train Loss: 0.008588 Val Loss: nan\n",
      "Epoch [2942/10000] Train Loss: 0.008610 Val Loss: nan\n",
      "Epoch [2943/10000] Train Loss: 0.008805 Val Loss: nan\n",
      "Epoch [2944/10000] Train Loss: 0.008984 Val Loss: nan\n",
      "Epoch [2945/10000] Train Loss: 0.008901 Val Loss: nan\n",
      "Epoch [2946/10000] Train Loss: 0.008662 Val Loss: nan\n",
      "Epoch [2947/10000] Train Loss: 0.008573 Val Loss: nan\n",
      "Epoch [2948/10000] Train Loss: 0.008744 Val Loss: nan\n",
      "Epoch [2949/10000] Train Loss: 0.008791 Val Loss: nan\n",
      "Epoch [2950/10000] Train Loss: 0.008745 Val Loss: nan\n",
      "Epoch [2951/10000] Train Loss: 0.008612 Val Loss: nan\n",
      "Epoch [2952/10000] Train Loss: 0.008891 Val Loss: nan\n",
      "Epoch [2953/10000] Train Loss: 0.008588 Val Loss: nan\n",
      "Epoch [2954/10000] Train Loss: 0.008646 Val Loss: nan\n",
      "Epoch [2955/10000] Train Loss: 0.008618 Val Loss: nan\n",
      "Epoch [2956/10000] Train Loss: 0.008914 Val Loss: nan\n",
      "Epoch [2957/10000] Train Loss: 0.008640 Val Loss: nan\n",
      "Epoch [2958/10000] Train Loss: 0.008628 Val Loss: nan\n",
      "Epoch [2959/10000] Train Loss: 0.008717 Val Loss: nan\n",
      "Epoch [2960/10000] Train Loss: 0.008601 Val Loss: nan\n",
      "Epoch [2961/10000] Train Loss: 0.008562 Val Loss: nan\n",
      "Epoch [2962/10000] Train Loss: 0.008665 Val Loss: nan\n",
      "Epoch [2963/10000] Train Loss: 0.008763 Val Loss: nan\n",
      "Epoch [2964/10000] Train Loss: 0.008649 Val Loss: nan\n",
      "Epoch [2965/10000] Train Loss: 0.008925 Val Loss: nan\n",
      "Epoch [2966/10000] Train Loss: 0.008829 Val Loss: nan\n",
      "Epoch [2967/10000] Train Loss: 0.008723 Val Loss: nan\n",
      "Epoch [2968/10000] Train Loss: 0.008515 Val Loss: nan\n",
      "Epoch [2969/10000] Train Loss: 0.008631 Val Loss: nan\n",
      "Epoch [2970/10000] Train Loss: 0.008590 Val Loss: nan\n",
      "Epoch [2971/10000] Train Loss: 0.008619 Val Loss: nan\n",
      "Epoch [2972/10000] Train Loss: 0.008596 Val Loss: nan\n",
      "Epoch [2973/10000] Train Loss: 0.008722 Val Loss: nan\n",
      "Epoch [2974/10000] Train Loss: 0.008593 Val Loss: nan\n",
      "Epoch [2975/10000] Train Loss: 0.008540 Val Loss: nan\n",
      "Epoch [2976/10000] Train Loss: 0.008637 Val Loss: nan\n",
      "Epoch [2977/10000] Train Loss: 0.008869 Val Loss: nan\n",
      "Epoch [2978/10000] Train Loss: 0.008552 Val Loss: nan\n",
      "Epoch [2979/10000] Train Loss: 0.008595 Val Loss: nan\n",
      "Epoch [2980/10000] Train Loss: 0.008581 Val Loss: nan\n",
      "Epoch [2981/10000] Train Loss: 0.008545 Val Loss: nan\n",
      "Epoch [2982/10000] Train Loss: 0.008966 Val Loss: nan\n",
      "Epoch [2983/10000] Train Loss: 0.008656 Val Loss: nan\n",
      "Epoch [2984/10000] Train Loss: 0.008970 Val Loss: nan\n",
      "Epoch [2985/10000] Train Loss: 0.008917 Val Loss: nan\n",
      "Epoch [2986/10000] Train Loss: 0.008803 Val Loss: nan\n",
      "Epoch [2987/10000] Train Loss: 0.008745 Val Loss: nan\n",
      "Epoch [2988/10000] Train Loss: 0.008758 Val Loss: nan\n",
      "Epoch [2989/10000] Train Loss: 0.008679 Val Loss: nan\n",
      "Epoch [2990/10000] Train Loss: 0.008789 Val Loss: nan\n",
      "Epoch [2991/10000] Train Loss: 0.008645 Val Loss: nan\n",
      "Epoch [2992/10000] Train Loss: 0.008715 Val Loss: nan\n",
      "Epoch [2993/10000] Train Loss: 0.008588 Val Loss: nan\n",
      "Epoch [2994/10000] Train Loss: 0.008706 Val Loss: nan\n",
      "Epoch [2995/10000] Train Loss: 0.009180 Val Loss: nan\n",
      "Epoch [2996/10000] Train Loss: 0.008652 Val Loss: nan\n",
      "Epoch [2997/10000] Train Loss: 0.008541 Val Loss: nan\n",
      "Epoch [2998/10000] Train Loss: 0.008730 Val Loss: nan\n",
      "Epoch [2999/10000] Train Loss: 0.008544 Val Loss: nan\n",
      "Epoch [3000/10000] Train Loss: 0.008667 Val Loss: nan\n",
      "Epoch [3001/10000] Train Loss: 0.008536 Val Loss: nan\n",
      "Epoch [3002/10000] Train Loss: 0.008697 Val Loss: nan\n",
      "Epoch [3003/10000] Train Loss: 0.008822 Val Loss: nan\n",
      "Epoch [3004/10000] Train Loss: 0.008716 Val Loss: nan\n",
      "Epoch [3005/10000] Train Loss: 0.008708 Val Loss: nan\n",
      "Epoch [3006/10000] Train Loss: 0.008627 Val Loss: nan\n",
      "Epoch [3007/10000] Train Loss: 0.008633 Val Loss: nan\n",
      "Epoch [3008/10000] Train Loss: 0.008540 Val Loss: nan\n",
      "Epoch [3009/10000] Train Loss: 0.008540 Val Loss: nan\n",
      "Epoch [3010/10000] Train Loss: 0.008815 Val Loss: nan\n",
      "Epoch [3011/10000] Train Loss: 0.008674 Val Loss: nan\n",
      "Epoch [3012/10000] Train Loss: 0.008535 Val Loss: nan\n",
      "Epoch [3013/10000] Train Loss: 0.008524 Val Loss: nan\n",
      "Epoch [3014/10000] Train Loss: 0.008556 Val Loss: nan\n",
      "Epoch [3015/10000] Train Loss: 0.008497 Val Loss: nan\n",
      "Epoch [3016/10000] Train Loss: 0.008745 Val Loss: nan\n",
      "Epoch [3017/10000] Train Loss: 0.008794 Val Loss: nan\n",
      "Epoch [3018/10000] Train Loss: 0.008533 Val Loss: nan\n",
      "Epoch [3019/10000] Train Loss: 0.008708 Val Loss: nan\n",
      "Epoch [3020/10000] Train Loss: 0.008621 Val Loss: nan\n",
      "Epoch [3021/10000] Train Loss: 0.008640 Val Loss: nan\n",
      "Epoch [3022/10000] Train Loss: 0.008677 Val Loss: nan\n",
      "Epoch [3023/10000] Train Loss: 0.008621 Val Loss: nan\n",
      "Epoch [3024/10000] Train Loss: 0.008760 Val Loss: nan\n",
      "Epoch [3025/10000] Train Loss: 0.008718 Val Loss: nan\n",
      "Epoch [3026/10000] Train Loss: 0.008646 Val Loss: nan\n",
      "Epoch [3027/10000] Train Loss: 0.008797 Val Loss: nan\n",
      "Epoch [3028/10000] Train Loss: 0.008647 Val Loss: nan\n",
      "Epoch [3029/10000] Train Loss: 0.008515 Val Loss: nan\n",
      "Epoch [3030/10000] Train Loss: 0.008683 Val Loss: nan\n",
      "Epoch [3031/10000] Train Loss: 0.008611 Val Loss: nan\n",
      "Epoch [3032/10000] Train Loss: 0.008699 Val Loss: nan\n",
      "Epoch [3033/10000] Train Loss: 0.008559 Val Loss: nan\n",
      "Epoch [3034/10000] Train Loss: 0.008522 Val Loss: nan\n",
      "Epoch [3035/10000] Train Loss: 0.008626 Val Loss: nan\n",
      "Epoch [3036/10000] Train Loss: 0.008564 Val Loss: nan\n",
      "Epoch [3037/10000] Train Loss: 0.008824 Val Loss: nan\n",
      "Epoch [3038/10000] Train Loss: 0.008680 Val Loss: nan\n",
      "Epoch [3039/10000] Train Loss: 0.008508 Val Loss: nan\n",
      "Epoch [3040/10000] Train Loss: 0.008486 Val Loss: nan\n",
      "Epoch [3041/10000] Train Loss: 0.008687 Val Loss: nan\n",
      "Epoch [3042/10000] Train Loss: 0.008470 Val Loss: nan\n",
      "Epoch [3043/10000] Train Loss: 0.008525 Val Loss: nan\n",
      "Epoch [3044/10000] Train Loss: 0.008644 Val Loss: nan\n",
      "Epoch [3045/10000] Train Loss: 0.008436 Val Loss: nan\n",
      "Epoch [3046/10000] Train Loss: 0.008508 Val Loss: nan\n",
      "Epoch [3047/10000] Train Loss: 0.008494 Val Loss: nan\n",
      "Epoch [3048/10000] Train Loss: 0.008626 Val Loss: nan\n",
      "Epoch [3049/10000] Train Loss: 0.008727 Val Loss: nan\n",
      "Epoch [3050/10000] Train Loss: 0.008665 Val Loss: nan\n",
      "Epoch [3051/10000] Train Loss: 0.008652 Val Loss: nan\n",
      "Epoch [3052/10000] Train Loss: 0.008521 Val Loss: nan\n",
      "Epoch [3053/10000] Train Loss: 0.008804 Val Loss: nan\n",
      "Epoch [3054/10000] Train Loss: 0.008821 Val Loss: nan\n",
      "Epoch [3055/10000] Train Loss: 0.008878 Val Loss: nan\n",
      "Epoch [3056/10000] Train Loss: 0.008651 Val Loss: nan\n",
      "Epoch [3057/10000] Train Loss: 0.008590 Val Loss: nan\n",
      "Epoch [3058/10000] Train Loss: 0.008528 Val Loss: nan\n",
      "Epoch [3059/10000] Train Loss: 0.008715 Val Loss: nan\n",
      "Epoch [3060/10000] Train Loss: 0.008555 Val Loss: nan\n",
      "Epoch [3061/10000] Train Loss: 0.008787 Val Loss: nan\n",
      "Epoch [3062/10000] Train Loss: 0.008473 Val Loss: nan\n",
      "Epoch [3063/10000] Train Loss: 0.008565 Val Loss: nan\n",
      "Epoch [3064/10000] Train Loss: 0.008629 Val Loss: nan\n",
      "Epoch [3065/10000] Train Loss: 0.008465 Val Loss: nan\n",
      "Epoch [3066/10000] Train Loss: 0.008565 Val Loss: nan\n",
      "Epoch [3067/10000] Train Loss: 0.008516 Val Loss: nan\n",
      "Epoch [3068/10000] Train Loss: 0.008563 Val Loss: nan\n",
      "Epoch [3069/10000] Train Loss: 0.008709 Val Loss: nan\n",
      "Epoch [3070/10000] Train Loss: 0.008552 Val Loss: nan\n",
      "Epoch [3071/10000] Train Loss: 0.008667 Val Loss: nan\n",
      "Epoch [3072/10000] Train Loss: 0.008494 Val Loss: nan\n",
      "Epoch [3073/10000] Train Loss: 0.008427 Val Loss: nan\n",
      "Epoch [3074/10000] Train Loss: 0.008491 Val Loss: nan\n",
      "Epoch [3075/10000] Train Loss: 0.008487 Val Loss: nan\n",
      "Epoch [3076/10000] Train Loss: 0.008542 Val Loss: nan\n",
      "Epoch [3077/10000] Train Loss: 0.008473 Val Loss: nan\n",
      "Epoch [3078/10000] Train Loss: 0.008483 Val Loss: nan\n",
      "Epoch [3079/10000] Train Loss: 0.008452 Val Loss: nan\n",
      "Epoch [3080/10000] Train Loss: 0.008489 Val Loss: nan\n",
      "Epoch [3081/10000] Train Loss: 0.008579 Val Loss: nan\n",
      "Epoch [3082/10000] Train Loss: 0.008439 Val Loss: nan\n",
      "Epoch [3083/10000] Train Loss: 0.008616 Val Loss: nan\n",
      "Epoch [3084/10000] Train Loss: 0.008429 Val Loss: nan\n",
      "Epoch [3085/10000] Train Loss: 0.008458 Val Loss: nan\n",
      "Epoch [3086/10000] Train Loss: 0.008580 Val Loss: nan\n",
      "Epoch [3087/10000] Train Loss: 0.008442 Val Loss: nan\n",
      "Epoch [3088/10000] Train Loss: 0.008670 Val Loss: nan\n",
      "Epoch [3089/10000] Train Loss: 0.008651 Val Loss: nan\n",
      "Epoch [3090/10000] Train Loss: 0.008694 Val Loss: nan\n",
      "Epoch [3091/10000] Train Loss: 0.008652 Val Loss: nan\n",
      "Epoch [3092/10000] Train Loss: 0.008666 Val Loss: nan\n",
      "Epoch [3093/10000] Train Loss: 0.008440 Val Loss: nan\n",
      "Epoch [3094/10000] Train Loss: 0.008557 Val Loss: nan\n",
      "Epoch [3095/10000] Train Loss: 0.008452 Val Loss: nan\n",
      "Epoch [3096/10000] Train Loss: 0.008451 Val Loss: nan\n",
      "Epoch [3097/10000] Train Loss: 0.008534 Val Loss: nan\n",
      "Epoch [3098/10000] Train Loss: 0.008527 Val Loss: nan\n",
      "Epoch [3099/10000] Train Loss: 0.008427 Val Loss: nan\n",
      "Epoch [3100/10000] Train Loss: 0.008558 Val Loss: nan\n",
      "Epoch [3101/10000] Train Loss: 0.008578 Val Loss: nan\n",
      "Epoch [3102/10000] Train Loss: 0.008468 Val Loss: nan\n",
      "Epoch [3103/10000] Train Loss: 0.008768 Val Loss: nan\n",
      "Epoch [3104/10000] Train Loss: 0.008429 Val Loss: nan\n",
      "Epoch [3105/10000] Train Loss: 0.008499 Val Loss: nan\n",
      "Epoch [3106/10000] Train Loss: 0.008546 Val Loss: nan\n",
      "Epoch [3107/10000] Train Loss: 0.008447 Val Loss: nan\n",
      "Epoch [3108/10000] Train Loss: 0.008668 Val Loss: nan\n",
      "Epoch [3109/10000] Train Loss: 0.008618 Val Loss: nan\n",
      "Epoch [3110/10000] Train Loss: 0.008491 Val Loss: nan\n",
      "Epoch [3111/10000] Train Loss: 0.008698 Val Loss: nan\n",
      "Epoch [3112/10000] Train Loss: 0.008542 Val Loss: nan\n",
      "Epoch [3113/10000] Train Loss: 0.008639 Val Loss: nan\n",
      "Epoch [3114/10000] Train Loss: 0.008719 Val Loss: nan\n",
      "Epoch [3115/10000] Train Loss: 0.008584 Val Loss: nan\n",
      "Epoch [3116/10000] Train Loss: 0.008698 Val Loss: nan\n",
      "Epoch [3117/10000] Train Loss: 0.008719 Val Loss: nan\n",
      "Epoch [3118/10000] Train Loss: 0.008553 Val Loss: nan\n",
      "Epoch [3119/10000] Train Loss: 0.008963 Val Loss: nan\n",
      "Epoch [3120/10000] Train Loss: 0.008592 Val Loss: nan\n",
      "Epoch [3121/10000] Train Loss: 0.008391 Val Loss: nan\n",
      "Epoch [3122/10000] Train Loss: 0.008613 Val Loss: nan\n",
      "Epoch [3123/10000] Train Loss: 0.008518 Val Loss: nan\n",
      "Epoch [3124/10000] Train Loss: 0.008622 Val Loss: nan\n",
      "Epoch [3125/10000] Train Loss: 0.008726 Val Loss: nan\n",
      "Epoch [3126/10000] Train Loss: 0.008438 Val Loss: nan\n",
      "Epoch [3127/10000] Train Loss: 0.008394 Val Loss: nan\n",
      "Epoch [3128/10000] Train Loss: 0.008621 Val Loss: nan\n",
      "Epoch [3129/10000] Train Loss: 0.008715 Val Loss: nan\n",
      "Epoch [3130/10000] Train Loss: 0.008564 Val Loss: nan\n",
      "Epoch [3131/10000] Train Loss: 0.008374 Val Loss: nan\n",
      "Epoch [3132/10000] Train Loss: 0.008381 Val Loss: nan\n",
      "Epoch [3133/10000] Train Loss: 0.008502 Val Loss: nan\n",
      "Epoch [3134/10000] Train Loss: 0.008457 Val Loss: nan\n",
      "Epoch [3135/10000] Train Loss: 0.008389 Val Loss: nan\n",
      "Epoch [3136/10000] Train Loss: 0.008523 Val Loss: nan\n",
      "Epoch [3137/10000] Train Loss: 0.008453 Val Loss: nan\n",
      "Epoch [3138/10000] Train Loss: 0.008523 Val Loss: nan\n",
      "Epoch [3139/10000] Train Loss: 0.008462 Val Loss: nan\n",
      "Epoch [3140/10000] Train Loss: 0.008538 Val Loss: nan\n",
      "Epoch [3141/10000] Train Loss: 0.008522 Val Loss: nan\n",
      "Epoch [3142/10000] Train Loss: 0.008401 Val Loss: nan\n",
      "Epoch [3143/10000] Train Loss: 0.008391 Val Loss: nan\n",
      "Epoch [3144/10000] Train Loss: 0.008633 Val Loss: nan\n",
      "Epoch [3145/10000] Train Loss: 0.008395 Val Loss: nan\n",
      "Epoch [3146/10000] Train Loss: 0.008538 Val Loss: nan\n",
      "Epoch [3147/10000] Train Loss: 0.008728 Val Loss: nan\n",
      "Epoch [3148/10000] Train Loss: 0.008563 Val Loss: nan\n",
      "Epoch [3149/10000] Train Loss: 0.008402 Val Loss: nan\n",
      "Epoch [3150/10000] Train Loss: 0.008664 Val Loss: nan\n",
      "Epoch [3151/10000] Train Loss: 0.008527 Val Loss: nan\n",
      "Epoch [3152/10000] Train Loss: 0.008500 Val Loss: nan\n",
      "Epoch [3153/10000] Train Loss: 0.008508 Val Loss: nan\n",
      "Epoch [3154/10000] Train Loss: 0.008404 Val Loss: nan\n",
      "Epoch [3155/10000] Train Loss: 0.008483 Val Loss: nan\n",
      "Epoch [3156/10000] Train Loss: 0.008464 Val Loss: nan\n",
      "Epoch [3157/10000] Train Loss: 0.008418 Val Loss: nan\n",
      "Epoch [3158/10000] Train Loss: 0.008371 Val Loss: nan\n",
      "Epoch [3159/10000] Train Loss: 0.008723 Val Loss: nan\n",
      "Epoch [3160/10000] Train Loss: 0.008555 Val Loss: nan\n",
      "Epoch [3161/10000] Train Loss: 0.008360 Val Loss: nan\n",
      "Epoch [3162/10000] Train Loss: 0.008464 Val Loss: nan\n",
      "Epoch [3163/10000] Train Loss: 0.008539 Val Loss: nan\n",
      "Epoch [3164/10000] Train Loss: 0.008657 Val Loss: nan\n",
      "Epoch [3165/10000] Train Loss: 0.008841 Val Loss: nan\n",
      "Epoch [3166/10000] Train Loss: 0.008448 Val Loss: nan\n",
      "Epoch [3167/10000] Train Loss: 0.008536 Val Loss: nan\n",
      "Epoch [3168/10000] Train Loss: 0.008676 Val Loss: nan\n",
      "Epoch [3169/10000] Train Loss: 0.008368 Val Loss: nan\n",
      "Epoch [3170/10000] Train Loss: 0.008487 Val Loss: nan\n",
      "Epoch [3171/10000] Train Loss: 0.008425 Val Loss: nan\n",
      "Epoch [3172/10000] Train Loss: 0.008617 Val Loss: nan\n",
      "Epoch [3173/10000] Train Loss: 0.008402 Val Loss: nan\n",
      "Epoch [3174/10000] Train Loss: 0.008601 Val Loss: nan\n",
      "Epoch [3175/10000] Train Loss: 0.008527 Val Loss: nan\n",
      "Epoch [3176/10000] Train Loss: 0.008455 Val Loss: nan\n",
      "Epoch [3177/10000] Train Loss: 0.008622 Val Loss: nan\n",
      "Epoch [3178/10000] Train Loss: 0.008365 Val Loss: nan\n",
      "Epoch [3179/10000] Train Loss: 0.008581 Val Loss: nan\n",
      "Epoch [3180/10000] Train Loss: 0.008467 Val Loss: nan\n",
      "Epoch [3181/10000] Train Loss: 0.008441 Val Loss: nan\n",
      "Epoch [3182/10000] Train Loss: 0.008357 Val Loss: nan\n",
      "Epoch [3183/10000] Train Loss: 0.008633 Val Loss: nan\n",
      "Epoch [3184/10000] Train Loss: 0.008660 Val Loss: nan\n",
      "Epoch [3185/10000] Train Loss: 0.008469 Val Loss: nan\n",
      "Epoch [3186/10000] Train Loss: 0.008582 Val Loss: nan\n",
      "Epoch [3187/10000] Train Loss: 0.008489 Val Loss: nan\n",
      "Epoch [3188/10000] Train Loss: 0.008453 Val Loss: nan\n",
      "Epoch [3189/10000] Train Loss: 0.008774 Val Loss: nan\n",
      "Epoch [3190/10000] Train Loss: 0.008309 Val Loss: nan\n",
      "Epoch [3191/10000] Train Loss: 0.008513 Val Loss: nan\n",
      "Epoch [3192/10000] Train Loss: 0.008447 Val Loss: nan\n",
      "Epoch [3193/10000] Train Loss: 0.008489 Val Loss: nan\n",
      "Epoch [3194/10000] Train Loss: 0.008731 Val Loss: nan\n",
      "Epoch [3195/10000] Train Loss: 0.008441 Val Loss: nan\n",
      "Epoch [3196/10000] Train Loss: 0.008284 Val Loss: nan\n",
      "Epoch [3197/10000] Train Loss: 0.008282 Val Loss: nan\n",
      "Epoch [3198/10000] Train Loss: 0.008368 Val Loss: nan\n",
      "Epoch [3199/10000] Train Loss: 0.008458 Val Loss: nan\n",
      "Epoch [3200/10000] Train Loss: 0.008441 Val Loss: nan\n",
      "Epoch [3201/10000] Train Loss: 0.008348 Val Loss: nan\n",
      "Epoch [3202/10000] Train Loss: 0.008563 Val Loss: nan\n",
      "Epoch [3203/10000] Train Loss: 0.008444 Val Loss: nan\n",
      "Epoch [3204/10000] Train Loss: 0.008744 Val Loss: nan\n",
      "Epoch [3205/10000] Train Loss: 0.008651 Val Loss: nan\n",
      "Epoch [3206/10000] Train Loss: 0.008434 Val Loss: nan\n",
      "Epoch [3207/10000] Train Loss: 0.008330 Val Loss: nan\n",
      "Epoch [3208/10000] Train Loss: 0.008499 Val Loss: nan\n",
      "Epoch [3209/10000] Train Loss: 0.008328 Val Loss: nan\n",
      "Epoch [3210/10000] Train Loss: 0.008324 Val Loss: nan\n",
      "Epoch [3211/10000] Train Loss: 0.008452 Val Loss: nan\n",
      "Epoch [3212/10000] Train Loss: 0.008511 Val Loss: nan\n",
      "Epoch [3213/10000] Train Loss: 0.008282 Val Loss: nan\n",
      "Epoch [3214/10000] Train Loss: 0.008599 Val Loss: nan\n",
      "Epoch [3215/10000] Train Loss: 0.008326 Val Loss: nan\n",
      "Epoch [3216/10000] Train Loss: 0.008548 Val Loss: nan\n",
      "Epoch [3217/10000] Train Loss: 0.008722 Val Loss: nan\n",
      "Epoch [3218/10000] Train Loss: 0.008482 Val Loss: nan\n",
      "Epoch [3219/10000] Train Loss: 0.008487 Val Loss: nan\n",
      "Epoch [3220/10000] Train Loss: 0.008288 Val Loss: nan\n",
      "Epoch [3221/10000] Train Loss: 0.008526 Val Loss: nan\n",
      "Epoch [3222/10000] Train Loss: 0.008658 Val Loss: nan\n",
      "Epoch [3223/10000] Train Loss: 0.008282 Val Loss: nan\n",
      "Epoch [3224/10000] Train Loss: 0.008526 Val Loss: nan\n",
      "Epoch [3225/10000] Train Loss: 0.008527 Val Loss: nan\n",
      "Epoch [3226/10000] Train Loss: 0.008406 Val Loss: nan\n",
      "Epoch [3227/10000] Train Loss: 0.008333 Val Loss: nan\n",
      "Epoch [3228/10000] Train Loss: 0.008310 Val Loss: nan\n",
      "Epoch [3229/10000] Train Loss: 0.008435 Val Loss: nan\n",
      "Epoch [3230/10000] Train Loss: 0.008502 Val Loss: nan\n",
      "Epoch [3231/10000] Train Loss: 0.008321 Val Loss: nan\n",
      "Epoch [3232/10000] Train Loss: 0.008664 Val Loss: nan\n",
      "Epoch [3233/10000] Train Loss: 0.008463 Val Loss: nan\n",
      "Epoch [3234/10000] Train Loss: 0.008408 Val Loss: nan\n",
      "Epoch [3235/10000] Train Loss: 0.008308 Val Loss: nan\n",
      "Epoch [3236/10000] Train Loss: 0.008403 Val Loss: nan\n",
      "Epoch [3237/10000] Train Loss: 0.008381 Val Loss: nan\n",
      "Epoch [3238/10000] Train Loss: 0.008568 Val Loss: nan\n",
      "Epoch [3239/10000] Train Loss: 0.008477 Val Loss: nan\n",
      "Epoch [3240/10000] Train Loss: 0.008541 Val Loss: nan\n",
      "Epoch [3241/10000] Train Loss: 0.008330 Val Loss: nan\n",
      "Epoch [3242/10000] Train Loss: 0.008415 Val Loss: nan\n",
      "Epoch [3243/10000] Train Loss: 0.008452 Val Loss: nan\n",
      "Epoch [3244/10000] Train Loss: 0.008553 Val Loss: nan\n",
      "Epoch [3245/10000] Train Loss: 0.008448 Val Loss: nan\n",
      "Epoch [3246/10000] Train Loss: 0.008578 Val Loss: nan\n",
      "Epoch [3247/10000] Train Loss: 0.008595 Val Loss: nan\n",
      "Epoch [3248/10000] Train Loss: 0.008429 Val Loss: nan\n",
      "Epoch [3249/10000] Train Loss: 0.008343 Val Loss: nan\n",
      "Epoch [3250/10000] Train Loss: 0.008291 Val Loss: nan\n",
      "Epoch [3251/10000] Train Loss: 0.008253 Val Loss: nan\n",
      "Epoch [3252/10000] Train Loss: 0.008256 Val Loss: nan\n",
      "Epoch [3253/10000] Train Loss: 0.008499 Val Loss: nan\n",
      "Epoch [3254/10000] Train Loss: 0.008661 Val Loss: nan\n",
      "Epoch [3255/10000] Train Loss: 0.008255 Val Loss: nan\n",
      "Epoch [3256/10000] Train Loss: 0.008467 Val Loss: nan\n",
      "Epoch [3257/10000] Train Loss: 0.008458 Val Loss: nan\n",
      "Epoch [3258/10000] Train Loss: 0.008275 Val Loss: nan\n",
      "Epoch [3259/10000] Train Loss: 0.008322 Val Loss: nan\n",
      "Epoch [3260/10000] Train Loss: 0.008682 Val Loss: nan\n",
      "Epoch [3261/10000] Train Loss: 0.008334 Val Loss: nan\n",
      "Epoch [3262/10000] Train Loss: 0.008492 Val Loss: nan\n",
      "Epoch [3263/10000] Train Loss: 0.008293 Val Loss: nan\n",
      "Epoch [3264/10000] Train Loss: 0.008432 Val Loss: nan\n",
      "Epoch [3265/10000] Train Loss: 0.008354 Val Loss: nan\n",
      "Epoch [3266/10000] Train Loss: 0.008288 Val Loss: nan\n",
      "Epoch [3267/10000] Train Loss: 0.008504 Val Loss: nan\n",
      "Epoch [3268/10000] Train Loss: 0.008320 Val Loss: nan\n",
      "Epoch [3269/10000] Train Loss: 0.008277 Val Loss: nan\n",
      "Epoch [3270/10000] Train Loss: 0.008386 Val Loss: nan\n",
      "Epoch [3271/10000] Train Loss: 0.008267 Val Loss: nan\n",
      "Epoch [3272/10000] Train Loss: 0.008415 Val Loss: nan\n",
      "Epoch [3273/10000] Train Loss: 0.008509 Val Loss: nan\n",
      "Epoch [3274/10000] Train Loss: 0.008324 Val Loss: nan\n",
      "Epoch [3275/10000] Train Loss: 0.008218 Val Loss: nan\n",
      "Epoch [3276/10000] Train Loss: 0.008362 Val Loss: nan\n",
      "Epoch [3277/10000] Train Loss: 0.008421 Val Loss: nan\n",
      "Epoch [3278/10000] Train Loss: 0.008334 Val Loss: nan\n",
      "Epoch [3279/10000] Train Loss: 0.008452 Val Loss: nan\n",
      "Epoch [3280/10000] Train Loss: 0.008227 Val Loss: nan\n",
      "Epoch [3281/10000] Train Loss: 0.008404 Val Loss: nan\n",
      "Epoch [3282/10000] Train Loss: 0.008388 Val Loss: nan\n",
      "Epoch [3283/10000] Train Loss: 0.008453 Val Loss: nan\n",
      "Epoch [3284/10000] Train Loss: 0.008381 Val Loss: nan\n",
      "Epoch [3285/10000] Train Loss: 0.008469 Val Loss: nan\n",
      "Epoch [3286/10000] Train Loss: 0.008452 Val Loss: nan\n",
      "Epoch [3287/10000] Train Loss: 0.008575 Val Loss: nan\n",
      "Epoch [3288/10000] Train Loss: 0.008234 Val Loss: nan\n",
      "Epoch [3289/10000] Train Loss: 0.008388 Val Loss: nan\n",
      "Epoch [3290/10000] Train Loss: 0.008208 Val Loss: nan\n",
      "Epoch [3291/10000] Train Loss: 0.008412 Val Loss: nan\n",
      "Epoch [3292/10000] Train Loss: 0.008536 Val Loss: nan\n",
      "Epoch [3293/10000] Train Loss: 0.008333 Val Loss: nan\n",
      "Epoch [3294/10000] Train Loss: 0.008300 Val Loss: nan\n",
      "Epoch [3295/10000] Train Loss: 0.008214 Val Loss: nan\n",
      "Epoch [3296/10000] Train Loss: 0.008215 Val Loss: nan\n",
      "Epoch [3297/10000] Train Loss: 0.008209 Val Loss: nan\n",
      "Epoch [3298/10000] Train Loss: 0.008386 Val Loss: nan\n",
      "Epoch [3299/10000] Train Loss: 0.008245 Val Loss: nan\n",
      "Epoch [3300/10000] Train Loss: 0.008219 Val Loss: nan\n",
      "Epoch [3301/10000] Train Loss: 0.008211 Val Loss: nan\n",
      "Epoch [3302/10000] Train Loss: 0.008449 Val Loss: nan\n",
      "Epoch [3303/10000] Train Loss: 0.008237 Val Loss: nan\n",
      "Epoch [3304/10000] Train Loss: 0.008517 Val Loss: nan\n",
      "Epoch [3305/10000] Train Loss: 0.008375 Val Loss: nan\n",
      "Epoch [3306/10000] Train Loss: 0.008690 Val Loss: nan\n",
      "Epoch [3307/10000] Train Loss: 0.008427 Val Loss: nan\n",
      "Epoch [3308/10000] Train Loss: 0.008372 Val Loss: nan\n",
      "Epoch [3309/10000] Train Loss: 0.008387 Val Loss: nan\n",
      "Epoch [3310/10000] Train Loss: 0.008439 Val Loss: nan\n",
      "Epoch [3311/10000] Train Loss: 0.008197 Val Loss: nan\n",
      "Epoch [3312/10000] Train Loss: 0.008219 Val Loss: nan\n",
      "Epoch [3313/10000] Train Loss: 0.008369 Val Loss: nan\n",
      "Epoch [3314/10000] Train Loss: 0.008283 Val Loss: nan\n",
      "Epoch [3315/10000] Train Loss: 0.008398 Val Loss: nan\n",
      "Epoch [3316/10000] Train Loss: 0.008598 Val Loss: nan\n",
      "Epoch [3317/10000] Train Loss: 0.008325 Val Loss: nan\n",
      "Epoch [3318/10000] Train Loss: 0.008546 Val Loss: nan\n",
      "Epoch [3319/10000] Train Loss: 0.008228 Val Loss: nan\n",
      "Epoch [3320/10000] Train Loss: 0.008239 Val Loss: nan\n",
      "Epoch [3321/10000] Train Loss: 0.008285 Val Loss: nan\n",
      "Epoch [3322/10000] Train Loss: 0.008298 Val Loss: nan\n",
      "Epoch [3323/10000] Train Loss: 0.008277 Val Loss: nan\n",
      "Epoch [3324/10000] Train Loss: 0.008389 Val Loss: nan\n",
      "Epoch [3325/10000] Train Loss: 0.008311 Val Loss: nan\n",
      "Epoch [3326/10000] Train Loss: 0.008205 Val Loss: nan\n",
      "Epoch [3327/10000] Train Loss: 0.008359 Val Loss: nan\n",
      "Epoch [3328/10000] Train Loss: 0.008373 Val Loss: nan\n",
      "Epoch [3329/10000] Train Loss: 0.008433 Val Loss: nan\n",
      "Epoch [3330/10000] Train Loss: 0.008526 Val Loss: nan\n",
      "Epoch [3331/10000] Train Loss: 0.008499 Val Loss: nan\n",
      "Epoch [3332/10000] Train Loss: 0.008258 Val Loss: nan\n",
      "Epoch [3333/10000] Train Loss: 0.008220 Val Loss: nan\n",
      "Epoch [3334/10000] Train Loss: 0.008236 Val Loss: nan\n",
      "Epoch [3335/10000] Train Loss: 0.008365 Val Loss: nan\n",
      "Epoch [3336/10000] Train Loss: 0.008219 Val Loss: nan\n",
      "Epoch [3337/10000] Train Loss: 0.008167 Val Loss: nan\n",
      "Epoch [3338/10000] Train Loss: 0.008205 Val Loss: nan\n",
      "Epoch [3339/10000] Train Loss: 0.008205 Val Loss: nan\n",
      "Epoch [3340/10000] Train Loss: 0.008259 Val Loss: nan\n",
      "Epoch [3341/10000] Train Loss: 0.008670 Val Loss: nan\n",
      "Epoch [3342/10000] Train Loss: 0.008308 Val Loss: nan\n",
      "Epoch [3343/10000] Train Loss: 0.008176 Val Loss: nan\n",
      "Epoch [3344/10000] Train Loss: 0.008493 Val Loss: nan\n",
      "Epoch [3345/10000] Train Loss: 0.008344 Val Loss: nan\n",
      "Epoch [3346/10000] Train Loss: 0.008352 Val Loss: nan\n",
      "Epoch [3347/10000] Train Loss: 0.008248 Val Loss: nan\n",
      "Epoch [3348/10000] Train Loss: 0.008184 Val Loss: nan\n",
      "Epoch [3349/10000] Train Loss: 0.008151 Val Loss: nan\n",
      "Epoch [3350/10000] Train Loss: 0.008334 Val Loss: nan\n",
      "Epoch [3351/10000] Train Loss: 0.008306 Val Loss: nan\n",
      "Epoch [3352/10000] Train Loss: 0.008342 Val Loss: nan\n",
      "Epoch [3353/10000] Train Loss: 0.008324 Val Loss: nan\n",
      "Epoch [3354/10000] Train Loss: 0.008431 Val Loss: nan\n",
      "Epoch [3355/10000] Train Loss: 0.008172 Val Loss: nan\n",
      "Epoch [3356/10000] Train Loss: 0.008171 Val Loss: nan\n",
      "Epoch [3357/10000] Train Loss: 0.008258 Val Loss: nan\n",
      "Epoch [3358/10000] Train Loss: 0.008268 Val Loss: nan\n",
      "Epoch [3359/10000] Train Loss: 0.008403 Val Loss: nan\n",
      "Epoch [3360/10000] Train Loss: 0.008454 Val Loss: nan\n",
      "Epoch [3361/10000] Train Loss: 0.008302 Val Loss: nan\n",
      "Epoch [3362/10000] Train Loss: 0.008215 Val Loss: nan\n",
      "Epoch [3363/10000] Train Loss: 0.008223 Val Loss: nan\n",
      "Epoch [3364/10000] Train Loss: 0.008495 Val Loss: nan\n",
      "Epoch [3365/10000] Train Loss: 0.008216 Val Loss: nan\n",
      "Epoch [3366/10000] Train Loss: 0.008209 Val Loss: nan\n",
      "Epoch [3367/10000] Train Loss: 0.008311 Val Loss: nan\n",
      "Epoch [3368/10000] Train Loss: 0.008580 Val Loss: nan\n",
      "Epoch [3369/10000] Train Loss: 0.008717 Val Loss: nan\n",
      "Epoch [3370/10000] Train Loss: 0.008320 Val Loss: nan\n",
      "Epoch [3371/10000] Train Loss: 0.008331 Val Loss: nan\n",
      "Epoch [3372/10000] Train Loss: 0.008178 Val Loss: nan\n",
      "Epoch [3373/10000] Train Loss: 0.008348 Val Loss: nan\n",
      "Epoch [3374/10000] Train Loss: 0.008267 Val Loss: nan\n",
      "Epoch [3375/10000] Train Loss: 0.008202 Val Loss: nan\n",
      "Epoch [3376/10000] Train Loss: 0.008309 Val Loss: nan\n",
      "Epoch [3377/10000] Train Loss: 0.008338 Val Loss: nan\n",
      "Epoch [3378/10000] Train Loss: 0.008320 Val Loss: nan\n",
      "Epoch [3379/10000] Train Loss: 0.008203 Val Loss: nan\n",
      "Epoch [3380/10000] Train Loss: 0.008324 Val Loss: nan\n",
      "Epoch [3381/10000] Train Loss: 0.008367 Val Loss: nan\n",
      "Epoch [3382/10000] Train Loss: 0.008220 Val Loss: nan\n",
      "Epoch [3383/10000] Train Loss: 0.008337 Val Loss: nan\n",
      "Epoch [3384/10000] Train Loss: 0.008427 Val Loss: nan\n",
      "Epoch [3385/10000] Train Loss: 0.008159 Val Loss: nan\n",
      "Epoch [3386/10000] Train Loss: 0.008170 Val Loss: nan\n",
      "Epoch [3387/10000] Train Loss: 0.008496 Val Loss: nan\n",
      "Epoch [3388/10000] Train Loss: 0.008511 Val Loss: nan\n",
      "Epoch [3389/10000] Train Loss: 0.008534 Val Loss: nan\n",
      "Epoch [3390/10000] Train Loss: 0.008351 Val Loss: nan\n",
      "Epoch [3391/10000] Train Loss: 0.008406 Val Loss: nan\n",
      "Epoch [3392/10000] Train Loss: 0.008546 Val Loss: nan\n",
      "Epoch [3393/10000] Train Loss: 0.008132 Val Loss: nan\n",
      "Epoch [3394/10000] Train Loss: 0.008182 Val Loss: nan\n",
      "Epoch [3395/10000] Train Loss: 0.008508 Val Loss: nan\n",
      "Epoch [3396/10000] Train Loss: 0.008330 Val Loss: nan\n",
      "Epoch [3397/10000] Train Loss: 0.008305 Val Loss: nan\n",
      "Epoch [3398/10000] Train Loss: 0.008140 Val Loss: nan\n",
      "Epoch [3399/10000] Train Loss: 0.008154 Val Loss: nan\n",
      "Epoch [3400/10000] Train Loss: 0.008275 Val Loss: nan\n",
      "Epoch [3401/10000] Train Loss: 0.008368 Val Loss: nan\n",
      "Epoch [3402/10000] Train Loss: 0.008387 Val Loss: nan\n",
      "Epoch [3403/10000] Train Loss: 0.008180 Val Loss: nan\n",
      "Epoch [3404/10000] Train Loss: 0.008137 Val Loss: nan\n",
      "Epoch [3405/10000] Train Loss: 0.008168 Val Loss: nan\n",
      "Epoch [3406/10000] Train Loss: 0.008387 Val Loss: nan\n",
      "Epoch [3407/10000] Train Loss: 0.008293 Val Loss: nan\n",
      "Epoch [3408/10000] Train Loss: 0.008192 Val Loss: nan\n",
      "Epoch [3409/10000] Train Loss: 0.008276 Val Loss: nan\n",
      "Epoch [3410/10000] Train Loss: 0.008189 Val Loss: nan\n",
      "Epoch [3411/10000] Train Loss: 0.008167 Val Loss: nan\n",
      "Epoch [3412/10000] Train Loss: 0.008308 Val Loss: nan\n",
      "Epoch [3413/10000] Train Loss: 0.008267 Val Loss: nan\n",
      "Epoch [3414/10000] Train Loss: 0.008246 Val Loss: nan\n",
      "Epoch [3415/10000] Train Loss: 0.008352 Val Loss: nan\n",
      "Epoch [3416/10000] Train Loss: 0.008252 Val Loss: nan\n",
      "Epoch [3417/10000] Train Loss: 0.008261 Val Loss: nan\n",
      "Epoch [3418/10000] Train Loss: 0.008295 Val Loss: nan\n",
      "Epoch [3419/10000] Train Loss: 0.008107 Val Loss: nan\n",
      "Epoch [3420/10000] Train Loss: 0.008298 Val Loss: nan\n",
      "Epoch [3421/10000] Train Loss: 0.008165 Val Loss: nan\n",
      "Epoch [3422/10000] Train Loss: 0.008451 Val Loss: nan\n",
      "Epoch [3423/10000] Train Loss: 0.008559 Val Loss: nan\n",
      "Epoch [3424/10000] Train Loss: 0.008255 Val Loss: nan\n",
      "Epoch [3425/10000] Train Loss: 0.008182 Val Loss: nan\n",
      "Epoch [3426/10000] Train Loss: 0.008377 Val Loss: nan\n",
      "Epoch [3427/10000] Train Loss: 0.008216 Val Loss: nan\n",
      "Epoch [3428/10000] Train Loss: 0.008283 Val Loss: nan\n",
      "Epoch [3429/10000] Train Loss: 0.008143 Val Loss: nan\n",
      "Epoch [3430/10000] Train Loss: 0.008135 Val Loss: nan\n",
      "Epoch [3431/10000] Train Loss: 0.008282 Val Loss: nan\n",
      "Epoch [3432/10000] Train Loss: 0.008395 Val Loss: nan\n",
      "Epoch [3433/10000] Train Loss: 0.008258 Val Loss: nan\n",
      "Epoch [3434/10000] Train Loss: 0.008247 Val Loss: nan\n",
      "Epoch [3435/10000] Train Loss: 0.008134 Val Loss: nan\n",
      "Epoch [3436/10000] Train Loss: 0.008466 Val Loss: nan\n",
      "Epoch [3437/10000] Train Loss: 0.008222 Val Loss: nan\n",
      "Epoch [3438/10000] Train Loss: 0.008193 Val Loss: nan\n",
      "Epoch [3439/10000] Train Loss: 0.008108 Val Loss: nan\n",
      "Epoch [3440/10000] Train Loss: 0.008150 Val Loss: nan\n",
      "Epoch [3441/10000] Train Loss: 0.008293 Val Loss: nan\n",
      "Epoch [3442/10000] Train Loss: 0.008318 Val Loss: nan\n",
      "Epoch [3443/10000] Train Loss: 0.008241 Val Loss: nan\n",
      "Epoch [3444/10000] Train Loss: 0.008420 Val Loss: nan\n",
      "Epoch [3445/10000] Train Loss: 0.008346 Val Loss: nan\n",
      "Epoch [3446/10000] Train Loss: 0.008267 Val Loss: nan\n",
      "Epoch [3447/10000] Train Loss: 0.008283 Val Loss: nan\n",
      "Epoch [3448/10000] Train Loss: 0.008099 Val Loss: nan\n",
      "Epoch [3449/10000] Train Loss: 0.008128 Val Loss: nan\n",
      "Epoch [3450/10000] Train Loss: 0.008326 Val Loss: nan\n",
      "Epoch [3451/10000] Train Loss: 0.008140 Val Loss: nan\n",
      "Epoch [3452/10000] Train Loss: 0.008526 Val Loss: nan\n",
      "Epoch [3453/10000] Train Loss: 0.008219 Val Loss: nan\n",
      "Epoch [3454/10000] Train Loss: 0.008295 Val Loss: nan\n",
      "Epoch [3455/10000] Train Loss: 0.008448 Val Loss: nan\n",
      "Epoch [3456/10000] Train Loss: 0.008515 Val Loss: nan\n",
      "Epoch [3457/10000] Train Loss: 0.008243 Val Loss: nan\n",
      "Epoch [3458/10000] Train Loss: 0.008241 Val Loss: nan\n",
      "Epoch [3459/10000] Train Loss: 0.008217 Val Loss: nan\n",
      "Epoch [3460/10000] Train Loss: 0.008232 Val Loss: nan\n",
      "Epoch [3461/10000] Train Loss: 0.008332 Val Loss: nan\n",
      "Epoch [3462/10000] Train Loss: 0.008219 Val Loss: nan\n",
      "Epoch [3463/10000] Train Loss: 0.008256 Val Loss: nan\n",
      "Epoch [3464/10000] Train Loss: 0.008337 Val Loss: nan\n",
      "Epoch [3465/10000] Train Loss: 0.008188 Val Loss: nan\n",
      "Epoch [3466/10000] Train Loss: 0.008175 Val Loss: nan\n",
      "Epoch [3467/10000] Train Loss: 0.008098 Val Loss: nan\n",
      "Epoch [3468/10000] Train Loss: 0.008088 Val Loss: nan\n",
      "Epoch [3469/10000] Train Loss: 0.008075 Val Loss: nan\n",
      "Epoch [3470/10000] Train Loss: 0.008076 Val Loss: nan\n",
      "Epoch [3471/10000] Train Loss: 0.008153 Val Loss: nan\n",
      "Epoch [3472/10000] Train Loss: 0.008072 Val Loss: nan\n",
      "Epoch [3473/10000] Train Loss: 0.008470 Val Loss: nan\n",
      "Epoch [3474/10000] Train Loss: 0.008319 Val Loss: nan\n",
      "Epoch [3475/10000] Train Loss: 0.008153 Val Loss: nan\n",
      "Epoch [3476/10000] Train Loss: 0.008233 Val Loss: nan\n",
      "Epoch [3477/10000] Train Loss: 0.008069 Val Loss: nan\n",
      "Epoch [3478/10000] Train Loss: 0.008337 Val Loss: nan\n",
      "Epoch [3479/10000] Train Loss: 0.008113 Val Loss: nan\n",
      "Epoch [3480/10000] Train Loss: 0.008313 Val Loss: nan\n",
      "Epoch [3481/10000] Train Loss: 0.008293 Val Loss: nan\n",
      "Epoch [3482/10000] Train Loss: 0.008084 Val Loss: nan\n",
      "Epoch [3483/10000] Train Loss: 0.008287 Val Loss: nan\n",
      "Epoch [3484/10000] Train Loss: 0.008149 Val Loss: nan\n",
      "Epoch [3485/10000] Train Loss: 0.008229 Val Loss: nan\n",
      "Epoch [3486/10000] Train Loss: 0.008152 Val Loss: nan\n",
      "Epoch [3487/10000] Train Loss: 0.008195 Val Loss: nan\n",
      "Epoch [3488/10000] Train Loss: 0.008118 Val Loss: nan\n",
      "Epoch [3489/10000] Train Loss: 0.008374 Val Loss: nan\n",
      "Epoch [3490/10000] Train Loss: 0.008162 Val Loss: nan\n",
      "Epoch [3491/10000] Train Loss: 0.008126 Val Loss: nan\n",
      "Epoch [3492/10000] Train Loss: 0.008372 Val Loss: nan\n",
      "Epoch [3493/10000] Train Loss: 0.008287 Val Loss: nan\n",
      "Epoch [3494/10000] Train Loss: 0.008213 Val Loss: nan\n",
      "Epoch [3495/10000] Train Loss: 0.008238 Val Loss: nan\n",
      "Epoch [3496/10000] Train Loss: 0.008093 Val Loss: nan\n",
      "Epoch [3497/10000] Train Loss: 0.008097 Val Loss: nan\n",
      "Epoch [3498/10000] Train Loss: 0.008063 Val Loss: nan\n",
      "Epoch [3499/10000] Train Loss: 0.008120 Val Loss: nan\n",
      "Epoch [3500/10000] Train Loss: 0.008132 Val Loss: nan\n",
      "Epoch [3501/10000] Train Loss: 0.008299 Val Loss: nan\n",
      "Epoch [3502/10000] Train Loss: 0.008128 Val Loss: nan\n",
      "Epoch [3503/10000] Train Loss: 0.008202 Val Loss: nan\n",
      "Epoch [3504/10000] Train Loss: 0.008205 Val Loss: nan\n",
      "Epoch [3505/10000] Train Loss: 0.008156 Val Loss: nan\n",
      "Epoch [3506/10000] Train Loss: 0.008181 Val Loss: nan\n",
      "Epoch [3507/10000] Train Loss: 0.008112 Val Loss: nan\n",
      "Epoch [3508/10000] Train Loss: 0.008055 Val Loss: nan\n",
      "Epoch [3509/10000] Train Loss: 0.008093 Val Loss: nan\n",
      "Epoch [3510/10000] Train Loss: 0.008293 Val Loss: nan\n",
      "Epoch [3511/10000] Train Loss: 0.008101 Val Loss: nan\n",
      "Epoch [3512/10000] Train Loss: 0.008147 Val Loss: nan\n",
      "Epoch [3513/10000] Train Loss: 0.008282 Val Loss: nan\n",
      "Epoch [3514/10000] Train Loss: 0.008127 Val Loss: nan\n",
      "Epoch [3515/10000] Train Loss: 0.008355 Val Loss: nan\n",
      "Epoch [3516/10000] Train Loss: 0.008099 Val Loss: nan\n",
      "Epoch [3517/10000] Train Loss: 0.008277 Val Loss: nan\n",
      "Epoch [3518/10000] Train Loss: 0.008299 Val Loss: nan\n",
      "Epoch [3519/10000] Train Loss: 0.008025 Val Loss: nan\n",
      "Epoch [3520/10000] Train Loss: 0.008186 Val Loss: nan\n",
      "Epoch [3521/10000] Train Loss: 0.008107 Val Loss: nan\n",
      "Epoch [3522/10000] Train Loss: 0.008329 Val Loss: nan\n",
      "Epoch [3523/10000] Train Loss: 0.008097 Val Loss: nan\n",
      "Epoch [3524/10000] Train Loss: 0.008247 Val Loss: nan\n",
      "Epoch [3525/10000] Train Loss: 0.008356 Val Loss: nan\n",
      "Epoch [3526/10000] Train Loss: 0.008118 Val Loss: nan\n",
      "Epoch [3527/10000] Train Loss: 0.008341 Val Loss: nan\n",
      "Epoch [3528/10000] Train Loss: 0.008141 Val Loss: nan\n",
      "Epoch [3529/10000] Train Loss: 0.008195 Val Loss: nan\n",
      "Epoch [3530/10000] Train Loss: 0.008424 Val Loss: nan\n",
      "Epoch [3531/10000] Train Loss: 0.008016 Val Loss: nan\n",
      "Epoch [3532/10000] Train Loss: 0.008153 Val Loss: nan\n",
      "Epoch [3533/10000] Train Loss: 0.008247 Val Loss: nan\n",
      "Epoch [3534/10000] Train Loss: 0.008185 Val Loss: nan\n",
      "Epoch [3535/10000] Train Loss: 0.008143 Val Loss: nan\n",
      "Epoch [3536/10000] Train Loss: 0.008241 Val Loss: nan\n",
      "Epoch [3537/10000] Train Loss: 0.008240 Val Loss: nan\n",
      "Epoch [3538/10000] Train Loss: 0.008068 Val Loss: nan\n",
      "Epoch [3539/10000] Train Loss: 0.008394 Val Loss: nan\n",
      "Epoch [3540/10000] Train Loss: 0.008193 Val Loss: nan\n",
      "Epoch [3541/10000] Train Loss: 0.008372 Val Loss: nan\n",
      "Epoch [3542/10000] Train Loss: 0.008548 Val Loss: nan\n",
      "Epoch [3543/10000] Train Loss: 0.008244 Val Loss: nan\n",
      "Epoch [3544/10000] Train Loss: 0.008172 Val Loss: nan\n",
      "Epoch [3545/10000] Train Loss: 0.008136 Val Loss: nan\n",
      "Epoch [3546/10000] Train Loss: 0.008221 Val Loss: nan\n",
      "Epoch [3547/10000] Train Loss: 0.008051 Val Loss: nan\n",
      "Epoch [3548/10000] Train Loss: 0.008220 Val Loss: nan\n",
      "Epoch [3549/10000] Train Loss: 0.008043 Val Loss: nan\n",
      "Epoch [3550/10000] Train Loss: 0.008051 Val Loss: nan\n",
      "Epoch [3551/10000] Train Loss: 0.008162 Val Loss: nan\n",
      "Epoch [3552/10000] Train Loss: 0.008094 Val Loss: nan\n",
      "Epoch [3553/10000] Train Loss: 0.008086 Val Loss: nan\n",
      "Epoch [3554/10000] Train Loss: 0.008149 Val Loss: nan\n",
      "Epoch [3555/10000] Train Loss: 0.008310 Val Loss: nan\n",
      "Epoch [3556/10000] Train Loss: 0.008511 Val Loss: nan\n",
      "Epoch [3557/10000] Train Loss: 0.008317 Val Loss: nan\n",
      "Epoch [3558/10000] Train Loss: 0.008173 Val Loss: nan\n",
      "Epoch [3559/10000] Train Loss: 0.008256 Val Loss: nan\n",
      "Epoch [3560/10000] Train Loss: 0.008088 Val Loss: nan\n",
      "Epoch [3561/10000] Train Loss: 0.008142 Val Loss: nan\n",
      "Epoch [3562/10000] Train Loss: 0.008210 Val Loss: nan\n",
      "Epoch [3563/10000] Train Loss: 0.008211 Val Loss: nan\n",
      "Epoch [3564/10000] Train Loss: 0.008012 Val Loss: nan\n",
      "Epoch [3565/10000] Train Loss: 0.008191 Val Loss: nan\n",
      "Epoch [3566/10000] Train Loss: 0.008199 Val Loss: nan\n",
      "Epoch [3567/10000] Train Loss: 0.008026 Val Loss: nan\n",
      "Epoch [3568/10000] Train Loss: 0.008040 Val Loss: nan\n",
      "Epoch [3569/10000] Train Loss: 0.008058 Val Loss: nan\n",
      "Epoch [3570/10000] Train Loss: 0.008277 Val Loss: nan\n",
      "Epoch [3571/10000] Train Loss: 0.008138 Val Loss: nan\n",
      "Epoch [3572/10000] Train Loss: 0.008143 Val Loss: nan\n",
      "Epoch [3573/10000] Train Loss: 0.008346 Val Loss: nan\n",
      "Epoch [3574/10000] Train Loss: 0.008139 Val Loss: nan\n",
      "Epoch [3575/10000] Train Loss: 0.008030 Val Loss: nan\n",
      "Epoch [3576/10000] Train Loss: 0.008181 Val Loss: nan\n",
      "Epoch [3577/10000] Train Loss: 0.008047 Val Loss: nan\n",
      "Epoch [3578/10000] Train Loss: 0.008299 Val Loss: nan\n",
      "Epoch [3579/10000] Train Loss: 0.008139 Val Loss: nan\n",
      "Epoch [3580/10000] Train Loss: 0.008321 Val Loss: nan\n",
      "Epoch [3581/10000] Train Loss: 0.008152 Val Loss: nan\n",
      "Epoch [3582/10000] Train Loss: 0.008115 Val Loss: nan\n",
      "Epoch [3583/10000] Train Loss: 0.008218 Val Loss: nan\n",
      "Epoch [3584/10000] Train Loss: 0.008083 Val Loss: nan\n",
      "Epoch [3585/10000] Train Loss: 0.008335 Val Loss: nan\n",
      "Epoch [3586/10000] Train Loss: 0.008085 Val Loss: nan\n",
      "Epoch [3587/10000] Train Loss: 0.008173 Val Loss: nan\n",
      "Epoch [3588/10000] Train Loss: 0.008063 Val Loss: nan\n",
      "Epoch [3589/10000] Train Loss: 0.008175 Val Loss: nan\n",
      "Epoch [3590/10000] Train Loss: 0.008280 Val Loss: nan\n",
      "Epoch [3591/10000] Train Loss: 0.008086 Val Loss: nan\n",
      "Epoch [3592/10000] Train Loss: 0.008296 Val Loss: nan\n",
      "Epoch [3593/10000] Train Loss: 0.008142 Val Loss: nan\n",
      "Epoch [3594/10000] Train Loss: 0.008273 Val Loss: nan\n",
      "Epoch [3595/10000] Train Loss: 0.008085 Val Loss: nan\n",
      "Epoch [3596/10000] Train Loss: 0.008036 Val Loss: nan\n",
      "Epoch [3597/10000] Train Loss: 0.008038 Val Loss: nan\n",
      "Epoch [3598/10000] Train Loss: 0.008262 Val Loss: nan\n",
      "Epoch [3599/10000] Train Loss: 0.008087 Val Loss: nan\n",
      "Epoch [3600/10000] Train Loss: 0.008054 Val Loss: nan\n",
      "Epoch [3601/10000] Train Loss: 0.008228 Val Loss: nan\n",
      "Epoch [3602/10000] Train Loss: 0.008045 Val Loss: nan\n",
      "Epoch [3603/10000] Train Loss: 0.008076 Val Loss: nan\n",
      "Epoch [3604/10000] Train Loss: 0.008165 Val Loss: nan\n",
      "Epoch [3605/10000] Train Loss: 0.008215 Val Loss: nan\n",
      "Epoch [3606/10000] Train Loss: 0.008086 Val Loss: nan\n",
      "Epoch [3607/10000] Train Loss: 0.008319 Val Loss: nan\n",
      "Epoch [3608/10000] Train Loss: 0.008158 Val Loss: nan\n",
      "Epoch [3609/10000] Train Loss: 0.008231 Val Loss: nan\n",
      "Epoch [3610/10000] Train Loss: 0.008287 Val Loss: nan\n",
      "Epoch [3611/10000] Train Loss: 0.008139 Val Loss: nan\n",
      "Epoch [3612/10000] Train Loss: 0.007988 Val Loss: nan\n",
      "Epoch [3613/10000] Train Loss: 0.007993 Val Loss: nan\n",
      "Epoch [3614/10000] Train Loss: 0.008206 Val Loss: nan\n",
      "Epoch [3615/10000] Train Loss: 0.008048 Val Loss: nan\n",
      "Epoch [3616/10000] Train Loss: 0.008133 Val Loss: nan\n",
      "Epoch [3617/10000] Train Loss: 0.008053 Val Loss: nan\n",
      "Epoch [3618/10000] Train Loss: 0.008105 Val Loss: nan\n",
      "Epoch [3619/10000] Train Loss: 0.008209 Val Loss: nan\n",
      "Epoch [3620/10000] Train Loss: 0.008019 Val Loss: nan\n",
      "Epoch [3621/10000] Train Loss: 0.008027 Val Loss: nan\n",
      "Epoch [3622/10000] Train Loss: 0.008072 Val Loss: nan\n",
      "Epoch [3623/10000] Train Loss: 0.008070 Val Loss: nan\n",
      "Epoch [3624/10000] Train Loss: 0.008024 Val Loss: nan\n",
      "Epoch [3625/10000] Train Loss: 0.008036 Val Loss: nan\n",
      "Epoch [3626/10000] Train Loss: 0.008182 Val Loss: nan\n",
      "Epoch [3627/10000] Train Loss: 0.008148 Val Loss: nan\n",
      "Epoch [3628/10000] Train Loss: 0.008107 Val Loss: nan\n",
      "Epoch [3629/10000] Train Loss: 0.008215 Val Loss: nan\n",
      "Epoch [3630/10000] Train Loss: 0.008006 Val Loss: nan\n",
      "Epoch [3631/10000] Train Loss: 0.008149 Val Loss: nan\n",
      "Epoch [3632/10000] Train Loss: 0.008071 Val Loss: nan\n",
      "Epoch [3633/10000] Train Loss: 0.007987 Val Loss: nan\n",
      "Epoch [3634/10000] Train Loss: 0.008012 Val Loss: nan\n",
      "Epoch [3635/10000] Train Loss: 0.008235 Val Loss: nan\n",
      "Epoch [3636/10000] Train Loss: 0.008028 Val Loss: nan\n",
      "Epoch [3637/10000] Train Loss: 0.007984 Val Loss: nan\n",
      "Epoch [3638/10000] Train Loss: 0.008116 Val Loss: nan\n",
      "Epoch [3639/10000] Train Loss: 0.008008 Val Loss: nan\n",
      "Epoch [3640/10000] Train Loss: 0.008285 Val Loss: nan\n",
      "Epoch [3641/10000] Train Loss: 0.008319 Val Loss: nan\n",
      "Epoch [3642/10000] Train Loss: 0.008086 Val Loss: nan\n",
      "Epoch [3643/10000] Train Loss: 0.008024 Val Loss: nan\n",
      "Epoch [3644/10000] Train Loss: 0.008094 Val Loss: nan\n",
      "Epoch [3645/10000] Train Loss: 0.008149 Val Loss: nan\n",
      "Epoch [3646/10000] Train Loss: 0.008033 Val Loss: nan\n",
      "Epoch [3647/10000] Train Loss: 0.008127 Val Loss: nan\n",
      "Epoch [3648/10000] Train Loss: 0.008034 Val Loss: nan\n",
      "Epoch [3649/10000] Train Loss: 0.008097 Val Loss: nan\n",
      "Epoch [3650/10000] Train Loss: 0.008162 Val Loss: nan\n",
      "Epoch [3651/10000] Train Loss: 0.008014 Val Loss: nan\n",
      "Epoch [3652/10000] Train Loss: 0.007985 Val Loss: nan\n",
      "Epoch [3653/10000] Train Loss: 0.008011 Val Loss: nan\n",
      "Epoch [3654/10000] Train Loss: 0.008137 Val Loss: nan\n",
      "Epoch [3655/10000] Train Loss: 0.007966 Val Loss: nan\n",
      "Epoch [3656/10000] Train Loss: 0.007967 Val Loss: nan\n",
      "Epoch [3657/10000] Train Loss: 0.008336 Val Loss: nan\n",
      "Epoch [3658/10000] Train Loss: 0.008115 Val Loss: nan\n",
      "Epoch [3659/10000] Train Loss: 0.008151 Val Loss: nan\n",
      "Epoch [3660/10000] Train Loss: 0.008116 Val Loss: nan\n",
      "Epoch [3661/10000] Train Loss: 0.008180 Val Loss: nan\n",
      "Epoch [3662/10000] Train Loss: 0.008326 Val Loss: nan\n",
      "Epoch [3663/10000] Train Loss: 0.007978 Val Loss: nan\n",
      "Epoch [3664/10000] Train Loss: 0.008093 Val Loss: nan\n",
      "Epoch [3665/10000] Train Loss: 0.008145 Val Loss: nan\n",
      "Epoch [3666/10000] Train Loss: 0.008251 Val Loss: nan\n",
      "Epoch [3667/10000] Train Loss: 0.007990 Val Loss: nan\n",
      "Epoch [3668/10000] Train Loss: 0.008503 Val Loss: nan\n",
      "Epoch [3669/10000] Train Loss: 0.007957 Val Loss: nan\n",
      "Epoch [3670/10000] Train Loss: 0.007971 Val Loss: nan\n",
      "Epoch [3671/10000] Train Loss: 0.007977 Val Loss: nan\n",
      "Epoch [3672/10000] Train Loss: 0.008159 Val Loss: nan\n",
      "Epoch [3673/10000] Train Loss: 0.008049 Val Loss: nan\n",
      "Epoch [3674/10000] Train Loss: 0.008110 Val Loss: nan\n",
      "Epoch [3675/10000] Train Loss: 0.007947 Val Loss: nan\n",
      "Epoch [3676/10000] Train Loss: 0.008138 Val Loss: nan\n",
      "Epoch [3677/10000] Train Loss: 0.008010 Val Loss: nan\n",
      "Epoch [3678/10000] Train Loss: 0.008013 Val Loss: nan\n",
      "Epoch [3679/10000] Train Loss: 0.008258 Val Loss: nan\n",
      "Epoch [3680/10000] Train Loss: 0.008033 Val Loss: nan\n",
      "Epoch [3681/10000] Train Loss: 0.008201 Val Loss: nan\n",
      "Epoch [3682/10000] Train Loss: 0.008125 Val Loss: nan\n",
      "Epoch [3683/10000] Train Loss: 0.008099 Val Loss: nan\n",
      "Epoch [3684/10000] Train Loss: 0.007926 Val Loss: nan\n",
      "Epoch [3685/10000] Train Loss: 0.008139 Val Loss: nan\n",
      "Epoch [3686/10000] Train Loss: 0.008079 Val Loss: nan\n",
      "Epoch [3687/10000] Train Loss: 0.008175 Val Loss: nan\n",
      "Epoch [3688/10000] Train Loss: 0.007972 Val Loss: nan\n",
      "Epoch [3689/10000] Train Loss: 0.008048 Val Loss: nan\n",
      "Epoch [3690/10000] Train Loss: 0.007908 Val Loss: nan\n",
      "Epoch [3691/10000] Train Loss: 0.007983 Val Loss: nan\n",
      "Epoch [3692/10000] Train Loss: 0.008164 Val Loss: nan\n",
      "Epoch [3693/10000] Train Loss: 0.008096 Val Loss: nan\n",
      "Epoch [3694/10000] Train Loss: 0.008276 Val Loss: nan\n",
      "Epoch [3695/10000] Train Loss: 0.007972 Val Loss: nan\n",
      "Epoch [3696/10000] Train Loss: 0.007972 Val Loss: nan\n",
      "Epoch [3697/10000] Train Loss: 0.008403 Val Loss: nan\n",
      "Epoch [3698/10000] Train Loss: 0.008143 Val Loss: nan\n",
      "Epoch [3699/10000] Train Loss: 0.008105 Val Loss: nan\n",
      "Epoch [3700/10000] Train Loss: 0.008027 Val Loss: nan\n",
      "Epoch [3701/10000] Train Loss: 0.008015 Val Loss: nan\n",
      "Epoch [3702/10000] Train Loss: 0.008105 Val Loss: nan\n",
      "Epoch [3703/10000] Train Loss: 0.007941 Val Loss: nan\n",
      "Epoch [3704/10000] Train Loss: 0.007949 Val Loss: nan\n",
      "Epoch [3705/10000] Train Loss: 0.007966 Val Loss: nan\n",
      "Epoch [3706/10000] Train Loss: 0.008031 Val Loss: nan\n",
      "Epoch [3707/10000] Train Loss: 0.008147 Val Loss: nan\n",
      "Epoch [3708/10000] Train Loss: 0.008422 Val Loss: nan\n",
      "Epoch [3709/10000] Train Loss: 0.008116 Val Loss: nan\n",
      "Epoch [3710/10000] Train Loss: 0.008155 Val Loss: nan\n",
      "Epoch [3711/10000] Train Loss: 0.008043 Val Loss: nan\n",
      "Epoch [3712/10000] Train Loss: 0.008063 Val Loss: nan\n",
      "Epoch [3713/10000] Train Loss: 0.008108 Val Loss: nan\n",
      "Epoch [3714/10000] Train Loss: 0.008159 Val Loss: nan\n",
      "Epoch [3715/10000] Train Loss: 0.008034 Val Loss: nan\n",
      "Epoch [3716/10000] Train Loss: 0.008098 Val Loss: nan\n",
      "Epoch [3717/10000] Train Loss: 0.008223 Val Loss: nan\n",
      "Epoch [3718/10000] Train Loss: 0.008012 Val Loss: nan\n",
      "Epoch [3719/10000] Train Loss: 0.008027 Val Loss: nan\n",
      "Epoch [3720/10000] Train Loss: 0.008176 Val Loss: nan\n",
      "Epoch [3721/10000] Train Loss: 0.008045 Val Loss: nan\n",
      "Epoch [3722/10000] Train Loss: 0.008110 Val Loss: nan\n",
      "Epoch [3723/10000] Train Loss: 0.007954 Val Loss: nan\n",
      "Epoch [3724/10000] Train Loss: 0.007937 Val Loss: nan\n",
      "Epoch [3725/10000] Train Loss: 0.008072 Val Loss: nan\n",
      "Epoch [3726/10000] Train Loss: 0.007940 Val Loss: nan\n",
      "Epoch [3727/10000] Train Loss: 0.008134 Val Loss: nan\n",
      "Epoch [3728/10000] Train Loss: 0.008188 Val Loss: nan\n",
      "Epoch [3729/10000] Train Loss: 0.008107 Val Loss: nan\n",
      "Epoch [3730/10000] Train Loss: 0.008174 Val Loss: nan\n",
      "Epoch [3731/10000] Train Loss: 0.008075 Val Loss: nan\n",
      "Epoch [3732/10000] Train Loss: 0.008081 Val Loss: nan\n",
      "Epoch [3733/10000] Train Loss: 0.007910 Val Loss: nan\n",
      "Epoch [3734/10000] Train Loss: 0.008073 Val Loss: nan\n",
      "Epoch [3735/10000] Train Loss: 0.008175 Val Loss: nan\n",
      "Epoch [3736/10000] Train Loss: 0.008191 Val Loss: nan\n",
      "Epoch [3737/10000] Train Loss: 0.008088 Val Loss: nan\n",
      "Epoch [3738/10000] Train Loss: 0.008223 Val Loss: nan\n",
      "Epoch [3739/10000] Train Loss: 0.007995 Val Loss: nan\n",
      "Epoch [3740/10000] Train Loss: 0.008063 Val Loss: nan\n",
      "Epoch [3741/10000] Train Loss: 0.008212 Val Loss: nan\n",
      "Epoch [3742/10000] Train Loss: 0.008084 Val Loss: nan\n",
      "Epoch [3743/10000] Train Loss: 0.008091 Val Loss: nan\n",
      "Epoch [3744/10000] Train Loss: 0.007957 Val Loss: nan\n",
      "Epoch [3745/10000] Train Loss: 0.007916 Val Loss: nan\n",
      "Epoch [3746/10000] Train Loss: 0.007984 Val Loss: nan\n",
      "Epoch [3747/10000] Train Loss: 0.008086 Val Loss: nan\n",
      "Epoch [3748/10000] Train Loss: 0.008201 Val Loss: nan\n",
      "Epoch [3749/10000] Train Loss: 0.007903 Val Loss: nan\n",
      "Epoch [3750/10000] Train Loss: 0.007939 Val Loss: nan\n",
      "Epoch [3751/10000] Train Loss: 0.008076 Val Loss: nan\n",
      "Epoch [3752/10000] Train Loss: 0.007941 Val Loss: nan\n",
      "Epoch [3753/10000] Train Loss: 0.007894 Val Loss: nan\n",
      "Epoch [3754/10000] Train Loss: 0.008008 Val Loss: nan\n",
      "Epoch [3755/10000] Train Loss: 0.007987 Val Loss: nan\n",
      "Epoch [3756/10000] Train Loss: 0.007944 Val Loss: nan\n",
      "Epoch [3757/10000] Train Loss: 0.007938 Val Loss: nan\n",
      "Epoch [3758/10000] Train Loss: 0.008077 Val Loss: nan\n",
      "Epoch [3759/10000] Train Loss: 0.007951 Val Loss: nan\n",
      "Epoch [3760/10000] Train Loss: 0.007954 Val Loss: nan\n",
      "Epoch [3761/10000] Train Loss: 0.007972 Val Loss: nan\n",
      "Epoch [3762/10000] Train Loss: 0.007929 Val Loss: nan\n",
      "Epoch [3763/10000] Train Loss: 0.008168 Val Loss: nan\n",
      "Epoch [3764/10000] Train Loss: 0.008071 Val Loss: nan\n",
      "Epoch [3765/10000] Train Loss: 0.008005 Val Loss: nan\n",
      "Epoch [3766/10000] Train Loss: 0.008140 Val Loss: nan\n",
      "Epoch [3767/10000] Train Loss: 0.007980 Val Loss: nan\n",
      "Epoch [3768/10000] Train Loss: 0.008129 Val Loss: nan\n",
      "Epoch [3769/10000] Train Loss: 0.007975 Val Loss: nan\n",
      "Epoch [3770/10000] Train Loss: 0.008117 Val Loss: nan\n",
      "Epoch [3771/10000] Train Loss: 0.008186 Val Loss: nan\n",
      "Epoch [3772/10000] Train Loss: 0.007991 Val Loss: nan\n",
      "Epoch [3773/10000] Train Loss: 0.008048 Val Loss: nan\n",
      "Epoch [3774/10000] Train Loss: 0.008231 Val Loss: nan\n",
      "Epoch [3775/10000] Train Loss: 0.008338 Val Loss: nan\n",
      "Epoch [3776/10000] Train Loss: 0.008092 Val Loss: nan\n",
      "Epoch [3777/10000] Train Loss: 0.008236 Val Loss: nan\n",
      "Epoch [3778/10000] Train Loss: 0.008065 Val Loss: nan\n",
      "Epoch [3779/10000] Train Loss: 0.007945 Val Loss: nan\n",
      "Epoch [3780/10000] Train Loss: 0.008165 Val Loss: nan\n",
      "Epoch [3781/10000] Train Loss: 0.008011 Val Loss: nan\n",
      "Epoch [3782/10000] Train Loss: 0.008062 Val Loss: nan\n",
      "Epoch [3783/10000] Train Loss: 0.008119 Val Loss: nan\n",
      "Epoch [3784/10000] Train Loss: 0.007912 Val Loss: nan\n",
      "Epoch [3785/10000] Train Loss: 0.007993 Val Loss: nan\n",
      "Epoch [3786/10000] Train Loss: 0.007920 Val Loss: nan\n",
      "Epoch [3787/10000] Train Loss: 0.008083 Val Loss: nan\n",
      "Epoch [3788/10000] Train Loss: 0.008157 Val Loss: nan\n",
      "Epoch [3789/10000] Train Loss: 0.008077 Val Loss: nan\n",
      "Epoch [3790/10000] Train Loss: 0.008067 Val Loss: nan\n",
      "Epoch [3791/10000] Train Loss: 0.007882 Val Loss: nan\n",
      "Epoch [3792/10000] Train Loss: 0.008128 Val Loss: nan\n",
      "Epoch [3793/10000] Train Loss: 0.007894 Val Loss: nan\n",
      "Epoch [3794/10000] Train Loss: 0.008050 Val Loss: nan\n",
      "Epoch [3795/10000] Train Loss: 0.008112 Val Loss: nan\n",
      "Epoch [3796/10000] Train Loss: 0.007928 Val Loss: nan\n",
      "Epoch [3797/10000] Train Loss: 0.008015 Val Loss: nan\n",
      "Epoch [3798/10000] Train Loss: 0.007903 Val Loss: nan\n",
      "Epoch [3799/10000] Train Loss: 0.007939 Val Loss: nan\n",
      "Epoch [3800/10000] Train Loss: 0.007956 Val Loss: nan\n",
      "Epoch [3801/10000] Train Loss: 0.008073 Val Loss: nan\n",
      "Epoch [3802/10000] Train Loss: 0.007933 Val Loss: nan\n",
      "Epoch [3803/10000] Train Loss: 0.008103 Val Loss: nan\n",
      "Epoch [3804/10000] Train Loss: 0.007928 Val Loss: nan\n",
      "Epoch [3805/10000] Train Loss: 0.007907 Val Loss: nan\n",
      "Epoch [3806/10000] Train Loss: 0.007884 Val Loss: nan\n",
      "Epoch [3807/10000] Train Loss: 0.007987 Val Loss: nan\n",
      "Epoch [3808/10000] Train Loss: 0.008030 Val Loss: nan\n",
      "Epoch [3809/10000] Train Loss: 0.007972 Val Loss: nan\n",
      "Epoch [3810/10000] Train Loss: 0.008039 Val Loss: nan\n",
      "Epoch [3811/10000] Train Loss: 0.008019 Val Loss: nan\n",
      "Epoch [3812/10000] Train Loss: 0.008058 Val Loss: nan\n",
      "Epoch [3813/10000] Train Loss: 0.008352 Val Loss: nan\n",
      "Epoch [3814/10000] Train Loss: 0.007904 Val Loss: nan\n",
      "Epoch [3815/10000] Train Loss: 0.007964 Val Loss: nan\n",
      "Epoch [3816/10000] Train Loss: 0.008021 Val Loss: nan\n",
      "Epoch [3817/10000] Train Loss: 0.007967 Val Loss: nan\n",
      "Epoch [3818/10000] Train Loss: 0.008050 Val Loss: nan\n",
      "Epoch [3819/10000] Train Loss: 0.007969 Val Loss: nan\n",
      "Epoch [3820/10000] Train Loss: 0.008305 Val Loss: nan\n",
      "Epoch [3821/10000] Train Loss: 0.007903 Val Loss: nan\n",
      "Epoch [3822/10000] Train Loss: 0.008045 Val Loss: nan\n",
      "Epoch [3823/10000] Train Loss: 0.008041 Val Loss: nan\n",
      "Epoch [3824/10000] Train Loss: 0.008044 Val Loss: nan\n",
      "Epoch [3825/10000] Train Loss: 0.008056 Val Loss: nan\n",
      "Epoch [3826/10000] Train Loss: 0.007868 Val Loss: nan\n",
      "Epoch [3827/10000] Train Loss: 0.007962 Val Loss: nan\n",
      "Epoch [3828/10000] Train Loss: 0.008085 Val Loss: nan\n",
      "Epoch [3829/10000] Train Loss: 0.007887 Val Loss: nan\n",
      "Epoch [3830/10000] Train Loss: 0.008025 Val Loss: nan\n",
      "Epoch [3831/10000] Train Loss: 0.008082 Val Loss: nan\n",
      "Epoch [3832/10000] Train Loss: 0.007966 Val Loss: nan\n",
      "Epoch [3833/10000] Train Loss: 0.008038 Val Loss: nan\n",
      "Epoch [3834/10000] Train Loss: 0.008018 Val Loss: nan\n",
      "Epoch [3835/10000] Train Loss: 0.007987 Val Loss: nan\n",
      "Epoch [3836/10000] Train Loss: 0.007901 Val Loss: nan\n",
      "Epoch [3837/10000] Train Loss: 0.007922 Val Loss: nan\n",
      "Epoch [3838/10000] Train Loss: 0.007886 Val Loss: nan\n",
      "Epoch [3839/10000] Train Loss: 0.007981 Val Loss: nan\n",
      "Epoch [3840/10000] Train Loss: 0.007925 Val Loss: nan\n",
      "Epoch [3841/10000] Train Loss: 0.008028 Val Loss: nan\n",
      "Epoch [3842/10000] Train Loss: 0.008131 Val Loss: nan\n",
      "Epoch [3843/10000] Train Loss: 0.007914 Val Loss: nan\n",
      "Epoch [3844/10000] Train Loss: 0.007873 Val Loss: nan\n",
      "Epoch [3845/10000] Train Loss: 0.007864 Val Loss: nan\n",
      "Epoch [3846/10000] Train Loss: 0.008086 Val Loss: nan\n",
      "Epoch [3847/10000] Train Loss: 0.007944 Val Loss: nan\n",
      "Epoch [3848/10000] Train Loss: 0.007927 Val Loss: nan\n",
      "Epoch [3849/10000] Train Loss: 0.007941 Val Loss: nan\n",
      "Epoch [3850/10000] Train Loss: 0.008006 Val Loss: nan\n",
      "Epoch [3851/10000] Train Loss: 0.008072 Val Loss: nan\n",
      "Epoch [3852/10000] Train Loss: 0.007894 Val Loss: nan\n",
      "Epoch [3853/10000] Train Loss: 0.008132 Val Loss: nan\n",
      "Epoch [3854/10000] Train Loss: 0.008073 Val Loss: nan\n",
      "Epoch [3855/10000] Train Loss: 0.007963 Val Loss: nan\n",
      "Epoch [3856/10000] Train Loss: 0.007924 Val Loss: nan\n",
      "Epoch [3857/10000] Train Loss: 0.007902 Val Loss: nan\n",
      "Epoch [3858/10000] Train Loss: 0.007935 Val Loss: nan\n",
      "Epoch [3859/10000] Train Loss: 0.008040 Val Loss: nan\n",
      "Epoch [3860/10000] Train Loss: 0.008012 Val Loss: nan\n",
      "Epoch [3861/10000] Train Loss: 0.007866 Val Loss: nan\n",
      "Epoch [3862/10000] Train Loss: 0.007948 Val Loss: nan\n",
      "Epoch [3863/10000] Train Loss: 0.008042 Val Loss: nan\n",
      "Epoch [3864/10000] Train Loss: 0.007861 Val Loss: nan\n",
      "Epoch [3865/10000] Train Loss: 0.007961 Val Loss: nan\n",
      "Epoch [3866/10000] Train Loss: 0.007901 Val Loss: nan\n",
      "Epoch [3867/10000] Train Loss: 0.007959 Val Loss: nan\n",
      "Epoch [3868/10000] Train Loss: 0.007887 Val Loss: nan\n",
      "Epoch [3869/10000] Train Loss: 0.008019 Val Loss: nan\n",
      "Epoch [3870/10000] Train Loss: 0.008072 Val Loss: nan\n",
      "Epoch [3871/10000] Train Loss: 0.008092 Val Loss: nan\n",
      "Epoch [3872/10000] Train Loss: 0.007847 Val Loss: nan\n",
      "Epoch [3873/10000] Train Loss: 0.008365 Val Loss: nan\n",
      "Epoch [3874/10000] Train Loss: 0.008195 Val Loss: nan\n",
      "Epoch [3875/10000] Train Loss: 0.007947 Val Loss: nan\n",
      "Epoch [3876/10000] Train Loss: 0.008025 Val Loss: nan\n",
      "Epoch [3877/10000] Train Loss: 0.007972 Val Loss: nan\n",
      "Epoch [3878/10000] Train Loss: 0.007944 Val Loss: nan\n",
      "Epoch [3879/10000] Train Loss: 0.008060 Val Loss: nan\n",
      "Epoch [3880/10000] Train Loss: 0.007882 Val Loss: nan\n",
      "Epoch [3881/10000] Train Loss: 0.007850 Val Loss: nan\n",
      "Epoch [3882/10000] Train Loss: 0.008118 Val Loss: nan\n",
      "Epoch [3883/10000] Train Loss: 0.007955 Val Loss: nan\n",
      "Epoch [3884/10000] Train Loss: 0.008044 Val Loss: nan\n",
      "Epoch [3885/10000] Train Loss: 0.007874 Val Loss: nan\n",
      "Epoch [3886/10000] Train Loss: 0.007914 Val Loss: nan\n",
      "Epoch [3887/10000] Train Loss: 0.007955 Val Loss: nan\n",
      "Epoch [3888/10000] Train Loss: 0.007893 Val Loss: nan\n",
      "Epoch [3889/10000] Train Loss: 0.008053 Val Loss: nan\n",
      "Epoch [3890/10000] Train Loss: 0.007970 Val Loss: nan\n",
      "Epoch [3891/10000] Train Loss: 0.007892 Val Loss: nan\n",
      "Epoch [3892/10000] Train Loss: 0.007989 Val Loss: nan\n",
      "Epoch [3893/10000] Train Loss: 0.008104 Val Loss: nan\n",
      "Epoch [3894/10000] Train Loss: 0.007876 Val Loss: nan\n",
      "Epoch [3895/10000] Train Loss: 0.007866 Val Loss: nan\n",
      "Epoch [3896/10000] Train Loss: 0.008093 Val Loss: nan\n",
      "Epoch [3897/10000] Train Loss: 0.008008 Val Loss: nan\n",
      "Epoch [3898/10000] Train Loss: 0.008103 Val Loss: nan\n",
      "Epoch [3899/10000] Train Loss: 0.008024 Val Loss: nan\n",
      "Epoch [3900/10000] Train Loss: 0.007896 Val Loss: nan\n",
      "Epoch [3901/10000] Train Loss: 0.007900 Val Loss: nan\n",
      "Epoch [3902/10000] Train Loss: 0.007917 Val Loss: nan\n",
      "Epoch [3903/10000] Train Loss: 0.008064 Val Loss: nan\n",
      "Epoch [3904/10000] Train Loss: 0.007899 Val Loss: nan\n",
      "Epoch [3905/10000] Train Loss: 0.007875 Val Loss: nan\n",
      "Epoch [3906/10000] Train Loss: 0.007838 Val Loss: nan\n",
      "Epoch [3907/10000] Train Loss: 0.007849 Val Loss: nan\n",
      "Epoch [3908/10000] Train Loss: 0.007838 Val Loss: nan\n",
      "Epoch [3909/10000] Train Loss: 0.008080 Val Loss: nan\n",
      "Epoch [3910/10000] Train Loss: 0.008017 Val Loss: nan\n",
      "Epoch [3911/10000] Train Loss: 0.008146 Val Loss: nan\n",
      "Epoch [3912/10000] Train Loss: 0.007816 Val Loss: nan\n",
      "Epoch [3913/10000] Train Loss: 0.007942 Val Loss: nan\n",
      "Epoch [3914/10000] Train Loss: 0.008110 Val Loss: nan\n",
      "Epoch [3915/10000] Train Loss: 0.008058 Val Loss: nan\n",
      "Epoch [3916/10000] Train Loss: 0.007882 Val Loss: nan\n",
      "Epoch [3917/10000] Train Loss: 0.007829 Val Loss: nan\n",
      "Epoch [3918/10000] Train Loss: 0.007856 Val Loss: nan\n",
      "Epoch [3919/10000] Train Loss: 0.007841 Val Loss: nan\n",
      "Epoch [3920/10000] Train Loss: 0.007844 Val Loss: nan\n",
      "Epoch [3921/10000] Train Loss: 0.007927 Val Loss: nan\n",
      "Epoch [3922/10000] Train Loss: 0.007954 Val Loss: nan\n",
      "Epoch [3923/10000] Train Loss: 0.007854 Val Loss: nan\n",
      "Epoch [3924/10000] Train Loss: 0.008001 Val Loss: nan\n",
      "Epoch [3925/10000] Train Loss: 0.007838 Val Loss: nan\n",
      "Epoch [3926/10000] Train Loss: 0.007852 Val Loss: nan\n",
      "Epoch [3927/10000] Train Loss: 0.008083 Val Loss: nan\n",
      "Epoch [3928/10000] Train Loss: 0.008065 Val Loss: nan\n",
      "Epoch [3929/10000] Train Loss: 0.007940 Val Loss: nan\n",
      "Epoch [3930/10000] Train Loss: 0.007916 Val Loss: nan\n",
      "Epoch [3931/10000] Train Loss: 0.007822 Val Loss: nan\n",
      "Epoch [3932/10000] Train Loss: 0.007867 Val Loss: nan\n",
      "Epoch [3933/10000] Train Loss: 0.007839 Val Loss: nan\n",
      "Epoch [3934/10000] Train Loss: 0.007966 Val Loss: nan\n",
      "Epoch [3935/10000] Train Loss: 0.007970 Val Loss: nan\n",
      "Epoch [3936/10000] Train Loss: 0.007888 Val Loss: nan\n",
      "Epoch [3937/10000] Train Loss: 0.008098 Val Loss: nan\n",
      "Epoch [3938/10000] Train Loss: 0.008007 Val Loss: nan\n",
      "Epoch [3939/10000] Train Loss: 0.008002 Val Loss: nan\n",
      "Epoch [3940/10000] Train Loss: 0.007987 Val Loss: nan\n",
      "Epoch [3941/10000] Train Loss: 0.007994 Val Loss: nan\n",
      "Epoch [3942/10000] Train Loss: 0.008024 Val Loss: nan\n",
      "Epoch [3943/10000] Train Loss: 0.007907 Val Loss: nan\n",
      "Epoch [3944/10000] Train Loss: 0.008222 Val Loss: nan\n",
      "Epoch [3945/10000] Train Loss: 0.007958 Val Loss: nan\n",
      "Epoch [3946/10000] Train Loss: 0.007841 Val Loss: nan\n",
      "Epoch [3947/10000] Train Loss: 0.007890 Val Loss: nan\n",
      "Epoch [3948/10000] Train Loss: 0.007899 Val Loss: nan\n",
      "Epoch [3949/10000] Train Loss: 0.008161 Val Loss: nan\n",
      "Epoch [3950/10000] Train Loss: 0.007877 Val Loss: nan\n",
      "Epoch [3951/10000] Train Loss: 0.007860 Val Loss: nan\n",
      "Epoch [3952/10000] Train Loss: 0.007904 Val Loss: nan\n",
      "Epoch [3953/10000] Train Loss: 0.008027 Val Loss: nan\n",
      "Epoch [3954/10000] Train Loss: 0.007847 Val Loss: nan\n",
      "Epoch [3955/10000] Train Loss: 0.007828 Val Loss: nan\n",
      "Epoch [3956/10000] Train Loss: 0.008243 Val Loss: nan\n",
      "Epoch [3957/10000] Train Loss: 0.008056 Val Loss: nan\n",
      "Epoch [3958/10000] Train Loss: 0.007901 Val Loss: nan\n",
      "Epoch [3959/10000] Train Loss: 0.008015 Val Loss: nan\n",
      "Epoch [3960/10000] Train Loss: 0.007821 Val Loss: nan\n",
      "Epoch [3961/10000] Train Loss: 0.007911 Val Loss: nan\n",
      "Epoch [3962/10000] Train Loss: 0.007874 Val Loss: nan\n",
      "Epoch [3963/10000] Train Loss: 0.007847 Val Loss: nan\n",
      "Epoch [3964/10000] Train Loss: 0.007815 Val Loss: nan\n",
      "Epoch [3965/10000] Train Loss: 0.007970 Val Loss: nan\n",
      "Epoch [3966/10000] Train Loss: 0.008140 Val Loss: nan\n",
      "Epoch [3967/10000] Train Loss: 0.008024 Val Loss: nan\n",
      "Epoch [3968/10000] Train Loss: 0.008092 Val Loss: nan\n",
      "Epoch [3969/10000] Train Loss: 0.007873 Val Loss: nan\n",
      "Epoch [3970/10000] Train Loss: 0.007799 Val Loss: nan\n",
      "Epoch [3971/10000] Train Loss: 0.007786 Val Loss: nan\n",
      "Epoch [3972/10000] Train Loss: 0.007846 Val Loss: nan\n",
      "Epoch [3973/10000] Train Loss: 0.007830 Val Loss: nan\n",
      "Epoch [3974/10000] Train Loss: 0.007840 Val Loss: nan\n",
      "Epoch [3975/10000] Train Loss: 0.007884 Val Loss: nan\n",
      "Epoch [3976/10000] Train Loss: 0.007901 Val Loss: nan\n",
      "Epoch [3977/10000] Train Loss: 0.007934 Val Loss: nan\n",
      "Epoch [3978/10000] Train Loss: 0.007812 Val Loss: nan\n",
      "Epoch [3979/10000] Train Loss: 0.007968 Val Loss: nan\n",
      "Epoch [3980/10000] Train Loss: 0.007936 Val Loss: nan\n",
      "Epoch [3981/10000] Train Loss: 0.007833 Val Loss: nan\n",
      "Epoch [3982/10000] Train Loss: 0.008184 Val Loss: nan\n",
      "Epoch [3983/10000] Train Loss: 0.008150 Val Loss: nan\n",
      "Epoch [3984/10000] Train Loss: 0.007930 Val Loss: nan\n",
      "Epoch [3985/10000] Train Loss: 0.007816 Val Loss: nan\n",
      "Epoch [3986/10000] Train Loss: 0.007791 Val Loss: nan\n",
      "Epoch [3987/10000] Train Loss: 0.007898 Val Loss: nan\n",
      "Epoch [3988/10000] Train Loss: 0.007888 Val Loss: nan\n",
      "Epoch [3989/10000] Train Loss: 0.007829 Val Loss: nan\n",
      "Epoch [3990/10000] Train Loss: 0.008076 Val Loss: nan\n",
      "Epoch [3991/10000] Train Loss: 0.007920 Val Loss: nan\n",
      "Epoch [3992/10000] Train Loss: 0.007944 Val Loss: nan\n",
      "Epoch [3993/10000] Train Loss: 0.007990 Val Loss: nan\n",
      "Epoch [3994/10000] Train Loss: 0.007845 Val Loss: nan\n",
      "Epoch [3995/10000] Train Loss: 0.007774 Val Loss: nan\n",
      "Epoch [3996/10000] Train Loss: 0.008010 Val Loss: nan\n",
      "Epoch [3997/10000] Train Loss: 0.007856 Val Loss: nan\n",
      "Epoch [3998/10000] Train Loss: 0.008070 Val Loss: nan\n",
      "Epoch [3999/10000] Train Loss: 0.008042 Val Loss: nan\n",
      "Epoch [4000/10000] Train Loss: 0.007961 Val Loss: nan\n",
      "Epoch [4001/10000] Train Loss: 0.007876 Val Loss: nan\n",
      "Epoch [4002/10000] Train Loss: 0.007880 Val Loss: nan\n",
      "Epoch [4003/10000] Train Loss: 0.007798 Val Loss: nan\n",
      "Epoch [4004/10000] Train Loss: 0.008011 Val Loss: nan\n",
      "Epoch [4005/10000] Train Loss: 0.007803 Val Loss: nan\n",
      "Epoch [4006/10000] Train Loss: 0.007831 Val Loss: nan\n",
      "Epoch [4007/10000] Train Loss: 0.007809 Val Loss: nan\n",
      "Epoch [4008/10000] Train Loss: 0.008041 Val Loss: nan\n",
      "Epoch [4009/10000] Train Loss: 0.007846 Val Loss: nan\n",
      "Epoch [4010/10000] Train Loss: 0.007932 Val Loss: nan\n",
      "Epoch [4011/10000] Train Loss: 0.007841 Val Loss: nan\n",
      "Epoch [4012/10000] Train Loss: 0.008010 Val Loss: nan\n",
      "Epoch [4013/10000] Train Loss: 0.007895 Val Loss: nan\n",
      "Epoch [4014/10000] Train Loss: 0.007929 Val Loss: nan\n",
      "Epoch [4015/10000] Train Loss: 0.007826 Val Loss: nan\n",
      "Epoch [4016/10000] Train Loss: 0.007849 Val Loss: nan\n",
      "Epoch [4017/10000] Train Loss: 0.007841 Val Loss: nan\n",
      "Epoch [4018/10000] Train Loss: 0.007810 Val Loss: nan\n",
      "Epoch [4019/10000] Train Loss: 0.008103 Val Loss: nan\n",
      "Epoch [4020/10000] Train Loss: 0.007792 Val Loss: nan\n",
      "Epoch [4021/10000] Train Loss: 0.008018 Val Loss: nan\n",
      "Epoch [4022/10000] Train Loss: 0.008141 Val Loss: nan\n",
      "Epoch [4023/10000] Train Loss: 0.007974 Val Loss: nan\n",
      "Epoch [4024/10000] Train Loss: 0.007883 Val Loss: nan\n",
      "Epoch [4025/10000] Train Loss: 0.007971 Val Loss: nan\n",
      "Epoch [4026/10000] Train Loss: 0.007950 Val Loss: nan\n",
      "Epoch [4027/10000] Train Loss: 0.007910 Val Loss: nan\n",
      "Epoch [4028/10000] Train Loss: 0.007771 Val Loss: nan\n",
      "Epoch [4029/10000] Train Loss: 0.007782 Val Loss: nan\n",
      "Epoch [4030/10000] Train Loss: 0.007886 Val Loss: nan\n",
      "Epoch [4031/10000] Train Loss: 0.008064 Val Loss: nan\n",
      "Epoch [4032/10000] Train Loss: 0.007843 Val Loss: nan\n",
      "Epoch [4033/10000] Train Loss: 0.007839 Val Loss: nan\n",
      "Epoch [4034/10000] Train Loss: 0.007847 Val Loss: nan\n",
      "Epoch [4035/10000] Train Loss: 0.008031 Val Loss: nan\n",
      "Epoch [4036/10000] Train Loss: 0.007772 Val Loss: nan\n",
      "Epoch [4037/10000] Train Loss: 0.007813 Val Loss: nan\n",
      "Epoch [4038/10000] Train Loss: 0.007926 Val Loss: nan\n",
      "Epoch [4039/10000] Train Loss: 0.007852 Val Loss: nan\n",
      "Epoch [4040/10000] Train Loss: 0.007773 Val Loss: nan\n",
      "Epoch [4041/10000] Train Loss: 0.007937 Val Loss: nan\n",
      "Epoch [4042/10000] Train Loss: 0.007902 Val Loss: nan\n",
      "Epoch [4043/10000] Train Loss: 0.008038 Val Loss: nan\n",
      "Epoch [4044/10000] Train Loss: 0.007910 Val Loss: nan\n",
      "Epoch [4045/10000] Train Loss: 0.007807 Val Loss: nan\n",
      "Epoch [4046/10000] Train Loss: 0.007794 Val Loss: nan\n",
      "Epoch [4047/10000] Train Loss: 0.007945 Val Loss: nan\n",
      "Epoch [4048/10000] Train Loss: 0.007930 Val Loss: nan\n",
      "Epoch [4049/10000] Train Loss: 0.007784 Val Loss: nan\n",
      "Epoch [4050/10000] Train Loss: 0.007904 Val Loss: nan\n",
      "Epoch [4051/10000] Train Loss: 0.007957 Val Loss: nan\n",
      "Epoch [4052/10000] Train Loss: 0.007917 Val Loss: nan\n",
      "Epoch [4053/10000] Train Loss: 0.007907 Val Loss: nan\n",
      "Epoch [4054/10000] Train Loss: 0.007827 Val Loss: nan\n",
      "Epoch [4055/10000] Train Loss: 0.007754 Val Loss: nan\n",
      "Epoch [4056/10000] Train Loss: 0.007940 Val Loss: nan\n",
      "Epoch [4057/10000] Train Loss: 0.007963 Val Loss: nan\n",
      "Epoch [4058/10000] Train Loss: 0.008081 Val Loss: nan\n",
      "Epoch [4059/10000] Train Loss: 0.007816 Val Loss: nan\n",
      "Epoch [4060/10000] Train Loss: 0.008052 Val Loss: nan\n",
      "Epoch [4061/10000] Train Loss: 0.008179 Val Loss: nan\n",
      "Epoch [4062/10000] Train Loss: 0.007935 Val Loss: nan\n",
      "Epoch [4063/10000] Train Loss: 0.007951 Val Loss: nan\n",
      "Epoch [4064/10000] Train Loss: 0.007889 Val Loss: nan\n",
      "Epoch [4065/10000] Train Loss: 0.007818 Val Loss: nan\n",
      "Epoch [4066/10000] Train Loss: 0.007819 Val Loss: nan\n",
      "Epoch [4067/10000] Train Loss: 0.007763 Val Loss: nan\n",
      "Epoch [4068/10000] Train Loss: 0.007833 Val Loss: nan\n",
      "Epoch [4069/10000] Train Loss: 0.007780 Val Loss: nan\n",
      "Epoch [4070/10000] Train Loss: 0.007933 Val Loss: nan\n",
      "Epoch [4071/10000] Train Loss: 0.007852 Val Loss: nan\n",
      "Epoch [4072/10000] Train Loss: 0.007937 Val Loss: nan\n",
      "Epoch [4073/10000] Train Loss: 0.007784 Val Loss: nan\n",
      "Epoch [4074/10000] Train Loss: 0.007807 Val Loss: nan\n",
      "Epoch [4075/10000] Train Loss: 0.008047 Val Loss: nan\n",
      "Epoch [4076/10000] Train Loss: 0.007906 Val Loss: nan\n",
      "Epoch [4077/10000] Train Loss: 0.007810 Val Loss: nan\n",
      "Epoch [4078/10000] Train Loss: 0.008064 Val Loss: nan\n",
      "Epoch [4079/10000] Train Loss: 0.007750 Val Loss: nan\n",
      "Epoch [4080/10000] Train Loss: 0.007767 Val Loss: nan\n",
      "Epoch [4081/10000] Train Loss: 0.007850 Val Loss: nan\n",
      "Epoch [4082/10000] Train Loss: 0.008021 Val Loss: nan\n",
      "Epoch [4083/10000] Train Loss: 0.007859 Val Loss: nan\n",
      "Epoch [4084/10000] Train Loss: 0.007789 Val Loss: nan\n",
      "Epoch [4085/10000] Train Loss: 0.007935 Val Loss: nan\n",
      "Epoch [4086/10000] Train Loss: 0.007887 Val Loss: nan\n",
      "Epoch [4087/10000] Train Loss: 0.007849 Val Loss: nan\n",
      "Epoch [4088/10000] Train Loss: 0.007995 Val Loss: nan\n",
      "Epoch [4089/10000] Train Loss: 0.008018 Val Loss: nan\n",
      "Epoch [4090/10000] Train Loss: 0.008080 Val Loss: nan\n",
      "Epoch [4091/10000] Train Loss: 0.007939 Val Loss: nan\n",
      "Epoch [4092/10000] Train Loss: 0.007802 Val Loss: nan\n",
      "Epoch [4093/10000] Train Loss: 0.008154 Val Loss: nan\n",
      "Epoch [4094/10000] Train Loss: 0.007824 Val Loss: nan\n",
      "Epoch [4095/10000] Train Loss: 0.007859 Val Loss: nan\n",
      "Epoch [4096/10000] Train Loss: 0.007868 Val Loss: nan\n",
      "Epoch [4097/10000] Train Loss: 0.007784 Val Loss: nan\n",
      "Epoch [4098/10000] Train Loss: 0.007904 Val Loss: nan\n",
      "Epoch [4099/10000] Train Loss: 0.007790 Val Loss: nan\n",
      "Epoch [4100/10000] Train Loss: 0.007746 Val Loss: nan\n",
      "Epoch [4101/10000] Train Loss: 0.007883 Val Loss: nan\n",
      "Epoch [4102/10000] Train Loss: 0.007754 Val Loss: nan\n",
      "Epoch [4103/10000] Train Loss: 0.007935 Val Loss: nan\n",
      "Epoch [4104/10000] Train Loss: 0.007843 Val Loss: nan\n",
      "Epoch [4105/10000] Train Loss: 0.007947 Val Loss: nan\n",
      "Epoch [4106/10000] Train Loss: 0.007970 Val Loss: nan\n",
      "Epoch [4107/10000] Train Loss: 0.008011 Val Loss: nan\n",
      "Epoch [4108/10000] Train Loss: 0.007951 Val Loss: nan\n",
      "Epoch [4109/10000] Train Loss: 0.007786 Val Loss: nan\n",
      "Epoch [4110/10000] Train Loss: 0.008107 Val Loss: nan\n",
      "Epoch [4111/10000] Train Loss: 0.007954 Val Loss: nan\n",
      "Epoch [4112/10000] Train Loss: 0.007952 Val Loss: nan\n",
      "Epoch [4113/10000] Train Loss: 0.007843 Val Loss: nan\n",
      "Epoch [4114/10000] Train Loss: 0.007968 Val Loss: nan\n",
      "Epoch [4115/10000] Train Loss: 0.008069 Val Loss: nan\n",
      "Epoch [4116/10000] Train Loss: 0.007896 Val Loss: nan\n",
      "Epoch [4117/10000] Train Loss: 0.007770 Val Loss: nan\n",
      "Epoch [4118/10000] Train Loss: 0.007737 Val Loss: nan\n",
      "Epoch [4119/10000] Train Loss: 0.008018 Val Loss: nan\n",
      "Epoch [4120/10000] Train Loss: 0.007854 Val Loss: nan\n",
      "Epoch [4121/10000] Train Loss: 0.008016 Val Loss: nan\n",
      "Epoch [4122/10000] Train Loss: 0.007878 Val Loss: nan\n",
      "Epoch [4123/10000] Train Loss: 0.007899 Val Loss: nan\n",
      "Epoch [4124/10000] Train Loss: 0.007734 Val Loss: nan\n",
      "Epoch [4125/10000] Train Loss: 0.007880 Val Loss: nan\n",
      "Epoch [4126/10000] Train Loss: 0.007923 Val Loss: nan\n",
      "Epoch [4127/10000] Train Loss: 0.007773 Val Loss: nan\n",
      "Epoch [4128/10000] Train Loss: 0.007879 Val Loss: nan\n",
      "Epoch [4129/10000] Train Loss: 0.007745 Val Loss: nan\n",
      "Epoch [4130/10000] Train Loss: 0.008038 Val Loss: nan\n",
      "Epoch [4131/10000] Train Loss: 0.007866 Val Loss: nan\n",
      "Epoch [4132/10000] Train Loss: 0.007969 Val Loss: nan\n",
      "Epoch [4133/10000] Train Loss: 0.007867 Val Loss: nan\n",
      "Epoch [4134/10000] Train Loss: 0.007748 Val Loss: nan\n",
      "Epoch [4135/10000] Train Loss: 0.007789 Val Loss: nan\n",
      "Epoch [4136/10000] Train Loss: 0.007779 Val Loss: nan\n",
      "Epoch [4137/10000] Train Loss: 0.007846 Val Loss: nan\n",
      "Epoch [4138/10000] Train Loss: 0.007759 Val Loss: nan\n",
      "Epoch [4139/10000] Train Loss: 0.007879 Val Loss: nan\n",
      "Epoch [4140/10000] Train Loss: 0.007899 Val Loss: nan\n",
      "Epoch [4141/10000] Train Loss: 0.007872 Val Loss: nan\n",
      "Epoch [4142/10000] Train Loss: 0.007846 Val Loss: nan\n",
      "Epoch [4143/10000] Train Loss: 0.007755 Val Loss: nan\n",
      "Epoch [4144/10000] Train Loss: 0.007838 Val Loss: nan\n",
      "Epoch [4145/10000] Train Loss: 0.007757 Val Loss: nan\n",
      "Epoch [4146/10000] Train Loss: 0.007901 Val Loss: nan\n",
      "Epoch [4147/10000] Train Loss: 0.007752 Val Loss: nan\n",
      "Epoch [4148/10000] Train Loss: 0.007952 Val Loss: nan\n",
      "Epoch [4149/10000] Train Loss: 0.007801 Val Loss: nan\n",
      "Epoch [4150/10000] Train Loss: 0.007898 Val Loss: nan\n",
      "Epoch [4151/10000] Train Loss: 0.007884 Val Loss: nan\n",
      "Epoch [4152/10000] Train Loss: 0.007889 Val Loss: nan\n",
      "Epoch [4153/10000] Train Loss: 0.007805 Val Loss: nan\n",
      "Epoch [4154/10000] Train Loss: 0.007773 Val Loss: nan\n",
      "Epoch [4155/10000] Train Loss: 0.007961 Val Loss: nan\n",
      "Epoch [4156/10000] Train Loss: 0.007949 Val Loss: nan\n",
      "Epoch [4157/10000] Train Loss: 0.007825 Val Loss: nan\n",
      "Epoch [4158/10000] Train Loss: 0.007959 Val Loss: nan\n",
      "Epoch [4159/10000] Train Loss: 0.007746 Val Loss: nan\n",
      "Epoch [4160/10000] Train Loss: 0.007890 Val Loss: nan\n",
      "Epoch [4161/10000] Train Loss: 0.008032 Val Loss: nan\n",
      "Epoch [4162/10000] Train Loss: 0.007811 Val Loss: nan\n",
      "Epoch [4163/10000] Train Loss: 0.007907 Val Loss: nan\n",
      "Epoch [4164/10000] Train Loss: 0.007996 Val Loss: nan\n",
      "Epoch [4165/10000] Train Loss: 0.007753 Val Loss: nan\n",
      "Epoch [4166/10000] Train Loss: 0.007846 Val Loss: nan\n",
      "Epoch [4167/10000] Train Loss: 0.008064 Val Loss: nan\n",
      "Epoch [4168/10000] Train Loss: 0.008049 Val Loss: nan\n",
      "Epoch [4169/10000] Train Loss: 0.007946 Val Loss: nan\n",
      "Epoch [4170/10000] Train Loss: 0.007856 Val Loss: nan\n",
      "Epoch [4171/10000] Train Loss: 0.007923 Val Loss: nan\n",
      "Epoch [4172/10000] Train Loss: 0.008105 Val Loss: nan\n",
      "Epoch [4173/10000] Train Loss: 0.008015 Val Loss: nan\n",
      "Epoch [4174/10000] Train Loss: 0.007825 Val Loss: nan\n",
      "Epoch [4175/10000] Train Loss: 0.007868 Val Loss: nan\n",
      "Epoch [4176/10000] Train Loss: 0.007837 Val Loss: nan\n",
      "Epoch [4177/10000] Train Loss: 0.007927 Val Loss: nan\n",
      "Epoch [4178/10000] Train Loss: 0.007740 Val Loss: nan\n",
      "Epoch [4179/10000] Train Loss: 0.008021 Val Loss: nan\n",
      "Epoch [4180/10000] Train Loss: 0.007752 Val Loss: nan\n",
      "Epoch [4181/10000] Train Loss: 0.007881 Val Loss: nan\n",
      "Epoch [4182/10000] Train Loss: 0.007895 Val Loss: nan\n",
      "Epoch [4183/10000] Train Loss: 0.007849 Val Loss: nan\n",
      "Epoch [4184/10000] Train Loss: 0.007724 Val Loss: nan\n",
      "Epoch [4185/10000] Train Loss: 0.007875 Val Loss: nan\n",
      "Epoch [4186/10000] Train Loss: 0.007832 Val Loss: nan\n",
      "Epoch [4187/10000] Train Loss: 0.007770 Val Loss: nan\n",
      "Epoch [4188/10000] Train Loss: 0.007929 Val Loss: nan\n",
      "Epoch [4189/10000] Train Loss: 0.007737 Val Loss: nan\n",
      "Epoch [4190/10000] Train Loss: 0.007723 Val Loss: nan\n",
      "Epoch [4191/10000] Train Loss: 0.007977 Val Loss: nan\n",
      "Epoch [4192/10000] Train Loss: 0.007836 Val Loss: nan\n",
      "Epoch [4193/10000] Train Loss: 0.007854 Val Loss: nan\n",
      "Epoch [4194/10000] Train Loss: 0.007796 Val Loss: nan\n",
      "Epoch [4195/10000] Train Loss: 0.007732 Val Loss: nan\n",
      "Epoch [4196/10000] Train Loss: 0.007736 Val Loss: nan\n",
      "Epoch [4197/10000] Train Loss: 0.007890 Val Loss: nan\n",
      "Epoch [4198/10000] Train Loss: 0.007973 Val Loss: nan\n",
      "Epoch [4199/10000] Train Loss: 0.007804 Val Loss: nan\n",
      "Epoch [4200/10000] Train Loss: 0.007838 Val Loss: nan\n",
      "Epoch [4201/10000] Train Loss: 0.007847 Val Loss: nan\n",
      "Epoch [4202/10000] Train Loss: 0.007712 Val Loss: nan\n",
      "Epoch [4203/10000] Train Loss: 0.007696 Val Loss: nan\n",
      "Epoch [4204/10000] Train Loss: 0.007755 Val Loss: nan\n",
      "Epoch [4205/10000] Train Loss: 0.007992 Val Loss: nan\n",
      "Epoch [4206/10000] Train Loss: 0.007711 Val Loss: nan\n",
      "Epoch [4207/10000] Train Loss: 0.007939 Val Loss: nan\n",
      "Epoch [4208/10000] Train Loss: 0.007890 Val Loss: nan\n",
      "Epoch [4209/10000] Train Loss: 0.007985 Val Loss: nan\n",
      "Epoch [4210/10000] Train Loss: 0.007744 Val Loss: nan\n",
      "Epoch [4211/10000] Train Loss: 0.007815 Val Loss: nan\n",
      "Epoch [4212/10000] Train Loss: 0.007741 Val Loss: nan\n",
      "Epoch [4213/10000] Train Loss: 0.007907 Val Loss: nan\n",
      "Epoch [4214/10000] Train Loss: 0.008240 Val Loss: nan\n",
      "Epoch [4215/10000] Train Loss: 0.007743 Val Loss: nan\n",
      "Epoch [4216/10000] Train Loss: 0.007785 Val Loss: nan\n",
      "Epoch [4217/10000] Train Loss: 0.007749 Val Loss: nan\n",
      "Epoch [4218/10000] Train Loss: 0.007713 Val Loss: nan\n",
      "Epoch [4219/10000] Train Loss: 0.007931 Val Loss: nan\n",
      "Epoch [4220/10000] Train Loss: 0.007955 Val Loss: nan\n",
      "Epoch [4221/10000] Train Loss: 0.007753 Val Loss: nan\n",
      "Epoch [4222/10000] Train Loss: 0.007765 Val Loss: nan\n",
      "Epoch [4223/10000] Train Loss: 0.008019 Val Loss: nan\n",
      "Epoch [4224/10000] Train Loss: 0.007819 Val Loss: nan\n",
      "Epoch [4225/10000] Train Loss: 0.007848 Val Loss: nan\n",
      "Epoch [4226/10000] Train Loss: 0.007780 Val Loss: nan\n",
      "Epoch [4227/10000] Train Loss: 0.007748 Val Loss: nan\n",
      "Epoch [4228/10000] Train Loss: 0.007816 Val Loss: nan\n",
      "Epoch [4229/10000] Train Loss: 0.008169 Val Loss: nan\n",
      "Epoch [4230/10000] Train Loss: 0.007738 Val Loss: nan\n",
      "Epoch [4231/10000] Train Loss: 0.007856 Val Loss: nan\n",
      "Epoch [4232/10000] Train Loss: 0.007797 Val Loss: nan\n",
      "Epoch [4233/10000] Train Loss: 0.007708 Val Loss: nan\n",
      "Epoch [4234/10000] Train Loss: 0.007679 Val Loss: nan\n",
      "Epoch [4235/10000] Train Loss: 0.007948 Val Loss: nan\n",
      "Epoch [4236/10000] Train Loss: 0.007738 Val Loss: nan\n",
      "Epoch [4237/10000] Train Loss: 0.007744 Val Loss: nan\n",
      "Epoch [4238/10000] Train Loss: 0.007873 Val Loss: nan\n",
      "Epoch [4239/10000] Train Loss: 0.007867 Val Loss: nan\n",
      "Epoch [4240/10000] Train Loss: 0.007861 Val Loss: nan\n",
      "Epoch [4241/10000] Train Loss: 0.007724 Val Loss: nan\n",
      "Epoch [4242/10000] Train Loss: 0.007709 Val Loss: nan\n",
      "Epoch [4243/10000] Train Loss: 0.007676 Val Loss: nan\n",
      "Epoch [4244/10000] Train Loss: 0.007813 Val Loss: nan\n",
      "Epoch [4245/10000] Train Loss: 0.007796 Val Loss: nan\n",
      "Epoch [4246/10000] Train Loss: 0.007818 Val Loss: nan\n",
      "Epoch [4247/10000] Train Loss: 0.007734 Val Loss: nan\n",
      "Epoch [4248/10000] Train Loss: 0.007864 Val Loss: nan\n",
      "Epoch [4249/10000] Train Loss: 0.007734 Val Loss: nan\n",
      "Epoch [4250/10000] Train Loss: 0.007941 Val Loss: nan\n",
      "Epoch [4251/10000] Train Loss: 0.007722 Val Loss: nan\n",
      "Epoch [4252/10000] Train Loss: 0.007687 Val Loss: nan\n",
      "Epoch [4253/10000] Train Loss: 0.007788 Val Loss: nan\n",
      "Epoch [4254/10000] Train Loss: 0.007910 Val Loss: nan\n",
      "Epoch [4255/10000] Train Loss: 0.007755 Val Loss: nan\n",
      "Epoch [4256/10000] Train Loss: 0.008133 Val Loss: nan\n",
      "Epoch [4257/10000] Train Loss: 0.007792 Val Loss: nan\n",
      "Epoch [4258/10000] Train Loss: 0.008101 Val Loss: nan\n",
      "Epoch [4259/10000] Train Loss: 0.007783 Val Loss: nan\n",
      "Epoch [4260/10000] Train Loss: 0.007801 Val Loss: nan\n",
      "Epoch [4261/10000] Train Loss: 0.007750 Val Loss: nan\n",
      "Epoch [4262/10000] Train Loss: 0.007832 Val Loss: nan\n",
      "Epoch [4263/10000] Train Loss: 0.007840 Val Loss: nan\n",
      "Epoch [4264/10000] Train Loss: 0.007734 Val Loss: nan\n",
      "Epoch [4265/10000] Train Loss: 0.007841 Val Loss: nan\n",
      "Epoch [4266/10000] Train Loss: 0.007750 Val Loss: nan\n",
      "Epoch [4267/10000] Train Loss: 0.007973 Val Loss: nan\n",
      "Epoch [4268/10000] Train Loss: 0.007869 Val Loss: nan\n",
      "Epoch [4269/10000] Train Loss: 0.007835 Val Loss: nan\n",
      "Epoch [4270/10000] Train Loss: 0.008236 Val Loss: nan\n",
      "Epoch [4271/10000] Train Loss: 0.008059 Val Loss: nan\n",
      "Epoch [4272/10000] Train Loss: 0.007964 Val Loss: nan\n",
      "Epoch [4273/10000] Train Loss: 0.007893 Val Loss: nan\n",
      "Epoch [4274/10000] Train Loss: 0.007907 Val Loss: nan\n",
      "Epoch [4275/10000] Train Loss: 0.007811 Val Loss: nan\n",
      "Epoch [4276/10000] Train Loss: 0.007705 Val Loss: nan\n",
      "Epoch [4277/10000] Train Loss: 0.007796 Val Loss: nan\n",
      "Epoch [4278/10000] Train Loss: 0.007776 Val Loss: nan\n",
      "Epoch [4279/10000] Train Loss: 0.007826 Val Loss: nan\n",
      "Epoch [4280/10000] Train Loss: 0.007735 Val Loss: nan\n",
      "Epoch [4281/10000] Train Loss: 0.007729 Val Loss: nan\n",
      "Epoch [4282/10000] Train Loss: 0.007727 Val Loss: nan\n",
      "Epoch [4283/10000] Train Loss: 0.007808 Val Loss: nan\n",
      "Epoch [4284/10000] Train Loss: 0.007945 Val Loss: nan\n",
      "Epoch [4285/10000] Train Loss: 0.007712 Val Loss: nan\n",
      "Epoch [4286/10000] Train Loss: 0.007738 Val Loss: nan\n",
      "Epoch [4287/10000] Train Loss: 0.007732 Val Loss: nan\n",
      "Epoch [4288/10000] Train Loss: 0.007715 Val Loss: nan\n",
      "Epoch [4289/10000] Train Loss: 0.007741 Val Loss: nan\n",
      "Epoch [4290/10000] Train Loss: 0.007674 Val Loss: nan\n",
      "Epoch [4291/10000] Train Loss: 0.007676 Val Loss: nan\n",
      "Epoch [4292/10000] Train Loss: 0.007988 Val Loss: nan\n",
      "Epoch [4293/10000] Train Loss: 0.007875 Val Loss: nan\n",
      "Epoch [4294/10000] Train Loss: 0.008038 Val Loss: nan\n",
      "Epoch [4295/10000] Train Loss: 0.007709 Val Loss: nan\n",
      "Epoch [4296/10000] Train Loss: 0.007886 Val Loss: nan\n",
      "Epoch [4297/10000] Train Loss: 0.007758 Val Loss: nan\n",
      "Epoch [4298/10000] Train Loss: 0.007835 Val Loss: nan\n",
      "Epoch [4299/10000] Train Loss: 0.007901 Val Loss: nan\n",
      "Epoch [4300/10000] Train Loss: 0.007664 Val Loss: nan\n",
      "Epoch [4301/10000] Train Loss: 0.007887 Val Loss: nan\n",
      "Epoch [4302/10000] Train Loss: 0.007756 Val Loss: nan\n",
      "Epoch [4303/10000] Train Loss: 0.007904 Val Loss: nan\n",
      "Epoch [4304/10000] Train Loss: 0.007680 Val Loss: nan\n",
      "Epoch [4305/10000] Train Loss: 0.007734 Val Loss: nan\n",
      "Epoch [4306/10000] Train Loss: 0.007748 Val Loss: nan\n",
      "Epoch [4307/10000] Train Loss: 0.007706 Val Loss: nan\n",
      "Epoch [4308/10000] Train Loss: 0.007676 Val Loss: nan\n",
      "Epoch [4309/10000] Train Loss: 0.007745 Val Loss: nan\n",
      "Epoch [4310/10000] Train Loss: 0.007688 Val Loss: nan\n",
      "Epoch [4311/10000] Train Loss: 0.007660 Val Loss: nan\n",
      "Epoch [4312/10000] Train Loss: 0.007719 Val Loss: nan\n",
      "Epoch [4313/10000] Train Loss: 0.007724 Val Loss: nan\n",
      "Epoch [4314/10000] Train Loss: 0.007726 Val Loss: nan\n",
      "Epoch [4315/10000] Train Loss: 0.007781 Val Loss: nan\n",
      "Epoch [4316/10000] Train Loss: 0.007843 Val Loss: nan\n",
      "Epoch [4317/10000] Train Loss: 0.008087 Val Loss: nan\n",
      "Epoch [4318/10000] Train Loss: 0.007702 Val Loss: nan\n",
      "Epoch [4319/10000] Train Loss: 0.007814 Val Loss: nan\n",
      "Epoch [4320/10000] Train Loss: 0.007709 Val Loss: nan\n",
      "Epoch [4321/10000] Train Loss: 0.007786 Val Loss: nan\n",
      "Epoch [4322/10000] Train Loss: 0.007933 Val Loss: nan\n",
      "Epoch [4323/10000] Train Loss: 0.007803 Val Loss: nan\n",
      "Epoch [4324/10000] Train Loss: 0.007766 Val Loss: nan\n",
      "Epoch [4325/10000] Train Loss: 0.007862 Val Loss: nan\n",
      "Epoch [4326/10000] Train Loss: 0.007708 Val Loss: nan\n",
      "Epoch [4327/10000] Train Loss: 0.007660 Val Loss: nan\n",
      "Epoch [4328/10000] Train Loss: 0.007813 Val Loss: nan\n",
      "Epoch [4329/10000] Train Loss: 0.007669 Val Loss: nan\n",
      "Epoch [4330/10000] Train Loss: 0.007705 Val Loss: nan\n",
      "Epoch [4331/10000] Train Loss: 0.007644 Val Loss: nan\n",
      "Epoch [4332/10000] Train Loss: 0.007714 Val Loss: nan\n",
      "Epoch [4333/10000] Train Loss: 0.007807 Val Loss: nan\n",
      "Epoch [4334/10000] Train Loss: 0.007707 Val Loss: nan\n",
      "Epoch [4335/10000] Train Loss: 0.007693 Val Loss: nan\n",
      "Epoch [4336/10000] Train Loss: 0.007807 Val Loss: nan\n",
      "Epoch [4337/10000] Train Loss: 0.007665 Val Loss: nan\n",
      "Epoch [4338/10000] Train Loss: 0.007683 Val Loss: nan\n",
      "Epoch [4339/10000] Train Loss: 0.007831 Val Loss: nan\n",
      "Epoch [4340/10000] Train Loss: 0.007798 Val Loss: nan\n",
      "Epoch [4341/10000] Train Loss: 0.007808 Val Loss: nan\n",
      "Epoch [4342/10000] Train Loss: 0.007699 Val Loss: nan\n",
      "Epoch [4343/10000] Train Loss: 0.007672 Val Loss: nan\n",
      "Epoch [4344/10000] Train Loss: 0.007759 Val Loss: nan\n",
      "Epoch [4345/10000] Train Loss: 0.007662 Val Loss: nan\n",
      "Epoch [4346/10000] Train Loss: 0.007783 Val Loss: nan\n",
      "Epoch [4347/10000] Train Loss: 0.007800 Val Loss: nan\n",
      "Epoch [4348/10000] Train Loss: 0.007825 Val Loss: nan\n",
      "Epoch [4349/10000] Train Loss: 0.007792 Val Loss: nan\n",
      "Epoch [4350/10000] Train Loss: 0.007706 Val Loss: nan\n",
      "Epoch [4351/10000] Train Loss: 0.007717 Val Loss: nan\n",
      "Epoch [4352/10000] Train Loss: 0.007715 Val Loss: nan\n",
      "Epoch [4353/10000] Train Loss: 0.007771 Val Loss: nan\n",
      "Epoch [4354/10000] Train Loss: 0.007842 Val Loss: nan\n",
      "Epoch [4355/10000] Train Loss: 0.007845 Val Loss: nan\n",
      "Epoch [4356/10000] Train Loss: 0.007798 Val Loss: nan\n",
      "Epoch [4357/10000] Train Loss: 0.007940 Val Loss: nan\n",
      "Epoch [4358/10000] Train Loss: 0.008161 Val Loss: nan\n",
      "Epoch [4359/10000] Train Loss: 0.007811 Val Loss: nan\n",
      "Epoch [4360/10000] Train Loss: 0.007774 Val Loss: nan\n",
      "Epoch [4361/10000] Train Loss: 0.007838 Val Loss: nan\n",
      "Epoch [4362/10000] Train Loss: 0.007843 Val Loss: nan\n",
      "Epoch [4363/10000] Train Loss: 0.007813 Val Loss: nan\n",
      "Epoch [4364/10000] Train Loss: 0.007910 Val Loss: nan\n",
      "Epoch [4365/10000] Train Loss: 0.007778 Val Loss: nan\n",
      "Epoch [4366/10000] Train Loss: 0.007817 Val Loss: nan\n",
      "Epoch [4367/10000] Train Loss: 0.007763 Val Loss: nan\n",
      "Epoch [4368/10000] Train Loss: 0.007645 Val Loss: nan\n",
      "Epoch [4369/10000] Train Loss: 0.007868 Val Loss: nan\n",
      "Epoch [4370/10000] Train Loss: 0.007895 Val Loss: nan\n",
      "Epoch [4371/10000] Train Loss: 0.007979 Val Loss: nan\n",
      "Epoch [4372/10000] Train Loss: 0.007676 Val Loss: nan\n",
      "Epoch [4373/10000] Train Loss: 0.007785 Val Loss: nan\n",
      "Epoch [4374/10000] Train Loss: 0.007679 Val Loss: nan\n",
      "Epoch [4375/10000] Train Loss: 0.007905 Val Loss: nan\n",
      "Epoch [4376/10000] Train Loss: 0.007820 Val Loss: nan\n",
      "Epoch [4377/10000] Train Loss: 0.007738 Val Loss: nan\n",
      "Epoch [4378/10000] Train Loss: 0.007901 Val Loss: nan\n",
      "Epoch [4379/10000] Train Loss: 0.007684 Val Loss: nan\n",
      "Epoch [4380/10000] Train Loss: 0.007642 Val Loss: nan\n",
      "Epoch [4381/10000] Train Loss: 0.007851 Val Loss: nan\n",
      "Epoch [4382/10000] Train Loss: 0.007772 Val Loss: nan\n",
      "Epoch [4383/10000] Train Loss: 0.007928 Val Loss: nan\n",
      "Epoch [4384/10000] Train Loss: 0.007716 Val Loss: nan\n",
      "Epoch [4385/10000] Train Loss: 0.007964 Val Loss: nan\n",
      "Epoch [4386/10000] Train Loss: 0.007734 Val Loss: nan\n",
      "Epoch [4387/10000] Train Loss: 0.007684 Val Loss: nan\n",
      "Epoch [4388/10000] Train Loss: 0.007794 Val Loss: nan\n",
      "Epoch [4389/10000] Train Loss: 0.007750 Val Loss: nan\n",
      "Epoch [4390/10000] Train Loss: 0.007658 Val Loss: nan\n",
      "Epoch [4391/10000] Train Loss: 0.007959 Val Loss: nan\n",
      "Epoch [4392/10000] Train Loss: 0.007899 Val Loss: nan\n",
      "Epoch [4393/10000] Train Loss: 0.007796 Val Loss: nan\n",
      "Epoch [4394/10000] Train Loss: 0.007646 Val Loss: nan\n",
      "Epoch [4395/10000] Train Loss: 0.007762 Val Loss: nan\n",
      "Epoch [4396/10000] Train Loss: 0.007754 Val Loss: nan\n",
      "Epoch [4397/10000] Train Loss: 0.007669 Val Loss: nan\n",
      "Epoch [4398/10000] Train Loss: 0.007731 Val Loss: nan\n",
      "Epoch [4399/10000] Train Loss: 0.007979 Val Loss: nan\n",
      "Epoch [4400/10000] Train Loss: 0.007724 Val Loss: nan\n",
      "Epoch [4401/10000] Train Loss: 0.008050 Val Loss: nan\n",
      "Epoch [4402/10000] Train Loss: 0.007818 Val Loss: nan\n",
      "Epoch [4403/10000] Train Loss: 0.007762 Val Loss: nan\n",
      "Epoch [4404/10000] Train Loss: 0.007881 Val Loss: nan\n",
      "Epoch [4405/10000] Train Loss: 0.008001 Val Loss: nan\n",
      "Epoch [4406/10000] Train Loss: 0.007680 Val Loss: nan\n",
      "Epoch [4407/10000] Train Loss: 0.007760 Val Loss: nan\n",
      "Epoch [4408/10000] Train Loss: 0.007759 Val Loss: nan\n",
      "Epoch [4409/10000] Train Loss: 0.007775 Val Loss: nan\n",
      "Epoch [4410/10000] Train Loss: 0.007788 Val Loss: nan\n",
      "Epoch [4411/10000] Train Loss: 0.007616 Val Loss: nan\n",
      "Epoch [4412/10000] Train Loss: 0.007779 Val Loss: nan\n",
      "Epoch [4413/10000] Train Loss: 0.007783 Val Loss: nan\n",
      "Epoch [4414/10000] Train Loss: 0.007646 Val Loss: nan\n",
      "Epoch [4415/10000] Train Loss: 0.007664 Val Loss: nan\n",
      "Epoch [4416/10000] Train Loss: 0.007753 Val Loss: nan\n",
      "Epoch [4417/10000] Train Loss: 0.007615 Val Loss: nan\n",
      "Epoch [4418/10000] Train Loss: 0.007880 Val Loss: nan\n",
      "Epoch [4419/10000] Train Loss: 0.007848 Val Loss: nan\n",
      "Epoch [4420/10000] Train Loss: 0.007752 Val Loss: nan\n",
      "Epoch [4421/10000] Train Loss: 0.007822 Val Loss: nan\n",
      "Epoch [4422/10000] Train Loss: 0.007892 Val Loss: nan\n",
      "Epoch [4423/10000] Train Loss: 0.007690 Val Loss: nan\n",
      "Epoch [4424/10000] Train Loss: 0.007795 Val Loss: nan\n",
      "Epoch [4425/10000] Train Loss: 0.007775 Val Loss: nan\n",
      "Epoch [4426/10000] Train Loss: 0.007735 Val Loss: nan\n",
      "Epoch [4427/10000] Train Loss: 0.007842 Val Loss: nan\n",
      "Epoch [4428/10000] Train Loss: 0.007821 Val Loss: nan\n",
      "Epoch [4429/10000] Train Loss: 0.007727 Val Loss: nan\n",
      "Epoch [4430/10000] Train Loss: 0.007685 Val Loss: nan\n",
      "Epoch [4431/10000] Train Loss: 0.007926 Val Loss: nan\n",
      "Epoch [4432/10000] Train Loss: 0.007653 Val Loss: nan\n",
      "Epoch [4433/10000] Train Loss: 0.007892 Val Loss: nan\n",
      "Epoch [4434/10000] Train Loss: 0.007786 Val Loss: nan\n",
      "Epoch [4435/10000] Train Loss: 0.007642 Val Loss: nan\n",
      "Epoch [4436/10000] Train Loss: 0.007623 Val Loss: nan\n",
      "Epoch [4437/10000] Train Loss: 0.007780 Val Loss: nan\n",
      "Epoch [4438/10000] Train Loss: 0.007975 Val Loss: nan\n",
      "Epoch [4439/10000] Train Loss: 0.008068 Val Loss: nan\n",
      "Epoch [4440/10000] Train Loss: 0.007654 Val Loss: nan\n",
      "Epoch [4441/10000] Train Loss: 0.007828 Val Loss: nan\n",
      "Epoch [4442/10000] Train Loss: 0.007782 Val Loss: nan\n",
      "Epoch [4443/10000] Train Loss: 0.007873 Val Loss: nan\n",
      "Epoch [4444/10000] Train Loss: 0.008084 Val Loss: nan\n",
      "Epoch [4445/10000] Train Loss: 0.007918 Val Loss: nan\n",
      "Epoch [4446/10000] Train Loss: 0.007774 Val Loss: nan\n",
      "Epoch [4447/10000] Train Loss: 0.007872 Val Loss: nan\n",
      "Epoch [4448/10000] Train Loss: 0.007980 Val Loss: nan\n",
      "Epoch [4449/10000] Train Loss: 0.007762 Val Loss: nan\n",
      "Epoch [4450/10000] Train Loss: 0.007932 Val Loss: nan\n",
      "Epoch [4451/10000] Train Loss: 0.007992 Val Loss: nan\n",
      "Epoch [4452/10000] Train Loss: 0.007748 Val Loss: nan\n",
      "Epoch [4453/10000] Train Loss: 0.007663 Val Loss: nan\n",
      "Epoch [4454/10000] Train Loss: 0.007656 Val Loss: nan\n",
      "Epoch [4455/10000] Train Loss: 0.007970 Val Loss: nan\n",
      "Epoch [4456/10000] Train Loss: 0.007757 Val Loss: nan\n",
      "Epoch [4457/10000] Train Loss: 0.007612 Val Loss: nan\n",
      "Epoch [4458/10000] Train Loss: 0.007786 Val Loss: nan\n",
      "Epoch [4459/10000] Train Loss: 0.007806 Val Loss: nan\n",
      "Epoch [4460/10000] Train Loss: 0.007704 Val Loss: nan\n",
      "Epoch [4461/10000] Train Loss: 0.007855 Val Loss: nan\n",
      "Epoch [4462/10000] Train Loss: 0.007744 Val Loss: nan\n",
      "Epoch [4463/10000] Train Loss: 0.007722 Val Loss: nan\n",
      "Epoch [4464/10000] Train Loss: 0.007606 Val Loss: nan\n",
      "Epoch [4465/10000] Train Loss: 0.007672 Val Loss: nan\n",
      "Epoch [4466/10000] Train Loss: 0.007608 Val Loss: nan\n",
      "Epoch [4467/10000] Train Loss: 0.007587 Val Loss: nan\n",
      "Epoch [4468/10000] Train Loss: 0.007571 Val Loss: nan\n",
      "Epoch [4469/10000] Train Loss: 0.007739 Val Loss: nan\n",
      "Epoch [4470/10000] Train Loss: 0.007864 Val Loss: nan\n",
      "Epoch [4471/10000] Train Loss: 0.007894 Val Loss: nan\n",
      "Epoch [4472/10000] Train Loss: 0.007638 Val Loss: nan\n",
      "Epoch [4473/10000] Train Loss: 0.007746 Val Loss: nan\n",
      "Epoch [4474/10000] Train Loss: 0.008046 Val Loss: nan\n",
      "Epoch [4475/10000] Train Loss: 0.007781 Val Loss: nan\n",
      "Epoch [4476/10000] Train Loss: 0.007680 Val Loss: nan\n",
      "Epoch [4477/10000] Train Loss: 0.007671 Val Loss: nan\n",
      "Epoch [4478/10000] Train Loss: 0.007696 Val Loss: nan\n",
      "Epoch [4479/10000] Train Loss: 0.007684 Val Loss: nan\n",
      "Epoch [4480/10000] Train Loss: 0.007974 Val Loss: nan\n",
      "Epoch [4481/10000] Train Loss: 0.007588 Val Loss: nan\n",
      "Epoch [4482/10000] Train Loss: 0.007669 Val Loss: nan\n",
      "Epoch [4483/10000] Train Loss: 0.007712 Val Loss: nan\n",
      "Epoch [4484/10000] Train Loss: 0.007724 Val Loss: nan\n",
      "Epoch [4485/10000] Train Loss: 0.007624 Val Loss: nan\n",
      "Epoch [4486/10000] Train Loss: 0.007829 Val Loss: nan\n",
      "Epoch [4487/10000] Train Loss: 0.007817 Val Loss: nan\n",
      "Epoch [4488/10000] Train Loss: 0.008171 Val Loss: nan\n",
      "Epoch [4489/10000] Train Loss: 0.007617 Val Loss: nan\n",
      "Epoch [4490/10000] Train Loss: 0.007629 Val Loss: nan\n",
      "Epoch [4491/10000] Train Loss: 0.007891 Val Loss: nan\n",
      "Epoch [4492/10000] Train Loss: 0.007732 Val Loss: nan\n",
      "Epoch [4493/10000] Train Loss: 0.007760 Val Loss: nan\n",
      "Epoch [4494/10000] Train Loss: 0.007703 Val Loss: nan\n",
      "Epoch [4495/10000] Train Loss: 0.007662 Val Loss: nan\n",
      "Epoch [4496/10000] Train Loss: 0.007740 Val Loss: nan\n",
      "Epoch [4497/10000] Train Loss: 0.007656 Val Loss: nan\n",
      "Epoch [4498/10000] Train Loss: 0.007704 Val Loss: nan\n",
      "Epoch [4499/10000] Train Loss: 0.007672 Val Loss: nan\n",
      "Epoch [4500/10000] Train Loss: 0.007670 Val Loss: nan\n",
      "Epoch [4501/10000] Train Loss: 0.007596 Val Loss: nan\n",
      "Epoch [4502/10000] Train Loss: 0.007606 Val Loss: nan\n",
      "Epoch [4503/10000] Train Loss: 0.007812 Val Loss: nan\n",
      "Epoch [4504/10000] Train Loss: 0.007580 Val Loss: nan\n",
      "Epoch [4505/10000] Train Loss: 0.007582 Val Loss: nan\n",
      "Epoch [4506/10000] Train Loss: 0.007711 Val Loss: nan\n",
      "Epoch [4507/10000] Train Loss: 0.007743 Val Loss: nan\n",
      "Epoch [4508/10000] Train Loss: 0.007740 Val Loss: nan\n",
      "Epoch [4509/10000] Train Loss: 0.007811 Val Loss: nan\n",
      "Epoch [4510/10000] Train Loss: 0.008020 Val Loss: nan\n",
      "Epoch [4511/10000] Train Loss: 0.007677 Val Loss: nan\n",
      "Epoch [4512/10000] Train Loss: 0.007785 Val Loss: nan\n",
      "Epoch [4513/10000] Train Loss: 0.007693 Val Loss: nan\n",
      "Epoch [4514/10000] Train Loss: 0.007777 Val Loss: nan\n",
      "Epoch [4515/10000] Train Loss: 0.007739 Val Loss: nan\n",
      "Epoch [4516/10000] Train Loss: 0.007744 Val Loss: nan\n",
      "Epoch [4517/10000] Train Loss: 0.007592 Val Loss: nan\n",
      "Epoch [4518/10000] Train Loss: 0.007741 Val Loss: nan\n",
      "Epoch [4519/10000] Train Loss: 0.007587 Val Loss: nan\n",
      "Epoch [4520/10000] Train Loss: 0.007693 Val Loss: nan\n",
      "Epoch [4521/10000] Train Loss: 0.007743 Val Loss: nan\n",
      "Epoch [4522/10000] Train Loss: 0.007792 Val Loss: nan\n",
      "Epoch [4523/10000] Train Loss: 0.007646 Val Loss: nan\n",
      "Epoch [4524/10000] Train Loss: 0.007623 Val Loss: nan\n",
      "Epoch [4525/10000] Train Loss: 0.007934 Val Loss: nan\n",
      "Epoch [4526/10000] Train Loss: 0.007659 Val Loss: nan\n",
      "Epoch [4527/10000] Train Loss: 0.007765 Val Loss: nan\n",
      "Epoch [4528/10000] Train Loss: 0.007593 Val Loss: nan\n",
      "Epoch [4529/10000] Train Loss: 0.007729 Val Loss: nan\n",
      "Epoch [4530/10000] Train Loss: 0.007957 Val Loss: nan\n",
      "Epoch [4531/10000] Train Loss: 0.007826 Val Loss: nan\n",
      "Epoch [4532/10000] Train Loss: 0.007642 Val Loss: nan\n",
      "Epoch [4533/10000] Train Loss: 0.007715 Val Loss: nan\n",
      "Epoch [4534/10000] Train Loss: 0.007584 Val Loss: nan\n",
      "Epoch [4535/10000] Train Loss: 0.007828 Val Loss: nan\n",
      "Epoch [4536/10000] Train Loss: 0.007671 Val Loss: nan\n",
      "Epoch [4537/10000] Train Loss: 0.007871 Val Loss: nan\n",
      "Epoch [4538/10000] Train Loss: 0.007705 Val Loss: nan\n",
      "Epoch [4539/10000] Train Loss: 0.007854 Val Loss: nan\n",
      "Epoch [4540/10000] Train Loss: 0.007793 Val Loss: nan\n",
      "Epoch [4541/10000] Train Loss: 0.007707 Val Loss: nan\n",
      "Epoch [4542/10000] Train Loss: 0.007735 Val Loss: nan\n",
      "Epoch [4543/10000] Train Loss: 0.007950 Val Loss: nan\n",
      "Epoch [4544/10000] Train Loss: 0.007638 Val Loss: nan\n",
      "Epoch [4545/10000] Train Loss: 0.007709 Val Loss: nan\n",
      "Epoch [4546/10000] Train Loss: 0.007667 Val Loss: nan\n",
      "Epoch [4547/10000] Train Loss: 0.007751 Val Loss: nan\n",
      "Epoch [4548/10000] Train Loss: 0.007722 Val Loss: nan\n",
      "Epoch [4549/10000] Train Loss: 0.007590 Val Loss: nan\n",
      "Epoch [4550/10000] Train Loss: 0.007602 Val Loss: nan\n",
      "Epoch [4551/10000] Train Loss: 0.007704 Val Loss: nan\n",
      "Epoch [4552/10000] Train Loss: 0.007667 Val Loss: nan\n",
      "Epoch [4553/10000] Train Loss: 0.007730 Val Loss: nan\n",
      "Epoch [4554/10000] Train Loss: 0.007597 Val Loss: nan\n",
      "Epoch [4555/10000] Train Loss: 0.007709 Val Loss: nan\n",
      "Epoch [4556/10000] Train Loss: 0.007667 Val Loss: nan\n",
      "Epoch [4557/10000] Train Loss: 0.007596 Val Loss: nan\n",
      "Epoch [4558/10000] Train Loss: 0.007825 Val Loss: nan\n",
      "Epoch [4559/10000] Train Loss: 0.007748 Val Loss: nan\n",
      "Epoch [4560/10000] Train Loss: 0.007684 Val Loss: nan\n",
      "Epoch [4561/10000] Train Loss: 0.007782 Val Loss: nan\n",
      "Epoch [4562/10000] Train Loss: 0.007690 Val Loss: nan\n",
      "Epoch [4563/10000] Train Loss: 0.007820 Val Loss: nan\n",
      "Epoch [4564/10000] Train Loss: 0.007760 Val Loss: nan\n",
      "Epoch [4565/10000] Train Loss: 0.007688 Val Loss: nan\n",
      "Epoch [4566/10000] Train Loss: 0.007821 Val Loss: nan\n",
      "Epoch [4567/10000] Train Loss: 0.007781 Val Loss: nan\n",
      "Epoch [4568/10000] Train Loss: 0.007644 Val Loss: nan\n",
      "Epoch [4569/10000] Train Loss: 0.007544 Val Loss: nan\n",
      "Epoch [4570/10000] Train Loss: 0.007764 Val Loss: nan\n",
      "Epoch [4571/10000] Train Loss: 0.007841 Val Loss: nan\n",
      "Epoch [4572/10000] Train Loss: 0.007677 Val Loss: nan\n",
      "Epoch [4573/10000] Train Loss: 0.007737 Val Loss: nan\n",
      "Epoch [4574/10000] Train Loss: 0.007836 Val Loss: nan\n",
      "Epoch [4575/10000] Train Loss: 0.007588 Val Loss: nan\n",
      "Epoch [4576/10000] Train Loss: 0.007677 Val Loss: nan\n",
      "Epoch [4577/10000] Train Loss: 0.007578 Val Loss: nan\n",
      "Epoch [4578/10000] Train Loss: 0.007578 Val Loss: nan\n",
      "Epoch [4579/10000] Train Loss: 0.007620 Val Loss: nan\n",
      "Epoch [4580/10000] Train Loss: 0.007685 Val Loss: nan\n",
      "Epoch [4581/10000] Train Loss: 0.008055 Val Loss: nan\n",
      "Epoch [4582/10000] Train Loss: 0.007688 Val Loss: nan\n",
      "Epoch [4583/10000] Train Loss: 0.007735 Val Loss: nan\n",
      "Epoch [4584/10000] Train Loss: 0.007803 Val Loss: nan\n",
      "Epoch [4585/10000] Train Loss: 0.007575 Val Loss: nan\n",
      "Epoch [4586/10000] Train Loss: 0.007572 Val Loss: nan\n",
      "Epoch [4587/10000] Train Loss: 0.007673 Val Loss: nan\n",
      "Epoch [4588/10000] Train Loss: 0.007574 Val Loss: nan\n",
      "Epoch [4589/10000] Train Loss: 0.007574 Val Loss: nan\n",
      "Epoch [4590/10000] Train Loss: 0.007632 Val Loss: nan\n",
      "Epoch [4591/10000] Train Loss: 0.007725 Val Loss: nan\n",
      "Epoch [4592/10000] Train Loss: 0.007722 Val Loss: nan\n",
      "Epoch [4593/10000] Train Loss: 0.007626 Val Loss: nan\n",
      "Epoch [4594/10000] Train Loss: 0.007743 Val Loss: nan\n",
      "Epoch [4595/10000] Train Loss: 0.007906 Val Loss: nan\n",
      "Epoch [4596/10000] Train Loss: 0.007874 Val Loss: nan\n",
      "Epoch [4597/10000] Train Loss: 0.007867 Val Loss: nan\n",
      "Epoch [4598/10000] Train Loss: 0.007920 Val Loss: nan\n",
      "Epoch [4599/10000] Train Loss: 0.007677 Val Loss: nan\n",
      "Epoch [4600/10000] Train Loss: 0.007743 Val Loss: nan\n",
      "Epoch [4601/10000] Train Loss: 0.007568 Val Loss: nan\n",
      "Epoch [4602/10000] Train Loss: 0.007593 Val Loss: nan\n",
      "Epoch [4603/10000] Train Loss: 0.007697 Val Loss: nan\n",
      "Epoch [4604/10000] Train Loss: 0.007604 Val Loss: nan\n",
      "Epoch [4605/10000] Train Loss: 0.007577 Val Loss: nan\n",
      "Epoch [4606/10000] Train Loss: 0.007631 Val Loss: nan\n",
      "Epoch [4607/10000] Train Loss: 0.007772 Val Loss: nan\n",
      "Epoch [4608/10000] Train Loss: 0.007554 Val Loss: nan\n",
      "Epoch [4609/10000] Train Loss: 0.007561 Val Loss: nan\n",
      "Epoch [4610/10000] Train Loss: 0.007691 Val Loss: nan\n",
      "Epoch [4611/10000] Train Loss: 0.007517 Val Loss: nan\n",
      "Epoch [4612/10000] Train Loss: 0.007663 Val Loss: nan\n",
      "Epoch [4613/10000] Train Loss: 0.007726 Val Loss: nan\n",
      "Epoch [4614/10000] Train Loss: 0.007736 Val Loss: nan\n",
      "Epoch [4615/10000] Train Loss: 0.007749 Val Loss: nan\n",
      "Epoch [4616/10000] Train Loss: 0.007553 Val Loss: nan\n",
      "Epoch [4617/10000] Train Loss: 0.007691 Val Loss: nan\n",
      "Epoch [4618/10000] Train Loss: 0.007677 Val Loss: nan\n",
      "Epoch [4619/10000] Train Loss: 0.007536 Val Loss: nan\n",
      "Epoch [4620/10000] Train Loss: 0.007532 Val Loss: nan\n",
      "Epoch [4621/10000] Train Loss: 0.007612 Val Loss: nan\n",
      "Epoch [4622/10000] Train Loss: 0.007578 Val Loss: nan\n",
      "Epoch [4623/10000] Train Loss: 0.007786 Val Loss: nan\n",
      "Epoch [4624/10000] Train Loss: 0.007629 Val Loss: nan\n",
      "Epoch [4625/10000] Train Loss: 0.007716 Val Loss: nan\n",
      "Epoch [4626/10000] Train Loss: 0.007643 Val Loss: nan\n",
      "Epoch [4627/10000] Train Loss: 0.007793 Val Loss: nan\n",
      "Epoch [4628/10000] Train Loss: 0.007710 Val Loss: nan\n",
      "Epoch [4629/10000] Train Loss: 0.007808 Val Loss: nan\n",
      "Epoch [4630/10000] Train Loss: 0.007837 Val Loss: nan\n",
      "Epoch [4631/10000] Train Loss: 0.007585 Val Loss: nan\n",
      "Epoch [4632/10000] Train Loss: 0.007623 Val Loss: nan\n",
      "Epoch [4633/10000] Train Loss: 0.007656 Val Loss: nan\n",
      "Epoch [4634/10000] Train Loss: 0.007759 Val Loss: nan\n",
      "Epoch [4635/10000] Train Loss: 0.007776 Val Loss: nan\n",
      "Epoch [4636/10000] Train Loss: 0.007752 Val Loss: nan\n",
      "Epoch [4637/10000] Train Loss: 0.007594 Val Loss: nan\n",
      "Epoch [4638/10000] Train Loss: 0.007705 Val Loss: nan\n",
      "Epoch [4639/10000] Train Loss: 0.007641 Val Loss: nan\n",
      "Epoch [4640/10000] Train Loss: 0.007703 Val Loss: nan\n",
      "Epoch [4641/10000] Train Loss: 0.007567 Val Loss: nan\n",
      "Epoch [4642/10000] Train Loss: 0.007577 Val Loss: nan\n",
      "Epoch [4643/10000] Train Loss: 0.007583 Val Loss: nan\n",
      "Epoch [4644/10000] Train Loss: 0.007949 Val Loss: nan\n",
      "Epoch [4645/10000] Train Loss: 0.007721 Val Loss: nan\n",
      "Epoch [4646/10000] Train Loss: 0.007620 Val Loss: nan\n",
      "Epoch [4647/10000] Train Loss: 0.007717 Val Loss: nan\n",
      "Epoch [4648/10000] Train Loss: 0.007581 Val Loss: nan\n",
      "Epoch [4649/10000] Train Loss: 0.007729 Val Loss: nan\n",
      "Epoch [4650/10000] Train Loss: 0.007588 Val Loss: nan\n",
      "Epoch [4651/10000] Train Loss: 0.007731 Val Loss: nan\n",
      "Epoch [4652/10000] Train Loss: 0.007622 Val Loss: nan\n",
      "Epoch [4653/10000] Train Loss: 0.007659 Val Loss: nan\n",
      "Epoch [4654/10000] Train Loss: 0.007736 Val Loss: nan\n",
      "Epoch [4655/10000] Train Loss: 0.007798 Val Loss: nan\n",
      "Epoch [4656/10000] Train Loss: 0.007654 Val Loss: nan\n",
      "Epoch [4657/10000] Train Loss: 0.007695 Val Loss: nan\n",
      "Epoch [4658/10000] Train Loss: 0.007679 Val Loss: nan\n",
      "Epoch [4659/10000] Train Loss: 0.007561 Val Loss: nan\n",
      "Epoch [4660/10000] Train Loss: 0.007612 Val Loss: nan\n",
      "Epoch [4661/10000] Train Loss: 0.007540 Val Loss: nan\n",
      "Epoch [4662/10000] Train Loss: 0.007683 Val Loss: nan\n",
      "Epoch [4663/10000] Train Loss: 0.007635 Val Loss: nan\n",
      "Epoch [4664/10000] Train Loss: 0.007767 Val Loss: nan\n",
      "Epoch [4665/10000] Train Loss: 0.007657 Val Loss: nan\n",
      "Epoch [4666/10000] Train Loss: 0.007623 Val Loss: nan\n",
      "Epoch [4667/10000] Train Loss: 0.007565 Val Loss: nan\n",
      "Epoch [4668/10000] Train Loss: 0.007594 Val Loss: nan\n",
      "Epoch [4669/10000] Train Loss: 0.007839 Val Loss: nan\n",
      "Epoch [4670/10000] Train Loss: 0.007705 Val Loss: nan\n",
      "Epoch [4671/10000] Train Loss: 0.007641 Val Loss: nan\n",
      "Epoch [4672/10000] Train Loss: 0.007546 Val Loss: nan\n",
      "Epoch [4673/10000] Train Loss: 0.007935 Val Loss: nan\n",
      "Epoch [4674/10000] Train Loss: 0.007517 Val Loss: nan\n",
      "Epoch [4675/10000] Train Loss: 0.007558 Val Loss: nan\n",
      "Epoch [4676/10000] Train Loss: 0.007794 Val Loss: nan\n",
      "Epoch [4677/10000] Train Loss: 0.007700 Val Loss: nan\n",
      "Epoch [4678/10000] Train Loss: 0.007832 Val Loss: nan\n",
      "Epoch [4679/10000] Train Loss: 0.007642 Val Loss: nan\n",
      "Epoch [4680/10000] Train Loss: 0.007642 Val Loss: nan\n",
      "Epoch [4681/10000] Train Loss: 0.007593 Val Loss: nan\n",
      "Epoch [4682/10000] Train Loss: 0.007576 Val Loss: nan\n",
      "Epoch [4683/10000] Train Loss: 0.007712 Val Loss: nan\n",
      "Epoch [4684/10000] Train Loss: 0.007585 Val Loss: nan\n",
      "Epoch [4685/10000] Train Loss: 0.007595 Val Loss: nan\n",
      "Epoch [4686/10000] Train Loss: 0.007636 Val Loss: nan\n",
      "Epoch [4687/10000] Train Loss: 0.007756 Val Loss: nan\n",
      "Epoch [4688/10000] Train Loss: 0.007592 Val Loss: nan\n",
      "Epoch [4689/10000] Train Loss: 0.007596 Val Loss: nan\n",
      "Epoch [4690/10000] Train Loss: 0.007631 Val Loss: nan\n",
      "Epoch [4691/10000] Train Loss: 0.007665 Val Loss: nan\n",
      "Epoch [4692/10000] Train Loss: 0.007800 Val Loss: nan\n",
      "Epoch [4693/10000] Train Loss: 0.007606 Val Loss: nan\n",
      "Epoch [4694/10000] Train Loss: 0.007657 Val Loss: nan\n",
      "Epoch [4695/10000] Train Loss: 0.007676 Val Loss: nan\n",
      "Epoch [4696/10000] Train Loss: 0.007601 Val Loss: nan\n",
      "Epoch [4697/10000] Train Loss: 0.007549 Val Loss: nan\n",
      "Epoch [4698/10000] Train Loss: 0.007512 Val Loss: nan\n",
      "Epoch [4699/10000] Train Loss: 0.007551 Val Loss: nan\n",
      "Epoch [4700/10000] Train Loss: 0.007535 Val Loss: nan\n",
      "Epoch [4701/10000] Train Loss: 0.007727 Val Loss: nan\n",
      "Epoch [4702/10000] Train Loss: 0.007577 Val Loss: nan\n",
      "Epoch [4703/10000] Train Loss: 0.007776 Val Loss: nan\n",
      "Epoch [4704/10000] Train Loss: 0.007794 Val Loss: nan\n",
      "Epoch [4705/10000] Train Loss: 0.007731 Val Loss: nan\n",
      "Epoch [4706/10000] Train Loss: 0.007602 Val Loss: nan\n",
      "Epoch [4707/10000] Train Loss: 0.007580 Val Loss: nan\n",
      "Epoch [4708/10000] Train Loss: 0.007518 Val Loss: nan\n",
      "Epoch [4709/10000] Train Loss: 0.007628 Val Loss: nan\n",
      "Epoch [4710/10000] Train Loss: 0.007518 Val Loss: nan\n",
      "Epoch [4711/10000] Train Loss: 0.007591 Val Loss: nan\n",
      "Epoch [4712/10000] Train Loss: 0.007917 Val Loss: nan\n",
      "Epoch [4713/10000] Train Loss: 0.007567 Val Loss: nan\n",
      "Epoch [4714/10000] Train Loss: 0.007683 Val Loss: nan\n",
      "Epoch [4715/10000] Train Loss: 0.007557 Val Loss: nan\n",
      "Epoch [4716/10000] Train Loss: 0.007633 Val Loss: nan\n",
      "Epoch [4717/10000] Train Loss: 0.007678 Val Loss: nan\n",
      "Epoch [4718/10000] Train Loss: 0.007575 Val Loss: nan\n",
      "Epoch [4719/10000] Train Loss: 0.007679 Val Loss: nan\n",
      "Epoch [4720/10000] Train Loss: 0.007667 Val Loss: nan\n",
      "Epoch [4721/10000] Train Loss: 0.007549 Val Loss: nan\n",
      "Epoch [4722/10000] Train Loss: 0.007731 Val Loss: nan\n",
      "Epoch [4723/10000] Train Loss: 0.007553 Val Loss: nan\n",
      "Epoch [4724/10000] Train Loss: 0.007882 Val Loss: nan\n",
      "Epoch [4725/10000] Train Loss: 0.007514 Val Loss: nan\n",
      "Epoch [4726/10000] Train Loss: 0.007648 Val Loss: nan\n",
      "Epoch [4727/10000] Train Loss: 0.007764 Val Loss: nan\n",
      "Epoch [4728/10000] Train Loss: 0.007640 Val Loss: nan\n",
      "Epoch [4729/10000] Train Loss: 0.007686 Val Loss: nan\n",
      "Epoch [4730/10000] Train Loss: 0.007831 Val Loss: nan\n",
      "Epoch [4731/10000] Train Loss: 0.007748 Val Loss: nan\n",
      "Epoch [4732/10000] Train Loss: 0.007705 Val Loss: nan\n",
      "Epoch [4733/10000] Train Loss: 0.007692 Val Loss: nan\n",
      "Epoch [4734/10000] Train Loss: 0.007637 Val Loss: nan\n",
      "Epoch [4735/10000] Train Loss: 0.007626 Val Loss: nan\n",
      "Epoch [4736/10000] Train Loss: 0.007765 Val Loss: nan\n",
      "Epoch [4737/10000] Train Loss: 0.007686 Val Loss: nan\n",
      "Epoch [4738/10000] Train Loss: 0.007558 Val Loss: nan\n",
      "Epoch [4739/10000] Train Loss: 0.007723 Val Loss: nan\n",
      "Epoch [4740/10000] Train Loss: 0.007677 Val Loss: nan\n",
      "Epoch [4741/10000] Train Loss: 0.007611 Val Loss: nan\n",
      "Epoch [4742/10000] Train Loss: 0.007504 Val Loss: nan\n",
      "Epoch [4743/10000] Train Loss: 0.007511 Val Loss: nan\n",
      "Epoch [4744/10000] Train Loss: 0.007630 Val Loss: nan\n",
      "Epoch [4745/10000] Train Loss: 0.007517 Val Loss: nan\n",
      "Epoch [4746/10000] Train Loss: 0.007539 Val Loss: nan\n",
      "Epoch [4747/10000] Train Loss: 0.007615 Val Loss: nan\n",
      "Epoch [4748/10000] Train Loss: 0.007685 Val Loss: nan\n",
      "Epoch [4749/10000] Train Loss: 0.007671 Val Loss: nan\n",
      "Epoch [4750/10000] Train Loss: 0.007719 Val Loss: nan\n",
      "Epoch [4751/10000] Train Loss: 0.007878 Val Loss: nan\n",
      "Epoch [4752/10000] Train Loss: 0.007495 Val Loss: nan\n",
      "Epoch [4753/10000] Train Loss: 0.007525 Val Loss: nan\n",
      "Epoch [4754/10000] Train Loss: 0.007569 Val Loss: nan\n",
      "Epoch [4755/10000] Train Loss: 0.007779 Val Loss: nan\n",
      "Epoch [4756/10000] Train Loss: 0.007943 Val Loss: nan\n",
      "Epoch [4757/10000] Train Loss: 0.007527 Val Loss: nan\n",
      "Epoch [4758/10000] Train Loss: 0.007633 Val Loss: nan\n",
      "Epoch [4759/10000] Train Loss: 0.007567 Val Loss: nan\n",
      "Epoch [4760/10000] Train Loss: 0.007523 Val Loss: nan\n",
      "Epoch [4761/10000] Train Loss: 0.007595 Val Loss: nan\n",
      "Epoch [4762/10000] Train Loss: 0.007615 Val Loss: nan\n",
      "Epoch [4763/10000] Train Loss: 0.007643 Val Loss: nan\n",
      "Epoch [4764/10000] Train Loss: 0.007661 Val Loss: nan\n",
      "Epoch [4765/10000] Train Loss: 0.007625 Val Loss: nan\n",
      "Epoch [4766/10000] Train Loss: 0.007641 Val Loss: nan\n",
      "Epoch [4767/10000] Train Loss: 0.007594 Val Loss: nan\n",
      "Epoch [4768/10000] Train Loss: 0.007707 Val Loss: nan\n",
      "Epoch [4769/10000] Train Loss: 0.007512 Val Loss: nan\n",
      "Epoch [4770/10000] Train Loss: 0.007517 Val Loss: nan\n",
      "Epoch [4771/10000] Train Loss: 0.007678 Val Loss: nan\n",
      "Epoch [4772/10000] Train Loss: 0.007490 Val Loss: nan\n",
      "Epoch [4773/10000] Train Loss: 0.007616 Val Loss: nan\n",
      "Epoch [4774/10000] Train Loss: 0.007511 Val Loss: nan\n",
      "Epoch [4775/10000] Train Loss: 0.007816 Val Loss: nan\n",
      "Epoch [4776/10000] Train Loss: 0.007671 Val Loss: nan\n",
      "Epoch [4777/10000] Train Loss: 0.007697 Val Loss: nan\n",
      "Epoch [4778/10000] Train Loss: 0.007565 Val Loss: nan\n",
      "Epoch [4779/10000] Train Loss: 0.007512 Val Loss: nan\n",
      "Epoch [4780/10000] Train Loss: 0.007617 Val Loss: nan\n",
      "Epoch [4781/10000] Train Loss: 0.007572 Val Loss: nan\n",
      "Epoch [4782/10000] Train Loss: 0.007779 Val Loss: nan\n",
      "Epoch [4783/10000] Train Loss: 0.007626 Val Loss: nan\n",
      "Epoch [4784/10000] Train Loss: 0.007622 Val Loss: nan\n",
      "Epoch [4785/10000] Train Loss: 0.007669 Val Loss: nan\n",
      "Epoch [4786/10000] Train Loss: 0.007532 Val Loss: nan\n",
      "Epoch [4787/10000] Train Loss: 0.007756 Val Loss: nan\n",
      "Epoch [4788/10000] Train Loss: 0.007567 Val Loss: nan\n",
      "Epoch [4789/10000] Train Loss: 0.007536 Val Loss: nan\n",
      "Epoch [4790/10000] Train Loss: 0.007628 Val Loss: nan\n",
      "Epoch [4791/10000] Train Loss: 0.007768 Val Loss: nan\n",
      "Epoch [4792/10000] Train Loss: 0.007628 Val Loss: nan\n",
      "Epoch [4793/10000] Train Loss: 0.007639 Val Loss: nan\n",
      "Epoch [4794/10000] Train Loss: 0.007658 Val Loss: nan\n",
      "Epoch [4795/10000] Train Loss: 0.007513 Val Loss: nan\n",
      "Epoch [4796/10000] Train Loss: 0.007775 Val Loss: nan\n",
      "Epoch [4797/10000] Train Loss: 0.007615 Val Loss: nan\n",
      "Epoch [4798/10000] Train Loss: 0.007605 Val Loss: nan\n",
      "Epoch [4799/10000] Train Loss: 0.007561 Val Loss: nan\n",
      "Epoch [4800/10000] Train Loss: 0.007886 Val Loss: nan\n",
      "Epoch [4801/10000] Train Loss: 0.007813 Val Loss: nan\n",
      "Epoch [4802/10000] Train Loss: 0.007564 Val Loss: nan\n",
      "Epoch [4803/10000] Train Loss: 0.007712 Val Loss: nan\n",
      "Epoch [4804/10000] Train Loss: 0.007524 Val Loss: nan\n",
      "Epoch [4805/10000] Train Loss: 0.007606 Val Loss: nan\n",
      "Epoch [4806/10000] Train Loss: 0.007627 Val Loss: nan\n",
      "Epoch [4807/10000] Train Loss: 0.007588 Val Loss: nan\n",
      "Epoch [4808/10000] Train Loss: 0.007494 Val Loss: nan\n",
      "Epoch [4809/10000] Train Loss: 0.007716 Val Loss: nan\n",
      "Epoch [4810/10000] Train Loss: 0.007587 Val Loss: nan\n",
      "Epoch [4811/10000] Train Loss: 0.007533 Val Loss: nan\n",
      "Epoch [4812/10000] Train Loss: 0.007624 Val Loss: nan\n",
      "Epoch [4813/10000] Train Loss: 0.007591 Val Loss: nan\n",
      "Epoch [4814/10000] Train Loss: 0.007541 Val Loss: nan\n",
      "Epoch [4815/10000] Train Loss: 0.007711 Val Loss: nan\n",
      "Epoch [4816/10000] Train Loss: 0.007638 Val Loss: nan\n",
      "Epoch [4817/10000] Train Loss: 0.007554 Val Loss: nan\n",
      "Epoch [4818/10000] Train Loss: 0.007555 Val Loss: nan\n",
      "Epoch [4819/10000] Train Loss: 0.007672 Val Loss: nan\n",
      "Epoch [4820/10000] Train Loss: 0.007714 Val Loss: nan\n",
      "Epoch [4821/10000] Train Loss: 0.007820 Val Loss: nan\n",
      "Epoch [4822/10000] Train Loss: 0.007624 Val Loss: nan\n",
      "Epoch [4823/10000] Train Loss: 0.007549 Val Loss: nan\n",
      "Epoch [4824/10000] Train Loss: 0.007790 Val Loss: nan\n",
      "Epoch [4825/10000] Train Loss: 0.007732 Val Loss: nan\n",
      "Epoch [4826/10000] Train Loss: 0.007641 Val Loss: nan\n",
      "Epoch [4827/10000] Train Loss: 0.007902 Val Loss: nan\n",
      "Epoch [4828/10000] Train Loss: 0.007762 Val Loss: nan\n",
      "Epoch [4829/10000] Train Loss: 0.007846 Val Loss: nan\n",
      "Epoch [4830/10000] Train Loss: 0.007804 Val Loss: nan\n",
      "Epoch [4831/10000] Train Loss: 0.007509 Val Loss: nan\n",
      "Epoch [4832/10000] Train Loss: 0.007481 Val Loss: nan\n",
      "Epoch [4833/10000] Train Loss: 0.007764 Val Loss: nan\n",
      "Epoch [4834/10000] Train Loss: 0.007516 Val Loss: nan\n",
      "Epoch [4835/10000] Train Loss: 0.007485 Val Loss: nan\n",
      "Epoch [4836/10000] Train Loss: 0.007492 Val Loss: nan\n",
      "Epoch [4837/10000] Train Loss: 0.007820 Val Loss: nan\n",
      "Epoch [4838/10000] Train Loss: 0.007711 Val Loss: nan\n",
      "Epoch [4839/10000] Train Loss: 0.007568 Val Loss: nan\n",
      "Epoch [4840/10000] Train Loss: 0.007617 Val Loss: nan\n",
      "Epoch [4841/10000] Train Loss: 0.007472 Val Loss: nan\n",
      "Epoch [4842/10000] Train Loss: 0.007632 Val Loss: nan\n",
      "Epoch [4843/10000] Train Loss: 0.007698 Val Loss: nan\n",
      "Epoch [4844/10000] Train Loss: 0.007496 Val Loss: nan\n",
      "Epoch [4845/10000] Train Loss: 0.007632 Val Loss: nan\n",
      "Epoch [4846/10000] Train Loss: 0.007613 Val Loss: nan\n",
      "Epoch [4847/10000] Train Loss: 0.007476 Val Loss: nan\n",
      "Epoch [4848/10000] Train Loss: 0.007518 Val Loss: nan\n",
      "Epoch [4849/10000] Train Loss: 0.007534 Val Loss: nan\n",
      "Epoch [4850/10000] Train Loss: 0.007802 Val Loss: nan\n",
      "Epoch [4851/10000] Train Loss: 0.007567 Val Loss: nan\n",
      "Epoch [4852/10000] Train Loss: 0.007692 Val Loss: nan\n",
      "Epoch [4853/10000] Train Loss: 0.007695 Val Loss: nan\n",
      "Epoch [4854/10000] Train Loss: 0.007581 Val Loss: nan\n",
      "Epoch [4855/10000] Train Loss: 0.007667 Val Loss: nan\n",
      "Epoch [4856/10000] Train Loss: 0.007921 Val Loss: nan\n",
      "Epoch [4857/10000] Train Loss: 0.007878 Val Loss: nan\n",
      "Epoch [4858/10000] Train Loss: 0.007667 Val Loss: nan\n",
      "Epoch [4859/10000] Train Loss: 0.007640 Val Loss: nan\n",
      "Epoch [4860/10000] Train Loss: 0.007536 Val Loss: nan\n",
      "Epoch [4861/10000] Train Loss: 0.007641 Val Loss: nan\n",
      "Epoch [4862/10000] Train Loss: 0.007518 Val Loss: nan\n",
      "Epoch [4863/10000] Train Loss: 0.007524 Val Loss: nan\n",
      "Epoch [4864/10000] Train Loss: 0.007644 Val Loss: nan\n",
      "Epoch [4865/10000] Train Loss: 0.007605 Val Loss: nan\n",
      "Epoch [4866/10000] Train Loss: 0.007460 Val Loss: nan\n",
      "Epoch [4867/10000] Train Loss: 0.007595 Val Loss: nan\n",
      "Epoch [4868/10000] Train Loss: 0.007592 Val Loss: nan\n",
      "Epoch [4869/10000] Train Loss: 0.007463 Val Loss: nan\n",
      "Epoch [4870/10000] Train Loss: 0.007727 Val Loss: nan\n",
      "Epoch [4871/10000] Train Loss: 0.007497 Val Loss: nan\n",
      "Epoch [4872/10000] Train Loss: 0.007748 Val Loss: nan\n",
      "Epoch [4873/10000] Train Loss: 0.007645 Val Loss: nan\n",
      "Epoch [4874/10000] Train Loss: 0.007595 Val Loss: nan\n",
      "Epoch [4875/10000] Train Loss: 0.007621 Val Loss: nan\n",
      "Epoch [4876/10000] Train Loss: 0.007697 Val Loss: nan\n",
      "Epoch [4877/10000] Train Loss: 0.007625 Val Loss: nan\n",
      "Epoch [4878/10000] Train Loss: 0.007495 Val Loss: nan\n",
      "Epoch [4879/10000] Train Loss: 0.007654 Val Loss: nan\n",
      "Epoch [4880/10000] Train Loss: 0.007627 Val Loss: nan\n",
      "Epoch [4881/10000] Train Loss: 0.007535 Val Loss: nan\n",
      "Epoch [4882/10000] Train Loss: 0.007648 Val Loss: nan\n",
      "Epoch [4883/10000] Train Loss: 0.007480 Val Loss: nan\n",
      "Epoch [4884/10000] Train Loss: 0.007615 Val Loss: nan\n",
      "Epoch [4885/10000] Train Loss: 0.007636 Val Loss: nan\n",
      "Epoch [4886/10000] Train Loss: 0.007459 Val Loss: nan\n",
      "Epoch [4887/10000] Train Loss: 0.007530 Val Loss: nan\n",
      "Epoch [4888/10000] Train Loss: 0.007468 Val Loss: nan\n",
      "Epoch [4889/10000] Train Loss: 0.007688 Val Loss: nan\n",
      "Epoch [4890/10000] Train Loss: 0.007588 Val Loss: nan\n",
      "Epoch [4891/10000] Train Loss: 0.007590 Val Loss: nan\n",
      "Epoch [4892/10000] Train Loss: 0.007601 Val Loss: nan\n",
      "Epoch [4893/10000] Train Loss: 0.007509 Val Loss: nan\n",
      "Epoch [4894/10000] Train Loss: 0.007559 Val Loss: nan\n",
      "Epoch [4895/10000] Train Loss: 0.007473 Val Loss: nan\n",
      "Epoch [4896/10000] Train Loss: 0.007591 Val Loss: nan\n",
      "Epoch [4897/10000] Train Loss: 0.007470 Val Loss: nan\n",
      "Epoch [4898/10000] Train Loss: 0.007574 Val Loss: nan\n",
      "Epoch [4899/10000] Train Loss: 0.007529 Val Loss: nan\n",
      "Epoch [4900/10000] Train Loss: 0.007628 Val Loss: nan\n",
      "Epoch [4901/10000] Train Loss: 0.007660 Val Loss: nan\n",
      "Epoch [4902/10000] Train Loss: 0.007582 Val Loss: nan\n",
      "Epoch [4903/10000] Train Loss: 0.007456 Val Loss: nan\n",
      "Epoch [4904/10000] Train Loss: 0.007464 Val Loss: nan\n",
      "Epoch [4905/10000] Train Loss: 0.007668 Val Loss: nan\n",
      "Epoch [4906/10000] Train Loss: 0.007669 Val Loss: nan\n",
      "Epoch [4907/10000] Train Loss: 0.007485 Val Loss: nan\n",
      "Epoch [4908/10000] Train Loss: 0.007468 Val Loss: nan\n",
      "Epoch [4909/10000] Train Loss: 0.007741 Val Loss: nan\n",
      "Epoch [4910/10000] Train Loss: 0.007630 Val Loss: nan\n",
      "Epoch [4911/10000] Train Loss: 0.007534 Val Loss: nan\n",
      "Epoch [4912/10000] Train Loss: 0.007608 Val Loss: nan\n",
      "Epoch [4913/10000] Train Loss: 0.007637 Val Loss: nan\n",
      "Epoch [4914/10000] Train Loss: 0.007619 Val Loss: nan\n",
      "Epoch [4915/10000] Train Loss: 0.007581 Val Loss: nan\n",
      "Epoch [4916/10000] Train Loss: 0.007560 Val Loss: nan\n",
      "Epoch [4917/10000] Train Loss: 0.007819 Val Loss: nan\n",
      "Epoch [4918/10000] Train Loss: 0.007791 Val Loss: nan\n",
      "Epoch [4919/10000] Train Loss: 0.007674 Val Loss: nan\n",
      "Epoch [4920/10000] Train Loss: 0.007523 Val Loss: nan\n",
      "Epoch [4921/10000] Train Loss: 0.007572 Val Loss: nan\n",
      "Epoch [4922/10000] Train Loss: 0.007485 Val Loss: nan\n",
      "Epoch [4923/10000] Train Loss: 0.007420 Val Loss: nan\n",
      "Epoch [4924/10000] Train Loss: 0.007477 Val Loss: nan\n",
      "Epoch [4925/10000] Train Loss: 0.007673 Val Loss: nan\n",
      "Epoch [4926/10000] Train Loss: 0.007543 Val Loss: nan\n",
      "Epoch [4927/10000] Train Loss: 0.007545 Val Loss: nan\n",
      "Epoch [4928/10000] Train Loss: 0.007453 Val Loss: nan\n",
      "Epoch [4929/10000] Train Loss: 0.007531 Val Loss: nan\n",
      "Epoch [4930/10000] Train Loss: 0.007613 Val Loss: nan\n",
      "Epoch [4931/10000] Train Loss: 0.007564 Val Loss: nan\n",
      "Epoch [4932/10000] Train Loss: 0.007665 Val Loss: nan\n",
      "Epoch [4933/10000] Train Loss: 0.007552 Val Loss: nan\n",
      "Epoch [4934/10000] Train Loss: 0.007439 Val Loss: nan\n",
      "Epoch [4935/10000] Train Loss: 0.007656 Val Loss: nan\n",
      "Epoch [4936/10000] Train Loss: 0.007472 Val Loss: nan\n",
      "Epoch [4937/10000] Train Loss: 0.007535 Val Loss: nan\n",
      "Epoch [4938/10000] Train Loss: 0.007447 Val Loss: nan\n",
      "Epoch [4939/10000] Train Loss: 0.007739 Val Loss: nan\n",
      "Epoch [4940/10000] Train Loss: 0.007416 Val Loss: nan\n",
      "Epoch [4941/10000] Train Loss: 0.007431 Val Loss: nan\n",
      "Epoch [4942/10000] Train Loss: 0.007604 Val Loss: nan\n",
      "Epoch [4943/10000] Train Loss: 0.007475 Val Loss: nan\n",
      "Epoch [4944/10000] Train Loss: 0.007668 Val Loss: nan\n",
      "Epoch [4945/10000] Train Loss: 0.007524 Val Loss: nan\n",
      "Epoch [4946/10000] Train Loss: 0.007595 Val Loss: nan\n",
      "Epoch [4947/10000] Train Loss: 0.007470 Val Loss: nan\n",
      "Epoch [4948/10000] Train Loss: 0.007455 Val Loss: nan\n",
      "Epoch [4949/10000] Train Loss: 0.007564 Val Loss: nan\n",
      "Epoch [4950/10000] Train Loss: 0.007472 Val Loss: nan\n",
      "Epoch [4951/10000] Train Loss: 0.007700 Val Loss: nan\n",
      "Epoch [4952/10000] Train Loss: 0.007594 Val Loss: nan\n",
      "Epoch [4953/10000] Train Loss: 0.007606 Val Loss: nan\n",
      "Epoch [4954/10000] Train Loss: 0.007539 Val Loss: nan\n",
      "Epoch [4955/10000] Train Loss: 0.007625 Val Loss: nan\n",
      "Epoch [4956/10000] Train Loss: 0.007465 Val Loss: nan\n",
      "Epoch [4957/10000] Train Loss: 0.007825 Val Loss: nan\n",
      "Epoch [4958/10000] Train Loss: 0.007591 Val Loss: nan\n",
      "Epoch [4959/10000] Train Loss: 0.007474 Val Loss: nan\n",
      "Epoch [4960/10000] Train Loss: 0.007639 Val Loss: nan\n",
      "Epoch [4961/10000] Train Loss: 0.007621 Val Loss: nan\n",
      "Epoch [4962/10000] Train Loss: 0.007680 Val Loss: nan\n",
      "Epoch [4963/10000] Train Loss: 0.007544 Val Loss: nan\n",
      "Epoch [4964/10000] Train Loss: 0.007550 Val Loss: nan\n",
      "Epoch [4965/10000] Train Loss: 0.007715 Val Loss: nan\n",
      "Epoch [4966/10000] Train Loss: 0.007493 Val Loss: nan\n",
      "Epoch [4967/10000] Train Loss: 0.007559 Val Loss: nan\n",
      "Epoch [4968/10000] Train Loss: 0.007795 Val Loss: nan\n",
      "Epoch [4969/10000] Train Loss: 0.007613 Val Loss: nan\n",
      "Epoch [4970/10000] Train Loss: 0.007604 Val Loss: nan\n",
      "Epoch [4971/10000] Train Loss: 0.007515 Val Loss: nan\n",
      "Epoch [4972/10000] Train Loss: 0.007722 Val Loss: nan\n",
      "Epoch [4973/10000] Train Loss: 0.007591 Val Loss: nan\n",
      "Epoch [4974/10000] Train Loss: 0.007584 Val Loss: nan\n",
      "Epoch [4975/10000] Train Loss: 0.007468 Val Loss: nan\n",
      "Epoch [4976/10000] Train Loss: 0.007599 Val Loss: nan\n",
      "Epoch [4977/10000] Train Loss: 0.007419 Val Loss: nan\n",
      "Epoch [4978/10000] Train Loss: 0.007549 Val Loss: nan\n",
      "Epoch [4979/10000] Train Loss: 0.007508 Val Loss: nan\n",
      "Epoch [4980/10000] Train Loss: 0.007482 Val Loss: nan\n",
      "Epoch [4981/10000] Train Loss: 0.007437 Val Loss: nan\n",
      "Epoch [4982/10000] Train Loss: 0.007636 Val Loss: nan\n",
      "Epoch [4983/10000] Train Loss: 0.007637 Val Loss: nan\n",
      "Epoch [4984/10000] Train Loss: 0.007463 Val Loss: nan\n",
      "Epoch [4985/10000] Train Loss: 0.007541 Val Loss: nan\n",
      "Epoch [4986/10000] Train Loss: 0.007522 Val Loss: nan\n",
      "Epoch [4987/10000] Train Loss: 0.007809 Val Loss: nan\n",
      "Epoch [4988/10000] Train Loss: 0.007762 Val Loss: nan\n",
      "Epoch [4989/10000] Train Loss: 0.007463 Val Loss: nan\n",
      "Epoch [4990/10000] Train Loss: 0.007601 Val Loss: nan\n",
      "Epoch [4991/10000] Train Loss: 0.007660 Val Loss: nan\n",
      "Epoch [4992/10000] Train Loss: 0.007433 Val Loss: nan\n",
      "Epoch [4993/10000] Train Loss: 0.007450 Val Loss: nan\n",
      "Epoch [4994/10000] Train Loss: 0.007484 Val Loss: nan\n",
      "Epoch [4995/10000] Train Loss: 0.007530 Val Loss: nan\n",
      "Epoch [4996/10000] Train Loss: 0.007812 Val Loss: nan\n",
      "Epoch [4997/10000] Train Loss: 0.007461 Val Loss: nan\n",
      "Epoch [4998/10000] Train Loss: 0.007445 Val Loss: nan\n",
      "Epoch [4999/10000] Train Loss: 0.007491 Val Loss: nan\n",
      "Epoch [5000/10000] Train Loss: 0.007468 Val Loss: nan\n",
      "Epoch [5001/10000] Train Loss: 0.007431 Val Loss: nan\n",
      "Epoch [5002/10000] Train Loss: 0.007657 Val Loss: nan\n",
      "Epoch [5003/10000] Train Loss: 0.007521 Val Loss: nan\n",
      "Epoch [5004/10000] Train Loss: 0.007718 Val Loss: nan\n",
      "Epoch [5005/10000] Train Loss: 0.007601 Val Loss: nan\n",
      "Epoch [5006/10000] Train Loss: 0.007512 Val Loss: nan\n",
      "Epoch [5007/10000] Train Loss: 0.007576 Val Loss: nan\n",
      "Epoch [5008/10000] Train Loss: 0.007413 Val Loss: nan\n",
      "Epoch [5009/10000] Train Loss: 0.007516 Val Loss: nan\n",
      "Epoch [5010/10000] Train Loss: 0.007408 Val Loss: nan\n",
      "Epoch [5011/10000] Train Loss: 0.007503 Val Loss: nan\n",
      "Epoch [5012/10000] Train Loss: 0.007797 Val Loss: nan\n",
      "Epoch [5013/10000] Train Loss: 0.007609 Val Loss: nan\n",
      "Epoch [5014/10000] Train Loss: 0.007627 Val Loss: nan\n",
      "Epoch [5015/10000] Train Loss: 0.007714 Val Loss: nan\n",
      "Epoch [5016/10000] Train Loss: 0.007463 Val Loss: nan\n",
      "Epoch [5017/10000] Train Loss: 0.007686 Val Loss: nan\n",
      "Epoch [5018/10000] Train Loss: 0.007747 Val Loss: nan\n",
      "Epoch [5019/10000] Train Loss: 0.007715 Val Loss: nan\n",
      "Epoch [5020/10000] Train Loss: 0.007663 Val Loss: nan\n",
      "Epoch [5021/10000] Train Loss: 0.007560 Val Loss: nan\n",
      "Epoch [5022/10000] Train Loss: 0.007472 Val Loss: nan\n",
      "Epoch [5023/10000] Train Loss: 0.007424 Val Loss: nan\n",
      "Epoch [5024/10000] Train Loss: 0.007590 Val Loss: nan\n",
      "Epoch [5025/10000] Train Loss: 0.007473 Val Loss: nan\n",
      "Epoch [5026/10000] Train Loss: 0.007775 Val Loss: nan\n",
      "Epoch [5027/10000] Train Loss: 0.007547 Val Loss: nan\n",
      "Epoch [5028/10000] Train Loss: 0.007548 Val Loss: nan\n",
      "Epoch [5029/10000] Train Loss: 0.007441 Val Loss: nan\n",
      "Epoch [5030/10000] Train Loss: 0.007569 Val Loss: nan\n",
      "Epoch [5031/10000] Train Loss: 0.007600 Val Loss: nan\n",
      "Epoch [5032/10000] Train Loss: 0.007441 Val Loss: nan\n",
      "Epoch [5033/10000] Train Loss: 0.007439 Val Loss: nan\n",
      "Epoch [5034/10000] Train Loss: 0.007466 Val Loss: nan\n",
      "Epoch [5035/10000] Train Loss: 0.007527 Val Loss: nan\n",
      "Epoch [5036/10000] Train Loss: 0.007414 Val Loss: nan\n",
      "Epoch [5037/10000] Train Loss: 0.007557 Val Loss: nan\n",
      "Epoch [5038/10000] Train Loss: 0.007681 Val Loss: nan\n",
      "Epoch [5039/10000] Train Loss: 0.007510 Val Loss: nan\n",
      "Epoch [5040/10000] Train Loss: 0.007584 Val Loss: nan\n",
      "Epoch [5041/10000] Train Loss: 0.007738 Val Loss: nan\n",
      "Epoch [5042/10000] Train Loss: 0.007522 Val Loss: nan\n",
      "Epoch [5043/10000] Train Loss: 0.007489 Val Loss: nan\n",
      "Epoch [5044/10000] Train Loss: 0.007388 Val Loss: nan\n",
      "Epoch [5045/10000] Train Loss: 0.007667 Val Loss: nan\n",
      "Epoch [5046/10000] Train Loss: 0.007459 Val Loss: nan\n",
      "Epoch [5047/10000] Train Loss: 0.007486 Val Loss: nan\n",
      "Epoch [5048/10000] Train Loss: 0.007705 Val Loss: nan\n",
      "Epoch [5049/10000] Train Loss: 0.007545 Val Loss: nan\n",
      "Epoch [5050/10000] Train Loss: 0.007506 Val Loss: nan\n",
      "Epoch [5051/10000] Train Loss: 0.007494 Val Loss: nan\n",
      "Epoch [5052/10000] Train Loss: 0.007830 Val Loss: nan\n",
      "Epoch [5053/10000] Train Loss: 0.007728 Val Loss: nan\n",
      "Epoch [5054/10000] Train Loss: 0.007591 Val Loss: nan\n",
      "Epoch [5055/10000] Train Loss: 0.007769 Val Loss: nan\n",
      "Epoch [5056/10000] Train Loss: 0.007580 Val Loss: nan\n",
      "Epoch [5057/10000] Train Loss: 0.007583 Val Loss: nan\n",
      "Epoch [5058/10000] Train Loss: 0.007526 Val Loss: nan\n",
      "Epoch [5059/10000] Train Loss: 0.007769 Val Loss: nan\n",
      "Epoch [5060/10000] Train Loss: 0.007423 Val Loss: nan\n",
      "Epoch [5061/10000] Train Loss: 0.007458 Val Loss: nan\n",
      "Epoch [5062/10000] Train Loss: 0.007388 Val Loss: nan\n",
      "Epoch [5063/10000] Train Loss: 0.007464 Val Loss: nan\n",
      "Epoch [5064/10000] Train Loss: 0.007587 Val Loss: nan\n",
      "Epoch [5065/10000] Train Loss: 0.007500 Val Loss: nan\n",
      "Epoch [5066/10000] Train Loss: 0.007523 Val Loss: nan\n",
      "Epoch [5067/10000] Train Loss: 0.007493 Val Loss: nan\n",
      "Epoch [5068/10000] Train Loss: 0.007440 Val Loss: nan\n",
      "Epoch [5069/10000] Train Loss: 0.007548 Val Loss: nan\n",
      "Epoch [5070/10000] Train Loss: 0.007559 Val Loss: nan\n",
      "Epoch [5071/10000] Train Loss: 0.007564 Val Loss: nan\n",
      "Epoch [5072/10000] Train Loss: 0.007522 Val Loss: nan\n",
      "Epoch [5073/10000] Train Loss: 0.007388 Val Loss: nan\n",
      "Epoch [5074/10000] Train Loss: 0.007631 Val Loss: nan\n",
      "Epoch [5075/10000] Train Loss: 0.007447 Val Loss: nan\n",
      "Epoch [5076/10000] Train Loss: 0.007523 Val Loss: nan\n",
      "Epoch [5077/10000] Train Loss: 0.007751 Val Loss: nan\n",
      "Epoch [5078/10000] Train Loss: 0.007550 Val Loss: nan\n",
      "Epoch [5079/10000] Train Loss: 0.007383 Val Loss: nan\n",
      "Epoch [5080/10000] Train Loss: 0.007385 Val Loss: nan\n",
      "Epoch [5081/10000] Train Loss: 0.007620 Val Loss: nan\n",
      "Epoch [5082/10000] Train Loss: 0.007529 Val Loss: nan\n",
      "Epoch [5083/10000] Train Loss: 0.007396 Val Loss: nan\n",
      "Epoch [5084/10000] Train Loss: 0.007541 Val Loss: nan\n",
      "Epoch [5085/10000] Train Loss: 0.007495 Val Loss: nan\n",
      "Epoch [5086/10000] Train Loss: 0.007858 Val Loss: nan\n",
      "Epoch [5087/10000] Train Loss: 0.007603 Val Loss: nan\n",
      "Epoch [5088/10000] Train Loss: 0.007395 Val Loss: nan\n",
      "Epoch [5089/10000] Train Loss: 0.007463 Val Loss: nan\n",
      "Epoch [5090/10000] Train Loss: 0.007423 Val Loss: nan\n",
      "Epoch [5091/10000] Train Loss: 0.007565 Val Loss: nan\n",
      "Epoch [5092/10000] Train Loss: 0.007543 Val Loss: nan\n",
      "Epoch [5093/10000] Train Loss: 0.007683 Val Loss: nan\n",
      "Epoch [5094/10000] Train Loss: 0.007557 Val Loss: nan\n",
      "Epoch [5095/10000] Train Loss: 0.007655 Val Loss: nan\n",
      "Epoch [5096/10000] Train Loss: 0.007745 Val Loss: nan\n",
      "Epoch [5097/10000] Train Loss: 0.007447 Val Loss: nan\n",
      "Epoch [5098/10000] Train Loss: 0.007392 Val Loss: nan\n",
      "Epoch [5099/10000] Train Loss: 0.007533 Val Loss: nan\n",
      "Epoch [5100/10000] Train Loss: 0.007641 Val Loss: nan\n",
      "Epoch [5101/10000] Train Loss: 0.007792 Val Loss: nan\n",
      "Epoch [5102/10000] Train Loss: 0.007607 Val Loss: nan\n",
      "Epoch [5103/10000] Train Loss: 0.007484 Val Loss: nan\n",
      "Epoch [5104/10000] Train Loss: 0.007685 Val Loss: nan\n",
      "Epoch [5105/10000] Train Loss: 0.007472 Val Loss: nan\n",
      "Epoch [5106/10000] Train Loss: 0.007568 Val Loss: nan\n",
      "Epoch [5107/10000] Train Loss: 0.007592 Val Loss: nan\n",
      "Epoch [5108/10000] Train Loss: 0.007624 Val Loss: nan\n",
      "Epoch [5109/10000] Train Loss: 0.007564 Val Loss: nan\n",
      "Epoch [5110/10000] Train Loss: 0.007483 Val Loss: nan\n",
      "Epoch [5111/10000] Train Loss: 0.007593 Val Loss: nan\n",
      "Epoch [5112/10000] Train Loss: 0.007397 Val Loss: nan\n",
      "Epoch [5113/10000] Train Loss: 0.007382 Val Loss: nan\n",
      "Epoch [5114/10000] Train Loss: 0.007438 Val Loss: nan\n",
      "Epoch [5115/10000] Train Loss: 0.007551 Val Loss: nan\n",
      "Epoch [5116/10000] Train Loss: 0.007794 Val Loss: nan\n",
      "Epoch [5117/10000] Train Loss: 0.007552 Val Loss: nan\n",
      "Epoch [5118/10000] Train Loss: 0.007402 Val Loss: nan\n",
      "Epoch [5119/10000] Train Loss: 0.007432 Val Loss: nan\n",
      "Epoch [5120/10000] Train Loss: 0.007428 Val Loss: nan\n",
      "Epoch [5121/10000] Train Loss: 0.007357 Val Loss: nan\n",
      "Epoch [5122/10000] Train Loss: 0.007402 Val Loss: nan\n",
      "Epoch [5123/10000] Train Loss: 0.007491 Val Loss: nan\n",
      "Epoch [5124/10000] Train Loss: 0.007654 Val Loss: nan\n",
      "Epoch [5125/10000] Train Loss: 0.007592 Val Loss: nan\n",
      "Epoch [5126/10000] Train Loss: 0.007644 Val Loss: nan\n",
      "Epoch [5127/10000] Train Loss: 0.007642 Val Loss: nan\n",
      "Epoch [5128/10000] Train Loss: 0.007403 Val Loss: nan\n",
      "Epoch [5129/10000] Train Loss: 0.007520 Val Loss: nan\n",
      "Epoch [5130/10000] Train Loss: 0.007517 Val Loss: nan\n",
      "Epoch [5131/10000] Train Loss: 0.007619 Val Loss: nan\n",
      "Epoch [5132/10000] Train Loss: 0.007556 Val Loss: nan\n",
      "Epoch [5133/10000] Train Loss: 0.007415 Val Loss: nan\n",
      "Epoch [5134/10000] Train Loss: 0.007557 Val Loss: nan\n",
      "Epoch [5135/10000] Train Loss: 0.007419 Val Loss: nan\n",
      "Epoch [5136/10000] Train Loss: 0.007725 Val Loss: nan\n",
      "Epoch [5137/10000] Train Loss: 0.007665 Val Loss: nan\n",
      "Epoch [5138/10000] Train Loss: 0.007531 Val Loss: nan\n",
      "Epoch [5139/10000] Train Loss: 0.007583 Val Loss: nan\n",
      "Epoch [5140/10000] Train Loss: 0.007450 Val Loss: nan\n",
      "Epoch [5141/10000] Train Loss: 0.007417 Val Loss: nan\n",
      "Epoch [5142/10000] Train Loss: 0.007575 Val Loss: nan\n",
      "Epoch [5143/10000] Train Loss: 0.007521 Val Loss: nan\n",
      "Epoch [5144/10000] Train Loss: 0.007552 Val Loss: nan\n",
      "Epoch [5145/10000] Train Loss: 0.007514 Val Loss: nan\n",
      "Epoch [5146/10000] Train Loss: 0.007425 Val Loss: nan\n",
      "Epoch [5147/10000] Train Loss: 0.007506 Val Loss: nan\n",
      "Epoch [5148/10000] Train Loss: 0.007528 Val Loss: nan\n",
      "Epoch [5149/10000] Train Loss: 0.007658 Val Loss: nan\n",
      "Epoch [5150/10000] Train Loss: 0.007404 Val Loss: nan\n",
      "Epoch [5151/10000] Train Loss: 0.007579 Val Loss: nan\n",
      "Epoch [5152/10000] Train Loss: 0.007412 Val Loss: nan\n",
      "Epoch [5153/10000] Train Loss: 0.007709 Val Loss: nan\n",
      "Epoch [5154/10000] Train Loss: 0.007564 Val Loss: nan\n",
      "Epoch [5155/10000] Train Loss: 0.007459 Val Loss: nan\n",
      "Epoch [5156/10000] Train Loss: 0.007546 Val Loss: nan\n",
      "Epoch [5157/10000] Train Loss: 0.007586 Val Loss: nan\n",
      "Epoch [5158/10000] Train Loss: 0.007551 Val Loss: nan\n",
      "Epoch [5159/10000] Train Loss: 0.007639 Val Loss: nan\n",
      "Epoch [5160/10000] Train Loss: 0.007449 Val Loss: nan\n",
      "Epoch [5161/10000] Train Loss: 0.007602 Val Loss: nan\n",
      "Epoch [5162/10000] Train Loss: 0.007404 Val Loss: nan\n",
      "Epoch [5163/10000] Train Loss: 0.007361 Val Loss: nan\n",
      "Epoch [5164/10000] Train Loss: 0.007409 Val Loss: nan\n",
      "Epoch [5165/10000] Train Loss: 0.007496 Val Loss: nan\n",
      "Epoch [5166/10000] Train Loss: 0.007534 Val Loss: nan\n",
      "Epoch [5167/10000] Train Loss: 0.007628 Val Loss: nan\n",
      "Epoch [5168/10000] Train Loss: 0.007423 Val Loss: nan\n",
      "Epoch [5169/10000] Train Loss: 0.007519 Val Loss: nan\n",
      "Epoch [5170/10000] Train Loss: 0.007551 Val Loss: nan\n",
      "Epoch [5171/10000] Train Loss: 0.007455 Val Loss: nan\n",
      "Epoch [5172/10000] Train Loss: 0.007424 Val Loss: nan\n",
      "Epoch [5173/10000] Train Loss: 0.007398 Val Loss: nan\n",
      "Epoch [5174/10000] Train Loss: 0.007499 Val Loss: nan\n",
      "Epoch [5175/10000] Train Loss: 0.007468 Val Loss: nan\n",
      "Epoch [5176/10000] Train Loss: 0.007367 Val Loss: nan\n",
      "Epoch [5177/10000] Train Loss: 0.007347 Val Loss: nan\n",
      "Epoch [5178/10000] Train Loss: 0.007612 Val Loss: nan\n",
      "Epoch [5179/10000] Train Loss: 0.007536 Val Loss: nan\n",
      "Epoch [5180/10000] Train Loss: 0.007547 Val Loss: nan\n",
      "Epoch [5181/10000] Train Loss: 0.007508 Val Loss: nan\n",
      "Epoch [5182/10000] Train Loss: 0.007397 Val Loss: nan\n",
      "Epoch [5183/10000] Train Loss: 0.007641 Val Loss: nan\n",
      "Epoch [5184/10000] Train Loss: 0.007412 Val Loss: nan\n",
      "Epoch [5185/10000] Train Loss: 0.007416 Val Loss: nan\n",
      "Epoch [5186/10000] Train Loss: 0.007535 Val Loss: nan\n",
      "Epoch [5187/10000] Train Loss: 0.007673 Val Loss: nan\n",
      "Epoch [5188/10000] Train Loss: 0.007506 Val Loss: nan\n",
      "Epoch [5189/10000] Train Loss: 0.007463 Val Loss: nan\n",
      "Epoch [5190/10000] Train Loss: 0.007438 Val Loss: nan\n",
      "Epoch [5191/10000] Train Loss: 0.007483 Val Loss: nan\n",
      "Epoch [5192/10000] Train Loss: 0.007655 Val Loss: nan\n",
      "Epoch [5193/10000] Train Loss: 0.007453 Val Loss: nan\n",
      "Epoch [5194/10000] Train Loss: 0.007523 Val Loss: nan\n",
      "Epoch [5195/10000] Train Loss: 0.007414 Val Loss: nan\n",
      "Epoch [5196/10000] Train Loss: 0.007508 Val Loss: nan\n",
      "Epoch [5197/10000] Train Loss: 0.007650 Val Loss: nan\n",
      "Epoch [5198/10000] Train Loss: 0.007605 Val Loss: nan\n",
      "Epoch [5199/10000] Train Loss: 0.007602 Val Loss: nan\n",
      "Epoch [5200/10000] Train Loss: 0.007378 Val Loss: nan\n",
      "Epoch [5201/10000] Train Loss: 0.007504 Val Loss: nan\n",
      "Epoch [5202/10000] Train Loss: 0.007954 Val Loss: nan\n",
      "Epoch [5203/10000] Train Loss: 0.007464 Val Loss: nan\n",
      "Epoch [5204/10000] Train Loss: 0.007435 Val Loss: nan\n",
      "Epoch [5205/10000] Train Loss: 0.007613 Val Loss: nan\n",
      "Epoch [5206/10000] Train Loss: 0.007432 Val Loss: nan\n",
      "Epoch [5207/10000] Train Loss: 0.007402 Val Loss: nan\n",
      "Epoch [5208/10000] Train Loss: 0.007600 Val Loss: nan\n",
      "Epoch [5209/10000] Train Loss: 0.007513 Val Loss: nan\n",
      "Epoch [5210/10000] Train Loss: 0.007474 Val Loss: nan\n",
      "Epoch [5211/10000] Train Loss: 0.007541 Val Loss: nan\n",
      "Epoch [5212/10000] Train Loss: 0.007588 Val Loss: nan\n",
      "Epoch [5213/10000] Train Loss: 0.007475 Val Loss: nan\n",
      "Epoch [5214/10000] Train Loss: 0.007349 Val Loss: nan\n",
      "Epoch [5215/10000] Train Loss: 0.007572 Val Loss: nan\n",
      "Epoch [5216/10000] Train Loss: 0.007551 Val Loss: nan\n",
      "Epoch [5217/10000] Train Loss: 0.007570 Val Loss: nan\n",
      "Epoch [5218/10000] Train Loss: 0.007562 Val Loss: nan\n",
      "Epoch [5219/10000] Train Loss: 0.007499 Val Loss: nan\n",
      "Epoch [5220/10000] Train Loss: 0.007404 Val Loss: nan\n",
      "Epoch [5221/10000] Train Loss: 0.007614 Val Loss: nan\n",
      "Epoch [5222/10000] Train Loss: 0.007518 Val Loss: nan\n",
      "Epoch [5223/10000] Train Loss: 0.007719 Val Loss: nan\n",
      "Epoch [5224/10000] Train Loss: 0.007495 Val Loss: nan\n",
      "Epoch [5225/10000] Train Loss: 0.007497 Val Loss: nan\n",
      "Epoch [5226/10000] Train Loss: 0.007436 Val Loss: nan\n",
      "Epoch [5227/10000] Train Loss: 0.007634 Val Loss: nan\n",
      "Epoch [5228/10000] Train Loss: 0.007428 Val Loss: nan\n",
      "Epoch [5229/10000] Train Loss: 0.007418 Val Loss: nan\n",
      "Epoch [5230/10000] Train Loss: 0.007527 Val Loss: nan\n",
      "Epoch [5231/10000] Train Loss: 0.007503 Val Loss: nan\n",
      "Epoch [5232/10000] Train Loss: 0.007457 Val Loss: nan\n",
      "Epoch [5233/10000] Train Loss: 0.007343 Val Loss: nan\n",
      "Epoch [5234/10000] Train Loss: 0.007380 Val Loss: nan\n",
      "Epoch [5235/10000] Train Loss: 0.007612 Val Loss: nan\n",
      "Epoch [5236/10000] Train Loss: 0.007467 Val Loss: nan\n",
      "Epoch [5237/10000] Train Loss: 0.007390 Val Loss: nan\n",
      "Epoch [5238/10000] Train Loss: 0.007531 Val Loss: nan\n",
      "Epoch [5239/10000] Train Loss: 0.007402 Val Loss: nan\n",
      "Epoch [5240/10000] Train Loss: 0.007550 Val Loss: nan\n",
      "Epoch [5241/10000] Train Loss: 0.007518 Val Loss: nan\n",
      "Epoch [5242/10000] Train Loss: 0.007425 Val Loss: nan\n",
      "Epoch [5243/10000] Train Loss: 0.007512 Val Loss: nan\n",
      "Epoch [5244/10000] Train Loss: 0.007511 Val Loss: nan\n",
      "Epoch [5245/10000] Train Loss: 0.007366 Val Loss: nan\n",
      "Epoch [5246/10000] Train Loss: 0.007549 Val Loss: nan\n",
      "Epoch [5247/10000] Train Loss: 0.007716 Val Loss: nan\n",
      "Epoch [5248/10000] Train Loss: 0.007618 Val Loss: nan\n",
      "Epoch [5249/10000] Train Loss: 0.007491 Val Loss: nan\n",
      "Epoch [5250/10000] Train Loss: 0.007539 Val Loss: nan\n",
      "Epoch [5251/10000] Train Loss: 0.007350 Val Loss: nan\n",
      "Epoch [5252/10000] Train Loss: 0.007411 Val Loss: nan\n",
      "Epoch [5253/10000] Train Loss: 0.007469 Val Loss: nan\n",
      "Epoch [5254/10000] Train Loss: 0.007653 Val Loss: nan\n",
      "Epoch [5255/10000] Train Loss: 0.007479 Val Loss: nan\n",
      "Epoch [5256/10000] Train Loss: 0.007417 Val Loss: nan\n",
      "Epoch [5257/10000] Train Loss: 0.007423 Val Loss: nan\n",
      "Epoch [5258/10000] Train Loss: 0.007547 Val Loss: nan\n",
      "Epoch [5259/10000] Train Loss: 0.007403 Val Loss: nan\n",
      "Epoch [5260/10000] Train Loss: 0.007462 Val Loss: nan\n",
      "Epoch [5261/10000] Train Loss: 0.007678 Val Loss: nan\n",
      "Epoch [5262/10000] Train Loss: 0.007462 Val Loss: nan\n",
      "Epoch [5263/10000] Train Loss: 0.007457 Val Loss: nan\n",
      "Epoch [5264/10000] Train Loss: 0.007664 Val Loss: nan\n",
      "Epoch [5265/10000] Train Loss: 0.007472 Val Loss: nan\n",
      "Epoch [5266/10000] Train Loss: 0.007630 Val Loss: nan\n",
      "Epoch [5267/10000] Train Loss: 0.007646 Val Loss: nan\n",
      "Epoch [5268/10000] Train Loss: 0.007395 Val Loss: nan\n",
      "Epoch [5269/10000] Train Loss: 0.007496 Val Loss: nan\n",
      "Epoch [5270/10000] Train Loss: 0.007544 Val Loss: nan\n",
      "Epoch [5271/10000] Train Loss: 0.007376 Val Loss: nan\n",
      "Epoch [5272/10000] Train Loss: 0.007467 Val Loss: nan\n",
      "Epoch [5273/10000] Train Loss: 0.007321 Val Loss: nan\n",
      "Epoch [5274/10000] Train Loss: 0.007388 Val Loss: nan\n",
      "Epoch [5275/10000] Train Loss: 0.007501 Val Loss: nan\n",
      "Epoch [5276/10000] Train Loss: 0.007366 Val Loss: nan\n",
      "Epoch [5277/10000] Train Loss: 0.007459 Val Loss: nan\n",
      "Epoch [5278/10000] Train Loss: 0.007434 Val Loss: nan\n",
      "Epoch [5279/10000] Train Loss: 0.007374 Val Loss: nan\n",
      "Epoch [5280/10000] Train Loss: 0.007381 Val Loss: nan\n",
      "Epoch [5281/10000] Train Loss: 0.007480 Val Loss: nan\n",
      "Epoch [5282/10000] Train Loss: 0.007473 Val Loss: nan\n",
      "Epoch [5283/10000] Train Loss: 0.007324 Val Loss: nan\n",
      "Epoch [5284/10000] Train Loss: 0.007466 Val Loss: nan\n",
      "Epoch [5285/10000] Train Loss: 0.007310 Val Loss: nan\n",
      "Epoch [5286/10000] Train Loss: 0.007382 Val Loss: nan\n",
      "Epoch [5287/10000] Train Loss: 0.007393 Val Loss: nan\n",
      "Epoch [5288/10000] Train Loss: 0.007441 Val Loss: nan\n",
      "Epoch [5289/10000] Train Loss: 0.007512 Val Loss: nan\n",
      "Epoch [5290/10000] Train Loss: 0.007652 Val Loss: nan\n",
      "Epoch [5291/10000] Train Loss: 0.007457 Val Loss: nan\n",
      "Epoch [5292/10000] Train Loss: 0.007519 Val Loss: nan\n",
      "Epoch [5293/10000] Train Loss: 0.007507 Val Loss: nan\n",
      "Epoch [5294/10000] Train Loss: 0.007470 Val Loss: nan\n",
      "Epoch [5295/10000] Train Loss: 0.007440 Val Loss: nan\n",
      "Epoch [5296/10000] Train Loss: 0.007388 Val Loss: nan\n",
      "Epoch [5297/10000] Train Loss: 0.007454 Val Loss: nan\n",
      "Epoch [5298/10000] Train Loss: 0.007443 Val Loss: nan\n",
      "Epoch [5299/10000] Train Loss: 0.007613 Val Loss: nan\n",
      "Epoch [5300/10000] Train Loss: 0.007385 Val Loss: nan\n",
      "Epoch [5301/10000] Train Loss: 0.007374 Val Loss: nan\n",
      "Epoch [5302/10000] Train Loss: 0.007751 Val Loss: nan\n",
      "Epoch [5303/10000] Train Loss: 0.007479 Val Loss: nan\n",
      "Epoch [5304/10000] Train Loss: 0.007606 Val Loss: nan\n",
      "Epoch [5305/10000] Train Loss: 0.007381 Val Loss: nan\n",
      "Epoch [5306/10000] Train Loss: 0.007601 Val Loss: nan\n",
      "Epoch [5307/10000] Train Loss: 0.007609 Val Loss: nan\n",
      "Epoch [5308/10000] Train Loss: 0.007681 Val Loss: nan\n",
      "Epoch [5309/10000] Train Loss: 0.007548 Val Loss: nan\n",
      "Epoch [5310/10000] Train Loss: 0.007500 Val Loss: nan\n",
      "Epoch [5311/10000] Train Loss: 0.007414 Val Loss: nan\n",
      "Epoch [5312/10000] Train Loss: 0.007502 Val Loss: nan\n",
      "Epoch [5313/10000] Train Loss: 0.007495 Val Loss: nan\n",
      "Epoch [5314/10000] Train Loss: 0.007401 Val Loss: nan\n",
      "Epoch [5315/10000] Train Loss: 0.007504 Val Loss: nan\n",
      "Epoch [5316/10000] Train Loss: 0.007436 Val Loss: nan\n",
      "Epoch [5317/10000] Train Loss: 0.007457 Val Loss: nan\n",
      "Epoch [5318/10000] Train Loss: 0.007501 Val Loss: nan\n",
      "Epoch [5319/10000] Train Loss: 0.007325 Val Loss: nan\n",
      "Epoch [5320/10000] Train Loss: 0.007494 Val Loss: nan\n",
      "Epoch [5321/10000] Train Loss: 0.007538 Val Loss: nan\n",
      "Epoch [5322/10000] Train Loss: 0.007363 Val Loss: nan\n",
      "Epoch [5323/10000] Train Loss: 0.007595 Val Loss: nan\n",
      "Epoch [5324/10000] Train Loss: 0.007496 Val Loss: nan\n",
      "Epoch [5325/10000] Train Loss: 0.007460 Val Loss: nan\n",
      "Epoch [5326/10000] Train Loss: 0.007419 Val Loss: nan\n",
      "Epoch [5327/10000] Train Loss: 0.007608 Val Loss: nan\n",
      "Epoch [5328/10000] Train Loss: 0.007866 Val Loss: nan\n",
      "Epoch [5329/10000] Train Loss: 0.007419 Val Loss: nan\n",
      "Epoch [5330/10000] Train Loss: 0.007393 Val Loss: nan\n",
      "Epoch [5331/10000] Train Loss: 0.007458 Val Loss: nan\n",
      "Epoch [5332/10000] Train Loss: 0.007439 Val Loss: nan\n",
      "Epoch [5333/10000] Train Loss: 0.007494 Val Loss: nan\n",
      "Epoch [5334/10000] Train Loss: 0.007570 Val Loss: nan\n",
      "Epoch [5335/10000] Train Loss: 0.007464 Val Loss: nan\n",
      "Epoch [5336/10000] Train Loss: 0.007362 Val Loss: nan\n",
      "Epoch [5337/10000] Train Loss: 0.007309 Val Loss: nan\n",
      "Epoch [5338/10000] Train Loss: 0.007340 Val Loss: nan\n",
      "Epoch [5339/10000] Train Loss: 0.007366 Val Loss: nan\n",
      "Epoch [5340/10000] Train Loss: 0.007337 Val Loss: nan\n",
      "Epoch [5341/10000] Train Loss: 0.007390 Val Loss: nan\n",
      "Epoch [5342/10000] Train Loss: 0.007531 Val Loss: nan\n",
      "Epoch [5343/10000] Train Loss: 0.007342 Val Loss: nan\n",
      "Epoch [5344/10000] Train Loss: 0.007652 Val Loss: nan\n",
      "Epoch [5345/10000] Train Loss: 0.007369 Val Loss: nan\n",
      "Epoch [5346/10000] Train Loss: 0.007533 Val Loss: nan\n",
      "Epoch [5347/10000] Train Loss: 0.007554 Val Loss: nan\n",
      "Epoch [5348/10000] Train Loss: 0.007463 Val Loss: nan\n",
      "Epoch [5349/10000] Train Loss: 0.007629 Val Loss: nan\n",
      "Epoch [5350/10000] Train Loss: 0.007459 Val Loss: nan\n",
      "Epoch [5351/10000] Train Loss: 0.007366 Val Loss: nan\n",
      "Epoch [5352/10000] Train Loss: 0.007343 Val Loss: nan\n",
      "Epoch [5353/10000] Train Loss: 0.007499 Val Loss: nan\n",
      "Epoch [5354/10000] Train Loss: 0.007408 Val Loss: nan\n",
      "Epoch [5355/10000] Train Loss: 0.007378 Val Loss: nan\n",
      "Epoch [5356/10000] Train Loss: 0.007600 Val Loss: nan\n",
      "Epoch [5357/10000] Train Loss: 0.007367 Val Loss: nan\n",
      "Epoch [5358/10000] Train Loss: 0.007468 Val Loss: nan\n",
      "Epoch [5359/10000] Train Loss: 0.007667 Val Loss: nan\n",
      "Epoch [5360/10000] Train Loss: 0.007379 Val Loss: nan\n",
      "Epoch [5361/10000] Train Loss: 0.007382 Val Loss: nan\n",
      "Epoch [5362/10000] Train Loss: 0.007356 Val Loss: nan\n",
      "Epoch [5363/10000] Train Loss: 0.007322 Val Loss: nan\n",
      "Epoch [5364/10000] Train Loss: 0.007476 Val Loss: nan\n",
      "Epoch [5365/10000] Train Loss: 0.007340 Val Loss: nan\n",
      "Epoch [5366/10000] Train Loss: 0.007347 Val Loss: nan\n",
      "Epoch [5367/10000] Train Loss: 0.007340 Val Loss: nan\n",
      "Epoch [5368/10000] Train Loss: 0.007565 Val Loss: nan\n",
      "Epoch [5369/10000] Train Loss: 0.007607 Val Loss: nan\n",
      "Epoch [5370/10000] Train Loss: 0.007479 Val Loss: nan\n",
      "Epoch [5371/10000] Train Loss: 0.007562 Val Loss: nan\n",
      "Epoch [5372/10000] Train Loss: 0.007547 Val Loss: nan\n",
      "Epoch [5373/10000] Train Loss: 0.007658 Val Loss: nan\n",
      "Epoch [5374/10000] Train Loss: 0.007363 Val Loss: nan\n",
      "Epoch [5375/10000] Train Loss: 0.007573 Val Loss: nan\n",
      "Epoch [5376/10000] Train Loss: 0.007374 Val Loss: nan\n",
      "Epoch [5377/10000] Train Loss: 0.007470 Val Loss: nan\n",
      "Epoch [5378/10000] Train Loss: 0.007389 Val Loss: nan\n",
      "Epoch [5379/10000] Train Loss: 0.007446 Val Loss: nan\n",
      "Epoch [5380/10000] Train Loss: 0.007484 Val Loss: nan\n",
      "Epoch [5381/10000] Train Loss: 0.007398 Val Loss: nan\n",
      "Epoch [5382/10000] Train Loss: 0.007332 Val Loss: nan\n",
      "Epoch [5383/10000] Train Loss: 0.007326 Val Loss: nan\n",
      "Epoch [5384/10000] Train Loss: 0.007390 Val Loss: nan\n",
      "Epoch [5385/10000] Train Loss: 0.007533 Val Loss: nan\n",
      "Epoch [5386/10000] Train Loss: 0.007456 Val Loss: nan\n",
      "Epoch [5387/10000] Train Loss: 0.007370 Val Loss: nan\n",
      "Epoch [5388/10000] Train Loss: 0.007535 Val Loss: nan\n",
      "Epoch [5389/10000] Train Loss: 0.007371 Val Loss: nan\n",
      "Epoch [5390/10000] Train Loss: 0.007359 Val Loss: nan\n",
      "Epoch [5391/10000] Train Loss: 0.007579 Val Loss: nan\n",
      "Epoch [5392/10000] Train Loss: 0.007432 Val Loss: nan\n",
      "Epoch [5393/10000] Train Loss: 0.007539 Val Loss: nan\n",
      "Epoch [5394/10000] Train Loss: 0.007419 Val Loss: nan\n",
      "Epoch [5395/10000] Train Loss: 0.007466 Val Loss: nan\n",
      "Epoch [5396/10000] Train Loss: 0.007455 Val Loss: nan\n",
      "Epoch [5397/10000] Train Loss: 0.007300 Val Loss: nan\n",
      "Epoch [5398/10000] Train Loss: 0.007328 Val Loss: nan\n",
      "Epoch [5399/10000] Train Loss: 0.007327 Val Loss: nan\n",
      "Epoch [5400/10000] Train Loss: 0.007509 Val Loss: nan\n",
      "Epoch [5401/10000] Train Loss: 0.007559 Val Loss: nan\n",
      "Epoch [5402/10000] Train Loss: 0.007541 Val Loss: nan\n",
      "Epoch [5403/10000] Train Loss: 0.007667 Val Loss: nan\n",
      "Epoch [5404/10000] Train Loss: 0.007481 Val Loss: nan\n",
      "Epoch [5405/10000] Train Loss: 0.007376 Val Loss: nan\n",
      "Epoch [5406/10000] Train Loss: 0.007630 Val Loss: nan\n",
      "Epoch [5407/10000] Train Loss: 0.007415 Val Loss: nan\n",
      "Epoch [5408/10000] Train Loss: 0.007575 Val Loss: nan\n",
      "Epoch [5409/10000] Train Loss: 0.007449 Val Loss: nan\n",
      "Epoch [5410/10000] Train Loss: 0.007451 Val Loss: nan\n",
      "Epoch [5411/10000] Train Loss: 0.007446 Val Loss: nan\n",
      "Epoch [5412/10000] Train Loss: 0.007424 Val Loss: nan\n",
      "Epoch [5413/10000] Train Loss: 0.007379 Val Loss: nan\n",
      "Epoch [5414/10000] Train Loss: 0.007340 Val Loss: nan\n",
      "Epoch [5415/10000] Train Loss: 0.007346 Val Loss: nan\n",
      "Epoch [5416/10000] Train Loss: 0.007439 Val Loss: nan\n",
      "Epoch [5417/10000] Train Loss: 0.007338 Val Loss: nan\n",
      "Epoch [5418/10000] Train Loss: 0.007449 Val Loss: nan\n",
      "Epoch [5419/10000] Train Loss: 0.007490 Val Loss: nan\n",
      "Epoch [5420/10000] Train Loss: 0.007466 Val Loss: nan\n",
      "Epoch [5421/10000] Train Loss: 0.007696 Val Loss: nan\n",
      "Epoch [5422/10000] Train Loss: 0.007539 Val Loss: nan\n",
      "Epoch [5423/10000] Train Loss: 0.007523 Val Loss: nan\n",
      "Epoch [5424/10000] Train Loss: 0.007513 Val Loss: nan\n",
      "Epoch [5425/10000] Train Loss: 0.007338 Val Loss: nan\n",
      "Epoch [5426/10000] Train Loss: 0.007346 Val Loss: nan\n",
      "Epoch [5427/10000] Train Loss: 0.007457 Val Loss: nan\n",
      "Epoch [5428/10000] Train Loss: 0.007374 Val Loss: nan\n",
      "Epoch [5429/10000] Train Loss: 0.007362 Val Loss: nan\n",
      "Epoch [5430/10000] Train Loss: 0.007438 Val Loss: nan\n",
      "Epoch [5431/10000] Train Loss: 0.007587 Val Loss: nan\n",
      "Epoch [5432/10000] Train Loss: 0.007650 Val Loss: nan\n",
      "Epoch [5433/10000] Train Loss: 0.007419 Val Loss: nan\n",
      "Epoch [5434/10000] Train Loss: 0.007357 Val Loss: nan\n",
      "Epoch [5435/10000] Train Loss: 0.007328 Val Loss: nan\n",
      "Epoch [5436/10000] Train Loss: 0.007295 Val Loss: nan\n",
      "Epoch [5437/10000] Train Loss: 0.007455 Val Loss: nan\n",
      "Epoch [5438/10000] Train Loss: 0.007755 Val Loss: nan\n",
      "Epoch [5439/10000] Train Loss: 0.007433 Val Loss: nan\n",
      "Epoch [5440/10000] Train Loss: 0.007526 Val Loss: nan\n",
      "Epoch [5441/10000] Train Loss: 0.007327 Val Loss: nan\n",
      "Epoch [5442/10000] Train Loss: 0.007449 Val Loss: nan\n",
      "Epoch [5443/10000] Train Loss: 0.007441 Val Loss: nan\n",
      "Epoch [5444/10000] Train Loss: 0.007327 Val Loss: nan\n",
      "Epoch [5445/10000] Train Loss: 0.007468 Val Loss: nan\n",
      "Epoch [5446/10000] Train Loss: 0.007447 Val Loss: nan\n",
      "Epoch [5447/10000] Train Loss: 0.007380 Val Loss: nan\n",
      "Epoch [5448/10000] Train Loss: 0.007464 Val Loss: nan\n",
      "Epoch [5449/10000] Train Loss: 0.007325 Val Loss: nan\n",
      "Epoch [5450/10000] Train Loss: 0.007524 Val Loss: nan\n",
      "Epoch [5451/10000] Train Loss: 0.007385 Val Loss: nan\n",
      "Epoch [5452/10000] Train Loss: 0.007495 Val Loss: nan\n",
      "Epoch [5453/10000] Train Loss: 0.007613 Val Loss: nan\n",
      "Epoch [5454/10000] Train Loss: 0.007523 Val Loss: nan\n",
      "Epoch [5455/10000] Train Loss: 0.007570 Val Loss: nan\n",
      "Epoch [5456/10000] Train Loss: 0.007496 Val Loss: nan\n",
      "Epoch [5457/10000] Train Loss: 0.007409 Val Loss: nan\n",
      "Epoch [5458/10000] Train Loss: 0.007418 Val Loss: nan\n",
      "Epoch [5459/10000] Train Loss: 0.007588 Val Loss: nan\n",
      "Epoch [5460/10000] Train Loss: 0.007413 Val Loss: nan\n",
      "Epoch [5461/10000] Train Loss: 0.007488 Val Loss: nan\n",
      "Epoch [5462/10000] Train Loss: 0.007413 Val Loss: nan\n",
      "Epoch [5463/10000] Train Loss: 0.007330 Val Loss: nan\n",
      "Epoch [5464/10000] Train Loss: 0.007480 Val Loss: nan\n",
      "Epoch [5465/10000] Train Loss: 0.007412 Val Loss: nan\n",
      "Epoch [5466/10000] Train Loss: 0.007330 Val Loss: nan\n",
      "Epoch [5467/10000] Train Loss: 0.007347 Val Loss: nan\n",
      "Epoch [5468/10000] Train Loss: 0.007463 Val Loss: nan\n",
      "Epoch [5469/10000] Train Loss: 0.007456 Val Loss: nan\n",
      "Epoch [5470/10000] Train Loss: 0.007424 Val Loss: nan\n",
      "Epoch [5471/10000] Train Loss: 0.007329 Val Loss: nan\n",
      "Epoch [5472/10000] Train Loss: 0.007319 Val Loss: nan\n",
      "Epoch [5473/10000] Train Loss: 0.007464 Val Loss: nan\n",
      "Epoch [5474/10000] Train Loss: 0.007472 Val Loss: nan\n",
      "Epoch [5475/10000] Train Loss: 0.007324 Val Loss: nan\n",
      "Epoch [5476/10000] Train Loss: 0.007321 Val Loss: nan\n",
      "Epoch [5477/10000] Train Loss: 0.007338 Val Loss: nan\n",
      "Epoch [5478/10000] Train Loss: 0.007467 Val Loss: nan\n",
      "Epoch [5479/10000] Train Loss: 0.007368 Val Loss: nan\n",
      "Epoch [5480/10000] Train Loss: 0.007597 Val Loss: nan\n",
      "Epoch [5481/10000] Train Loss: 0.007330 Val Loss: nan\n",
      "Epoch [5482/10000] Train Loss: 0.007281 Val Loss: nan\n",
      "Epoch [5483/10000] Train Loss: 0.007444 Val Loss: nan\n",
      "Epoch [5484/10000] Train Loss: 0.007324 Val Loss: nan\n",
      "Epoch [5485/10000] Train Loss: 0.007289 Val Loss: nan\n",
      "Epoch [5486/10000] Train Loss: 0.007306 Val Loss: nan\n",
      "Epoch [5487/10000] Train Loss: 0.007435 Val Loss: nan\n",
      "Epoch [5488/10000] Train Loss: 0.007406 Val Loss: nan\n",
      "Epoch [5489/10000] Train Loss: 0.007432 Val Loss: nan\n",
      "Epoch [5490/10000] Train Loss: 0.007642 Val Loss: nan\n",
      "Epoch [5491/10000] Train Loss: 0.007374 Val Loss: nan\n",
      "Epoch [5492/10000] Train Loss: 0.007426 Val Loss: nan\n",
      "Epoch [5493/10000] Train Loss: 0.007434 Val Loss: nan\n",
      "Epoch [5494/10000] Train Loss: 0.007558 Val Loss: nan\n",
      "Epoch [5495/10000] Train Loss: 0.007465 Val Loss: nan\n",
      "Epoch [5496/10000] Train Loss: 0.007444 Val Loss: nan\n",
      "Epoch [5497/10000] Train Loss: 0.007508 Val Loss: nan\n",
      "Epoch [5498/10000] Train Loss: 0.007431 Val Loss: nan\n",
      "Epoch [5499/10000] Train Loss: 0.007327 Val Loss: nan\n",
      "Epoch [5500/10000] Train Loss: 0.007504 Val Loss: nan\n",
      "Epoch [5501/10000] Train Loss: 0.007351 Val Loss: nan\n",
      "Epoch [5502/10000] Train Loss: 0.007458 Val Loss: nan\n",
      "Epoch [5503/10000] Train Loss: 0.007376 Val Loss: nan\n",
      "Epoch [5504/10000] Train Loss: 0.007475 Val Loss: nan\n",
      "Epoch [5505/10000] Train Loss: 0.007520 Val Loss: nan\n",
      "Epoch [5506/10000] Train Loss: 0.007490 Val Loss: nan\n",
      "Epoch [5507/10000] Train Loss: 0.007370 Val Loss: nan\n",
      "Epoch [5508/10000] Train Loss: 0.007456 Val Loss: nan\n",
      "Epoch [5509/10000] Train Loss: 0.007480 Val Loss: nan\n",
      "Epoch [5510/10000] Train Loss: 0.007411 Val Loss: nan\n",
      "Epoch [5511/10000] Train Loss: 0.007422 Val Loss: nan\n",
      "Epoch [5512/10000] Train Loss: 0.007340 Val Loss: nan\n",
      "Epoch [5513/10000] Train Loss: 0.007371 Val Loss: nan\n",
      "Epoch [5514/10000] Train Loss: 0.007434 Val Loss: nan\n",
      "Epoch [5515/10000] Train Loss: 0.007602 Val Loss: nan\n",
      "Epoch [5516/10000] Train Loss: 0.007311 Val Loss: nan\n",
      "Epoch [5517/10000] Train Loss: 0.007323 Val Loss: nan\n",
      "Epoch [5518/10000] Train Loss: 0.007475 Val Loss: nan\n",
      "Epoch [5519/10000] Train Loss: 0.007332 Val Loss: nan\n",
      "Epoch [5520/10000] Train Loss: 0.007323 Val Loss: nan\n",
      "Epoch [5521/10000] Train Loss: 0.007486 Val Loss: nan\n",
      "Epoch [5522/10000] Train Loss: 0.007326 Val Loss: nan\n",
      "Epoch [5523/10000] Train Loss: 0.007413 Val Loss: nan\n",
      "Epoch [5524/10000] Train Loss: 0.007380 Val Loss: nan\n",
      "Epoch [5525/10000] Train Loss: 0.007323 Val Loss: nan\n",
      "Epoch [5526/10000] Train Loss: 0.007315 Val Loss: nan\n",
      "Epoch [5527/10000] Train Loss: 0.007572 Val Loss: nan\n",
      "Epoch [5528/10000] Train Loss: 0.007407 Val Loss: nan\n",
      "Epoch [5529/10000] Train Loss: 0.007553 Val Loss: nan\n",
      "Epoch [5530/10000] Train Loss: 0.007547 Val Loss: nan\n",
      "Epoch [5531/10000] Train Loss: 0.007391 Val Loss: nan\n",
      "Epoch [5532/10000] Train Loss: 0.007530 Val Loss: nan\n",
      "Epoch [5533/10000] Train Loss: 0.007448 Val Loss: nan\n",
      "Epoch [5534/10000] Train Loss: 0.007469 Val Loss: nan\n",
      "Epoch [5535/10000] Train Loss: 0.007514 Val Loss: nan\n",
      "Epoch [5536/10000] Train Loss: 0.007363 Val Loss: nan\n",
      "Epoch [5537/10000] Train Loss: 0.007507 Val Loss: nan\n",
      "Epoch [5538/10000] Train Loss: 0.007312 Val Loss: nan\n",
      "Epoch [5539/10000] Train Loss: 0.007323 Val Loss: nan\n",
      "Epoch [5540/10000] Train Loss: 0.007343 Val Loss: nan\n",
      "Epoch [5541/10000] Train Loss: 0.007347 Val Loss: nan\n",
      "Epoch [5542/10000] Train Loss: 0.007292 Val Loss: nan\n",
      "Epoch [5543/10000] Train Loss: 0.007350 Val Loss: nan\n",
      "Epoch [5544/10000] Train Loss: 0.007424 Val Loss: nan\n",
      "Epoch [5545/10000] Train Loss: 0.007515 Val Loss: nan\n",
      "Epoch [5546/10000] Train Loss: 0.007262 Val Loss: nan\n",
      "Epoch [5547/10000] Train Loss: 0.007472 Val Loss: nan\n",
      "Epoch [5548/10000] Train Loss: 0.007481 Val Loss: nan\n",
      "Epoch [5549/10000] Train Loss: 0.007428 Val Loss: nan\n",
      "Epoch [5550/10000] Train Loss: 0.007414 Val Loss: nan\n",
      "Epoch [5551/10000] Train Loss: 0.007485 Val Loss: nan\n",
      "Epoch [5552/10000] Train Loss: 0.007532 Val Loss: nan\n",
      "Epoch [5553/10000] Train Loss: 0.007414 Val Loss: nan\n",
      "Epoch [5554/10000] Train Loss: 0.007543 Val Loss: nan\n",
      "Epoch [5555/10000] Train Loss: 0.007436 Val Loss: nan\n",
      "Epoch [5556/10000] Train Loss: 0.007355 Val Loss: nan\n",
      "Epoch [5557/10000] Train Loss: 0.007435 Val Loss: nan\n",
      "Epoch [5558/10000] Train Loss: 0.007336 Val Loss: nan\n",
      "Epoch [5559/10000] Train Loss: 0.007404 Val Loss: nan\n",
      "Epoch [5560/10000] Train Loss: 0.007422 Val Loss: nan\n",
      "Epoch [5561/10000] Train Loss: 0.007279 Val Loss: nan\n",
      "Epoch [5562/10000] Train Loss: 0.007458 Val Loss: nan\n",
      "Epoch [5563/10000] Train Loss: 0.007394 Val Loss: nan\n",
      "Epoch [5564/10000] Train Loss: 0.007314 Val Loss: nan\n",
      "Epoch [5565/10000] Train Loss: 0.007499 Val Loss: nan\n",
      "Epoch [5566/10000] Train Loss: 0.007356 Val Loss: nan\n",
      "Epoch [5567/10000] Train Loss: 0.007382 Val Loss: nan\n",
      "Epoch [5568/10000] Train Loss: 0.007314 Val Loss: nan\n",
      "Epoch [5569/10000] Train Loss: 0.007622 Val Loss: nan\n",
      "Epoch [5570/10000] Train Loss: 0.007368 Val Loss: nan\n",
      "Epoch [5571/10000] Train Loss: 0.007358 Val Loss: nan\n",
      "Epoch [5572/10000] Train Loss: 0.007328 Val Loss: nan\n",
      "Epoch [5573/10000] Train Loss: 0.007396 Val Loss: nan\n",
      "Epoch [5574/10000] Train Loss: 0.007406 Val Loss: nan\n",
      "Epoch [5575/10000] Train Loss: 0.007368 Val Loss: nan\n",
      "Epoch [5576/10000] Train Loss: 0.007667 Val Loss: nan\n",
      "Epoch [5577/10000] Train Loss: 0.007599 Val Loss: nan\n",
      "Epoch [5578/10000] Train Loss: 0.007365 Val Loss: nan\n",
      "Epoch [5579/10000] Train Loss: 0.007540 Val Loss: nan\n",
      "Epoch [5580/10000] Train Loss: 0.007445 Val Loss: nan\n",
      "Epoch [5581/10000] Train Loss: 0.007531 Val Loss: nan\n",
      "Epoch [5582/10000] Train Loss: 0.007284 Val Loss: nan\n",
      "Epoch [5583/10000] Train Loss: 0.007422 Val Loss: nan\n",
      "Epoch [5584/10000] Train Loss: 0.007471 Val Loss: nan\n",
      "Epoch [5585/10000] Train Loss: 0.007270 Val Loss: nan\n",
      "Epoch [5586/10000] Train Loss: 0.007552 Val Loss: nan\n",
      "Epoch [5587/10000] Train Loss: 0.007301 Val Loss: nan\n",
      "Epoch [5588/10000] Train Loss: 0.007338 Val Loss: nan\n",
      "Epoch [5589/10000] Train Loss: 0.007265 Val Loss: nan\n",
      "Epoch [5590/10000] Train Loss: 0.007429 Val Loss: nan\n",
      "Epoch [5591/10000] Train Loss: 0.007370 Val Loss: nan\n",
      "Epoch [5592/10000] Train Loss: 0.007519 Val Loss: nan\n",
      "Epoch [5593/10000] Train Loss: 0.007314 Val Loss: nan\n",
      "Epoch [5594/10000] Train Loss: 0.007540 Val Loss: nan\n",
      "Epoch [5595/10000] Train Loss: 0.007609 Val Loss: nan\n",
      "Epoch [5596/10000] Train Loss: 0.007259 Val Loss: nan\n",
      "Epoch [5597/10000] Train Loss: 0.007535 Val Loss: nan\n",
      "Epoch [5598/10000] Train Loss: 0.007439 Val Loss: nan\n",
      "Epoch [5599/10000] Train Loss: 0.007529 Val Loss: nan\n",
      "Epoch [5600/10000] Train Loss: 0.007486 Val Loss: nan\n",
      "Epoch [5601/10000] Train Loss: 0.007339 Val Loss: nan\n",
      "Epoch [5602/10000] Train Loss: 0.007453 Val Loss: nan\n",
      "Epoch [5603/10000] Train Loss: 0.007383 Val Loss: nan\n",
      "Epoch [5604/10000] Train Loss: 0.007303 Val Loss: nan\n",
      "Epoch [5605/10000] Train Loss: 0.007354 Val Loss: nan\n",
      "Epoch [5606/10000] Train Loss: 0.007441 Val Loss: nan\n",
      "Epoch [5607/10000] Train Loss: 0.007280 Val Loss: nan\n",
      "Epoch [5608/10000] Train Loss: 0.007633 Val Loss: nan\n",
      "Epoch [5609/10000] Train Loss: 0.007282 Val Loss: nan\n",
      "Epoch [5610/10000] Train Loss: 0.007329 Val Loss: nan\n",
      "Epoch [5611/10000] Train Loss: 0.007379 Val Loss: nan\n",
      "Epoch [5612/10000] Train Loss: 0.007484 Val Loss: nan\n",
      "Epoch [5613/10000] Train Loss: 0.007270 Val Loss: nan\n",
      "Epoch [5614/10000] Train Loss: 0.007275 Val Loss: nan\n",
      "Epoch [5615/10000] Train Loss: 0.007285 Val Loss: nan\n",
      "Epoch [5616/10000] Train Loss: 0.007358 Val Loss: nan\n",
      "Epoch [5617/10000] Train Loss: 0.007439 Val Loss: nan\n",
      "Epoch [5618/10000] Train Loss: 0.007444 Val Loss: nan\n",
      "Epoch [5619/10000] Train Loss: 0.007298 Val Loss: nan\n",
      "Epoch [5620/10000] Train Loss: 0.007357 Val Loss: nan\n",
      "Epoch [5621/10000] Train Loss: 0.007289 Val Loss: nan\n",
      "Epoch [5622/10000] Train Loss: 0.007340 Val Loss: nan\n",
      "Epoch [5623/10000] Train Loss: 0.007374 Val Loss: nan\n",
      "Epoch [5624/10000] Train Loss: 0.007446 Val Loss: nan\n",
      "Epoch [5625/10000] Train Loss: 0.007444 Val Loss: nan\n",
      "Epoch [5626/10000] Train Loss: 0.007527 Val Loss: nan\n",
      "Epoch [5627/10000] Train Loss: 0.007617 Val Loss: nan\n",
      "Epoch [5628/10000] Train Loss: 0.007385 Val Loss: nan\n",
      "Epoch [5629/10000] Train Loss: 0.007384 Val Loss: nan\n",
      "Epoch [5630/10000] Train Loss: 0.007430 Val Loss: nan\n",
      "Epoch [5631/10000] Train Loss: 0.007409 Val Loss: nan\n",
      "Epoch [5632/10000] Train Loss: 0.007428 Val Loss: nan\n",
      "Epoch [5633/10000] Train Loss: 0.007416 Val Loss: nan\n",
      "Epoch [5634/10000] Train Loss: 0.007258 Val Loss: nan\n",
      "Epoch [5635/10000] Train Loss: 0.007523 Val Loss: nan\n",
      "Epoch [5636/10000] Train Loss: 0.007273 Val Loss: nan\n",
      "Epoch [5637/10000] Train Loss: 0.007377 Val Loss: nan\n",
      "Epoch [5638/10000] Train Loss: 0.007402 Val Loss: nan\n",
      "Epoch [5639/10000] Train Loss: 0.007347 Val Loss: nan\n",
      "Epoch [5640/10000] Train Loss: 0.007306 Val Loss: nan\n",
      "Epoch [5641/10000] Train Loss: 0.007497 Val Loss: nan\n",
      "Epoch [5642/10000] Train Loss: 0.007446 Val Loss: nan\n",
      "Epoch [5643/10000] Train Loss: 0.007387 Val Loss: nan\n",
      "Epoch [5644/10000] Train Loss: 0.007431 Val Loss: nan\n",
      "Epoch [5645/10000] Train Loss: 0.007283 Val Loss: nan\n",
      "Epoch [5646/10000] Train Loss: 0.007666 Val Loss: nan\n",
      "Epoch [5647/10000] Train Loss: 0.007370 Val Loss: nan\n",
      "Epoch [5648/10000] Train Loss: 0.007426 Val Loss: nan\n",
      "Epoch [5649/10000] Train Loss: 0.007451 Val Loss: nan\n",
      "Epoch [5650/10000] Train Loss: 0.007398 Val Loss: nan\n",
      "Epoch [5651/10000] Train Loss: 0.007299 Val Loss: nan\n",
      "Epoch [5652/10000] Train Loss: 0.007467 Val Loss: nan\n",
      "Epoch [5653/10000] Train Loss: 0.007292 Val Loss: nan\n",
      "Epoch [5654/10000] Train Loss: 0.007550 Val Loss: nan\n",
      "Epoch [5655/10000] Train Loss: 0.007409 Val Loss: nan\n",
      "Epoch [5656/10000] Train Loss: 0.007370 Val Loss: nan\n",
      "Epoch [5657/10000] Train Loss: 0.007341 Val Loss: nan\n",
      "Epoch [5658/10000] Train Loss: 0.007359 Val Loss: nan\n",
      "Epoch [5659/10000] Train Loss: 0.007417 Val Loss: nan\n",
      "Epoch [5660/10000] Train Loss: 0.007346 Val Loss: nan\n",
      "Epoch [5661/10000] Train Loss: 0.007426 Val Loss: nan\n",
      "Epoch [5662/10000] Train Loss: 0.007435 Val Loss: nan\n",
      "Epoch [5663/10000] Train Loss: 0.007554 Val Loss: nan\n",
      "Epoch [5664/10000] Train Loss: 0.007557 Val Loss: nan\n",
      "Epoch [5665/10000] Train Loss: 0.007541 Val Loss: nan\n",
      "Epoch [5666/10000] Train Loss: 0.007325 Val Loss: nan\n",
      "Epoch [5667/10000] Train Loss: 0.007290 Val Loss: nan\n",
      "Epoch [5668/10000] Train Loss: 0.007470 Val Loss: nan\n",
      "Epoch [5669/10000] Train Loss: 0.007314 Val Loss: nan\n",
      "Epoch [5670/10000] Train Loss: 0.007464 Val Loss: nan\n",
      "Epoch [5671/10000] Train Loss: 0.007434 Val Loss: nan\n",
      "Epoch [5672/10000] Train Loss: 0.007393 Val Loss: nan\n",
      "Epoch [5673/10000] Train Loss: 0.007384 Val Loss: nan\n",
      "Epoch [5674/10000] Train Loss: 0.007402 Val Loss: nan\n",
      "Epoch [5675/10000] Train Loss: 0.007410 Val Loss: nan\n",
      "Epoch [5676/10000] Train Loss: 0.007440 Val Loss: nan\n",
      "Epoch [5677/10000] Train Loss: 0.007263 Val Loss: nan\n",
      "Epoch [5678/10000] Train Loss: 0.007292 Val Loss: nan\n",
      "Epoch [5679/10000] Train Loss: 0.007395 Val Loss: nan\n",
      "Epoch [5680/10000] Train Loss: 0.007400 Val Loss: nan\n",
      "Epoch [5681/10000] Train Loss: 0.007372 Val Loss: nan\n",
      "Epoch [5682/10000] Train Loss: 0.007406 Val Loss: nan\n",
      "Epoch [5683/10000] Train Loss: 0.007303 Val Loss: nan\n",
      "Epoch [5684/10000] Train Loss: 0.007309 Val Loss: nan\n",
      "Epoch [5685/10000] Train Loss: 0.007430 Val Loss: nan\n",
      "Epoch [5686/10000] Train Loss: 0.007434 Val Loss: nan\n",
      "Epoch [5687/10000] Train Loss: 0.007305 Val Loss: nan\n",
      "Epoch [5688/10000] Train Loss: 0.007509 Val Loss: nan\n",
      "Epoch [5689/10000] Train Loss: 0.007388 Val Loss: nan\n",
      "Epoch [5690/10000] Train Loss: 0.007406 Val Loss: nan\n",
      "Epoch [5691/10000] Train Loss: 0.007349 Val Loss: nan\n",
      "Epoch [5692/10000] Train Loss: 0.007443 Val Loss: nan\n",
      "Epoch [5693/10000] Train Loss: 0.007369 Val Loss: nan\n",
      "Epoch [5694/10000] Train Loss: 0.007405 Val Loss: nan\n",
      "Epoch [5695/10000] Train Loss: 0.007285 Val Loss: nan\n",
      "Epoch [5696/10000] Train Loss: 0.007258 Val Loss: nan\n",
      "Epoch [5697/10000] Train Loss: 0.007398 Val Loss: nan\n",
      "Epoch [5698/10000] Train Loss: 0.007322 Val Loss: nan\n",
      "Epoch [5699/10000] Train Loss: 0.007364 Val Loss: nan\n",
      "Epoch [5700/10000] Train Loss: 0.007267 Val Loss: nan\n",
      "Epoch [5701/10000] Train Loss: 0.007272 Val Loss: nan\n",
      "Epoch [5702/10000] Train Loss: 0.007531 Val Loss: nan\n",
      "Epoch [5703/10000] Train Loss: 0.007350 Val Loss: nan\n",
      "Epoch [5704/10000] Train Loss: 0.007301 Val Loss: nan\n",
      "Epoch [5705/10000] Train Loss: 0.007348 Val Loss: nan\n",
      "Epoch [5706/10000] Train Loss: 0.007414 Val Loss: nan\n",
      "Epoch [5707/10000] Train Loss: 0.007354 Val Loss: nan\n",
      "Epoch [5708/10000] Train Loss: 0.007280 Val Loss: nan\n",
      "Epoch [5709/10000] Train Loss: 0.007262 Val Loss: nan\n",
      "Epoch [5710/10000] Train Loss: 0.007377 Val Loss: nan\n",
      "Epoch [5711/10000] Train Loss: 0.007320 Val Loss: nan\n",
      "Epoch [5712/10000] Train Loss: 0.007452 Val Loss: nan\n",
      "Epoch [5713/10000] Train Loss: 0.007272 Val Loss: nan\n",
      "Epoch [5714/10000] Train Loss: 0.007388 Val Loss: nan\n",
      "Epoch [5715/10000] Train Loss: 0.007393 Val Loss: nan\n",
      "Epoch [5716/10000] Train Loss: 0.007526 Val Loss: nan\n",
      "Epoch [5717/10000] Train Loss: 0.007503 Val Loss: nan\n",
      "Epoch [5718/10000] Train Loss: 0.007378 Val Loss: nan\n",
      "Epoch [5719/10000] Train Loss: 0.007392 Val Loss: nan\n",
      "Epoch [5720/10000] Train Loss: 0.007400 Val Loss: nan\n",
      "Epoch [5721/10000] Train Loss: 0.007330 Val Loss: nan\n",
      "Epoch [5722/10000] Train Loss: 0.007267 Val Loss: nan\n",
      "Epoch [5723/10000] Train Loss: 0.007391 Val Loss: nan\n",
      "Epoch [5724/10000] Train Loss: 0.007392 Val Loss: nan\n",
      "Epoch [5725/10000] Train Loss: 0.007678 Val Loss: nan\n",
      "Epoch [5726/10000] Train Loss: 0.007306 Val Loss: nan\n",
      "Epoch [5727/10000] Train Loss: 0.007275 Val Loss: nan\n",
      "Epoch [5728/10000] Train Loss: 0.007349 Val Loss: nan\n",
      "Epoch [5729/10000] Train Loss: 0.007393 Val Loss: nan\n",
      "Epoch [5730/10000] Train Loss: 0.007301 Val Loss: nan\n",
      "Epoch [5731/10000] Train Loss: 0.007264 Val Loss: nan\n",
      "Epoch [5732/10000] Train Loss: 0.007285 Val Loss: nan\n",
      "Epoch [5733/10000] Train Loss: 0.007247 Val Loss: nan\n",
      "Epoch [5734/10000] Train Loss: 0.007439 Val Loss: nan\n",
      "Epoch [5735/10000] Train Loss: 0.007330 Val Loss: nan\n",
      "Epoch [5736/10000] Train Loss: 0.007434 Val Loss: nan\n",
      "Epoch [5737/10000] Train Loss: 0.007344 Val Loss: nan\n",
      "Epoch [5738/10000] Train Loss: 0.007460 Val Loss: nan\n",
      "Epoch [5739/10000] Train Loss: 0.007436 Val Loss: nan\n",
      "Epoch [5740/10000] Train Loss: 0.007473 Val Loss: nan\n",
      "Epoch [5741/10000] Train Loss: 0.007364 Val Loss: nan\n",
      "Epoch [5742/10000] Train Loss: 0.007354 Val Loss: nan\n",
      "Epoch [5743/10000] Train Loss: 0.007273 Val Loss: nan\n",
      "Epoch [5744/10000] Train Loss: 0.007361 Val Loss: nan\n",
      "Epoch [5745/10000] Train Loss: 0.007299 Val Loss: nan\n",
      "Epoch [5746/10000] Train Loss: 0.007324 Val Loss: nan\n",
      "Epoch [5747/10000] Train Loss: 0.007305 Val Loss: nan\n",
      "Epoch [5748/10000] Train Loss: 0.007331 Val Loss: nan\n",
      "Epoch [5749/10000] Train Loss: 0.007311 Val Loss: nan\n",
      "Epoch [5750/10000] Train Loss: 0.007259 Val Loss: nan\n",
      "Epoch [5751/10000] Train Loss: 0.007505 Val Loss: nan\n",
      "Epoch [5752/10000] Train Loss: 0.007319 Val Loss: nan\n",
      "Epoch [5753/10000] Train Loss: 0.007487 Val Loss: nan\n",
      "Epoch [5754/10000] Train Loss: 0.007291 Val Loss: nan\n",
      "Epoch [5755/10000] Train Loss: 0.007358 Val Loss: nan\n",
      "Epoch [5756/10000] Train Loss: 0.007417 Val Loss: nan\n",
      "Epoch [5757/10000] Train Loss: 0.007255 Val Loss: nan\n",
      "Epoch [5758/10000] Train Loss: 0.007506 Val Loss: nan\n",
      "Epoch [5759/10000] Train Loss: 0.007259 Val Loss: nan\n",
      "Epoch [5760/10000] Train Loss: 0.007398 Val Loss: nan\n",
      "Epoch [5761/10000] Train Loss: 0.007565 Val Loss: nan\n",
      "Epoch [5762/10000] Train Loss: 0.007332 Val Loss: nan\n",
      "Epoch [5763/10000] Train Loss: 0.007301 Val Loss: nan\n",
      "Epoch [5764/10000] Train Loss: 0.007409 Val Loss: nan\n",
      "Epoch [5765/10000] Train Loss: 0.007231 Val Loss: nan\n",
      "Epoch [5766/10000] Train Loss: 0.007407 Val Loss: nan\n",
      "Epoch [5767/10000] Train Loss: 0.007295 Val Loss: nan\n",
      "Epoch [5768/10000] Train Loss: 0.007536 Val Loss: nan\n",
      "Epoch [5769/10000] Train Loss: 0.007322 Val Loss: nan\n",
      "Epoch [5770/10000] Train Loss: 0.007382 Val Loss: nan\n",
      "Epoch [5771/10000] Train Loss: 0.007304 Val Loss: nan\n",
      "Epoch [5772/10000] Train Loss: 0.007646 Val Loss: nan\n",
      "Epoch [5773/10000] Train Loss: 0.007389 Val Loss: nan\n",
      "Epoch [5774/10000] Train Loss: 0.007259 Val Loss: nan\n",
      "Epoch [5775/10000] Train Loss: 0.007234 Val Loss: nan\n",
      "Epoch [5776/10000] Train Loss: 0.007264 Val Loss: nan\n",
      "Epoch [5777/10000] Train Loss: 0.007316 Val Loss: nan\n",
      "Epoch [5778/10000] Train Loss: 0.007384 Val Loss: nan\n",
      "Epoch [5779/10000] Train Loss: 0.007314 Val Loss: nan\n",
      "Epoch [5780/10000] Train Loss: 0.007405 Val Loss: nan\n",
      "Epoch [5781/10000] Train Loss: 0.007238 Val Loss: nan\n",
      "Epoch [5782/10000] Train Loss: 0.007258 Val Loss: nan\n",
      "Epoch [5783/10000] Train Loss: 0.007375 Val Loss: nan\n",
      "Epoch [5784/10000] Train Loss: 0.007262 Val Loss: nan\n",
      "Epoch [5785/10000] Train Loss: 0.007292 Val Loss: nan\n",
      "Epoch [5786/10000] Train Loss: 0.007249 Val Loss: nan\n",
      "Epoch [5787/10000] Train Loss: 0.007434 Val Loss: nan\n",
      "Epoch [5788/10000] Train Loss: 0.007280 Val Loss: nan\n",
      "Epoch [5789/10000] Train Loss: 0.007463 Val Loss: nan\n",
      "Epoch [5790/10000] Train Loss: 0.007232 Val Loss: nan\n",
      "Epoch [5791/10000] Train Loss: 0.007390 Val Loss: nan\n",
      "Epoch [5792/10000] Train Loss: 0.007256 Val Loss: nan\n",
      "Epoch [5793/10000] Train Loss: 0.007379 Val Loss: nan\n",
      "Epoch [5794/10000] Train Loss: 0.007339 Val Loss: nan\n",
      "Epoch [5795/10000] Train Loss: 0.007276 Val Loss: nan\n",
      "Epoch [5796/10000] Train Loss: 0.007381 Val Loss: nan\n",
      "Epoch [5797/10000] Train Loss: 0.007267 Val Loss: nan\n",
      "Epoch [5798/10000] Train Loss: 0.007327 Val Loss: nan\n",
      "Epoch [5799/10000] Train Loss: 0.007240 Val Loss: nan\n",
      "Epoch [5800/10000] Train Loss: 0.007389 Val Loss: nan\n",
      "Epoch [5801/10000] Train Loss: 0.007396 Val Loss: nan\n",
      "Epoch [5802/10000] Train Loss: 0.007415 Val Loss: nan\n",
      "Epoch [5803/10000] Train Loss: 0.007260 Val Loss: nan\n",
      "Epoch [5804/10000] Train Loss: 0.007275 Val Loss: nan\n",
      "Epoch [5805/10000] Train Loss: 0.007371 Val Loss: nan\n",
      "Epoch [5806/10000] Train Loss: 0.007370 Val Loss: nan\n",
      "Epoch [5807/10000] Train Loss: 0.007253 Val Loss: nan\n",
      "Epoch [5808/10000] Train Loss: 0.007260 Val Loss: nan\n",
      "Epoch [5809/10000] Train Loss: 0.007363 Val Loss: nan\n",
      "Epoch [5810/10000] Train Loss: 0.007247 Val Loss: nan\n",
      "Epoch [5811/10000] Train Loss: 0.007327 Val Loss: nan\n",
      "Epoch [5812/10000] Train Loss: 0.007284 Val Loss: nan\n",
      "Epoch [5813/10000] Train Loss: 0.007580 Val Loss: nan\n",
      "Epoch [5814/10000] Train Loss: 0.007390 Val Loss: nan\n",
      "Epoch [5815/10000] Train Loss: 0.007328 Val Loss: nan\n",
      "Epoch [5816/10000] Train Loss: 0.007383 Val Loss: nan\n",
      "Epoch [5817/10000] Train Loss: 0.007516 Val Loss: nan\n",
      "Epoch [5818/10000] Train Loss: 0.007246 Val Loss: nan\n",
      "Epoch [5819/10000] Train Loss: 0.007356 Val Loss: nan\n",
      "Epoch [5820/10000] Train Loss: 0.007341 Val Loss: nan\n",
      "Epoch [5821/10000] Train Loss: 0.007402 Val Loss: nan\n",
      "Epoch [5822/10000] Train Loss: 0.007527 Val Loss: nan\n",
      "Epoch [5823/10000] Train Loss: 0.007262 Val Loss: nan\n",
      "Epoch [5824/10000] Train Loss: 0.007261 Val Loss: nan\n",
      "Epoch [5825/10000] Train Loss: 0.007227 Val Loss: nan\n",
      "Epoch [5826/10000] Train Loss: 0.007424 Val Loss: nan\n",
      "Epoch [5827/10000] Train Loss: 0.007361 Val Loss: nan\n",
      "Epoch [5828/10000] Train Loss: 0.007286 Val Loss: nan\n",
      "Epoch [5829/10000] Train Loss: 0.007230 Val Loss: nan\n",
      "Epoch [5830/10000] Train Loss: 0.007283 Val Loss: nan\n",
      "Epoch [5831/10000] Train Loss: 0.007226 Val Loss: nan\n",
      "Epoch [5832/10000] Train Loss: 0.007266 Val Loss: nan\n",
      "Epoch [5833/10000] Train Loss: 0.007354 Val Loss: nan\n",
      "Epoch [5834/10000] Train Loss: 0.007326 Val Loss: nan\n",
      "Epoch [5835/10000] Train Loss: 0.007343 Val Loss: nan\n",
      "Epoch [5836/10000] Train Loss: 0.007377 Val Loss: nan\n",
      "Epoch [5837/10000] Train Loss: 0.007381 Val Loss: nan\n",
      "Epoch [5838/10000] Train Loss: 0.007268 Val Loss: nan\n",
      "Epoch [5839/10000] Train Loss: 0.007292 Val Loss: nan\n",
      "Epoch [5840/10000] Train Loss: 0.007224 Val Loss: nan\n",
      "Epoch [5841/10000] Train Loss: 0.007337 Val Loss: nan\n",
      "Epoch [5842/10000] Train Loss: 0.007281 Val Loss: nan\n",
      "Epoch [5843/10000] Train Loss: 0.007343 Val Loss: nan\n",
      "Epoch [5844/10000] Train Loss: 0.007254 Val Loss: nan\n",
      "Epoch [5845/10000] Train Loss: 0.007303 Val Loss: nan\n",
      "Epoch [5846/10000] Train Loss: 0.007354 Val Loss: nan\n",
      "Epoch [5847/10000] Train Loss: 0.007380 Val Loss: nan\n",
      "Epoch [5848/10000] Train Loss: 0.007237 Val Loss: nan\n",
      "Epoch [5849/10000] Train Loss: 0.007405 Val Loss: nan\n",
      "Epoch [5850/10000] Train Loss: 0.007214 Val Loss: nan\n",
      "Epoch [5851/10000] Train Loss: 0.007216 Val Loss: nan\n",
      "Epoch [5852/10000] Train Loss: 0.007349 Val Loss: nan\n",
      "Epoch [5853/10000] Train Loss: 0.007358 Val Loss: nan\n",
      "Epoch [5854/10000] Train Loss: 0.007509 Val Loss: nan\n",
      "Epoch [5855/10000] Train Loss: 0.007243 Val Loss: nan\n",
      "Epoch [5856/10000] Train Loss: 0.007361 Val Loss: nan\n",
      "Epoch [5857/10000] Train Loss: 0.007406 Val Loss: nan\n",
      "Epoch [5858/10000] Train Loss: 0.007298 Val Loss: nan\n",
      "Epoch [5859/10000] Train Loss: 0.007287 Val Loss: nan\n",
      "Epoch [5860/10000] Train Loss: 0.007350 Val Loss: nan\n",
      "Epoch [5861/10000] Train Loss: 0.007349 Val Loss: nan\n",
      "Epoch [5862/10000] Train Loss: 0.007296 Val Loss: nan\n",
      "Epoch [5863/10000] Train Loss: 0.007277 Val Loss: nan\n",
      "Epoch [5864/10000] Train Loss: 0.007362 Val Loss: nan\n",
      "Epoch [5865/10000] Train Loss: 0.007327 Val Loss: nan\n",
      "Epoch [5866/10000] Train Loss: 0.007224 Val Loss: nan\n",
      "Epoch [5867/10000] Train Loss: 0.007344 Val Loss: nan\n",
      "Epoch [5868/10000] Train Loss: 0.007390 Val Loss: nan\n",
      "Epoch [5869/10000] Train Loss: 0.007390 Val Loss: nan\n",
      "Epoch [5870/10000] Train Loss: 0.007474 Val Loss: nan\n",
      "Epoch [5871/10000] Train Loss: 0.007230 Val Loss: nan\n",
      "Epoch [5872/10000] Train Loss: 0.007427 Val Loss: nan\n",
      "Epoch [5873/10000] Train Loss: 0.007276 Val Loss: nan\n",
      "Epoch [5874/10000] Train Loss: 0.007494 Val Loss: nan\n",
      "Epoch [5875/10000] Train Loss: 0.007263 Val Loss: nan\n",
      "Epoch [5876/10000] Train Loss: 0.007273 Val Loss: nan\n",
      "Epoch [5877/10000] Train Loss: 0.007230 Val Loss: nan\n",
      "Epoch [5878/10000] Train Loss: 0.007250 Val Loss: nan\n",
      "Epoch [5879/10000] Train Loss: 0.007354 Val Loss: nan\n",
      "Epoch [5880/10000] Train Loss: 0.007541 Val Loss: nan\n",
      "Epoch [5881/10000] Train Loss: 0.007598 Val Loss: nan\n",
      "Epoch [5882/10000] Train Loss: 0.007297 Val Loss: nan\n",
      "Epoch [5883/10000] Train Loss: 0.007607 Val Loss: nan\n",
      "Epoch [5884/10000] Train Loss: 0.007353 Val Loss: nan\n",
      "Epoch [5885/10000] Train Loss: 0.007262 Val Loss: nan\n",
      "Epoch [5886/10000] Train Loss: 0.007459 Val Loss: nan\n",
      "Epoch [5887/10000] Train Loss: 0.007314 Val Loss: nan\n",
      "Epoch [5888/10000] Train Loss: 0.007609 Val Loss: nan\n",
      "Epoch [5889/10000] Train Loss: 0.007480 Val Loss: nan\n",
      "Epoch [5890/10000] Train Loss: 0.007326 Val Loss: nan\n",
      "Epoch [5891/10000] Train Loss: 0.007488 Val Loss: nan\n",
      "Epoch [5892/10000] Train Loss: 0.007443 Val Loss: nan\n",
      "Epoch [5893/10000] Train Loss: 0.007716 Val Loss: nan\n",
      "Epoch [5894/10000] Train Loss: 0.007421 Val Loss: nan\n",
      "Epoch [5895/10000] Train Loss: 0.007382 Val Loss: nan\n",
      "Epoch [5896/10000] Train Loss: 0.007520 Val Loss: nan\n",
      "Epoch [5897/10000] Train Loss: 0.007201 Val Loss: nan\n",
      "Epoch [5898/10000] Train Loss: 0.007358 Val Loss: nan\n",
      "Epoch [5899/10000] Train Loss: 0.007458 Val Loss: nan\n",
      "Epoch [5900/10000] Train Loss: 0.007523 Val Loss: nan\n",
      "Epoch [5901/10000] Train Loss: 0.007305 Val Loss: nan\n",
      "Epoch [5902/10000] Train Loss: 0.007245 Val Loss: nan\n",
      "Epoch [5903/10000] Train Loss: 0.007380 Val Loss: nan\n",
      "Epoch [5904/10000] Train Loss: 0.007233 Val Loss: nan\n",
      "Epoch [5905/10000] Train Loss: 0.007321 Val Loss: nan\n",
      "Epoch [5906/10000] Train Loss: 0.007260 Val Loss: nan\n",
      "Epoch [5907/10000] Train Loss: 0.007264 Val Loss: nan\n",
      "Epoch [5908/10000] Train Loss: 0.007298 Val Loss: nan\n",
      "Epoch [5909/10000] Train Loss: 0.007342 Val Loss: nan\n",
      "Epoch [5910/10000] Train Loss: 0.007538 Val Loss: nan\n",
      "Epoch [5911/10000] Train Loss: 0.007361 Val Loss: nan\n",
      "Epoch [5912/10000] Train Loss: 0.007305 Val Loss: nan\n",
      "Epoch [5913/10000] Train Loss: 0.007355 Val Loss: nan\n",
      "Epoch [5914/10000] Train Loss: 0.007551 Val Loss: nan\n",
      "Epoch [5915/10000] Train Loss: 0.007319 Val Loss: nan\n",
      "Epoch [5916/10000] Train Loss: 0.007340 Val Loss: nan\n",
      "Epoch [5917/10000] Train Loss: 0.007334 Val Loss: nan\n",
      "Epoch [5918/10000] Train Loss: 0.007560 Val Loss: nan\n",
      "Epoch [5919/10000] Train Loss: 0.007448 Val Loss: nan\n",
      "Epoch [5920/10000] Train Loss: 0.007289 Val Loss: nan\n",
      "Epoch [5921/10000] Train Loss: 0.007233 Val Loss: nan\n",
      "Epoch [5922/10000] Train Loss: 0.007402 Val Loss: nan\n",
      "Epoch [5923/10000] Train Loss: 0.007258 Val Loss: nan\n",
      "Epoch [5924/10000] Train Loss: 0.007380 Val Loss: nan\n",
      "Epoch [5925/10000] Train Loss: 0.007497 Val Loss: nan\n",
      "Epoch [5926/10000] Train Loss: 0.007645 Val Loss: nan\n",
      "Epoch [5927/10000] Train Loss: 0.007404 Val Loss: nan\n",
      "Epoch [5928/10000] Train Loss: 0.007651 Val Loss: nan\n",
      "Epoch [5929/10000] Train Loss: 0.007299 Val Loss: nan\n",
      "Epoch [5930/10000] Train Loss: 0.007259 Val Loss: nan\n",
      "Epoch [5931/10000] Train Loss: 0.007219 Val Loss: nan\n",
      "Epoch [5932/10000] Train Loss: 0.007266 Val Loss: nan\n",
      "Epoch [5933/10000] Train Loss: 0.007327 Val Loss: nan\n",
      "Epoch [5934/10000] Train Loss: 0.007272 Val Loss: nan\n",
      "Epoch [5935/10000] Train Loss: 0.007258 Val Loss: nan\n",
      "Epoch [5936/10000] Train Loss: 0.007469 Val Loss: nan\n",
      "Epoch [5937/10000] Train Loss: 0.007364 Val Loss: nan\n",
      "Epoch [5938/10000] Train Loss: 0.007357 Val Loss: nan\n",
      "Epoch [5939/10000] Train Loss: 0.007241 Val Loss: nan\n",
      "Epoch [5940/10000] Train Loss: 0.007366 Val Loss: nan\n",
      "Epoch [5941/10000] Train Loss: 0.007235 Val Loss: nan\n",
      "Epoch [5942/10000] Train Loss: 0.007386 Val Loss: nan\n",
      "Epoch [5943/10000] Train Loss: 0.007330 Val Loss: nan\n",
      "Epoch [5944/10000] Train Loss: 0.007353 Val Loss: nan\n",
      "Epoch [5945/10000] Train Loss: 0.007350 Val Loss: nan\n",
      "Epoch [5946/10000] Train Loss: 0.007335 Val Loss: nan\n",
      "Epoch [5947/10000] Train Loss: 0.007269 Val Loss: nan\n",
      "Epoch [5948/10000] Train Loss: 0.007312 Val Loss: nan\n",
      "Epoch [5949/10000] Train Loss: 0.007226 Val Loss: nan\n",
      "Epoch [5950/10000] Train Loss: 0.007291 Val Loss: nan\n",
      "Epoch [5951/10000] Train Loss: 0.007211 Val Loss: nan\n",
      "Epoch [5952/10000] Train Loss: 0.007219 Val Loss: nan\n",
      "Epoch [5953/10000] Train Loss: 0.007445 Val Loss: nan\n",
      "Epoch [5954/10000] Train Loss: 0.007451 Val Loss: nan\n",
      "Epoch [5955/10000] Train Loss: 0.007499 Val Loss: nan\n",
      "Epoch [5956/10000] Train Loss: 0.007276 Val Loss: nan\n",
      "Epoch [5957/10000] Train Loss: 0.007257 Val Loss: nan\n",
      "Epoch [5958/10000] Train Loss: 0.007486 Val Loss: nan\n",
      "Epoch [5959/10000] Train Loss: 0.007600 Val Loss: nan\n",
      "Epoch [5960/10000] Train Loss: 0.007393 Val Loss: nan\n",
      "Epoch [5961/10000] Train Loss: 0.007301 Val Loss: nan\n",
      "Epoch [5962/10000] Train Loss: 0.007270 Val Loss: nan\n",
      "Epoch [5963/10000] Train Loss: 0.007271 Val Loss: nan\n",
      "Epoch [5964/10000] Train Loss: 0.007384 Val Loss: nan\n",
      "Epoch [5965/10000] Train Loss: 0.007418 Val Loss: nan\n",
      "Epoch [5966/10000] Train Loss: 0.007367 Val Loss: nan\n",
      "Epoch [5967/10000] Train Loss: 0.007253 Val Loss: nan\n",
      "Epoch [5968/10000] Train Loss: 0.007290 Val Loss: nan\n",
      "Epoch [5969/10000] Train Loss: 0.007369 Val Loss: nan\n",
      "Epoch [5970/10000] Train Loss: 0.007399 Val Loss: nan\n",
      "Epoch [5971/10000] Train Loss: 0.007228 Val Loss: nan\n",
      "Epoch [5972/10000] Train Loss: 0.007278 Val Loss: nan\n",
      "Epoch [5973/10000] Train Loss: 0.007343 Val Loss: nan\n",
      "Epoch [5974/10000] Train Loss: 0.007240 Val Loss: nan\n",
      "Epoch [5975/10000] Train Loss: 0.007324 Val Loss: nan\n",
      "Epoch [5976/10000] Train Loss: 0.007236 Val Loss: nan\n",
      "Epoch [5977/10000] Train Loss: 0.007349 Val Loss: nan\n",
      "Epoch [5978/10000] Train Loss: 0.007367 Val Loss: nan\n",
      "Epoch [5979/10000] Train Loss: 0.007323 Val Loss: nan\n",
      "Epoch [5980/10000] Train Loss: 0.007293 Val Loss: nan\n",
      "Epoch [5981/10000] Train Loss: 0.007446 Val Loss: nan\n",
      "Epoch [5982/10000] Train Loss: 0.007283 Val Loss: nan\n",
      "Epoch [5983/10000] Train Loss: 0.007247 Val Loss: nan\n",
      "Epoch [5984/10000] Train Loss: 0.007276 Val Loss: nan\n",
      "Epoch [5985/10000] Train Loss: 0.007384 Val Loss: nan\n",
      "Epoch [5986/10000] Train Loss: 0.007580 Val Loss: nan\n",
      "Epoch [5987/10000] Train Loss: 0.007296 Val Loss: nan\n",
      "Epoch [5988/10000] Train Loss: 0.007239 Val Loss: nan\n",
      "Epoch [5989/10000] Train Loss: 0.007369 Val Loss: nan\n",
      "Epoch [5990/10000] Train Loss: 0.007455 Val Loss: nan\n",
      "Epoch [5991/10000] Train Loss: 0.007326 Val Loss: nan\n",
      "Epoch [5992/10000] Train Loss: 0.007259 Val Loss: nan\n",
      "Epoch [5993/10000] Train Loss: 0.007466 Val Loss: nan\n",
      "Epoch [5994/10000] Train Loss: 0.007340 Val Loss: nan\n",
      "Epoch [5995/10000] Train Loss: 0.007215 Val Loss: nan\n",
      "Epoch [5996/10000] Train Loss: 0.007462 Val Loss: nan\n",
      "Epoch [5997/10000] Train Loss: 0.007239 Val Loss: nan\n",
      "Epoch [5998/10000] Train Loss: 0.007269 Val Loss: nan\n",
      "Epoch [5999/10000] Train Loss: 0.007509 Val Loss: nan\n",
      "Epoch [6000/10000] Train Loss: 0.007303 Val Loss: nan\n",
      "Epoch [6001/10000] Train Loss: 0.007494 Val Loss: nan\n",
      "Epoch [6002/10000] Train Loss: 0.007410 Val Loss: nan\n",
      "Epoch [6003/10000] Train Loss: 0.007221 Val Loss: nan\n",
      "Epoch [6004/10000] Train Loss: 0.007319 Val Loss: nan\n",
      "Epoch [6005/10000] Train Loss: 0.007204 Val Loss: nan\n",
      "Epoch [6006/10000] Train Loss: 0.007377 Val Loss: nan\n",
      "Epoch [6007/10000] Train Loss: 0.007281 Val Loss: nan\n",
      "Epoch [6008/10000] Train Loss: 0.007261 Val Loss: nan\n",
      "Epoch [6009/10000] Train Loss: 0.007348 Val Loss: nan\n",
      "Epoch [6010/10000] Train Loss: 0.007349 Val Loss: nan\n",
      "Epoch [6011/10000] Train Loss: 0.007341 Val Loss: nan\n",
      "Epoch [6012/10000] Train Loss: 0.007265 Val Loss: nan\n",
      "Epoch [6013/10000] Train Loss: 0.007294 Val Loss: nan\n",
      "Epoch [6014/10000] Train Loss: 0.007548 Val Loss: nan\n",
      "Epoch [6015/10000] Train Loss: 0.007276 Val Loss: nan\n",
      "Epoch [6016/10000] Train Loss: 0.007433 Val Loss: nan\n",
      "Epoch [6017/10000] Train Loss: 0.007218 Val Loss: nan\n",
      "Epoch [6018/10000] Train Loss: 0.007418 Val Loss: nan\n",
      "Epoch [6019/10000] Train Loss: 0.007222 Val Loss: nan\n",
      "Epoch [6020/10000] Train Loss: 0.007520 Val Loss: nan\n",
      "Epoch [6021/10000] Train Loss: 0.007330 Val Loss: nan\n",
      "Epoch [6022/10000] Train Loss: 0.007281 Val Loss: nan\n",
      "Epoch [6023/10000] Train Loss: 0.007341 Val Loss: nan\n",
      "Epoch [6024/10000] Train Loss: 0.007321 Val Loss: nan\n",
      "Epoch [6025/10000] Train Loss: 0.007228 Val Loss: nan\n",
      "Epoch [6026/10000] Train Loss: 0.007246 Val Loss: nan\n",
      "Epoch [6027/10000] Train Loss: 0.007235 Val Loss: nan\n",
      "Epoch [6028/10000] Train Loss: 0.007319 Val Loss: nan\n",
      "Epoch [6029/10000] Train Loss: 0.007315 Val Loss: nan\n",
      "Epoch [6030/10000] Train Loss: 0.007446 Val Loss: nan\n",
      "Epoch [6031/10000] Train Loss: 0.007485 Val Loss: nan\n",
      "Epoch [6032/10000] Train Loss: 0.007396 Val Loss: nan\n",
      "Epoch [6033/10000] Train Loss: 0.007216 Val Loss: nan\n",
      "Epoch [6034/10000] Train Loss: 0.007465 Val Loss: nan\n",
      "Epoch [6035/10000] Train Loss: 0.007459 Val Loss: nan\n",
      "Epoch [6036/10000] Train Loss: 0.007235 Val Loss: nan\n",
      "Epoch [6037/10000] Train Loss: 0.007354 Val Loss: nan\n",
      "Epoch [6038/10000] Train Loss: 0.007222 Val Loss: nan\n",
      "Epoch [6039/10000] Train Loss: 0.007265 Val Loss: nan\n",
      "Epoch [6040/10000] Train Loss: 0.007359 Val Loss: nan\n",
      "Epoch [6041/10000] Train Loss: 0.007305 Val Loss: nan\n",
      "Epoch [6042/10000] Train Loss: 0.007276 Val Loss: nan\n",
      "Epoch [6043/10000] Train Loss: 0.007365 Val Loss: nan\n",
      "Epoch [6044/10000] Train Loss: 0.007302 Val Loss: nan\n",
      "Epoch [6045/10000] Train Loss: 0.007226 Val Loss: nan\n",
      "Epoch [6046/10000] Train Loss: 0.007292 Val Loss: nan\n",
      "Epoch [6047/10000] Train Loss: 0.007334 Val Loss: nan\n",
      "Epoch [6048/10000] Train Loss: 0.007409 Val Loss: nan\n",
      "Epoch [6049/10000] Train Loss: 0.007321 Val Loss: nan\n",
      "Epoch [6050/10000] Train Loss: 0.007254 Val Loss: nan\n",
      "Epoch [6051/10000] Train Loss: 0.007259 Val Loss: nan\n",
      "Epoch [6052/10000] Train Loss: 0.007431 Val Loss: nan\n",
      "Epoch [6053/10000] Train Loss: 0.007265 Val Loss: nan\n",
      "Epoch [6054/10000] Train Loss: 0.007341 Val Loss: nan\n",
      "Epoch [6055/10000] Train Loss: 0.007438 Val Loss: nan\n",
      "Epoch [6056/10000] Train Loss: 0.007314 Val Loss: nan\n",
      "Epoch [6057/10000] Train Loss: 0.007422 Val Loss: nan\n",
      "Epoch [6058/10000] Train Loss: 0.007476 Val Loss: nan\n",
      "Epoch [6059/10000] Train Loss: 0.007712 Val Loss: nan\n",
      "Epoch [6060/10000] Train Loss: 0.007295 Val Loss: nan\n",
      "Epoch [6061/10000] Train Loss: 0.007518 Val Loss: nan\n",
      "Epoch [6062/10000] Train Loss: 0.007494 Val Loss: nan\n",
      "Epoch [6063/10000] Train Loss: 0.007371 Val Loss: nan\n",
      "Epoch [6064/10000] Train Loss: 0.007395 Val Loss: nan\n",
      "Epoch [6065/10000] Train Loss: 0.007339 Val Loss: nan\n",
      "Epoch [6066/10000] Train Loss: 0.007338 Val Loss: nan\n",
      "Epoch [6067/10000] Train Loss: 0.007428 Val Loss: nan\n",
      "Epoch [6068/10000] Train Loss: 0.007542 Val Loss: nan\n",
      "Epoch [6069/10000] Train Loss: 0.007445 Val Loss: nan\n",
      "Epoch [6070/10000] Train Loss: 0.007229 Val Loss: nan\n",
      "Epoch [6071/10000] Train Loss: 0.007333 Val Loss: nan\n",
      "Epoch [6072/10000] Train Loss: 0.007471 Val Loss: nan\n",
      "Epoch [6073/10000] Train Loss: 0.007244 Val Loss: nan\n",
      "Epoch [6074/10000] Train Loss: 0.007570 Val Loss: nan\n",
      "Epoch [6075/10000] Train Loss: 0.007204 Val Loss: nan\n",
      "Epoch [6076/10000] Train Loss: 0.007227 Val Loss: nan\n",
      "Epoch [6077/10000] Train Loss: 0.007415 Val Loss: nan\n",
      "Epoch [6078/10000] Train Loss: 0.007288 Val Loss: nan\n",
      "Epoch [6079/10000] Train Loss: 0.007394 Val Loss: nan\n",
      "Epoch [6080/10000] Train Loss: 0.007278 Val Loss: nan\n",
      "Epoch [6081/10000] Train Loss: 0.007520 Val Loss: nan\n",
      "Epoch [6082/10000] Train Loss: 0.007417 Val Loss: nan\n",
      "Epoch [6083/10000] Train Loss: 0.007270 Val Loss: nan\n",
      "Epoch [6084/10000] Train Loss: 0.007341 Val Loss: nan\n",
      "Epoch [6085/10000] Train Loss: 0.007247 Val Loss: nan\n",
      "Epoch [6086/10000] Train Loss: 0.007379 Val Loss: nan\n",
      "Epoch [6087/10000] Train Loss: 0.007200 Val Loss: nan\n",
      "Epoch [6088/10000] Train Loss: 0.007326 Val Loss: nan\n",
      "Epoch [6089/10000] Train Loss: 0.007438 Val Loss: nan\n",
      "Epoch [6090/10000] Train Loss: 0.007453 Val Loss: nan\n",
      "Epoch [6091/10000] Train Loss: 0.007247 Val Loss: nan\n",
      "Epoch [6092/10000] Train Loss: 0.007275 Val Loss: nan\n",
      "Epoch [6093/10000] Train Loss: 0.007289 Val Loss: nan\n",
      "Epoch [6094/10000] Train Loss: 0.007659 Val Loss: nan\n",
      "Epoch [6095/10000] Train Loss: 0.007236 Val Loss: nan\n",
      "Epoch [6096/10000] Train Loss: 0.007261 Val Loss: nan\n",
      "Epoch [6097/10000] Train Loss: 0.007232 Val Loss: nan\n",
      "Epoch [6098/10000] Train Loss: 0.007265 Val Loss: nan\n",
      "Epoch [6099/10000] Train Loss: 0.007194 Val Loss: nan\n",
      "Epoch [6100/10000] Train Loss: 0.007551 Val Loss: nan\n",
      "Epoch [6101/10000] Train Loss: 0.007597 Val Loss: nan\n",
      "Epoch [6102/10000] Train Loss: 0.007724 Val Loss: nan\n",
      "Epoch [6103/10000] Train Loss: 0.007389 Val Loss: nan\n",
      "Epoch [6104/10000] Train Loss: 0.007329 Val Loss: nan\n",
      "Epoch [6105/10000] Train Loss: 0.007401 Val Loss: nan\n",
      "Epoch [6106/10000] Train Loss: 0.007275 Val Loss: nan\n",
      "Epoch [6107/10000] Train Loss: 0.007382 Val Loss: nan\n",
      "Epoch [6108/10000] Train Loss: 0.007328 Val Loss: nan\n",
      "Epoch [6109/10000] Train Loss: 0.007361 Val Loss: nan\n",
      "Epoch [6110/10000] Train Loss: 0.007271 Val Loss: nan\n",
      "Epoch [6111/10000] Train Loss: 0.007334 Val Loss: nan\n",
      "Epoch [6112/10000] Train Loss: 0.007225 Val Loss: nan\n",
      "Epoch [6113/10000] Train Loss: 0.007206 Val Loss: nan\n",
      "Epoch [6114/10000] Train Loss: 0.007283 Val Loss: nan\n",
      "Epoch [6115/10000] Train Loss: 0.007197 Val Loss: nan\n",
      "Epoch [6116/10000] Train Loss: 0.007364 Val Loss: nan\n",
      "Epoch [6117/10000] Train Loss: 0.007508 Val Loss: nan\n",
      "Epoch [6118/10000] Train Loss: 0.007270 Val Loss: nan\n",
      "Epoch [6119/10000] Train Loss: 0.007355 Val Loss: nan\n",
      "Epoch [6120/10000] Train Loss: 0.007389 Val Loss: nan\n",
      "Epoch [6121/10000] Train Loss: 0.007204 Val Loss: nan\n",
      "Epoch [6122/10000] Train Loss: 0.007213 Val Loss: nan\n",
      "Epoch [6123/10000] Train Loss: 0.007238 Val Loss: nan\n",
      "Epoch [6124/10000] Train Loss: 0.007388 Val Loss: nan\n",
      "Epoch [6125/10000] Train Loss: 0.007340 Val Loss: nan\n",
      "Epoch [6126/10000] Train Loss: 0.007349 Val Loss: nan\n",
      "Epoch [6127/10000] Train Loss: 0.007276 Val Loss: nan\n",
      "Epoch [6128/10000] Train Loss: 0.007230 Val Loss: nan\n",
      "Epoch [6129/10000] Train Loss: 0.007214 Val Loss: nan\n",
      "Epoch [6130/10000] Train Loss: 0.007158 Val Loss: nan\n",
      "Epoch [6131/10000] Train Loss: 0.007249 Val Loss: nan\n",
      "Epoch [6132/10000] Train Loss: 0.007372 Val Loss: nan\n",
      "Epoch [6133/10000] Train Loss: 0.007401 Val Loss: nan\n",
      "Epoch [6134/10000] Train Loss: 0.007296 Val Loss: nan\n",
      "Epoch [6135/10000] Train Loss: 0.007191 Val Loss: nan\n",
      "Epoch [6136/10000] Train Loss: 0.007181 Val Loss: nan\n",
      "Epoch [6137/10000] Train Loss: 0.007288 Val Loss: nan\n",
      "Epoch [6138/10000] Train Loss: 0.007355 Val Loss: nan\n",
      "Epoch [6139/10000] Train Loss: 0.007344 Val Loss: nan\n",
      "Epoch [6140/10000] Train Loss: 0.007260 Val Loss: nan\n",
      "Epoch [6141/10000] Train Loss: 0.007216 Val Loss: nan\n",
      "Epoch [6142/10000] Train Loss: 0.007427 Val Loss: nan\n",
      "Epoch [6143/10000] Train Loss: 0.007213 Val Loss: nan\n",
      "Epoch [6144/10000] Train Loss: 0.007385 Val Loss: nan\n",
      "Epoch [6145/10000] Train Loss: 0.007516 Val Loss: nan\n",
      "Epoch [6146/10000] Train Loss: 0.007273 Val Loss: nan\n",
      "Epoch [6147/10000] Train Loss: 0.007304 Val Loss: nan\n",
      "Epoch [6148/10000] Train Loss: 0.007189 Val Loss: nan\n",
      "Epoch [6149/10000] Train Loss: 0.007197 Val Loss: nan\n",
      "Epoch [6150/10000] Train Loss: 0.007304 Val Loss: nan\n",
      "Epoch [6151/10000] Train Loss: 0.007432 Val Loss: nan\n",
      "Epoch [6152/10000] Train Loss: 0.007218 Val Loss: nan\n",
      "Epoch [6153/10000] Train Loss: 0.007269 Val Loss: nan\n",
      "Epoch [6154/10000] Train Loss: 0.007319 Val Loss: nan\n",
      "Epoch [6155/10000] Train Loss: 0.007450 Val Loss: nan\n",
      "Epoch [6156/10000] Train Loss: 0.007448 Val Loss: nan\n",
      "Epoch [6157/10000] Train Loss: 0.007238 Val Loss: nan\n",
      "Epoch [6158/10000] Train Loss: 0.007222 Val Loss: nan\n",
      "Epoch [6159/10000] Train Loss: 0.007256 Val Loss: nan\n",
      "Epoch [6160/10000] Train Loss: 0.007371 Val Loss: nan\n",
      "Epoch [6161/10000] Train Loss: 0.007354 Val Loss: nan\n",
      "Epoch [6162/10000] Train Loss: 0.007207 Val Loss: nan\n",
      "Epoch [6163/10000] Train Loss: 0.007258 Val Loss: nan\n",
      "Epoch [6164/10000] Train Loss: 0.007446 Val Loss: nan\n",
      "Epoch [6165/10000] Train Loss: 0.007414 Val Loss: nan\n",
      "Epoch [6166/10000] Train Loss: 0.007212 Val Loss: nan\n",
      "Epoch [6167/10000] Train Loss: 0.007326 Val Loss: nan\n",
      "Epoch [6168/10000] Train Loss: 0.007266 Val Loss: nan\n",
      "Epoch [6169/10000] Train Loss: 0.007204 Val Loss: nan\n",
      "Epoch [6170/10000] Train Loss: 0.007193 Val Loss: nan\n",
      "Epoch [6171/10000] Train Loss: 0.007437 Val Loss: nan\n",
      "Epoch [6172/10000] Train Loss: 0.007311 Val Loss: nan\n",
      "Epoch [6173/10000] Train Loss: 0.007183 Val Loss: nan\n",
      "Epoch [6174/10000] Train Loss: 0.007282 Val Loss: nan\n",
      "Epoch [6175/10000] Train Loss: 0.007193 Val Loss: nan\n",
      "Epoch [6176/10000] Train Loss: 0.007310 Val Loss: nan\n",
      "Epoch [6177/10000] Train Loss: 0.007283 Val Loss: nan\n",
      "Epoch [6178/10000] Train Loss: 0.007170 Val Loss: nan\n",
      "Epoch [6179/10000] Train Loss: 0.007168 Val Loss: nan\n",
      "Epoch [6180/10000] Train Loss: 0.007380 Val Loss: nan\n",
      "Epoch [6181/10000] Train Loss: 0.007217 Val Loss: nan\n",
      "Epoch [6182/10000] Train Loss: 0.007451 Val Loss: nan\n",
      "Epoch [6183/10000] Train Loss: 0.007202 Val Loss: nan\n",
      "Epoch [6184/10000] Train Loss: 0.007421 Val Loss: nan\n",
      "Epoch [6185/10000] Train Loss: 0.007213 Val Loss: nan\n",
      "Epoch [6186/10000] Train Loss: 0.007420 Val Loss: nan\n",
      "Epoch [6187/10000] Train Loss: 0.007188 Val Loss: nan\n",
      "Epoch [6188/10000] Train Loss: 0.007152 Val Loss: nan\n",
      "Epoch [6189/10000] Train Loss: 0.007376 Val Loss: nan\n",
      "Epoch [6190/10000] Train Loss: 0.007311 Val Loss: nan\n",
      "Epoch [6191/10000] Train Loss: 0.007172 Val Loss: nan\n",
      "Epoch [6192/10000] Train Loss: 0.007174 Val Loss: nan\n",
      "Epoch [6193/10000] Train Loss: 0.007288 Val Loss: nan\n",
      "Epoch [6194/10000] Train Loss: 0.007391 Val Loss: nan\n",
      "Epoch [6195/10000] Train Loss: 0.007230 Val Loss: nan\n",
      "Epoch [6196/10000] Train Loss: 0.007386 Val Loss: nan\n",
      "Epoch [6197/10000] Train Loss: 0.007459 Val Loss: nan\n",
      "Epoch [6198/10000] Train Loss: 0.007263 Val Loss: nan\n",
      "Epoch [6199/10000] Train Loss: 0.007541 Val Loss: nan\n",
      "Epoch [6200/10000] Train Loss: 0.007444 Val Loss: nan\n",
      "Epoch [6201/10000] Train Loss: 0.007326 Val Loss: nan\n",
      "Epoch [6202/10000] Train Loss: 0.007426 Val Loss: nan\n",
      "Epoch [6203/10000] Train Loss: 0.007362 Val Loss: nan\n",
      "Epoch [6204/10000] Train Loss: 0.007235 Val Loss: nan\n",
      "Epoch [6205/10000] Train Loss: 0.007478 Val Loss: nan\n",
      "Epoch [6206/10000] Train Loss: 0.007437 Val Loss: nan\n",
      "Epoch [6207/10000] Train Loss: 0.007221 Val Loss: nan\n",
      "Epoch [6208/10000] Train Loss: 0.007243 Val Loss: nan\n",
      "Epoch [6209/10000] Train Loss: 0.007190 Val Loss: nan\n",
      "Epoch [6210/10000] Train Loss: 0.007277 Val Loss: nan\n",
      "Epoch [6211/10000] Train Loss: 0.007392 Val Loss: nan\n",
      "Epoch [6212/10000] Train Loss: 0.007181 Val Loss: nan\n",
      "Epoch [6213/10000] Train Loss: 0.007269 Val Loss: nan\n",
      "Epoch [6214/10000] Train Loss: 0.007406 Val Loss: nan\n",
      "Epoch [6215/10000] Train Loss: 0.007282 Val Loss: nan\n",
      "Epoch [6216/10000] Train Loss: 0.007485 Val Loss: nan\n",
      "Epoch [6217/10000] Train Loss: 0.007453 Val Loss: nan\n",
      "Epoch [6218/10000] Train Loss: 0.007454 Val Loss: nan\n",
      "Epoch [6219/10000] Train Loss: 0.007326 Val Loss: nan\n",
      "Epoch [6220/10000] Train Loss: 0.007181 Val Loss: nan\n",
      "Epoch [6221/10000] Train Loss: 0.007176 Val Loss: nan\n",
      "Epoch [6222/10000] Train Loss: 0.007438 Val Loss: nan\n",
      "Epoch [6223/10000] Train Loss: 0.007323 Val Loss: nan\n",
      "Epoch [6224/10000] Train Loss: 0.007272 Val Loss: nan\n",
      "Epoch [6225/10000] Train Loss: 0.007296 Val Loss: nan\n",
      "Epoch [6226/10000] Train Loss: 0.007220 Val Loss: nan\n",
      "Epoch [6227/10000] Train Loss: 0.007214 Val Loss: nan\n",
      "Epoch [6228/10000] Train Loss: 0.007383 Val Loss: nan\n",
      "Epoch [6229/10000] Train Loss: 0.007502 Val Loss: nan\n",
      "Epoch [6230/10000] Train Loss: 0.007289 Val Loss: nan\n",
      "Epoch [6231/10000] Train Loss: 0.007337 Val Loss: nan\n",
      "Epoch [6232/10000] Train Loss: 0.007415 Val Loss: nan\n",
      "Epoch [6233/10000] Train Loss: 0.007301 Val Loss: nan\n",
      "Epoch [6234/10000] Train Loss: 0.007471 Val Loss: nan\n",
      "Epoch [6235/10000] Train Loss: 0.007180 Val Loss: nan\n",
      "Epoch [6236/10000] Train Loss: 0.007184 Val Loss: nan\n",
      "Epoch [6237/10000] Train Loss: 0.007183 Val Loss: nan\n",
      "Epoch [6238/10000] Train Loss: 0.007329 Val Loss: nan\n",
      "Epoch [6239/10000] Train Loss: 0.007246 Val Loss: nan\n",
      "Epoch [6240/10000] Train Loss: 0.007336 Val Loss: nan\n",
      "Epoch [6241/10000] Train Loss: 0.007360 Val Loss: nan\n",
      "Epoch [6242/10000] Train Loss: 0.007205 Val Loss: nan\n",
      "Epoch [6243/10000] Train Loss: 0.007360 Val Loss: nan\n",
      "Epoch [6244/10000] Train Loss: 0.007359 Val Loss: nan\n",
      "Epoch [6245/10000] Train Loss: 0.007199 Val Loss: nan\n",
      "Epoch [6246/10000] Train Loss: 0.007321 Val Loss: nan\n",
      "Epoch [6247/10000] Train Loss: 0.007370 Val Loss: nan\n",
      "Epoch [6248/10000] Train Loss: 0.007278 Val Loss: nan\n",
      "Epoch [6249/10000] Train Loss: 0.007262 Val Loss: nan\n",
      "Epoch [6250/10000] Train Loss: 0.007464 Val Loss: nan\n",
      "Epoch [6251/10000] Train Loss: 0.007269 Val Loss: nan\n",
      "Epoch [6252/10000] Train Loss: 0.007231 Val Loss: nan\n",
      "Epoch [6253/10000] Train Loss: 0.007262 Val Loss: nan\n",
      "Epoch [6254/10000] Train Loss: 0.007408 Val Loss: nan\n",
      "Epoch [6255/10000] Train Loss: 0.007225 Val Loss: nan\n",
      "Epoch [6256/10000] Train Loss: 0.007219 Val Loss: nan\n",
      "Epoch [6257/10000] Train Loss: 0.007251 Val Loss: nan\n",
      "Epoch [6258/10000] Train Loss: 0.007272 Val Loss: nan\n",
      "Epoch [6259/10000] Train Loss: 0.007171 Val Loss: nan\n",
      "Epoch [6260/10000] Train Loss: 0.007248 Val Loss: nan\n",
      "Epoch [6261/10000] Train Loss: 0.007271 Val Loss: nan\n",
      "Epoch [6262/10000] Train Loss: 0.007395 Val Loss: nan\n",
      "Epoch [6263/10000] Train Loss: 0.007288 Val Loss: nan\n",
      "Epoch [6264/10000] Train Loss: 0.007407 Val Loss: nan\n",
      "Epoch [6265/10000] Train Loss: 0.007303 Val Loss: nan\n",
      "Epoch [6266/10000] Train Loss: 0.007567 Val Loss: nan\n",
      "Epoch [6267/10000] Train Loss: 0.007165 Val Loss: nan\n",
      "Epoch [6268/10000] Train Loss: 0.007199 Val Loss: nan\n",
      "Epoch [6269/10000] Train Loss: 0.007261 Val Loss: nan\n",
      "Epoch [6270/10000] Train Loss: 0.007576 Val Loss: nan\n",
      "Epoch [6271/10000] Train Loss: 0.007239 Val Loss: nan\n",
      "Epoch [6272/10000] Train Loss: 0.007326 Val Loss: nan\n",
      "Epoch [6273/10000] Train Loss: 0.007246 Val Loss: nan\n",
      "Epoch [6274/10000] Train Loss: 0.007190 Val Loss: nan\n",
      "Epoch [6275/10000] Train Loss: 0.007202 Val Loss: nan\n",
      "Epoch [6276/10000] Train Loss: 0.007216 Val Loss: nan\n",
      "Epoch [6277/10000] Train Loss: 0.007400 Val Loss: nan\n",
      "Epoch [6278/10000] Train Loss: 0.007261 Val Loss: nan\n",
      "Epoch [6279/10000] Train Loss: 0.007422 Val Loss: nan\n",
      "Epoch [6280/10000] Train Loss: 0.007262 Val Loss: nan\n",
      "Epoch [6281/10000] Train Loss: 0.007521 Val Loss: nan\n",
      "Epoch [6282/10000] Train Loss: 0.007458 Val Loss: nan\n",
      "Epoch [6283/10000] Train Loss: 0.007350 Val Loss: nan\n",
      "Epoch [6284/10000] Train Loss: 0.007181 Val Loss: nan\n",
      "Epoch [6285/10000] Train Loss: 0.007262 Val Loss: nan\n",
      "Epoch [6286/10000] Train Loss: 0.007378 Val Loss: nan\n",
      "Epoch [6287/10000] Train Loss: 0.007230 Val Loss: nan\n",
      "Epoch [6288/10000] Train Loss: 0.007236 Val Loss: nan\n",
      "Epoch [6289/10000] Train Loss: 0.007267 Val Loss: nan\n",
      "Epoch [6290/10000] Train Loss: 0.007396 Val Loss: nan\n",
      "Epoch [6291/10000] Train Loss: 0.007434 Val Loss: nan\n",
      "Epoch [6292/10000] Train Loss: 0.007494 Val Loss: nan\n",
      "Epoch [6293/10000] Train Loss: 0.007192 Val Loss: nan\n",
      "Epoch [6294/10000] Train Loss: 0.007211 Val Loss: nan\n",
      "Epoch [6295/10000] Train Loss: 0.007181 Val Loss: nan\n",
      "Epoch [6296/10000] Train Loss: 0.007286 Val Loss: nan\n",
      "Epoch [6297/10000] Train Loss: 0.007320 Val Loss: nan\n",
      "Epoch [6298/10000] Train Loss: 0.007276 Val Loss: nan\n",
      "Epoch [6299/10000] Train Loss: 0.007166 Val Loss: nan\n",
      "Epoch [6300/10000] Train Loss: 0.007193 Val Loss: nan\n",
      "Epoch [6301/10000] Train Loss: 0.007180 Val Loss: nan\n",
      "Epoch [6302/10000] Train Loss: 0.007202 Val Loss: nan\n",
      "Epoch [6303/10000] Train Loss: 0.007314 Val Loss: nan\n",
      "Epoch [6304/10000] Train Loss: 0.007328 Val Loss: nan\n",
      "Epoch [6305/10000] Train Loss: 0.007410 Val Loss: nan\n",
      "Epoch [6306/10000] Train Loss: 0.007274 Val Loss: nan\n",
      "Epoch [6307/10000] Train Loss: 0.007196 Val Loss: nan\n",
      "Epoch [6308/10000] Train Loss: 0.007181 Val Loss: nan\n",
      "Epoch [6309/10000] Train Loss: 0.007208 Val Loss: nan\n",
      "Epoch [6310/10000] Train Loss: 0.007157 Val Loss: nan\n",
      "Epoch [6311/10000] Train Loss: 0.007259 Val Loss: nan\n",
      "Epoch [6312/10000] Train Loss: 0.007170 Val Loss: nan\n",
      "Epoch [6313/10000] Train Loss: 0.007285 Val Loss: nan\n",
      "Epoch [6314/10000] Train Loss: 0.007392 Val Loss: nan\n",
      "Epoch [6315/10000] Train Loss: 0.007356 Val Loss: nan\n",
      "Epoch [6316/10000] Train Loss: 0.007352 Val Loss: nan\n",
      "Epoch [6317/10000] Train Loss: 0.007356 Val Loss: nan\n",
      "Epoch [6318/10000] Train Loss: 0.007176 Val Loss: nan\n",
      "Epoch [6319/10000] Train Loss: 0.007371 Val Loss: nan\n",
      "Epoch [6320/10000] Train Loss: 0.007326 Val Loss: nan\n",
      "Epoch [6321/10000] Train Loss: 0.007390 Val Loss: nan\n",
      "Epoch [6322/10000] Train Loss: 0.007286 Val Loss: nan\n",
      "Epoch [6323/10000] Train Loss: 0.007200 Val Loss: nan\n",
      "Epoch [6324/10000] Train Loss: 0.007392 Val Loss: nan\n",
      "Epoch [6325/10000] Train Loss: 0.007284 Val Loss: nan\n",
      "Epoch [6326/10000] Train Loss: 0.007325 Val Loss: nan\n",
      "Epoch [6327/10000] Train Loss: 0.007277 Val Loss: nan\n",
      "Epoch [6328/10000] Train Loss: 0.007321 Val Loss: nan\n",
      "Epoch [6329/10000] Train Loss: 0.007288 Val Loss: nan\n",
      "Epoch [6330/10000] Train Loss: 0.007409 Val Loss: nan\n",
      "Epoch [6331/10000] Train Loss: 0.007289 Val Loss: nan\n",
      "Epoch [6332/10000] Train Loss: 0.007308 Val Loss: nan\n",
      "Epoch [6333/10000] Train Loss: 0.007307 Val Loss: nan\n",
      "Epoch [6334/10000] Train Loss: 0.007172 Val Loss: nan\n",
      "Epoch [6335/10000] Train Loss: 0.007624 Val Loss: nan\n",
      "Epoch [6336/10000] Train Loss: 0.007288 Val Loss: nan\n",
      "Epoch [6337/10000] Train Loss: 0.007339 Val Loss: nan\n",
      "Epoch [6338/10000] Train Loss: 0.007267 Val Loss: nan\n",
      "Epoch [6339/10000] Train Loss: 0.007168 Val Loss: nan\n",
      "Epoch [6340/10000] Train Loss: 0.007149 Val Loss: nan\n",
      "Epoch [6341/10000] Train Loss: 0.007142 Val Loss: nan\n",
      "Epoch [6342/10000] Train Loss: 0.007280 Val Loss: nan\n",
      "Epoch [6343/10000] Train Loss: 0.007332 Val Loss: nan\n",
      "Epoch [6344/10000] Train Loss: 0.007413 Val Loss: nan\n",
      "Epoch [6345/10000] Train Loss: 0.007294 Val Loss: nan\n",
      "Epoch [6346/10000] Train Loss: 0.007369 Val Loss: nan\n",
      "Epoch [6347/10000] Train Loss: 0.007200 Val Loss: nan\n",
      "Epoch [6348/10000] Train Loss: 0.007221 Val Loss: nan\n",
      "Epoch [6349/10000] Train Loss: 0.007168 Val Loss: nan\n",
      "Epoch [6350/10000] Train Loss: 0.007243 Val Loss: nan\n",
      "Epoch [6351/10000] Train Loss: 0.007341 Val Loss: nan\n",
      "Epoch [6352/10000] Train Loss: 0.007310 Val Loss: nan\n",
      "Epoch [6353/10000] Train Loss: 0.007173 Val Loss: nan\n",
      "Epoch [6354/10000] Train Loss: 0.007189 Val Loss: nan\n",
      "Epoch [6355/10000] Train Loss: 0.007267 Val Loss: nan\n",
      "Epoch [6356/10000] Train Loss: 0.007273 Val Loss: nan\n",
      "Epoch [6357/10000] Train Loss: 0.007373 Val Loss: nan\n",
      "Epoch [6358/10000] Train Loss: 0.007261 Val Loss: nan\n",
      "Epoch [6359/10000] Train Loss: 0.007267 Val Loss: nan\n",
      "Epoch [6360/10000] Train Loss: 0.007202 Val Loss: nan\n",
      "Epoch [6361/10000] Train Loss: 0.007265 Val Loss: nan\n",
      "Epoch [6362/10000] Train Loss: 0.007230 Val Loss: nan\n",
      "Epoch [6363/10000] Train Loss: 0.007262 Val Loss: nan\n",
      "Epoch [6364/10000] Train Loss: 0.007281 Val Loss: nan\n",
      "Epoch [6365/10000] Train Loss: 0.007183 Val Loss: nan\n",
      "Epoch [6366/10000] Train Loss: 0.007287 Val Loss: nan\n",
      "Epoch [6367/10000] Train Loss: 0.007227 Val Loss: nan\n",
      "Epoch [6368/10000] Train Loss: 0.007224 Val Loss: nan\n",
      "Epoch [6369/10000] Train Loss: 0.007188 Val Loss: nan\n",
      "Epoch [6370/10000] Train Loss: 0.007176 Val Loss: nan\n",
      "Epoch [6371/10000] Train Loss: 0.007199 Val Loss: nan\n",
      "Epoch [6372/10000] Train Loss: 0.007280 Val Loss: nan\n",
      "Epoch [6373/10000] Train Loss: 0.007445 Val Loss: nan\n",
      "Epoch [6374/10000] Train Loss: 0.007423 Val Loss: nan\n",
      "Epoch [6375/10000] Train Loss: 0.007348 Val Loss: nan\n",
      "Epoch [6376/10000] Train Loss: 0.007272 Val Loss: nan\n",
      "Epoch [6377/10000] Train Loss: 0.007202 Val Loss: nan\n",
      "Epoch [6378/10000] Train Loss: 0.007146 Val Loss: nan\n",
      "Epoch [6379/10000] Train Loss: 0.007220 Val Loss: nan\n",
      "Epoch [6380/10000] Train Loss: 0.007187 Val Loss: nan\n",
      "Epoch [6381/10000] Train Loss: 0.007343 Val Loss: nan\n",
      "Epoch [6382/10000] Train Loss: 0.007246 Val Loss: nan\n",
      "Epoch [6383/10000] Train Loss: 0.007203 Val Loss: nan\n",
      "Epoch [6384/10000] Train Loss: 0.007505 Val Loss: nan\n",
      "Epoch [6385/10000] Train Loss: 0.007523 Val Loss: nan\n",
      "Epoch [6386/10000] Train Loss: 0.007342 Val Loss: nan\n",
      "Epoch [6387/10000] Train Loss: 0.007165 Val Loss: nan\n",
      "Epoch [6388/10000] Train Loss: 0.007347 Val Loss: nan\n",
      "Epoch [6389/10000] Train Loss: 0.007306 Val Loss: nan\n",
      "Epoch [6390/10000] Train Loss: 0.007313 Val Loss: nan\n",
      "Epoch [6391/10000] Train Loss: 0.007149 Val Loss: nan\n",
      "Epoch [6392/10000] Train Loss: 0.007224 Val Loss: nan\n",
      "Epoch [6393/10000] Train Loss: 0.007158 Val Loss: nan\n",
      "Epoch [6394/10000] Train Loss: 0.007203 Val Loss: nan\n",
      "Epoch [6395/10000] Train Loss: 0.007194 Val Loss: nan\n",
      "Epoch [6396/10000] Train Loss: 0.007353 Val Loss: nan\n",
      "Epoch [6397/10000] Train Loss: 0.007264 Val Loss: nan\n",
      "Epoch [6398/10000] Train Loss: 0.007235 Val Loss: nan\n",
      "Epoch [6399/10000] Train Loss: 0.007226 Val Loss: nan\n",
      "Epoch [6400/10000] Train Loss: 0.007440 Val Loss: nan\n",
      "Epoch [6401/10000] Train Loss: 0.007303 Val Loss: nan\n",
      "Epoch [6402/10000] Train Loss: 0.007305 Val Loss: nan\n",
      "Epoch [6403/10000] Train Loss: 0.007510 Val Loss: nan\n",
      "Epoch [6404/10000] Train Loss: 0.007347 Val Loss: nan\n",
      "Epoch [6405/10000] Train Loss: 0.007393 Val Loss: nan\n",
      "Epoch [6406/10000] Train Loss: 0.007408 Val Loss: nan\n",
      "Epoch [6407/10000] Train Loss: 0.007284 Val Loss: nan\n",
      "Epoch [6408/10000] Train Loss: 0.007327 Val Loss: nan\n",
      "Epoch [6409/10000] Train Loss: 0.007486 Val Loss: nan\n",
      "Epoch [6410/10000] Train Loss: 0.007276 Val Loss: nan\n",
      "Epoch [6411/10000] Train Loss: 0.007173 Val Loss: nan\n",
      "Epoch [6412/10000] Train Loss: 0.007218 Val Loss: nan\n",
      "Epoch [6413/10000] Train Loss: 0.007251 Val Loss: nan\n",
      "Epoch [6414/10000] Train Loss: 0.007277 Val Loss: nan\n",
      "Epoch [6415/10000] Train Loss: 0.007243 Val Loss: nan\n",
      "Epoch [6416/10000] Train Loss: 0.007154 Val Loss: nan\n",
      "Epoch [6417/10000] Train Loss: 0.007241 Val Loss: nan\n",
      "Epoch [6418/10000] Train Loss: 0.007244 Val Loss: nan\n",
      "Epoch [6419/10000] Train Loss: 0.007259 Val Loss: nan\n",
      "Epoch [6420/10000] Train Loss: 0.007450 Val Loss: nan\n",
      "Epoch [6421/10000] Train Loss: 0.007346 Val Loss: nan\n",
      "Epoch [6422/10000] Train Loss: 0.007252 Val Loss: nan\n",
      "Epoch [6423/10000] Train Loss: 0.007145 Val Loss: nan\n",
      "Epoch [6424/10000] Train Loss: 0.007207 Val Loss: nan\n",
      "Epoch [6425/10000] Train Loss: 0.007292 Val Loss: nan\n",
      "Epoch [6426/10000] Train Loss: 0.007446 Val Loss: nan\n",
      "Epoch [6427/10000] Train Loss: 0.007286 Val Loss: nan\n",
      "Epoch [6428/10000] Train Loss: 0.007312 Val Loss: nan\n",
      "Epoch [6429/10000] Train Loss: 0.007234 Val Loss: nan\n",
      "Epoch [6430/10000] Train Loss: 0.007400 Val Loss: nan\n",
      "Epoch [6431/10000] Train Loss: 0.007249 Val Loss: nan\n",
      "Epoch [6432/10000] Train Loss: 0.007290 Val Loss: nan\n",
      "Epoch [6433/10000] Train Loss: 0.007186 Val Loss: nan\n",
      "Epoch [6434/10000] Train Loss: 0.007188 Val Loss: nan\n",
      "Epoch [6435/10000] Train Loss: 0.007207 Val Loss: nan\n",
      "Epoch [6436/10000] Train Loss: 0.007419 Val Loss: nan\n",
      "Epoch [6437/10000] Train Loss: 0.007259 Val Loss: nan\n",
      "Epoch [6438/10000] Train Loss: 0.007200 Val Loss: nan\n",
      "Epoch [6439/10000] Train Loss: 0.007263 Val Loss: nan\n",
      "Epoch [6440/10000] Train Loss: 0.007342 Val Loss: nan\n",
      "Epoch [6441/10000] Train Loss: 0.007215 Val Loss: nan\n",
      "Epoch [6442/10000] Train Loss: 0.007277 Val Loss: nan\n",
      "Epoch [6443/10000] Train Loss: 0.007340 Val Loss: nan\n",
      "Epoch [6444/10000] Train Loss: 0.007217 Val Loss: nan\n",
      "Epoch [6445/10000] Train Loss: 0.007353 Val Loss: nan\n",
      "Epoch [6446/10000] Train Loss: 0.007457 Val Loss: nan\n",
      "Epoch [6447/10000] Train Loss: 0.007378 Val Loss: nan\n",
      "Epoch [6448/10000] Train Loss: 0.007295 Val Loss: nan\n",
      "Epoch [6449/10000] Train Loss: 0.007180 Val Loss: nan\n",
      "Epoch [6450/10000] Train Loss: 0.007247 Val Loss: nan\n",
      "Epoch [6451/10000] Train Loss: 0.007239 Val Loss: nan\n",
      "Epoch [6452/10000] Train Loss: 0.007277 Val Loss: nan\n",
      "Epoch [6453/10000] Train Loss: 0.007135 Val Loss: nan\n",
      "Epoch [6454/10000] Train Loss: 0.007334 Val Loss: nan\n",
      "Epoch [6455/10000] Train Loss: 0.007272 Val Loss: nan\n",
      "Epoch [6456/10000] Train Loss: 0.007228 Val Loss: nan\n",
      "Epoch [6457/10000] Train Loss: 0.007411 Val Loss: nan\n",
      "Epoch [6458/10000] Train Loss: 0.007267 Val Loss: nan\n",
      "Epoch [6459/10000] Train Loss: 0.007216 Val Loss: nan\n",
      "Epoch [6460/10000] Train Loss: 0.007259 Val Loss: nan\n",
      "Epoch [6461/10000] Train Loss: 0.007182 Val Loss: nan\n",
      "Epoch [6462/10000] Train Loss: 0.007147 Val Loss: nan\n",
      "Epoch [6463/10000] Train Loss: 0.007358 Val Loss: nan\n",
      "Epoch [6464/10000] Train Loss: 0.007210 Val Loss: nan\n",
      "Epoch [6465/10000] Train Loss: 0.007502 Val Loss: nan\n",
      "Epoch [6466/10000] Train Loss: 0.007246 Val Loss: nan\n",
      "Epoch [6467/10000] Train Loss: 0.007474 Val Loss: nan\n",
      "Epoch [6468/10000] Train Loss: 0.007265 Val Loss: nan\n",
      "Epoch [6469/10000] Train Loss: 0.007185 Val Loss: nan\n",
      "Epoch [6470/10000] Train Loss: 0.007401 Val Loss: nan\n",
      "Epoch [6471/10000] Train Loss: 0.007194 Val Loss: nan\n",
      "Epoch [6472/10000] Train Loss: 0.007297 Val Loss: nan\n",
      "Epoch [6473/10000] Train Loss: 0.007142 Val Loss: nan\n",
      "Epoch [6474/10000] Train Loss: 0.007137 Val Loss: nan\n",
      "Epoch [6475/10000] Train Loss: 0.007242 Val Loss: nan\n",
      "Epoch [6476/10000] Train Loss: 0.007345 Val Loss: nan\n",
      "Epoch [6477/10000] Train Loss: 0.007210 Val Loss: nan\n",
      "Epoch [6478/10000] Train Loss: 0.007269 Val Loss: nan\n",
      "Epoch [6479/10000] Train Loss: 0.007134 Val Loss: nan\n",
      "Epoch [6480/10000] Train Loss: 0.007338 Val Loss: nan\n",
      "Epoch [6481/10000] Train Loss: 0.007158 Val Loss: nan\n",
      "Epoch [6482/10000] Train Loss: 0.007132 Val Loss: nan\n",
      "Epoch [6483/10000] Train Loss: 0.007258 Val Loss: nan\n",
      "Epoch [6484/10000] Train Loss: 0.007231 Val Loss: nan\n",
      "Epoch [6485/10000] Train Loss: 0.007169 Val Loss: nan\n",
      "Epoch [6486/10000] Train Loss: 0.007203 Val Loss: nan\n",
      "Epoch [6487/10000] Train Loss: 0.007232 Val Loss: nan\n",
      "Epoch [6488/10000] Train Loss: 0.007139 Val Loss: nan\n",
      "Epoch [6489/10000] Train Loss: 0.007341 Val Loss: nan\n",
      "Epoch [6490/10000] Train Loss: 0.007184 Val Loss: nan\n",
      "Epoch [6491/10000] Train Loss: 0.007259 Val Loss: nan\n",
      "Epoch [6492/10000] Train Loss: 0.007175 Val Loss: nan\n",
      "Epoch [6493/10000] Train Loss: 0.007283 Val Loss: nan\n",
      "Epoch [6494/10000] Train Loss: 0.007614 Val Loss: nan\n",
      "Epoch [6495/10000] Train Loss: 0.007175 Val Loss: nan\n",
      "Epoch [6496/10000] Train Loss: 0.007098 Val Loss: nan\n",
      "Epoch [6497/10000] Train Loss: 0.007430 Val Loss: nan\n",
      "Epoch [6498/10000] Train Loss: 0.007240 Val Loss: nan\n",
      "Epoch [6499/10000] Train Loss: 0.007140 Val Loss: nan\n",
      "Epoch [6500/10000] Train Loss: 0.007241 Val Loss: nan\n",
      "Epoch [6501/10000] Train Loss: 0.007223 Val Loss: nan\n",
      "Epoch [6502/10000] Train Loss: 0.007139 Val Loss: nan\n",
      "Epoch [6503/10000] Train Loss: 0.007121 Val Loss: nan\n",
      "Epoch [6504/10000] Train Loss: 0.007155 Val Loss: nan\n",
      "Epoch [6505/10000] Train Loss: 0.007238 Val Loss: nan\n",
      "Epoch [6506/10000] Train Loss: 0.007111 Val Loss: nan\n",
      "Epoch [6507/10000] Train Loss: 0.007293 Val Loss: nan\n",
      "Epoch [6508/10000] Train Loss: 0.007208 Val Loss: nan\n",
      "Epoch [6509/10000] Train Loss: 0.007141 Val Loss: nan\n",
      "Epoch [6510/10000] Train Loss: 0.007175 Val Loss: nan\n",
      "Epoch [6511/10000] Train Loss: 0.007178 Val Loss: nan\n",
      "Epoch [6512/10000] Train Loss: 0.007244 Val Loss: nan\n",
      "Epoch [6513/10000] Train Loss: 0.007257 Val Loss: nan\n",
      "Epoch [6514/10000] Train Loss: 0.007152 Val Loss: nan\n",
      "Epoch [6515/10000] Train Loss: 0.007381 Val Loss: nan\n",
      "Epoch [6516/10000] Train Loss: 0.007266 Val Loss: nan\n",
      "Epoch [6517/10000] Train Loss: 0.007370 Val Loss: nan\n",
      "Epoch [6518/10000] Train Loss: 0.007279 Val Loss: nan\n",
      "Epoch [6519/10000] Train Loss: 0.007169 Val Loss: nan\n",
      "Epoch [6520/10000] Train Loss: 0.007237 Val Loss: nan\n",
      "Epoch [6521/10000] Train Loss: 0.007152 Val Loss: nan\n",
      "Epoch [6522/10000] Train Loss: 0.007278 Val Loss: nan\n",
      "Epoch [6523/10000] Train Loss: 0.007261 Val Loss: nan\n",
      "Epoch [6524/10000] Train Loss: 0.007165 Val Loss: nan\n",
      "Epoch [6525/10000] Train Loss: 0.007130 Val Loss: nan\n",
      "Epoch [6526/10000] Train Loss: 0.007173 Val Loss: nan\n",
      "Epoch [6527/10000] Train Loss: 0.007283 Val Loss: nan\n",
      "Epoch [6528/10000] Train Loss: 0.007156 Val Loss: nan\n",
      "Epoch [6529/10000] Train Loss: 0.007194 Val Loss: nan\n",
      "Epoch [6530/10000] Train Loss: 0.007428 Val Loss: nan\n",
      "Epoch [6531/10000] Train Loss: 0.007347 Val Loss: nan\n",
      "Epoch [6532/10000] Train Loss: 0.007360 Val Loss: nan\n",
      "Epoch [6533/10000] Train Loss: 0.007134 Val Loss: nan\n",
      "Epoch [6534/10000] Train Loss: 0.007145 Val Loss: nan\n",
      "Epoch [6535/10000] Train Loss: 0.007334 Val Loss: nan\n",
      "Epoch [6536/10000] Train Loss: 0.007406 Val Loss: nan\n",
      "Epoch [6537/10000] Train Loss: 0.007291 Val Loss: nan\n",
      "Epoch [6538/10000] Train Loss: 0.007260 Val Loss: nan\n",
      "Epoch [6539/10000] Train Loss: 0.007313 Val Loss: nan\n",
      "Epoch [6540/10000] Train Loss: 0.007329 Val Loss: nan\n",
      "Epoch [6541/10000] Train Loss: 0.007222 Val Loss: nan\n",
      "Epoch [6542/10000] Train Loss: 0.007277 Val Loss: nan\n",
      "Epoch [6543/10000] Train Loss: 0.007265 Val Loss: nan\n",
      "Epoch [6544/10000] Train Loss: 0.007373 Val Loss: nan\n",
      "Epoch [6545/10000] Train Loss: 0.007317 Val Loss: nan\n",
      "Epoch [6546/10000] Train Loss: 0.007176 Val Loss: nan\n",
      "Epoch [6547/10000] Train Loss: 0.007283 Val Loss: nan\n",
      "Epoch [6548/10000] Train Loss: 0.007132 Val Loss: nan\n",
      "Epoch [6549/10000] Train Loss: 0.007352 Val Loss: nan\n",
      "Epoch [6550/10000] Train Loss: 0.007401 Val Loss: nan\n",
      "Epoch [6551/10000] Train Loss: 0.007305 Val Loss: nan\n",
      "Epoch [6552/10000] Train Loss: 0.007322 Val Loss: nan\n",
      "Epoch [6553/10000] Train Loss: 0.007517 Val Loss: nan\n",
      "Epoch [6554/10000] Train Loss: 0.007365 Val Loss: nan\n",
      "Epoch [6555/10000] Train Loss: 0.007239 Val Loss: nan\n",
      "Epoch [6556/10000] Train Loss: 0.007228 Val Loss: nan\n",
      "Epoch [6557/10000] Train Loss: 0.007250 Val Loss: nan\n",
      "Epoch [6558/10000] Train Loss: 0.007283 Val Loss: nan\n",
      "Epoch [6559/10000] Train Loss: 0.007270 Val Loss: nan\n",
      "Epoch [6560/10000] Train Loss: 0.007169 Val Loss: nan\n",
      "Epoch [6561/10000] Train Loss: 0.007310 Val Loss: nan\n",
      "Epoch [6562/10000] Train Loss: 0.007219 Val Loss: nan\n",
      "Epoch [6563/10000] Train Loss: 0.007256 Val Loss: nan\n",
      "Epoch [6564/10000] Train Loss: 0.007200 Val Loss: nan\n",
      "Epoch [6565/10000] Train Loss: 0.007176 Val Loss: nan\n",
      "Epoch [6566/10000] Train Loss: 0.007244 Val Loss: nan\n",
      "Epoch [6567/10000] Train Loss: 0.007197 Val Loss: nan\n",
      "Epoch [6568/10000] Train Loss: 0.007144 Val Loss: nan\n",
      "Epoch [6569/10000] Train Loss: 0.007203 Val Loss: nan\n",
      "Epoch [6570/10000] Train Loss: 0.007358 Val Loss: nan\n",
      "Epoch [6571/10000] Train Loss: 0.007378 Val Loss: nan\n",
      "Epoch [6572/10000] Train Loss: 0.007326 Val Loss: nan\n",
      "Epoch [6573/10000] Train Loss: 0.007303 Val Loss: nan\n",
      "Epoch [6574/10000] Train Loss: 0.007187 Val Loss: nan\n",
      "Epoch [6575/10000] Train Loss: 0.007476 Val Loss: nan\n",
      "Epoch [6576/10000] Train Loss: 0.007361 Val Loss: nan\n",
      "Epoch [6577/10000] Train Loss: 0.007261 Val Loss: nan\n",
      "Epoch [6578/10000] Train Loss: 0.007329 Val Loss: nan\n",
      "Epoch [6579/10000] Train Loss: 0.007202 Val Loss: nan\n",
      "Epoch [6580/10000] Train Loss: 0.007269 Val Loss: nan\n",
      "Epoch [6581/10000] Train Loss: 0.007136 Val Loss: nan\n",
      "Epoch [6582/10000] Train Loss: 0.007298 Val Loss: nan\n",
      "Epoch [6583/10000] Train Loss: 0.007128 Val Loss: nan\n",
      "Epoch [6584/10000] Train Loss: 0.007145 Val Loss: nan\n",
      "Epoch [6585/10000] Train Loss: 0.007154 Val Loss: nan\n",
      "Epoch [6586/10000] Train Loss: 0.007221 Val Loss: nan\n",
      "Epoch [6587/10000] Train Loss: 0.007229 Val Loss: nan\n",
      "Epoch [6588/10000] Train Loss: 0.007154 Val Loss: nan\n",
      "Epoch [6589/10000] Train Loss: 0.007361 Val Loss: nan\n",
      "Epoch [6590/10000] Train Loss: 0.007233 Val Loss: nan\n",
      "Epoch [6591/10000] Train Loss: 0.007388 Val Loss: nan\n",
      "Epoch [6592/10000] Train Loss: 0.007235 Val Loss: nan\n",
      "Epoch [6593/10000] Train Loss: 0.007353 Val Loss: nan\n",
      "Epoch [6594/10000] Train Loss: 0.007160 Val Loss: nan\n",
      "Epoch [6595/10000] Train Loss: 0.007352 Val Loss: nan\n",
      "Epoch [6596/10000] Train Loss: 0.007245 Val Loss: nan\n",
      "Epoch [6597/10000] Train Loss: 0.007109 Val Loss: nan\n",
      "Epoch [6598/10000] Train Loss: 0.007141 Val Loss: nan\n",
      "Epoch [6599/10000] Train Loss: 0.007339 Val Loss: nan\n",
      "Epoch [6600/10000] Train Loss: 0.007225 Val Loss: nan\n",
      "Epoch [6601/10000] Train Loss: 0.007137 Val Loss: nan\n",
      "Epoch [6602/10000] Train Loss: 0.007275 Val Loss: nan\n",
      "Epoch [6603/10000] Train Loss: 0.007441 Val Loss: nan\n",
      "Epoch [6604/10000] Train Loss: 0.007271 Val Loss: nan\n",
      "Epoch [6605/10000] Train Loss: 0.007122 Val Loss: nan\n",
      "Epoch [6606/10000] Train Loss: 0.007158 Val Loss: nan\n",
      "Epoch [6607/10000] Train Loss: 0.007201 Val Loss: nan\n",
      "Epoch [6608/10000] Train Loss: 0.007383 Val Loss: nan\n",
      "Epoch [6609/10000] Train Loss: 0.007127 Val Loss: nan\n",
      "Epoch [6610/10000] Train Loss: 0.007487 Val Loss: nan\n",
      "Epoch [6611/10000] Train Loss: 0.007170 Val Loss: nan\n",
      "Epoch [6612/10000] Train Loss: 0.007206 Val Loss: nan\n",
      "Epoch [6613/10000] Train Loss: 0.007177 Val Loss: nan\n",
      "Epoch [6614/10000] Train Loss: 0.007119 Val Loss: nan\n",
      "Epoch [6615/10000] Train Loss: 0.007110 Val Loss: nan\n",
      "Epoch [6616/10000] Train Loss: 0.007191 Val Loss: nan\n",
      "Epoch [6617/10000] Train Loss: 0.007228 Val Loss: nan\n",
      "Epoch [6618/10000] Train Loss: 0.007250 Val Loss: nan\n",
      "Epoch [6619/10000] Train Loss: 0.007161 Val Loss: nan\n",
      "Epoch [6620/10000] Train Loss: 0.007296 Val Loss: nan\n",
      "Epoch [6621/10000] Train Loss: 0.007388 Val Loss: nan\n",
      "Epoch [6622/10000] Train Loss: 0.007139 Val Loss: nan\n",
      "Epoch [6623/10000] Train Loss: 0.007246 Val Loss: nan\n",
      "Epoch [6624/10000] Train Loss: 0.007211 Val Loss: nan\n",
      "Epoch [6625/10000] Train Loss: 0.007457 Val Loss: nan\n",
      "Epoch [6626/10000] Train Loss: 0.007287 Val Loss: nan\n",
      "Epoch [6627/10000] Train Loss: 0.007162 Val Loss: nan\n",
      "Epoch [6628/10000] Train Loss: 0.007385 Val Loss: nan\n",
      "Epoch [6629/10000] Train Loss: 0.007111 Val Loss: nan\n",
      "Epoch [6630/10000] Train Loss: 0.007099 Val Loss: nan\n",
      "Epoch [6631/10000] Train Loss: 0.007304 Val Loss: nan\n",
      "Epoch [6632/10000] Train Loss: 0.007381 Val Loss: nan\n",
      "Epoch [6633/10000] Train Loss: 0.007362 Val Loss: nan\n",
      "Epoch [6634/10000] Train Loss: 0.007110 Val Loss: nan\n",
      "Epoch [6635/10000] Train Loss: 0.007128 Val Loss: nan\n",
      "Epoch [6636/10000] Train Loss: 0.007209 Val Loss: nan\n",
      "Epoch [6637/10000] Train Loss: 0.007259 Val Loss: nan\n",
      "Epoch [6638/10000] Train Loss: 0.007355 Val Loss: nan\n",
      "Epoch [6639/10000] Train Loss: 0.007221 Val Loss: nan\n",
      "Epoch [6640/10000] Train Loss: 0.007343 Val Loss: nan\n",
      "Epoch [6641/10000] Train Loss: 0.007233 Val Loss: nan\n",
      "Epoch [6642/10000] Train Loss: 0.007316 Val Loss: nan\n",
      "Epoch [6643/10000] Train Loss: 0.007364 Val Loss: nan\n",
      "Epoch [6644/10000] Train Loss: 0.007207 Val Loss: nan\n",
      "Epoch [6645/10000] Train Loss: 0.007251 Val Loss: nan\n",
      "Epoch [6646/10000] Train Loss: 0.007346 Val Loss: nan\n",
      "Epoch [6647/10000] Train Loss: 0.007260 Val Loss: nan\n",
      "Epoch [6648/10000] Train Loss: 0.007125 Val Loss: nan\n",
      "Epoch [6649/10000] Train Loss: 0.007236 Val Loss: nan\n",
      "Epoch [6650/10000] Train Loss: 0.007129 Val Loss: nan\n",
      "Epoch [6651/10000] Train Loss: 0.007441 Val Loss: nan\n",
      "Epoch [6652/10000] Train Loss: 0.007435 Val Loss: nan\n",
      "Epoch [6653/10000] Train Loss: 0.007156 Val Loss: nan\n",
      "Epoch [6654/10000] Train Loss: 0.007366 Val Loss: nan\n",
      "Epoch [6655/10000] Train Loss: 0.007183 Val Loss: nan\n",
      "Epoch [6656/10000] Train Loss: 0.007388 Val Loss: nan\n",
      "Epoch [6657/10000] Train Loss: 0.007108 Val Loss: nan\n",
      "Epoch [6658/10000] Train Loss: 0.007251 Val Loss: nan\n",
      "Epoch [6659/10000] Train Loss: 0.007252 Val Loss: nan\n",
      "Epoch [6660/10000] Train Loss: 0.007223 Val Loss: nan\n",
      "Epoch [6661/10000] Train Loss: 0.007213 Val Loss: nan\n",
      "Epoch [6662/10000] Train Loss: 0.007299 Val Loss: nan\n",
      "Epoch [6663/10000] Train Loss: 0.007150 Val Loss: nan\n",
      "Epoch [6664/10000] Train Loss: 0.007150 Val Loss: nan\n",
      "Epoch [6665/10000] Train Loss: 0.007194 Val Loss: nan\n",
      "Epoch [6666/10000] Train Loss: 0.007298 Val Loss: nan\n",
      "Epoch [6667/10000] Train Loss: 0.007280 Val Loss: nan\n",
      "Epoch [6668/10000] Train Loss: 0.007281 Val Loss: nan\n",
      "Epoch [6669/10000] Train Loss: 0.007503 Val Loss: nan\n",
      "Epoch [6670/10000] Train Loss: 0.007132 Val Loss: nan\n",
      "Epoch [6671/10000] Train Loss: 0.007227 Val Loss: nan\n",
      "Epoch [6672/10000] Train Loss: 0.007172 Val Loss: nan\n",
      "Epoch [6673/10000] Train Loss: 0.007382 Val Loss: nan\n",
      "Epoch [6674/10000] Train Loss: 0.007352 Val Loss: nan\n",
      "Epoch [6675/10000] Train Loss: 0.007165 Val Loss: nan\n",
      "Epoch [6676/10000] Train Loss: 0.007471 Val Loss: nan\n",
      "Epoch [6677/10000] Train Loss: 0.007139 Val Loss: nan\n",
      "Epoch [6678/10000] Train Loss: 0.007102 Val Loss: nan\n",
      "Epoch [6679/10000] Train Loss: 0.007224 Val Loss: nan\n",
      "Epoch [6680/10000] Train Loss: 0.007125 Val Loss: nan\n",
      "Epoch [6681/10000] Train Loss: 0.007125 Val Loss: nan\n",
      "Epoch [6682/10000] Train Loss: 0.007218 Val Loss: nan\n",
      "Epoch [6683/10000] Train Loss: 0.007217 Val Loss: nan\n",
      "Epoch [6684/10000] Train Loss: 0.007281 Val Loss: nan\n",
      "Epoch [6685/10000] Train Loss: 0.007338 Val Loss: nan\n",
      "Epoch [6686/10000] Train Loss: 0.007229 Val Loss: nan\n",
      "Epoch [6687/10000] Train Loss: 0.007230 Val Loss: nan\n",
      "Epoch [6688/10000] Train Loss: 0.007213 Val Loss: nan\n",
      "Epoch [6689/10000] Train Loss: 0.007306 Val Loss: nan\n",
      "Epoch [6690/10000] Train Loss: 0.007237 Val Loss: nan\n",
      "Epoch [6691/10000] Train Loss: 0.007334 Val Loss: nan\n",
      "Epoch [6692/10000] Train Loss: 0.007348 Val Loss: nan\n",
      "Epoch [6693/10000] Train Loss: 0.007167 Val Loss: nan\n",
      "Epoch [6694/10000] Train Loss: 0.007259 Val Loss: nan\n",
      "Epoch [6695/10000] Train Loss: 0.007542 Val Loss: nan\n",
      "Epoch [6696/10000] Train Loss: 0.007146 Val Loss: nan\n",
      "Epoch [6697/10000] Train Loss: 0.007129 Val Loss: nan\n",
      "Epoch [6698/10000] Train Loss: 0.007230 Val Loss: nan\n",
      "Epoch [6699/10000] Train Loss: 0.007242 Val Loss: nan\n",
      "Epoch [6700/10000] Train Loss: 0.007250 Val Loss: nan\n",
      "Epoch [6701/10000] Train Loss: 0.007196 Val Loss: nan\n",
      "Epoch [6702/10000] Train Loss: 0.007337 Val Loss: nan\n",
      "Epoch [6703/10000] Train Loss: 0.007129 Val Loss: nan\n",
      "Epoch [6704/10000] Train Loss: 0.007272 Val Loss: nan\n",
      "Epoch [6705/10000] Train Loss: 0.007226 Val Loss: nan\n",
      "Epoch [6706/10000] Train Loss: 0.007227 Val Loss: nan\n",
      "Epoch [6707/10000] Train Loss: 0.007157 Val Loss: nan\n",
      "Epoch [6708/10000] Train Loss: 0.007111 Val Loss: nan\n",
      "Epoch [6709/10000] Train Loss: 0.007377 Val Loss: nan\n",
      "Epoch [6710/10000] Train Loss: 0.007223 Val Loss: nan\n",
      "Epoch [6711/10000] Train Loss: 0.007133 Val Loss: nan\n",
      "Epoch [6712/10000] Train Loss: 0.007143 Val Loss: nan\n",
      "Epoch [6713/10000] Train Loss: 0.007210 Val Loss: nan\n",
      "Epoch [6714/10000] Train Loss: 0.007158 Val Loss: nan\n",
      "Epoch [6715/10000] Train Loss: 0.007141 Val Loss: nan\n",
      "Epoch [6716/10000] Train Loss: 0.007372 Val Loss: nan\n",
      "Epoch [6717/10000] Train Loss: 0.007112 Val Loss: nan\n",
      "Epoch [6718/10000] Train Loss: 0.007332 Val Loss: nan\n",
      "Epoch [6719/10000] Train Loss: 0.007345 Val Loss: nan\n",
      "Epoch [6720/10000] Train Loss: 0.007112 Val Loss: nan\n",
      "Epoch [6721/10000] Train Loss: 0.007200 Val Loss: nan\n",
      "Epoch [6722/10000] Train Loss: 0.007212 Val Loss: nan\n",
      "Epoch [6723/10000] Train Loss: 0.007189 Val Loss: nan\n",
      "Epoch [6724/10000] Train Loss: 0.007180 Val Loss: nan\n",
      "Epoch [6725/10000] Train Loss: 0.007332 Val Loss: nan\n",
      "Epoch [6726/10000] Train Loss: 0.007137 Val Loss: nan\n",
      "Epoch [6727/10000] Train Loss: 0.007150 Val Loss: nan\n",
      "Epoch [6728/10000] Train Loss: 0.007179 Val Loss: nan\n",
      "Epoch [6729/10000] Train Loss: 0.007258 Val Loss: nan\n",
      "Epoch [6730/10000] Train Loss: 0.007166 Val Loss: nan\n",
      "Epoch [6731/10000] Train Loss: 0.007350 Val Loss: nan\n",
      "Epoch [6732/10000] Train Loss: 0.007125 Val Loss: nan\n",
      "Epoch [6733/10000] Train Loss: 0.007281 Val Loss: nan\n",
      "Epoch [6734/10000] Train Loss: 0.007089 Val Loss: nan\n",
      "Epoch [6735/10000] Train Loss: 0.007198 Val Loss: nan\n",
      "Epoch [6736/10000] Train Loss: 0.007358 Val Loss: nan\n",
      "Epoch [6737/10000] Train Loss: 0.007293 Val Loss: nan\n",
      "Epoch [6738/10000] Train Loss: 0.007217 Val Loss: nan\n",
      "Epoch [6739/10000] Train Loss: 0.007340 Val Loss: nan\n",
      "Epoch [6740/10000] Train Loss: 0.007353 Val Loss: nan\n",
      "Epoch [6741/10000] Train Loss: 0.007265 Val Loss: nan\n",
      "Epoch [6742/10000] Train Loss: 0.007166 Val Loss: nan\n",
      "Epoch [6743/10000] Train Loss: 0.007450 Val Loss: nan\n",
      "Epoch [6744/10000] Train Loss: 0.007334 Val Loss: nan\n",
      "Epoch [6745/10000] Train Loss: 0.007256 Val Loss: nan\n",
      "Epoch [6746/10000] Train Loss: 0.007267 Val Loss: nan\n",
      "Epoch [6747/10000] Train Loss: 0.007326 Val Loss: nan\n",
      "Epoch [6748/10000] Train Loss: 0.007119 Val Loss: nan\n",
      "Epoch [6749/10000] Train Loss: 0.007361 Val Loss: nan\n",
      "Epoch [6750/10000] Train Loss: 0.007289 Val Loss: nan\n",
      "Epoch [6751/10000] Train Loss: 0.007231 Val Loss: nan\n",
      "Epoch [6752/10000] Train Loss: 0.007174 Val Loss: nan\n",
      "Epoch [6753/10000] Train Loss: 0.007197 Val Loss: nan\n",
      "Epoch [6754/10000] Train Loss: 0.007162 Val Loss: nan\n",
      "Epoch [6755/10000] Train Loss: 0.007155 Val Loss: nan\n",
      "Epoch [6756/10000] Train Loss: 0.007215 Val Loss: nan\n",
      "Epoch [6757/10000] Train Loss: 0.007236 Val Loss: nan\n",
      "Epoch [6758/10000] Train Loss: 0.007177 Val Loss: nan\n",
      "Epoch [6759/10000] Train Loss: 0.007333 Val Loss: nan\n",
      "Epoch [6760/10000] Train Loss: 0.007190 Val Loss: nan\n",
      "Epoch [6761/10000] Train Loss: 0.007120 Val Loss: nan\n",
      "Epoch [6762/10000] Train Loss: 0.007077 Val Loss: nan\n",
      "Epoch [6763/10000] Train Loss: 0.007364 Val Loss: nan\n",
      "Epoch [6764/10000] Train Loss: 0.007225 Val Loss: nan\n",
      "Epoch [6765/10000] Train Loss: 0.007101 Val Loss: nan\n",
      "Epoch [6766/10000] Train Loss: 0.007187 Val Loss: nan\n",
      "Epoch [6767/10000] Train Loss: 0.007196 Val Loss: nan\n",
      "Epoch [6768/10000] Train Loss: 0.007155 Val Loss: nan\n",
      "Epoch [6769/10000] Train Loss: 0.007185 Val Loss: nan\n",
      "Epoch [6770/10000] Train Loss: 0.007111 Val Loss: nan\n",
      "Epoch [6771/10000] Train Loss: 0.007376 Val Loss: nan\n",
      "Epoch [6772/10000] Train Loss: 0.007211 Val Loss: nan\n",
      "Epoch [6773/10000] Train Loss: 0.007106 Val Loss: nan\n",
      "Epoch [6774/10000] Train Loss: 0.007097 Val Loss: nan\n",
      "Epoch [6775/10000] Train Loss: 0.007138 Val Loss: nan\n",
      "Epoch [6776/10000] Train Loss: 0.007275 Val Loss: nan\n",
      "Epoch [6777/10000] Train Loss: 0.007112 Val Loss: nan\n",
      "Epoch [6778/10000] Train Loss: 0.007315 Val Loss: nan\n",
      "Epoch [6779/10000] Train Loss: 0.007136 Val Loss: nan\n",
      "Epoch [6780/10000] Train Loss: 0.007174 Val Loss: nan\n",
      "Epoch [6781/10000] Train Loss: 0.007118 Val Loss: nan\n",
      "Epoch [6782/10000] Train Loss: 0.007226 Val Loss: nan\n",
      "Epoch [6783/10000] Train Loss: 0.007137 Val Loss: nan\n",
      "Epoch [6784/10000] Train Loss: 0.007098 Val Loss: nan\n",
      "Epoch [6785/10000] Train Loss: 0.007228 Val Loss: nan\n",
      "Epoch [6786/10000] Train Loss: 0.007331 Val Loss: nan\n",
      "Epoch [6787/10000] Train Loss: 0.007290 Val Loss: nan\n",
      "Epoch [6788/10000] Train Loss: 0.007324 Val Loss: nan\n",
      "Epoch [6789/10000] Train Loss: 0.007384 Val Loss: nan\n",
      "Epoch [6790/10000] Train Loss: 0.007179 Val Loss: nan\n",
      "Epoch [6791/10000] Train Loss: 0.007252 Val Loss: nan\n",
      "Epoch [6792/10000] Train Loss: 0.007159 Val Loss: nan\n",
      "Epoch [6793/10000] Train Loss: 0.007228 Val Loss: nan\n",
      "Epoch [6794/10000] Train Loss: 0.007145 Val Loss: nan\n",
      "Epoch [6795/10000] Train Loss: 0.007101 Val Loss: nan\n",
      "Epoch [6796/10000] Train Loss: 0.007233 Val Loss: nan\n",
      "Epoch [6797/10000] Train Loss: 0.007182 Val Loss: nan\n",
      "Epoch [6798/10000] Train Loss: 0.007221 Val Loss: nan\n",
      "Epoch [6799/10000] Train Loss: 0.007088 Val Loss: nan\n",
      "Epoch [6800/10000] Train Loss: 0.007176 Val Loss: nan\n",
      "Epoch [6801/10000] Train Loss: 0.007260 Val Loss: nan\n",
      "Epoch [6802/10000] Train Loss: 0.007174 Val Loss: nan\n",
      "Epoch [6803/10000] Train Loss: 0.007253 Val Loss: nan\n",
      "Epoch [6804/10000] Train Loss: 0.007272 Val Loss: nan\n",
      "Epoch [6805/10000] Train Loss: 0.007160 Val Loss: nan\n",
      "Epoch [6806/10000] Train Loss: 0.007402 Val Loss: nan\n",
      "Epoch [6807/10000] Train Loss: 0.007112 Val Loss: nan\n",
      "Epoch [6808/10000] Train Loss: 0.007182 Val Loss: nan\n",
      "Epoch [6809/10000] Train Loss: 0.007133 Val Loss: nan\n",
      "Epoch [6810/10000] Train Loss: 0.007127 Val Loss: nan\n",
      "Epoch [6811/10000] Train Loss: 0.007138 Val Loss: nan\n",
      "Epoch [6812/10000] Train Loss: 0.007152 Val Loss: nan\n",
      "Epoch [6813/10000] Train Loss: 0.007217 Val Loss: nan\n",
      "Epoch [6814/10000] Train Loss: 0.007146 Val Loss: nan\n",
      "Epoch [6815/10000] Train Loss: 0.007325 Val Loss: nan\n",
      "Epoch [6816/10000] Train Loss: 0.007225 Val Loss: nan\n",
      "Epoch [6817/10000] Train Loss: 0.007141 Val Loss: nan\n",
      "Epoch [6818/10000] Train Loss: 0.007309 Val Loss: nan\n",
      "Epoch [6819/10000] Train Loss: 0.007077 Val Loss: nan\n",
      "Epoch [6820/10000] Train Loss: 0.007202 Val Loss: nan\n",
      "Epoch [6821/10000] Train Loss: 0.007234 Val Loss: nan\n",
      "Epoch [6822/10000] Train Loss: 0.007229 Val Loss: nan\n",
      "Epoch [6823/10000] Train Loss: 0.007303 Val Loss: nan\n",
      "Epoch [6824/10000] Train Loss: 0.007370 Val Loss: nan\n",
      "Epoch [6825/10000] Train Loss: 0.007332 Val Loss: nan\n",
      "Epoch [6826/10000] Train Loss: 0.007105 Val Loss: nan\n",
      "Epoch [6827/10000] Train Loss: 0.007259 Val Loss: nan\n",
      "Epoch [6828/10000] Train Loss: 0.007264 Val Loss: nan\n",
      "Epoch [6829/10000] Train Loss: 0.007299 Val Loss: nan\n",
      "Epoch [6830/10000] Train Loss: 0.007171 Val Loss: nan\n",
      "Epoch [6831/10000] Train Loss: 0.007220 Val Loss: nan\n",
      "Epoch [6832/10000] Train Loss: 0.007050 Val Loss: nan\n",
      "Epoch [6833/10000] Train Loss: 0.007248 Val Loss: nan\n",
      "Epoch [6834/10000] Train Loss: 0.007175 Val Loss: nan\n",
      "Epoch [6835/10000] Train Loss: 0.007203 Val Loss: nan\n",
      "Epoch [6836/10000] Train Loss: 0.007094 Val Loss: nan\n",
      "Epoch [6837/10000] Train Loss: 0.007302 Val Loss: nan\n",
      "Epoch [6838/10000] Train Loss: 0.007135 Val Loss: nan\n",
      "Epoch [6839/10000] Train Loss: 0.007381 Val Loss: nan\n",
      "Epoch [6840/10000] Train Loss: 0.007274 Val Loss: nan\n",
      "Epoch [6841/10000] Train Loss: 0.007289 Val Loss: nan\n",
      "Epoch [6842/10000] Train Loss: 0.007325 Val Loss: nan\n",
      "Epoch [6843/10000] Train Loss: 0.007217 Val Loss: nan\n",
      "Epoch [6844/10000] Train Loss: 0.007328 Val Loss: nan\n",
      "Epoch [6845/10000] Train Loss: 0.007163 Val Loss: nan\n",
      "Epoch [6846/10000] Train Loss: 0.007171 Val Loss: nan\n",
      "Epoch [6847/10000] Train Loss: 0.007155 Val Loss: nan\n",
      "Epoch [6848/10000] Train Loss: 0.007109 Val Loss: nan\n",
      "Epoch [6849/10000] Train Loss: 0.007168 Val Loss: nan\n",
      "Epoch [6850/10000] Train Loss: 0.007205 Val Loss: nan\n",
      "Epoch [6851/10000] Train Loss: 0.007282 Val Loss: nan\n",
      "Epoch [6852/10000] Train Loss: 0.007261 Val Loss: nan\n",
      "Epoch [6853/10000] Train Loss: 0.007529 Val Loss: nan\n",
      "Epoch [6854/10000] Train Loss: 0.007114 Val Loss: nan\n",
      "Epoch [6855/10000] Train Loss: 0.007232 Val Loss: nan\n",
      "Epoch [6856/10000] Train Loss: 0.007195 Val Loss: nan\n",
      "Epoch [6857/10000] Train Loss: 0.007313 Val Loss: nan\n",
      "Epoch [6858/10000] Train Loss: 0.007199 Val Loss: nan\n",
      "Epoch [6859/10000] Train Loss: 0.007137 Val Loss: nan\n",
      "Epoch [6860/10000] Train Loss: 0.007293 Val Loss: nan\n",
      "Epoch [6861/10000] Train Loss: 0.007323 Val Loss: nan\n",
      "Epoch [6862/10000] Train Loss: 0.007487 Val Loss: nan\n",
      "Epoch [6863/10000] Train Loss: 0.007387 Val Loss: nan\n",
      "Epoch [6864/10000] Train Loss: 0.007315 Val Loss: nan\n",
      "Epoch [6865/10000] Train Loss: 0.007218 Val Loss: nan\n",
      "Epoch [6866/10000] Train Loss: 0.007083 Val Loss: nan\n",
      "Epoch [6867/10000] Train Loss: 0.007362 Val Loss: nan\n",
      "Epoch [6868/10000] Train Loss: 0.007343 Val Loss: nan\n",
      "Epoch [6869/10000] Train Loss: 0.007267 Val Loss: nan\n",
      "Epoch [6870/10000] Train Loss: 0.007229 Val Loss: nan\n",
      "Epoch [6871/10000] Train Loss: 0.007162 Val Loss: nan\n",
      "Epoch [6872/10000] Train Loss: 0.007189 Val Loss: nan\n",
      "Epoch [6873/10000] Train Loss: 0.007226 Val Loss: nan\n",
      "Epoch [6874/10000] Train Loss: 0.007188 Val Loss: nan\n",
      "Epoch [6875/10000] Train Loss: 0.007217 Val Loss: nan\n",
      "Epoch [6876/10000] Train Loss: 0.007088 Val Loss: nan\n",
      "Epoch [6877/10000] Train Loss: 0.007103 Val Loss: nan\n",
      "Epoch [6878/10000] Train Loss: 0.007221 Val Loss: nan\n",
      "Epoch [6879/10000] Train Loss: 0.007088 Val Loss: nan\n",
      "Epoch [6880/10000] Train Loss: 0.007309 Val Loss: nan\n",
      "Epoch [6881/10000] Train Loss: 0.007083 Val Loss: nan\n",
      "Epoch [6882/10000] Train Loss: 0.007309 Val Loss: nan\n",
      "Epoch [6883/10000] Train Loss: 0.007105 Val Loss: nan\n",
      "Epoch [6884/10000] Train Loss: 0.007055 Val Loss: nan\n",
      "Epoch [6885/10000] Train Loss: 0.007206 Val Loss: nan\n",
      "Epoch [6886/10000] Train Loss: 0.007171 Val Loss: nan\n",
      "Epoch [6887/10000] Train Loss: 0.007191 Val Loss: nan\n",
      "Epoch [6888/10000] Train Loss: 0.007121 Val Loss: nan\n",
      "Epoch [6889/10000] Train Loss: 0.007136 Val Loss: nan\n",
      "Epoch [6890/10000] Train Loss: 0.007281 Val Loss: nan\n",
      "Epoch [6891/10000] Train Loss: 0.007100 Val Loss: nan\n",
      "Epoch [6892/10000] Train Loss: 0.007120 Val Loss: nan\n",
      "Epoch [6893/10000] Train Loss: 0.007152 Val Loss: nan\n",
      "Epoch [6894/10000] Train Loss: 0.007094 Val Loss: nan\n",
      "Epoch [6895/10000] Train Loss: 0.007313 Val Loss: nan\n",
      "Epoch [6896/10000] Train Loss: 0.007093 Val Loss: nan\n",
      "Epoch [6897/10000] Train Loss: 0.007094 Val Loss: nan\n",
      "Epoch [6898/10000] Train Loss: 0.007089 Val Loss: nan\n",
      "Epoch [6899/10000] Train Loss: 0.007084 Val Loss: nan\n",
      "Epoch [6900/10000] Train Loss: 0.007118 Val Loss: nan\n",
      "Epoch [6901/10000] Train Loss: 0.007096 Val Loss: nan\n",
      "Epoch [6902/10000] Train Loss: 0.007223 Val Loss: nan\n",
      "Epoch [6903/10000] Train Loss: 0.007096 Val Loss: nan\n",
      "Epoch [6904/10000] Train Loss: 0.007283 Val Loss: nan\n",
      "Epoch [6905/10000] Train Loss: 0.007169 Val Loss: nan\n",
      "Epoch [6906/10000] Train Loss: 0.007240 Val Loss: nan\n",
      "Epoch [6907/10000] Train Loss: 0.007435 Val Loss: nan\n",
      "Epoch [6908/10000] Train Loss: 0.007071 Val Loss: nan\n",
      "Epoch [6909/10000] Train Loss: 0.007099 Val Loss: nan\n",
      "Epoch [6910/10000] Train Loss: 0.007198 Val Loss: nan\n",
      "Epoch [6911/10000] Train Loss: 0.007218 Val Loss: nan\n",
      "Epoch [6912/10000] Train Loss: 0.007115 Val Loss: nan\n",
      "Epoch [6913/10000] Train Loss: 0.007482 Val Loss: nan\n",
      "Epoch [6914/10000] Train Loss: 0.007093 Val Loss: nan\n",
      "Epoch [6915/10000] Train Loss: 0.007195 Val Loss: nan\n",
      "Epoch [6916/10000] Train Loss: 0.007202 Val Loss: nan\n",
      "Epoch [6917/10000] Train Loss: 0.007146 Val Loss: nan\n",
      "Epoch [6918/10000] Train Loss: 0.007068 Val Loss: nan\n",
      "Epoch [6919/10000] Train Loss: 0.007244 Val Loss: nan\n",
      "Epoch [6920/10000] Train Loss: 0.007303 Val Loss: nan\n",
      "Epoch [6921/10000] Train Loss: 0.007221 Val Loss: nan\n",
      "Epoch [6922/10000] Train Loss: 0.007144 Val Loss: nan\n",
      "Epoch [6923/10000] Train Loss: 0.007235 Val Loss: nan\n",
      "Epoch [6924/10000] Train Loss: 0.007122 Val Loss: nan\n",
      "Epoch [6925/10000] Train Loss: 0.007204 Val Loss: nan\n",
      "Epoch [6926/10000] Train Loss: 0.007233 Val Loss: nan\n",
      "Epoch [6927/10000] Train Loss: 0.007217 Val Loss: nan\n",
      "Epoch [6928/10000] Train Loss: 0.007297 Val Loss: nan\n",
      "Epoch [6929/10000] Train Loss: 0.007115 Val Loss: nan\n",
      "Epoch [6930/10000] Train Loss: 0.007185 Val Loss: nan\n",
      "Epoch [6931/10000] Train Loss: 0.007086 Val Loss: nan\n",
      "Epoch [6932/10000] Train Loss: 0.007072 Val Loss: nan\n",
      "Epoch [6933/10000] Train Loss: 0.007223 Val Loss: nan\n",
      "Epoch [6934/10000] Train Loss: 0.007150 Val Loss: nan\n",
      "Epoch [6935/10000] Train Loss: 0.007241 Val Loss: nan\n",
      "Epoch [6936/10000] Train Loss: 0.007379 Val Loss: nan\n",
      "Epoch [6937/10000] Train Loss: 0.007068 Val Loss: nan\n",
      "Epoch [6938/10000] Train Loss: 0.007438 Val Loss: nan\n",
      "Epoch [6939/10000] Train Loss: 0.007063 Val Loss: nan\n",
      "Epoch [6940/10000] Train Loss: 0.007057 Val Loss: nan\n",
      "Epoch [6941/10000] Train Loss: 0.007362 Val Loss: nan\n",
      "Epoch [6942/10000] Train Loss: 0.007238 Val Loss: nan\n",
      "Epoch [6943/10000] Train Loss: 0.007109 Val Loss: nan\n",
      "Epoch [6944/10000] Train Loss: 0.007164 Val Loss: nan\n",
      "Epoch [6945/10000] Train Loss: 0.007314 Val Loss: nan\n",
      "Epoch [6946/10000] Train Loss: 0.007157 Val Loss: nan\n",
      "Epoch [6947/10000] Train Loss: 0.007333 Val Loss: nan\n",
      "Epoch [6948/10000] Train Loss: 0.007072 Val Loss: nan\n",
      "Epoch [6949/10000] Train Loss: 0.007204 Val Loss: nan\n",
      "Epoch [6950/10000] Train Loss: 0.007217 Val Loss: nan\n",
      "Epoch [6951/10000] Train Loss: 0.007378 Val Loss: nan\n",
      "Epoch [6952/10000] Train Loss: 0.007381 Val Loss: nan\n",
      "Epoch [6953/10000] Train Loss: 0.007173 Val Loss: nan\n",
      "Epoch [6954/10000] Train Loss: 0.007153 Val Loss: nan\n",
      "Epoch [6955/10000] Train Loss: 0.007099 Val Loss: nan\n",
      "Epoch [6956/10000] Train Loss: 0.007310 Val Loss: nan\n",
      "Epoch [6957/10000] Train Loss: 0.007280 Val Loss: nan\n",
      "Epoch [6958/10000] Train Loss: 0.007372 Val Loss: nan\n",
      "Epoch [6959/10000] Train Loss: 0.007220 Val Loss: nan\n",
      "Epoch [6960/10000] Train Loss: 0.007186 Val Loss: nan\n",
      "Epoch [6961/10000] Train Loss: 0.007208 Val Loss: nan\n",
      "Epoch [6962/10000] Train Loss: 0.007266 Val Loss: nan\n",
      "Epoch [6963/10000] Train Loss: 0.007269 Val Loss: nan\n",
      "Epoch [6964/10000] Train Loss: 0.007220 Val Loss: nan\n",
      "Epoch [6965/10000] Train Loss: 0.007287 Val Loss: nan\n",
      "Epoch [6966/10000] Train Loss: 0.007386 Val Loss: nan\n",
      "Epoch [6967/10000] Train Loss: 0.007309 Val Loss: nan\n",
      "Epoch [6968/10000] Train Loss: 0.007404 Val Loss: nan\n",
      "Epoch [6969/10000] Train Loss: 0.007179 Val Loss: nan\n",
      "Epoch [6970/10000] Train Loss: 0.007165 Val Loss: nan\n",
      "Epoch [6971/10000] Train Loss: 0.007157 Val Loss: nan\n",
      "Epoch [6972/10000] Train Loss: 0.007329 Val Loss: nan\n",
      "Epoch [6973/10000] Train Loss: 0.007074 Val Loss: nan\n",
      "Epoch [6974/10000] Train Loss: 0.007243 Val Loss: nan\n",
      "Epoch [6975/10000] Train Loss: 0.007260 Val Loss: nan\n",
      "Epoch [6976/10000] Train Loss: 0.007083 Val Loss: nan\n",
      "Epoch [6977/10000] Train Loss: 0.007197 Val Loss: nan\n",
      "Epoch [6978/10000] Train Loss: 0.007252 Val Loss: nan\n",
      "Epoch [6979/10000] Train Loss: 0.007181 Val Loss: nan\n",
      "Epoch [6980/10000] Train Loss: 0.007164 Val Loss: nan\n",
      "Epoch [6981/10000] Train Loss: 0.007286 Val Loss: nan\n",
      "Epoch [6982/10000] Train Loss: 0.007220 Val Loss: nan\n",
      "Epoch [6983/10000] Train Loss: 0.007382 Val Loss: nan\n",
      "Epoch [6984/10000] Train Loss: 0.007249 Val Loss: nan\n",
      "Epoch [6985/10000] Train Loss: 0.007327 Val Loss: nan\n",
      "Epoch [6986/10000] Train Loss: 0.007123 Val Loss: nan\n",
      "Epoch [6987/10000] Train Loss: 0.007131 Val Loss: nan\n",
      "Epoch [6988/10000] Train Loss: 0.007200 Val Loss: nan\n",
      "Epoch [6989/10000] Train Loss: 0.007274 Val Loss: nan\n",
      "Epoch [6990/10000] Train Loss: 0.007250 Val Loss: nan\n",
      "Epoch [6991/10000] Train Loss: 0.007262 Val Loss: nan\n",
      "Epoch [6992/10000] Train Loss: 0.007325 Val Loss: nan\n",
      "Epoch [6993/10000] Train Loss: 0.007167 Val Loss: nan\n",
      "Epoch [6994/10000] Train Loss: 0.007161 Val Loss: nan\n",
      "Epoch [6995/10000] Train Loss: 0.007167 Val Loss: nan\n",
      "Epoch [6996/10000] Train Loss: 0.007135 Val Loss: nan\n",
      "Epoch [6997/10000] Train Loss: 0.007191 Val Loss: nan\n",
      "Epoch [6998/10000] Train Loss: 0.007234 Val Loss: nan\n",
      "Epoch [6999/10000] Train Loss: 0.007289 Val Loss: nan\n",
      "Epoch [7000/10000] Train Loss: 0.007049 Val Loss: nan\n",
      "Epoch [7001/10000] Train Loss: 0.007057 Val Loss: nan\n",
      "Epoch [7002/10000] Train Loss: 0.007158 Val Loss: nan\n",
      "Epoch [7003/10000] Train Loss: 0.007393 Val Loss: nan\n",
      "Epoch [7004/10000] Train Loss: 0.007206 Val Loss: nan\n",
      "Epoch [7005/10000] Train Loss: 0.007105 Val Loss: nan\n",
      "Epoch [7006/10000] Train Loss: 0.007341 Val Loss: nan\n",
      "Epoch [7007/10000] Train Loss: 0.007178 Val Loss: nan\n",
      "Epoch [7008/10000] Train Loss: 0.007345 Val Loss: nan\n",
      "Epoch [7009/10000] Train Loss: 0.007220 Val Loss: nan\n",
      "Epoch [7010/10000] Train Loss: 0.007162 Val Loss: nan\n",
      "Epoch [7011/10000] Train Loss: 0.007036 Val Loss: nan\n",
      "Epoch [7012/10000] Train Loss: 0.007137 Val Loss: nan\n",
      "Epoch [7013/10000] Train Loss: 0.007236 Val Loss: nan\n",
      "Epoch [7014/10000] Train Loss: 0.007147 Val Loss: nan\n",
      "Epoch [7015/10000] Train Loss: 0.007392 Val Loss: nan\n",
      "Epoch [7016/10000] Train Loss: 0.007124 Val Loss: nan\n",
      "Epoch [7017/10000] Train Loss: 0.007103 Val Loss: nan\n",
      "Epoch [7018/10000] Train Loss: 0.007229 Val Loss: nan\n",
      "Epoch [7019/10000] Train Loss: 0.007066 Val Loss: nan\n",
      "Epoch [7020/10000] Train Loss: 0.007221 Val Loss: nan\n",
      "Epoch [7021/10000] Train Loss: 0.007190 Val Loss: nan\n",
      "Epoch [7022/10000] Train Loss: 0.007128 Val Loss: nan\n",
      "Epoch [7023/10000] Train Loss: 0.007118 Val Loss: nan\n",
      "Epoch [7024/10000] Train Loss: 0.007217 Val Loss: nan\n",
      "Epoch [7025/10000] Train Loss: 0.007229 Val Loss: nan\n",
      "Epoch [7026/10000] Train Loss: 0.007093 Val Loss: nan\n",
      "Epoch [7027/10000] Train Loss: 0.007058 Val Loss: nan\n",
      "Epoch [7028/10000] Train Loss: 0.007099 Val Loss: nan\n",
      "Epoch [7029/10000] Train Loss: 0.007069 Val Loss: nan\n",
      "Epoch [7030/10000] Train Loss: 0.007121 Val Loss: nan\n",
      "Epoch [7031/10000] Train Loss: 0.007312 Val Loss: nan\n",
      "Epoch [7032/10000] Train Loss: 0.007083 Val Loss: nan\n",
      "Epoch [7033/10000] Train Loss: 0.007268 Val Loss: nan\n",
      "Epoch [7034/10000] Train Loss: 0.007073 Val Loss: nan\n",
      "Epoch [7035/10000] Train Loss: 0.007060 Val Loss: nan\n",
      "Epoch [7036/10000] Train Loss: 0.007238 Val Loss: nan\n",
      "Epoch [7037/10000] Train Loss: 0.007195 Val Loss: nan\n",
      "Epoch [7038/10000] Train Loss: 0.007120 Val Loss: nan\n",
      "Epoch [7039/10000] Train Loss: 0.007147 Val Loss: nan\n",
      "Epoch [7040/10000] Train Loss: 0.007094 Val Loss: nan\n",
      "Epoch [7041/10000] Train Loss: 0.007130 Val Loss: nan\n",
      "Epoch [7042/10000] Train Loss: 0.007070 Val Loss: nan\n",
      "Epoch [7043/10000] Train Loss: 0.007041 Val Loss: nan\n",
      "Epoch [7044/10000] Train Loss: 0.007076 Val Loss: nan\n",
      "Epoch [7045/10000] Train Loss: 0.007166 Val Loss: nan\n",
      "Epoch [7046/10000] Train Loss: 0.007110 Val Loss: nan\n",
      "Epoch [7047/10000] Train Loss: 0.007061 Val Loss: nan\n",
      "Epoch [7048/10000] Train Loss: 0.007180 Val Loss: nan\n",
      "Epoch [7049/10000] Train Loss: 0.007092 Val Loss: nan\n",
      "Epoch [7050/10000] Train Loss: 0.007111 Val Loss: nan\n",
      "Epoch [7051/10000] Train Loss: 0.007154 Val Loss: nan\n",
      "Epoch [7052/10000] Train Loss: 0.007109 Val Loss: nan\n",
      "Epoch [7053/10000] Train Loss: 0.007323 Val Loss: nan\n",
      "Epoch [7054/10000] Train Loss: 0.007106 Val Loss: nan\n",
      "Epoch [7055/10000] Train Loss: 0.007125 Val Loss: nan\n",
      "Epoch [7056/10000] Train Loss: 0.007057 Val Loss: nan\n",
      "Epoch [7057/10000] Train Loss: 0.007050 Val Loss: nan\n",
      "Epoch [7058/10000] Train Loss: 0.007199 Val Loss: nan\n",
      "Epoch [7059/10000] Train Loss: 0.007341 Val Loss: nan\n",
      "Epoch [7060/10000] Train Loss: 0.007322 Val Loss: nan\n",
      "Epoch [7061/10000] Train Loss: 0.007257 Val Loss: nan\n",
      "Epoch [7062/10000] Train Loss: 0.007193 Val Loss: nan\n",
      "Epoch [7063/10000] Train Loss: 0.007075 Val Loss: nan\n",
      "Epoch [7064/10000] Train Loss: 0.007182 Val Loss: nan\n",
      "Epoch [7065/10000] Train Loss: 0.007141 Val Loss: nan\n",
      "Epoch [7066/10000] Train Loss: 0.007209 Val Loss: nan\n",
      "Epoch [7067/10000] Train Loss: 0.007092 Val Loss: nan\n",
      "Epoch [7068/10000] Train Loss: 0.007188 Val Loss: nan\n",
      "Epoch [7069/10000] Train Loss: 0.007207 Val Loss: nan\n",
      "Epoch [7070/10000] Train Loss: 0.007019 Val Loss: nan\n",
      "Epoch [7071/10000] Train Loss: 0.007041 Val Loss: nan\n",
      "Epoch [7072/10000] Train Loss: 0.007042 Val Loss: nan\n",
      "Epoch [7073/10000] Train Loss: 0.007072 Val Loss: nan\n",
      "Epoch [7074/10000] Train Loss: 0.007262 Val Loss: nan\n",
      "Epoch [7075/10000] Train Loss: 0.007150 Val Loss: nan\n",
      "Epoch [7076/10000] Train Loss: 0.007170 Val Loss: nan\n",
      "Epoch [7077/10000] Train Loss: 0.007271 Val Loss: nan\n",
      "Epoch [7078/10000] Train Loss: 0.007116 Val Loss: nan\n",
      "Epoch [7079/10000] Train Loss: 0.007129 Val Loss: nan\n",
      "Epoch [7080/10000] Train Loss: 0.007096 Val Loss: nan\n",
      "Epoch [7081/10000] Train Loss: 0.007189 Val Loss: nan\n",
      "Epoch [7082/10000] Train Loss: 0.007346 Val Loss: nan\n",
      "Epoch [7083/10000] Train Loss: 0.007145 Val Loss: nan\n",
      "Epoch [7084/10000] Train Loss: 0.007385 Val Loss: nan\n",
      "Epoch [7085/10000] Train Loss: 0.007148 Val Loss: nan\n",
      "Epoch [7086/10000] Train Loss: 0.007049 Val Loss: nan\n",
      "Epoch [7087/10000] Train Loss: 0.007172 Val Loss: nan\n",
      "Epoch [7088/10000] Train Loss: 0.007278 Val Loss: nan\n",
      "Epoch [7089/10000] Train Loss: 0.007283 Val Loss: nan\n",
      "Epoch [7090/10000] Train Loss: 0.007171 Val Loss: nan\n",
      "Epoch [7091/10000] Train Loss: 0.007183 Val Loss: nan\n",
      "Epoch [7092/10000] Train Loss: 0.007107 Val Loss: nan\n",
      "Epoch [7093/10000] Train Loss: 0.007118 Val Loss: nan\n",
      "Epoch [7094/10000] Train Loss: 0.007143 Val Loss: nan\n",
      "Epoch [7095/10000] Train Loss: 0.007085 Val Loss: nan\n",
      "Epoch [7096/10000] Train Loss: 0.007192 Val Loss: nan\n",
      "Epoch [7097/10000] Train Loss: 0.007216 Val Loss: nan\n",
      "Epoch [7098/10000] Train Loss: 0.007102 Val Loss: nan\n",
      "Epoch [7099/10000] Train Loss: 0.007157 Val Loss: nan\n",
      "Epoch [7100/10000] Train Loss: 0.007057 Val Loss: nan\n",
      "Epoch [7101/10000] Train Loss: 0.007196 Val Loss: nan\n",
      "Epoch [7102/10000] Train Loss: 0.007111 Val Loss: nan\n",
      "Epoch [7103/10000] Train Loss: 0.007118 Val Loss: nan\n",
      "Epoch [7104/10000] Train Loss: 0.007140 Val Loss: nan\n",
      "Epoch [7105/10000] Train Loss: 0.007161 Val Loss: nan\n",
      "Epoch [7106/10000] Train Loss: 0.007382 Val Loss: nan\n",
      "Epoch [7107/10000] Train Loss: 0.007110 Val Loss: nan\n",
      "Epoch [7108/10000] Train Loss: 0.007282 Val Loss: nan\n",
      "Epoch [7109/10000] Train Loss: 0.007288 Val Loss: nan\n",
      "Epoch [7110/10000] Train Loss: 0.007365 Val Loss: nan\n",
      "Epoch [7111/10000] Train Loss: 0.007119 Val Loss: nan\n",
      "Epoch [7112/10000] Train Loss: 0.007219 Val Loss: nan\n",
      "Epoch [7113/10000] Train Loss: 0.007196 Val Loss: nan\n",
      "Epoch [7114/10000] Train Loss: 0.007091 Val Loss: nan\n",
      "Epoch [7115/10000] Train Loss: 0.007037 Val Loss: nan\n",
      "Epoch [7116/10000] Train Loss: 0.007144 Val Loss: nan\n",
      "Epoch [7117/10000] Train Loss: 0.007143 Val Loss: nan\n",
      "Epoch [7118/10000] Train Loss: 0.007136 Val Loss: nan\n",
      "Epoch [7119/10000] Train Loss: 0.007119 Val Loss: nan\n",
      "Epoch [7120/10000] Train Loss: 0.007078 Val Loss: nan\n",
      "Epoch [7121/10000] Train Loss: 0.007068 Val Loss: nan\n",
      "Epoch [7122/10000] Train Loss: 0.007058 Val Loss: nan\n",
      "Epoch [7123/10000] Train Loss: 0.007047 Val Loss: nan\n",
      "Epoch [7124/10000] Train Loss: 0.007054 Val Loss: nan\n",
      "Epoch [7125/10000] Train Loss: 0.007319 Val Loss: nan\n",
      "Epoch [7126/10000] Train Loss: 0.007209 Val Loss: nan\n",
      "Epoch [7127/10000] Train Loss: 0.007232 Val Loss: nan\n",
      "Epoch [7128/10000] Train Loss: 0.007160 Val Loss: nan\n",
      "Epoch [7129/10000] Train Loss: 0.007173 Val Loss: nan\n",
      "Epoch [7130/10000] Train Loss: 0.007117 Val Loss: nan\n",
      "Epoch [7131/10000] Train Loss: 0.007204 Val Loss: nan\n",
      "Epoch [7132/10000] Train Loss: 0.007108 Val Loss: nan\n",
      "Epoch [7133/10000] Train Loss: 0.007169 Val Loss: nan\n",
      "Epoch [7134/10000] Train Loss: 0.007173 Val Loss: nan\n",
      "Epoch [7135/10000] Train Loss: 0.007447 Val Loss: nan\n",
      "Epoch [7136/10000] Train Loss: 0.007356 Val Loss: nan\n",
      "Epoch [7137/10000] Train Loss: 0.007147 Val Loss: nan\n",
      "Epoch [7138/10000] Train Loss: 0.007167 Val Loss: nan\n",
      "Epoch [7139/10000] Train Loss: 0.007084 Val Loss: nan\n",
      "Epoch [7140/10000] Train Loss: 0.007083 Val Loss: nan\n",
      "Epoch [7141/10000] Train Loss: 0.007124 Val Loss: nan\n",
      "Epoch [7142/10000] Train Loss: 0.007118 Val Loss: nan\n",
      "Epoch [7143/10000] Train Loss: 0.007086 Val Loss: nan\n",
      "Epoch [7144/10000] Train Loss: 0.007037 Val Loss: nan\n",
      "Epoch [7145/10000] Train Loss: 0.007154 Val Loss: nan\n",
      "Epoch [7146/10000] Train Loss: 0.007614 Val Loss: nan\n",
      "Epoch [7147/10000] Train Loss: 0.007183 Val Loss: nan\n",
      "Epoch [7148/10000] Train Loss: 0.007078 Val Loss: nan\n",
      "Epoch [7149/10000] Train Loss: 0.007345 Val Loss: nan\n",
      "Epoch [7150/10000] Train Loss: 0.007087 Val Loss: nan\n",
      "Epoch [7151/10000] Train Loss: 0.007343 Val Loss: nan\n",
      "Epoch [7152/10000] Train Loss: 0.007116 Val Loss: nan\n",
      "Epoch [7153/10000] Train Loss: 0.007306 Val Loss: nan\n",
      "Epoch [7154/10000] Train Loss: 0.007194 Val Loss: nan\n",
      "Epoch [7155/10000] Train Loss: 0.007180 Val Loss: nan\n",
      "Epoch [7156/10000] Train Loss: 0.007010 Val Loss: nan\n",
      "Epoch [7157/10000] Train Loss: 0.007195 Val Loss: nan\n",
      "Epoch [7158/10000] Train Loss: 0.007092 Val Loss: nan\n",
      "Epoch [7159/10000] Train Loss: 0.007380 Val Loss: nan\n",
      "Epoch [7160/10000] Train Loss: 0.007359 Val Loss: nan\n",
      "Epoch [7161/10000] Train Loss: 0.007202 Val Loss: nan\n",
      "Epoch [7162/10000] Train Loss: 0.007082 Val Loss: nan\n",
      "Epoch [7163/10000] Train Loss: 0.007318 Val Loss: nan\n",
      "Epoch [7164/10000] Train Loss: 0.007068 Val Loss: nan\n",
      "Epoch [7165/10000] Train Loss: 0.007184 Val Loss: nan\n",
      "Epoch [7166/10000] Train Loss: 0.007134 Val Loss: nan\n",
      "Epoch [7167/10000] Train Loss: 0.007086 Val Loss: nan\n",
      "Epoch [7168/10000] Train Loss: 0.007063 Val Loss: nan\n",
      "Epoch [7169/10000] Train Loss: 0.007063 Val Loss: nan\n",
      "Epoch [7170/10000] Train Loss: 0.007136 Val Loss: nan\n",
      "Epoch [7171/10000] Train Loss: 0.007081 Val Loss: nan\n",
      "Epoch [7172/10000] Train Loss: 0.007114 Val Loss: nan\n",
      "Epoch [7173/10000] Train Loss: 0.007191 Val Loss: nan\n",
      "Epoch [7174/10000] Train Loss: 0.007154 Val Loss: nan\n",
      "Epoch [7175/10000] Train Loss: 0.007327 Val Loss: nan\n",
      "Epoch [7176/10000] Train Loss: 0.007427 Val Loss: nan\n",
      "Epoch [7177/10000] Train Loss: 0.007404 Val Loss: nan\n",
      "Epoch [7178/10000] Train Loss: 0.007272 Val Loss: nan\n",
      "Epoch [7179/10000] Train Loss: 0.007028 Val Loss: nan\n",
      "Epoch [7180/10000] Train Loss: 0.007178 Val Loss: nan\n",
      "Epoch [7181/10000] Train Loss: 0.007295 Val Loss: nan\n",
      "Epoch [7182/10000] Train Loss: 0.007117 Val Loss: nan\n",
      "Epoch [7183/10000] Train Loss: 0.007222 Val Loss: nan\n",
      "Epoch [7184/10000] Train Loss: 0.007171 Val Loss: nan\n",
      "Epoch [7185/10000] Train Loss: 0.007274 Val Loss: nan\n",
      "Epoch [7186/10000] Train Loss: 0.007122 Val Loss: nan\n",
      "Epoch [7187/10000] Train Loss: 0.007118 Val Loss: nan\n",
      "Epoch [7188/10000] Train Loss: 0.007133 Val Loss: nan\n",
      "Epoch [7189/10000] Train Loss: 0.007041 Val Loss: nan\n",
      "Epoch [7190/10000] Train Loss: 0.007196 Val Loss: nan\n",
      "Epoch [7191/10000] Train Loss: 0.007121 Val Loss: nan\n",
      "Epoch [7192/10000] Train Loss: 0.007248 Val Loss: nan\n",
      "Epoch [7193/10000] Train Loss: 0.007264 Val Loss: nan\n",
      "Epoch [7194/10000] Train Loss: 0.007317 Val Loss: nan\n",
      "Epoch [7195/10000] Train Loss: 0.007195 Val Loss: nan\n",
      "Epoch [7196/10000] Train Loss: 0.007046 Val Loss: nan\n",
      "Epoch [7197/10000] Train Loss: 0.007057 Val Loss: nan\n",
      "Epoch [7198/10000] Train Loss: 0.007067 Val Loss: nan\n",
      "Epoch [7199/10000] Train Loss: 0.007223 Val Loss: nan\n",
      "Epoch [7200/10000] Train Loss: 0.007376 Val Loss: nan\n",
      "Epoch [7201/10000] Train Loss: 0.007046 Val Loss: nan\n",
      "Epoch [7202/10000] Train Loss: 0.007111 Val Loss: nan\n",
      "Epoch [7203/10000] Train Loss: 0.007073 Val Loss: nan\n",
      "Epoch [7204/10000] Train Loss: 0.007392 Val Loss: nan\n",
      "Epoch [7205/10000] Train Loss: 0.007143 Val Loss: nan\n",
      "Epoch [7206/10000] Train Loss: 0.007147 Val Loss: nan\n",
      "Epoch [7207/10000] Train Loss: 0.007307 Val Loss: nan\n",
      "Epoch [7208/10000] Train Loss: 0.007209 Val Loss: nan\n",
      "Epoch [7209/10000] Train Loss: 0.007129 Val Loss: nan\n",
      "Epoch [7210/10000] Train Loss: 0.007149 Val Loss: nan\n",
      "Epoch [7211/10000] Train Loss: 0.007049 Val Loss: nan\n",
      "Epoch [7212/10000] Train Loss: 0.007081 Val Loss: nan\n",
      "Epoch [7213/10000] Train Loss: 0.007161 Val Loss: nan\n",
      "Epoch [7214/10000] Train Loss: 0.007141 Val Loss: nan\n",
      "Epoch [7215/10000] Train Loss: 0.007149 Val Loss: nan\n",
      "Epoch [7216/10000] Train Loss: 0.007089 Val Loss: nan\n",
      "Epoch [7217/10000] Train Loss: 0.007024 Val Loss: nan\n",
      "Epoch [7218/10000] Train Loss: 0.007144 Val Loss: nan\n",
      "Epoch [7219/10000] Train Loss: 0.007018 Val Loss: nan\n",
      "Epoch [7220/10000] Train Loss: 0.007238 Val Loss: nan\n",
      "Epoch [7221/10000] Train Loss: 0.007120 Val Loss: nan\n",
      "Epoch [7222/10000] Train Loss: 0.007071 Val Loss: nan\n",
      "Epoch [7223/10000] Train Loss: 0.007242 Val Loss: nan\n",
      "Epoch [7224/10000] Train Loss: 0.007180 Val Loss: nan\n",
      "Epoch [7225/10000] Train Loss: 0.007119 Val Loss: nan\n",
      "Epoch [7226/10000] Train Loss: 0.007048 Val Loss: nan\n",
      "Epoch [7227/10000] Train Loss: 0.007070 Val Loss: nan\n",
      "Epoch [7228/10000] Train Loss: 0.007076 Val Loss: nan\n",
      "Epoch [7229/10000] Train Loss: 0.007217 Val Loss: nan\n",
      "Epoch [7230/10000] Train Loss: 0.007110 Val Loss: nan\n",
      "Epoch [7231/10000] Train Loss: 0.007252 Val Loss: nan\n",
      "Epoch [7232/10000] Train Loss: 0.007196 Val Loss: nan\n",
      "Epoch [7233/10000] Train Loss: 0.007283 Val Loss: nan\n",
      "Epoch [7234/10000] Train Loss: 0.007405 Val Loss: nan\n",
      "Epoch [7235/10000] Train Loss: 0.007207 Val Loss: nan\n",
      "Epoch [7236/10000] Train Loss: 0.007075 Val Loss: nan\n",
      "Epoch [7237/10000] Train Loss: 0.007265 Val Loss: nan\n",
      "Epoch [7238/10000] Train Loss: 0.007107 Val Loss: nan\n",
      "Epoch [7239/10000] Train Loss: 0.007421 Val Loss: nan\n",
      "Epoch [7240/10000] Train Loss: 0.007087 Val Loss: nan\n",
      "Epoch [7241/10000] Train Loss: 0.007280 Val Loss: nan\n",
      "Epoch [7242/10000] Train Loss: 0.007110 Val Loss: nan\n",
      "Epoch [7243/10000] Train Loss: 0.007245 Val Loss: nan\n",
      "Epoch [7244/10000] Train Loss: 0.007213 Val Loss: nan\n",
      "Epoch [7245/10000] Train Loss: 0.007079 Val Loss: nan\n",
      "Epoch [7246/10000] Train Loss: 0.007265 Val Loss: nan\n",
      "Epoch [7247/10000] Train Loss: 0.007196 Val Loss: nan\n",
      "Epoch [7248/10000] Train Loss: 0.007163 Val Loss: nan\n",
      "Epoch [7249/10000] Train Loss: 0.007004 Val Loss: nan\n",
      "Epoch [7250/10000] Train Loss: 0.007206 Val Loss: nan\n",
      "Epoch [7251/10000] Train Loss: 0.007060 Val Loss: nan\n",
      "Epoch [7252/10000] Train Loss: 0.007263 Val Loss: nan\n",
      "Epoch [7253/10000] Train Loss: 0.007046 Val Loss: nan\n",
      "Epoch [7254/10000] Train Loss: 0.007194 Val Loss: nan\n",
      "Epoch [7255/10000] Train Loss: 0.007376 Val Loss: nan\n",
      "Epoch [7256/10000] Train Loss: 0.007093 Val Loss: nan\n",
      "Epoch [7257/10000] Train Loss: 0.007164 Val Loss: nan\n",
      "Epoch [7258/10000] Train Loss: 0.007093 Val Loss: nan\n",
      "Epoch [7259/10000] Train Loss: 0.007175 Val Loss: nan\n",
      "Epoch [7260/10000] Train Loss: 0.007073 Val Loss: nan\n",
      "Epoch [7261/10000] Train Loss: 0.007304 Val Loss: nan\n",
      "Epoch [7262/10000] Train Loss: 0.007045 Val Loss: nan\n",
      "Epoch [7263/10000] Train Loss: 0.007312 Val Loss: nan\n",
      "Epoch [7264/10000] Train Loss: 0.007129 Val Loss: nan\n",
      "Epoch [7265/10000] Train Loss: 0.007073 Val Loss: nan\n",
      "Epoch [7266/10000] Train Loss: 0.007084 Val Loss: nan\n",
      "Epoch [7267/10000] Train Loss: 0.007228 Val Loss: nan\n",
      "Epoch [7268/10000] Train Loss: 0.007221 Val Loss: nan\n",
      "Epoch [7269/10000] Train Loss: 0.007118 Val Loss: nan\n",
      "Epoch [7270/10000] Train Loss: 0.007283 Val Loss: nan\n",
      "Epoch [7271/10000] Train Loss: 0.007304 Val Loss: nan\n",
      "Epoch [7272/10000] Train Loss: 0.007062 Val Loss: nan\n",
      "Epoch [7273/10000] Train Loss: 0.007161 Val Loss: nan\n",
      "Epoch [7274/10000] Train Loss: 0.007022 Val Loss: nan\n",
      "Epoch [7275/10000] Train Loss: 0.007073 Val Loss: nan\n",
      "Epoch [7276/10000] Train Loss: 0.007443 Val Loss: nan\n",
      "Epoch [7277/10000] Train Loss: 0.007100 Val Loss: nan\n",
      "Epoch [7278/10000] Train Loss: 0.007040 Val Loss: nan\n",
      "Epoch [7279/10000] Train Loss: 0.007172 Val Loss: nan\n",
      "Epoch [7280/10000] Train Loss: 0.007146 Val Loss: nan\n",
      "Epoch [7281/10000] Train Loss: 0.007172 Val Loss: nan\n",
      "Epoch [7282/10000] Train Loss: 0.007184 Val Loss: nan\n",
      "Epoch [7283/10000] Train Loss: 0.007167 Val Loss: nan\n",
      "Epoch [7284/10000] Train Loss: 0.007078 Val Loss: nan\n",
      "Epoch [7285/10000] Train Loss: 0.007037 Val Loss: nan\n",
      "Epoch [7286/10000] Train Loss: 0.007131 Val Loss: nan\n",
      "Epoch [7287/10000] Train Loss: 0.007228 Val Loss: nan\n",
      "Epoch [7288/10000] Train Loss: 0.007103 Val Loss: nan\n",
      "Epoch [7289/10000] Train Loss: 0.007466 Val Loss: nan\n",
      "Epoch [7290/10000] Train Loss: 0.007122 Val Loss: nan\n",
      "Epoch [7291/10000] Train Loss: 0.007358 Val Loss: nan\n",
      "Epoch [7292/10000] Train Loss: 0.007206 Val Loss: nan\n",
      "Epoch [7293/10000] Train Loss: 0.007060 Val Loss: nan\n",
      "Epoch [7294/10000] Train Loss: 0.007018 Val Loss: nan\n",
      "Epoch [7295/10000] Train Loss: 0.007035 Val Loss: nan\n",
      "Epoch [7296/10000] Train Loss: 0.007108 Val Loss: nan\n",
      "Epoch [7297/10000] Train Loss: 0.007163 Val Loss: nan\n",
      "Epoch [7298/10000] Train Loss: 0.007151 Val Loss: nan\n",
      "Epoch [7299/10000] Train Loss: 0.007209 Val Loss: nan\n",
      "Epoch [7300/10000] Train Loss: 0.007052 Val Loss: nan\n",
      "Epoch [7301/10000] Train Loss: 0.007041 Val Loss: nan\n",
      "Epoch [7302/10000] Train Loss: 0.007055 Val Loss: nan\n",
      "Epoch [7303/10000] Train Loss: 0.007271 Val Loss: nan\n",
      "Epoch [7304/10000] Train Loss: 0.007059 Val Loss: nan\n",
      "Epoch [7305/10000] Train Loss: 0.007227 Val Loss: nan\n",
      "Epoch [7306/10000] Train Loss: 0.007023 Val Loss: nan\n",
      "Epoch [7307/10000] Train Loss: 0.007070 Val Loss: nan\n",
      "Epoch [7308/10000] Train Loss: 0.007257 Val Loss: nan\n",
      "Epoch [7309/10000] Train Loss: 0.007107 Val Loss: nan\n",
      "Epoch [7310/10000] Train Loss: 0.007039 Val Loss: nan\n",
      "Epoch [7311/10000] Train Loss: 0.007062 Val Loss: nan\n",
      "Epoch [7312/10000] Train Loss: 0.007240 Val Loss: nan\n",
      "Epoch [7313/10000] Train Loss: 0.007074 Val Loss: nan\n",
      "Epoch [7314/10000] Train Loss: 0.007085 Val Loss: nan\n",
      "Epoch [7315/10000] Train Loss: 0.007351 Val Loss: nan\n",
      "Epoch [7316/10000] Train Loss: 0.007083 Val Loss: nan\n",
      "Epoch [7317/10000] Train Loss: 0.007073 Val Loss: nan\n",
      "Epoch [7318/10000] Train Loss: 0.007107 Val Loss: nan\n",
      "Epoch [7319/10000] Train Loss: 0.006988 Val Loss: nan\n",
      "Epoch [7320/10000] Train Loss: 0.007013 Val Loss: nan\n",
      "Epoch [7321/10000] Train Loss: 0.007130 Val Loss: nan\n",
      "Epoch [7322/10000] Train Loss: 0.007117 Val Loss: nan\n",
      "Epoch [7323/10000] Train Loss: 0.007229 Val Loss: nan\n",
      "Epoch [7324/10000] Train Loss: 0.007162 Val Loss: nan\n",
      "Epoch [7325/10000] Train Loss: 0.007148 Val Loss: nan\n",
      "Epoch [7326/10000] Train Loss: 0.007087 Val Loss: nan\n",
      "Epoch [7327/10000] Train Loss: 0.007095 Val Loss: nan\n",
      "Epoch [7328/10000] Train Loss: 0.007118 Val Loss: nan\n",
      "Epoch [7329/10000] Train Loss: 0.007059 Val Loss: nan\n",
      "Epoch [7330/10000] Train Loss: 0.006994 Val Loss: nan\n",
      "Epoch [7331/10000] Train Loss: 0.007160 Val Loss: nan\n",
      "Epoch [7332/10000] Train Loss: 0.007123 Val Loss: nan\n",
      "Epoch [7333/10000] Train Loss: 0.007035 Val Loss: nan\n",
      "Epoch [7334/10000] Train Loss: 0.007198 Val Loss: nan\n",
      "Epoch [7335/10000] Train Loss: 0.007065 Val Loss: nan\n",
      "Epoch [7336/10000] Train Loss: 0.007241 Val Loss: nan\n",
      "Epoch [7337/10000] Train Loss: 0.007095 Val Loss: nan\n",
      "Epoch [7338/10000] Train Loss: 0.007192 Val Loss: nan\n",
      "Epoch [7339/10000] Train Loss: 0.007313 Val Loss: nan\n",
      "Epoch [7340/10000] Train Loss: 0.007189 Val Loss: nan\n",
      "Epoch [7341/10000] Train Loss: 0.007090 Val Loss: nan\n",
      "Epoch [7342/10000] Train Loss: 0.007370 Val Loss: nan\n",
      "Epoch [7343/10000] Train Loss: 0.007064 Val Loss: nan\n",
      "Epoch [7344/10000] Train Loss: 0.007093 Val Loss: nan\n",
      "Epoch [7345/10000] Train Loss: 0.007033 Val Loss: nan\n",
      "Epoch [7346/10000] Train Loss: 0.007048 Val Loss: nan\n",
      "Epoch [7347/10000] Train Loss: 0.007100 Val Loss: nan\n",
      "Epoch [7348/10000] Train Loss: 0.007208 Val Loss: nan\n",
      "Epoch [7349/10000] Train Loss: 0.007174 Val Loss: nan\n",
      "Epoch [7350/10000] Train Loss: 0.007156 Val Loss: nan\n",
      "Epoch [7351/10000] Train Loss: 0.007187 Val Loss: nan\n",
      "Epoch [7352/10000] Train Loss: 0.007126 Val Loss: nan\n",
      "Epoch [7353/10000] Train Loss: 0.007117 Val Loss: nan\n",
      "Epoch [7354/10000] Train Loss: 0.007140 Val Loss: nan\n",
      "Epoch [7355/10000] Train Loss: 0.006995 Val Loss: nan\n",
      "Epoch [7356/10000] Train Loss: 0.007163 Val Loss: nan\n",
      "Epoch [7357/10000] Train Loss: 0.007190 Val Loss: nan\n",
      "Epoch [7358/10000] Train Loss: 0.007295 Val Loss: nan\n",
      "Epoch [7359/10000] Train Loss: 0.007309 Val Loss: nan\n",
      "Epoch [7360/10000] Train Loss: 0.007130 Val Loss: nan\n",
      "Epoch [7361/10000] Train Loss: 0.007214 Val Loss: nan\n",
      "Epoch [7362/10000] Train Loss: 0.007085 Val Loss: nan\n",
      "Epoch [7363/10000] Train Loss: 0.007105 Val Loss: nan\n",
      "Epoch [7364/10000] Train Loss: 0.007177 Val Loss: nan\n",
      "Epoch [7365/10000] Train Loss: 0.007064 Val Loss: nan\n",
      "Epoch [7366/10000] Train Loss: 0.007263 Val Loss: nan\n",
      "Epoch [7367/10000] Train Loss: 0.007144 Val Loss: nan\n",
      "Epoch [7368/10000] Train Loss: 0.007036 Val Loss: nan\n",
      "Epoch [7369/10000] Train Loss: 0.007196 Val Loss: nan\n",
      "Epoch [7370/10000] Train Loss: 0.007071 Val Loss: nan\n",
      "Epoch [7371/10000] Train Loss: 0.007118 Val Loss: nan\n",
      "Epoch [7372/10000] Train Loss: 0.007130 Val Loss: nan\n",
      "Epoch [7373/10000] Train Loss: 0.007116 Val Loss: nan\n",
      "Epoch [7374/10000] Train Loss: 0.007009 Val Loss: nan\n",
      "Epoch [7375/10000] Train Loss: 0.007108 Val Loss: nan\n",
      "Epoch [7376/10000] Train Loss: 0.007054 Val Loss: nan\n",
      "Epoch [7377/10000] Train Loss: 0.007117 Val Loss: nan\n",
      "Epoch [7378/10000] Train Loss: 0.007214 Val Loss: nan\n",
      "Epoch [7379/10000] Train Loss: 0.007105 Val Loss: nan\n",
      "Epoch [7380/10000] Train Loss: 0.007110 Val Loss: nan\n",
      "Epoch [7381/10000] Train Loss: 0.007244 Val Loss: nan\n",
      "Epoch [7382/10000] Train Loss: 0.007161 Val Loss: nan\n",
      "Epoch [7383/10000] Train Loss: 0.007027 Val Loss: nan\n",
      "Epoch [7384/10000] Train Loss: 0.007066 Val Loss: nan\n",
      "Epoch [7385/10000] Train Loss: 0.007277 Val Loss: nan\n",
      "Epoch [7386/10000] Train Loss: 0.007184 Val Loss: nan\n",
      "Epoch [7387/10000] Train Loss: 0.007139 Val Loss: nan\n",
      "Epoch [7388/10000] Train Loss: 0.007194 Val Loss: nan\n",
      "Epoch [7389/10000] Train Loss: 0.007191 Val Loss: nan\n",
      "Epoch [7390/10000] Train Loss: 0.007165 Val Loss: nan\n",
      "Epoch [7391/10000] Train Loss: 0.007213 Val Loss: nan\n",
      "Epoch [7392/10000] Train Loss: 0.007025 Val Loss: nan\n",
      "Epoch [7393/10000] Train Loss: 0.007144 Val Loss: nan\n",
      "Epoch [7394/10000] Train Loss: 0.007154 Val Loss: nan\n",
      "Epoch [7395/10000] Train Loss: 0.007039 Val Loss: nan\n",
      "Epoch [7396/10000] Train Loss: 0.007100 Val Loss: nan\n",
      "Epoch [7397/10000] Train Loss: 0.007068 Val Loss: nan\n",
      "Epoch [7398/10000] Train Loss: 0.007072 Val Loss: nan\n",
      "Epoch [7399/10000] Train Loss: 0.007170 Val Loss: nan\n",
      "Epoch [7400/10000] Train Loss: 0.007136 Val Loss: nan\n",
      "Epoch [7401/10000] Train Loss: 0.007036 Val Loss: nan\n",
      "Epoch [7402/10000] Train Loss: 0.007128 Val Loss: nan\n",
      "Epoch [7403/10000] Train Loss: 0.007095 Val Loss: nan\n",
      "Epoch [7404/10000] Train Loss: 0.007266 Val Loss: nan\n",
      "Epoch [7405/10000] Train Loss: 0.007128 Val Loss: nan\n",
      "Epoch [7406/10000] Train Loss: 0.007197 Val Loss: nan\n",
      "Epoch [7407/10000] Train Loss: 0.007153 Val Loss: nan\n",
      "Epoch [7408/10000] Train Loss: 0.007199 Val Loss: nan\n",
      "Epoch [7409/10000] Train Loss: 0.007129 Val Loss: nan\n",
      "Epoch [7410/10000] Train Loss: 0.007032 Val Loss: nan\n",
      "Epoch [7411/10000] Train Loss: 0.007017 Val Loss: nan\n",
      "Epoch [7412/10000] Train Loss: 0.007023 Val Loss: nan\n",
      "Epoch [7413/10000] Train Loss: 0.007190 Val Loss: nan\n",
      "Epoch [7414/10000] Train Loss: 0.007204 Val Loss: nan\n",
      "Epoch [7415/10000] Train Loss: 0.007205 Val Loss: nan\n",
      "Epoch [7416/10000] Train Loss: 0.007267 Val Loss: nan\n",
      "Epoch [7417/10000] Train Loss: 0.007058 Val Loss: nan\n",
      "Epoch [7418/10000] Train Loss: 0.007085 Val Loss: nan\n",
      "Epoch [7419/10000] Train Loss: 0.007214 Val Loss: nan\n",
      "Epoch [7420/10000] Train Loss: 0.007076 Val Loss: nan\n",
      "Epoch [7421/10000] Train Loss: 0.007016 Val Loss: nan\n",
      "Epoch [7422/10000] Train Loss: 0.007076 Val Loss: nan\n",
      "Epoch [7423/10000] Train Loss: 0.007168 Val Loss: nan\n",
      "Epoch [7424/10000] Train Loss: 0.007022 Val Loss: nan\n",
      "Epoch [7425/10000] Train Loss: 0.007138 Val Loss: nan\n",
      "Epoch [7426/10000] Train Loss: 0.007152 Val Loss: nan\n",
      "Epoch [7427/10000] Train Loss: 0.007116 Val Loss: nan\n",
      "Epoch [7428/10000] Train Loss: 0.007153 Val Loss: nan\n",
      "Epoch [7429/10000] Train Loss: 0.007029 Val Loss: nan\n",
      "Epoch [7430/10000] Train Loss: 0.007225 Val Loss: nan\n",
      "Epoch [7431/10000] Train Loss: 0.007027 Val Loss: nan\n",
      "Epoch [7432/10000] Train Loss: 0.007089 Val Loss: nan\n",
      "Epoch [7433/10000] Train Loss: 0.007033 Val Loss: nan\n",
      "Epoch [7434/10000] Train Loss: 0.007034 Val Loss: nan\n",
      "Epoch [7435/10000] Train Loss: 0.006983 Val Loss: nan\n",
      "Epoch [7436/10000] Train Loss: 0.007133 Val Loss: nan\n",
      "Epoch [7437/10000] Train Loss: 0.007174 Val Loss: nan\n",
      "Epoch [7438/10000] Train Loss: 0.007247 Val Loss: nan\n",
      "Epoch [7439/10000] Train Loss: 0.007132 Val Loss: nan\n",
      "Epoch [7440/10000] Train Loss: 0.007097 Val Loss: nan\n",
      "Epoch [7441/10000] Train Loss: 0.007239 Val Loss: nan\n",
      "Epoch [7442/10000] Train Loss: 0.007170 Val Loss: nan\n",
      "Epoch [7443/10000] Train Loss: 0.007131 Val Loss: nan\n",
      "Epoch [7444/10000] Train Loss: 0.007139 Val Loss: nan\n",
      "Epoch [7445/10000] Train Loss: 0.007022 Val Loss: nan\n",
      "Epoch [7446/10000] Train Loss: 0.007375 Val Loss: nan\n",
      "Epoch [7447/10000] Train Loss: 0.007028 Val Loss: nan\n",
      "Epoch [7448/10000] Train Loss: 0.007245 Val Loss: nan\n",
      "Epoch [7449/10000] Train Loss: 0.007119 Val Loss: nan\n",
      "Epoch [7450/10000] Train Loss: 0.007052 Val Loss: nan\n",
      "Epoch [7451/10000] Train Loss: 0.007005 Val Loss: nan\n",
      "Epoch [7452/10000] Train Loss: 0.007180 Val Loss: nan\n",
      "Epoch [7453/10000] Train Loss: 0.007189 Val Loss: nan\n",
      "Epoch [7454/10000] Train Loss: 0.007027 Val Loss: nan\n",
      "Epoch [7455/10000] Train Loss: 0.007093 Val Loss: nan\n",
      "Epoch [7456/10000] Train Loss: 0.007182 Val Loss: nan\n",
      "Epoch [7457/10000] Train Loss: 0.007224 Val Loss: nan\n",
      "Epoch [7458/10000] Train Loss: 0.007200 Val Loss: nan\n",
      "Epoch [7459/10000] Train Loss: 0.007184 Val Loss: nan\n",
      "Epoch [7460/10000] Train Loss: 0.007123 Val Loss: nan\n",
      "Epoch [7461/10000] Train Loss: 0.007046 Val Loss: nan\n",
      "Epoch [7462/10000] Train Loss: 0.007051 Val Loss: nan\n",
      "Epoch [7463/10000] Train Loss: 0.007003 Val Loss: nan\n",
      "Epoch [7464/10000] Train Loss: 0.007142 Val Loss: nan\n",
      "Epoch [7465/10000] Train Loss: 0.007071 Val Loss: nan\n",
      "Epoch [7466/10000] Train Loss: 0.007300 Val Loss: nan\n",
      "Epoch [7467/10000] Train Loss: 0.007150 Val Loss: nan\n",
      "Epoch [7468/10000] Train Loss: 0.007054 Val Loss: nan\n",
      "Epoch [7469/10000] Train Loss: 0.007253 Val Loss: nan\n",
      "Epoch [7470/10000] Train Loss: 0.007132 Val Loss: nan\n",
      "Epoch [7471/10000] Train Loss: 0.007050 Val Loss: nan\n",
      "Epoch [7472/10000] Train Loss: 0.007027 Val Loss: nan\n",
      "Epoch [7473/10000] Train Loss: 0.007080 Val Loss: nan\n",
      "Epoch [7474/10000] Train Loss: 0.007276 Val Loss: nan\n",
      "Epoch [7475/10000] Train Loss: 0.007040 Val Loss: nan\n",
      "Epoch [7476/10000] Train Loss: 0.007263 Val Loss: nan\n",
      "Epoch [7477/10000] Train Loss: 0.007144 Val Loss: nan\n",
      "Epoch [7478/10000] Train Loss: 0.007229 Val Loss: nan\n",
      "Epoch [7479/10000] Train Loss: 0.007310 Val Loss: nan\n",
      "Epoch [7480/10000] Train Loss: 0.007117 Val Loss: nan\n",
      "Epoch [7481/10000] Train Loss: 0.007160 Val Loss: nan\n",
      "Epoch [7482/10000] Train Loss: 0.007116 Val Loss: nan\n",
      "Epoch [7483/10000] Train Loss: 0.007050 Val Loss: nan\n",
      "Epoch [7484/10000] Train Loss: 0.006988 Val Loss: nan\n",
      "Epoch [7485/10000] Train Loss: 0.007030 Val Loss: nan\n",
      "Epoch [7486/10000] Train Loss: 0.006979 Val Loss: nan\n",
      "Epoch [7487/10000] Train Loss: 0.007005 Val Loss: nan\n",
      "Epoch [7488/10000] Train Loss: 0.007026 Val Loss: nan\n",
      "Epoch [7489/10000] Train Loss: 0.007020 Val Loss: nan\n",
      "Epoch [7490/10000] Train Loss: 0.007129 Val Loss: nan\n",
      "Epoch [7491/10000] Train Loss: 0.007146 Val Loss: nan\n",
      "Epoch [7492/10000] Train Loss: 0.007154 Val Loss: nan\n",
      "Epoch [7493/10000] Train Loss: 0.007327 Val Loss: nan\n",
      "Epoch [7494/10000] Train Loss: 0.007163 Val Loss: nan\n",
      "Epoch [7495/10000] Train Loss: 0.007107 Val Loss: nan\n",
      "Epoch [7496/10000] Train Loss: 0.007091 Val Loss: nan\n",
      "Epoch [7497/10000] Train Loss: 0.007148 Val Loss: nan\n",
      "Epoch [7498/10000] Train Loss: 0.007120 Val Loss: nan\n",
      "Epoch [7499/10000] Train Loss: 0.007277 Val Loss: nan\n",
      "Epoch [7500/10000] Train Loss: 0.007151 Val Loss: nan\n",
      "Epoch [7501/10000] Train Loss: 0.007025 Val Loss: nan\n",
      "Epoch [7502/10000] Train Loss: 0.006998 Val Loss: nan\n",
      "Epoch [7503/10000] Train Loss: 0.007014 Val Loss: nan\n",
      "Epoch [7504/10000] Train Loss: 0.007099 Val Loss: nan\n",
      "Epoch [7505/10000] Train Loss: 0.007153 Val Loss: nan\n",
      "Epoch [7506/10000] Train Loss: 0.007058 Val Loss: nan\n",
      "Epoch [7507/10000] Train Loss: 0.007249 Val Loss: nan\n",
      "Epoch [7508/10000] Train Loss: 0.007314 Val Loss: nan\n",
      "Epoch [7509/10000] Train Loss: 0.007113 Val Loss: nan\n",
      "Epoch [7510/10000] Train Loss: 0.007256 Val Loss: nan\n",
      "Epoch [7511/10000] Train Loss: 0.007198 Val Loss: nan\n",
      "Epoch [7512/10000] Train Loss: 0.006992 Val Loss: nan\n",
      "Epoch [7513/10000] Train Loss: 0.007212 Val Loss: nan\n",
      "Epoch [7514/10000] Train Loss: 0.007143 Val Loss: nan\n",
      "Epoch [7515/10000] Train Loss: 0.007033 Val Loss: nan\n",
      "Epoch [7516/10000] Train Loss: 0.007137 Val Loss: nan\n",
      "Epoch [7517/10000] Train Loss: 0.007107 Val Loss: nan\n",
      "Epoch [7518/10000] Train Loss: 0.007012 Val Loss: nan\n",
      "Epoch [7519/10000] Train Loss: 0.007193 Val Loss: nan\n",
      "Epoch [7520/10000] Train Loss: 0.007184 Val Loss: nan\n",
      "Epoch [7521/10000] Train Loss: 0.007062 Val Loss: nan\n",
      "Epoch [7522/10000] Train Loss: 0.007137 Val Loss: nan\n",
      "Epoch [7523/10000] Train Loss: 0.007247 Val Loss: nan\n",
      "Epoch [7524/10000] Train Loss: 0.007047 Val Loss: nan\n",
      "Epoch [7525/10000] Train Loss: 0.007124 Val Loss: nan\n",
      "Epoch [7526/10000] Train Loss: 0.007100 Val Loss: nan\n",
      "Epoch [7527/10000] Train Loss: 0.007119 Val Loss: nan\n",
      "Epoch [7528/10000] Train Loss: 0.007079 Val Loss: nan\n",
      "Epoch [7529/10000] Train Loss: 0.007279 Val Loss: nan\n",
      "Epoch [7530/10000] Train Loss: 0.007110 Val Loss: nan\n",
      "Epoch [7531/10000] Train Loss: 0.007025 Val Loss: nan\n",
      "Epoch [7532/10000] Train Loss: 0.007161 Val Loss: nan\n",
      "Epoch [7533/10000] Train Loss: 0.007035 Val Loss: nan\n",
      "Epoch [7534/10000] Train Loss: 0.007146 Val Loss: nan\n",
      "Epoch [7535/10000] Train Loss: 0.007078 Val Loss: nan\n",
      "Epoch [7536/10000] Train Loss: 0.007127 Val Loss: nan\n",
      "Epoch [7537/10000] Train Loss: 0.007040 Val Loss: nan\n",
      "Epoch [7538/10000] Train Loss: 0.007016 Val Loss: nan\n",
      "Epoch [7539/10000] Train Loss: 0.007289 Val Loss: nan\n",
      "Epoch [7540/10000] Train Loss: 0.007113 Val Loss: nan\n",
      "Epoch [7541/10000] Train Loss: 0.007142 Val Loss: nan\n",
      "Epoch [7542/10000] Train Loss: 0.007467 Val Loss: nan\n",
      "Epoch [7543/10000] Train Loss: 0.007392 Val Loss: nan\n",
      "Epoch [7544/10000] Train Loss: 0.006991 Val Loss: nan\n",
      "Epoch [7545/10000] Train Loss: 0.007129 Val Loss: nan\n",
      "Epoch [7546/10000] Train Loss: 0.007091 Val Loss: nan\n",
      "Epoch [7547/10000] Train Loss: 0.007339 Val Loss: nan\n",
      "Epoch [7548/10000] Train Loss: 0.007123 Val Loss: nan\n",
      "Epoch [7549/10000] Train Loss: 0.007104 Val Loss: nan\n",
      "Epoch [7550/10000] Train Loss: 0.007102 Val Loss: nan\n",
      "Epoch [7551/10000] Train Loss: 0.007144 Val Loss: nan\n",
      "Epoch [7552/10000] Train Loss: 0.007239 Val Loss: nan\n",
      "Epoch [7553/10000] Train Loss: 0.007174 Val Loss: nan\n",
      "Epoch [7554/10000] Train Loss: 0.007033 Val Loss: nan\n",
      "Epoch [7555/10000] Train Loss: 0.006975 Val Loss: nan\n",
      "Epoch [7556/10000] Train Loss: 0.007019 Val Loss: nan\n",
      "Epoch [7557/10000] Train Loss: 0.007025 Val Loss: nan\n",
      "Epoch [7558/10000] Train Loss: 0.007089 Val Loss: nan\n",
      "Epoch [7559/10000] Train Loss: 0.007136 Val Loss: nan\n",
      "Epoch [7560/10000] Train Loss: 0.007074 Val Loss: nan\n",
      "Epoch [7561/10000] Train Loss: 0.006986 Val Loss: nan\n",
      "Epoch [7562/10000] Train Loss: 0.007246 Val Loss: nan\n",
      "Epoch [7563/10000] Train Loss: 0.007293 Val Loss: nan\n",
      "Epoch [7564/10000] Train Loss: 0.007160 Val Loss: nan\n",
      "Epoch [7565/10000] Train Loss: 0.007248 Val Loss: nan\n",
      "Epoch [7566/10000] Train Loss: 0.006993 Val Loss: nan\n",
      "Epoch [7567/10000] Train Loss: 0.007039 Val Loss: nan\n",
      "Epoch [7568/10000] Train Loss: 0.007032 Val Loss: nan\n",
      "Epoch [7569/10000] Train Loss: 0.007097 Val Loss: nan\n",
      "Epoch [7570/10000] Train Loss: 0.007001 Val Loss: nan\n",
      "Epoch [7571/10000] Train Loss: 0.007087 Val Loss: nan\n",
      "Epoch [7572/10000] Train Loss: 0.007027 Val Loss: nan\n",
      "Epoch [7573/10000] Train Loss: 0.007049 Val Loss: nan\n",
      "Epoch [7574/10000] Train Loss: 0.007222 Val Loss: nan\n",
      "Epoch [7575/10000] Train Loss: 0.007175 Val Loss: nan\n",
      "Epoch [7576/10000] Train Loss: 0.006995 Val Loss: nan\n",
      "Epoch [7577/10000] Train Loss: 0.007005 Val Loss: nan\n",
      "Epoch [7578/10000] Train Loss: 0.007014 Val Loss: nan\n",
      "Epoch [7579/10000] Train Loss: 0.007036 Val Loss: nan\n",
      "Epoch [7580/10000] Train Loss: 0.007093 Val Loss: nan\n",
      "Epoch [7581/10000] Train Loss: 0.007309 Val Loss: nan\n",
      "Epoch [7582/10000] Train Loss: 0.007364 Val Loss: nan\n",
      "Epoch [7583/10000] Train Loss: 0.007127 Val Loss: nan\n",
      "Epoch [7584/10000] Train Loss: 0.007112 Val Loss: nan\n",
      "Epoch [7585/10000] Train Loss: 0.006971 Val Loss: nan\n",
      "Epoch [7586/10000] Train Loss: 0.006990 Val Loss: nan\n",
      "Epoch [7587/10000] Train Loss: 0.006984 Val Loss: nan\n",
      "Epoch [7588/10000] Train Loss: 0.007014 Val Loss: nan\n",
      "Epoch [7589/10000] Train Loss: 0.007095 Val Loss: nan\n",
      "Epoch [7590/10000] Train Loss: 0.007164 Val Loss: nan\n",
      "Epoch [7591/10000] Train Loss: 0.007093 Val Loss: nan\n",
      "Epoch [7592/10000] Train Loss: 0.007146 Val Loss: nan\n",
      "Epoch [7593/10000] Train Loss: 0.007187 Val Loss: nan\n",
      "Epoch [7594/10000] Train Loss: 0.007092 Val Loss: nan\n",
      "Epoch [7595/10000] Train Loss: 0.007025 Val Loss: nan\n",
      "Epoch [7596/10000] Train Loss: 0.007009 Val Loss: nan\n",
      "Epoch [7597/10000] Train Loss: 0.007008 Val Loss: nan\n",
      "Epoch [7598/10000] Train Loss: 0.007004 Val Loss: nan\n",
      "Epoch [7599/10000] Train Loss: 0.007230 Val Loss: nan\n",
      "Epoch [7600/10000] Train Loss: 0.007208 Val Loss: nan\n",
      "Epoch [7601/10000] Train Loss: 0.007307 Val Loss: nan\n",
      "Epoch [7602/10000] Train Loss: 0.007072 Val Loss: nan\n",
      "Epoch [7603/10000] Train Loss: 0.007038 Val Loss: nan\n",
      "Epoch [7604/10000] Train Loss: 0.007136 Val Loss: nan\n",
      "Epoch [7605/10000] Train Loss: 0.007072 Val Loss: nan\n",
      "Epoch [7606/10000] Train Loss: 0.007137 Val Loss: nan\n",
      "Epoch [7607/10000] Train Loss: 0.007287 Val Loss: nan\n",
      "Epoch [7608/10000] Train Loss: 0.006963 Val Loss: nan\n",
      "Epoch [7609/10000] Train Loss: 0.007032 Val Loss: nan\n",
      "Epoch [7610/10000] Train Loss: 0.007016 Val Loss: nan\n",
      "Epoch [7611/10000] Train Loss: 0.007021 Val Loss: nan\n",
      "Epoch [7612/10000] Train Loss: 0.007096 Val Loss: nan\n",
      "Epoch [7613/10000] Train Loss: 0.007227 Val Loss: nan\n",
      "Epoch [7614/10000] Train Loss: 0.007208 Val Loss: nan\n",
      "Epoch [7615/10000] Train Loss: 0.007209 Val Loss: nan\n",
      "Epoch [7616/10000] Train Loss: 0.007019 Val Loss: nan\n",
      "Epoch [7617/10000] Train Loss: 0.007364 Val Loss: nan\n",
      "Epoch [7618/10000] Train Loss: 0.007130 Val Loss: nan\n",
      "Epoch [7619/10000] Train Loss: 0.007075 Val Loss: nan\n",
      "Epoch [7620/10000] Train Loss: 0.007025 Val Loss: nan\n",
      "Epoch [7621/10000] Train Loss: 0.007002 Val Loss: nan\n",
      "Epoch [7622/10000] Train Loss: 0.007057 Val Loss: nan\n",
      "Epoch [7623/10000] Train Loss: 0.007017 Val Loss: nan\n",
      "Epoch [7624/10000] Train Loss: 0.007253 Val Loss: nan\n",
      "Epoch [7625/10000] Train Loss: 0.007139 Val Loss: nan\n",
      "Epoch [7626/10000] Train Loss: 0.007100 Val Loss: nan\n",
      "Epoch [7627/10000] Train Loss: 0.007208 Val Loss: nan\n",
      "Epoch [7628/10000] Train Loss: 0.007289 Val Loss: nan\n",
      "Epoch [7629/10000] Train Loss: 0.007062 Val Loss: nan\n",
      "Epoch [7630/10000] Train Loss: 0.007055 Val Loss: nan\n",
      "Epoch [7631/10000] Train Loss: 0.007166 Val Loss: nan\n",
      "Epoch [7632/10000] Train Loss: 0.007052 Val Loss: nan\n",
      "Epoch [7633/10000] Train Loss: 0.007133 Val Loss: nan\n",
      "Epoch [7634/10000] Train Loss: 0.007381 Val Loss: nan\n",
      "Epoch [7635/10000] Train Loss: 0.007313 Val Loss: nan\n",
      "Epoch [7636/10000] Train Loss: 0.007135 Val Loss: nan\n",
      "Epoch [7637/10000] Train Loss: 0.007039 Val Loss: nan\n",
      "Epoch [7638/10000] Train Loss: 0.007486 Val Loss: nan\n",
      "Epoch [7639/10000] Train Loss: 0.006981 Val Loss: nan\n",
      "Epoch [7640/10000] Train Loss: 0.007132 Val Loss: nan\n",
      "Epoch [7641/10000] Train Loss: 0.007033 Val Loss: nan\n",
      "Epoch [7642/10000] Train Loss: 0.007057 Val Loss: nan\n",
      "Epoch [7643/10000] Train Loss: 0.007025 Val Loss: nan\n",
      "Epoch [7644/10000] Train Loss: 0.006997 Val Loss: nan\n",
      "Epoch [7645/10000] Train Loss: 0.007125 Val Loss: nan\n",
      "Epoch [7646/10000] Train Loss: 0.007128 Val Loss: nan\n",
      "Epoch [7647/10000] Train Loss: 0.007068 Val Loss: nan\n",
      "Epoch [7648/10000] Train Loss: 0.007090 Val Loss: nan\n",
      "Epoch [7649/10000] Train Loss: 0.007229 Val Loss: nan\n",
      "Epoch [7650/10000] Train Loss: 0.007010 Val Loss: nan\n",
      "Epoch [7651/10000] Train Loss: 0.007009 Val Loss: nan\n",
      "Epoch [7652/10000] Train Loss: 0.007036 Val Loss: nan\n",
      "Epoch [7653/10000] Train Loss: 0.006989 Val Loss: nan\n",
      "Epoch [7654/10000] Train Loss: 0.006990 Val Loss: nan\n",
      "Epoch [7655/10000] Train Loss: 0.007083 Val Loss: nan\n",
      "Epoch [7656/10000] Train Loss: 0.007189 Val Loss: nan\n",
      "Epoch [7657/10000] Train Loss: 0.007052 Val Loss: nan\n",
      "Epoch [7658/10000] Train Loss: 0.007046 Val Loss: nan\n",
      "Epoch [7659/10000] Train Loss: 0.007065 Val Loss: nan\n",
      "Epoch [7660/10000] Train Loss: 0.007058 Val Loss: nan\n",
      "Epoch [7661/10000] Train Loss: 0.007080 Val Loss: nan\n",
      "Epoch [7662/10000] Train Loss: 0.007357 Val Loss: nan\n",
      "Epoch [7663/10000] Train Loss: 0.007125 Val Loss: nan\n",
      "Epoch [7664/10000] Train Loss: 0.007101 Val Loss: nan\n",
      "Epoch [7665/10000] Train Loss: 0.007238 Val Loss: nan\n",
      "Epoch [7666/10000] Train Loss: 0.007111 Val Loss: nan\n",
      "Epoch [7667/10000] Train Loss: 0.007024 Val Loss: nan\n",
      "Epoch [7668/10000] Train Loss: 0.007004 Val Loss: nan\n",
      "Epoch [7669/10000] Train Loss: 0.007033 Val Loss: nan\n",
      "Epoch [7670/10000] Train Loss: 0.007109 Val Loss: nan\n",
      "Epoch [7671/10000] Train Loss: 0.007026 Val Loss: nan\n",
      "Epoch [7672/10000] Train Loss: 0.007128 Val Loss: nan\n",
      "Epoch [7673/10000] Train Loss: 0.007214 Val Loss: nan\n",
      "Epoch [7674/10000] Train Loss: 0.007045 Val Loss: nan\n",
      "Epoch [7675/10000] Train Loss: 0.006973 Val Loss: nan\n",
      "Epoch [7676/10000] Train Loss: 0.007161 Val Loss: nan\n",
      "Epoch [7677/10000] Train Loss: 0.007150 Val Loss: nan\n",
      "Epoch [7678/10000] Train Loss: 0.007274 Val Loss: nan\n",
      "Epoch [7679/10000] Train Loss: 0.007158 Val Loss: nan\n",
      "Epoch [7680/10000] Train Loss: 0.007081 Val Loss: nan\n",
      "Epoch [7681/10000] Train Loss: 0.007365 Val Loss: nan\n",
      "Epoch [7682/10000] Train Loss: 0.007209 Val Loss: nan\n",
      "Epoch [7683/10000] Train Loss: 0.007146 Val Loss: nan\n",
      "Epoch [7684/10000] Train Loss: 0.006987 Val Loss: nan\n",
      "Epoch [7685/10000] Train Loss: 0.007021 Val Loss: nan\n",
      "Epoch [7686/10000] Train Loss: 0.007112 Val Loss: nan\n",
      "Epoch [7687/10000] Train Loss: 0.006999 Val Loss: nan\n",
      "Epoch [7688/10000] Train Loss: 0.007023 Val Loss: nan\n",
      "Epoch [7689/10000] Train Loss: 0.007075 Val Loss: nan\n",
      "Epoch [7690/10000] Train Loss: 0.007000 Val Loss: nan\n",
      "Epoch [7691/10000] Train Loss: 0.006991 Val Loss: nan\n",
      "Epoch [7692/10000] Train Loss: 0.007112 Val Loss: nan\n",
      "Epoch [7693/10000] Train Loss: 0.007024 Val Loss: nan\n",
      "Epoch [7694/10000] Train Loss: 0.007138 Val Loss: nan\n",
      "Epoch [7695/10000] Train Loss: 0.007019 Val Loss: nan\n",
      "Epoch [7696/10000] Train Loss: 0.007004 Val Loss: nan\n",
      "Epoch [7697/10000] Train Loss: 0.007010 Val Loss: nan\n",
      "Epoch [7698/10000] Train Loss: 0.007199 Val Loss: nan\n",
      "Epoch [7699/10000] Train Loss: 0.007030 Val Loss: nan\n",
      "Epoch [7700/10000] Train Loss: 0.007037 Val Loss: nan\n",
      "Epoch [7701/10000] Train Loss: 0.007000 Val Loss: nan\n",
      "Epoch [7702/10000] Train Loss: 0.006986 Val Loss: nan\n",
      "Epoch [7703/10000] Train Loss: 0.007147 Val Loss: nan\n",
      "Epoch [7704/10000] Train Loss: 0.007156 Val Loss: nan\n",
      "Epoch [7705/10000] Train Loss: 0.006996 Val Loss: nan\n",
      "Epoch [7706/10000] Train Loss: 0.007152 Val Loss: nan\n",
      "Epoch [7707/10000] Train Loss: 0.007170 Val Loss: nan\n",
      "Epoch [7708/10000] Train Loss: 0.007030 Val Loss: nan\n",
      "Epoch [7709/10000] Train Loss: 0.007073 Val Loss: nan\n",
      "Epoch [7710/10000] Train Loss: 0.007012 Val Loss: nan\n",
      "Epoch [7711/10000] Train Loss: 0.007150 Val Loss: nan\n",
      "Epoch [7712/10000] Train Loss: 0.007153 Val Loss: nan\n",
      "Epoch [7713/10000] Train Loss: 0.007151 Val Loss: nan\n",
      "Epoch [7714/10000] Train Loss: 0.007113 Val Loss: nan\n",
      "Epoch [7715/10000] Train Loss: 0.007080 Val Loss: nan\n",
      "Epoch [7716/10000] Train Loss: 0.007000 Val Loss: nan\n",
      "Epoch [7717/10000] Train Loss: 0.006978 Val Loss: nan\n",
      "Epoch [7718/10000] Train Loss: 0.007101 Val Loss: nan\n",
      "Epoch [7719/10000] Train Loss: 0.007115 Val Loss: nan\n",
      "Epoch [7720/10000] Train Loss: 0.006973 Val Loss: nan\n",
      "Epoch [7721/10000] Train Loss: 0.007111 Val Loss: nan\n",
      "Epoch [7722/10000] Train Loss: 0.007171 Val Loss: nan\n",
      "Epoch [7723/10000] Train Loss: 0.007143 Val Loss: nan\n",
      "Epoch [7724/10000] Train Loss: 0.006959 Val Loss: nan\n",
      "Epoch [7725/10000] Train Loss: 0.007123 Val Loss: nan\n",
      "Epoch [7726/10000] Train Loss: 0.007147 Val Loss: nan\n",
      "Epoch [7727/10000] Train Loss: 0.007124 Val Loss: nan\n",
      "Epoch [7728/10000] Train Loss: 0.007243 Val Loss: nan\n",
      "Epoch [7729/10000] Train Loss: 0.007225 Val Loss: nan\n",
      "Epoch [7730/10000] Train Loss: 0.007147 Val Loss: nan\n",
      "Epoch [7731/10000] Train Loss: 0.006995 Val Loss: nan\n",
      "Epoch [7732/10000] Train Loss: 0.006971 Val Loss: nan\n",
      "Epoch [7733/10000] Train Loss: 0.007025 Val Loss: nan\n",
      "Epoch [7734/10000] Train Loss: 0.007059 Val Loss: nan\n",
      "Epoch [7735/10000] Train Loss: 0.007066 Val Loss: nan\n",
      "Epoch [7736/10000] Train Loss: 0.007093 Val Loss: nan\n",
      "Epoch [7737/10000] Train Loss: 0.007152 Val Loss: nan\n",
      "Epoch [7738/10000] Train Loss: 0.007020 Val Loss: nan\n",
      "Epoch [7739/10000] Train Loss: 0.007370 Val Loss: nan\n",
      "Epoch [7740/10000] Train Loss: 0.007009 Val Loss: nan\n",
      "Epoch [7741/10000] Train Loss: 0.007087 Val Loss: nan\n",
      "Epoch [7742/10000] Train Loss: 0.006992 Val Loss: nan\n",
      "Epoch [7743/10000] Train Loss: 0.006963 Val Loss: nan\n",
      "Epoch [7744/10000] Train Loss: 0.007108 Val Loss: nan\n",
      "Epoch [7745/10000] Train Loss: 0.007042 Val Loss: nan\n",
      "Epoch [7746/10000] Train Loss: 0.006989 Val Loss: nan\n",
      "Epoch [7747/10000] Train Loss: 0.007244 Val Loss: nan\n",
      "Epoch [7748/10000] Train Loss: 0.006998 Val Loss: nan\n",
      "Epoch [7749/10000] Train Loss: 0.007005 Val Loss: nan\n",
      "Epoch [7750/10000] Train Loss: 0.006968 Val Loss: nan\n",
      "Epoch [7751/10000] Train Loss: 0.007092 Val Loss: nan\n",
      "Epoch [7752/10000] Train Loss: 0.007101 Val Loss: nan\n",
      "Epoch [7753/10000] Train Loss: 0.007065 Val Loss: nan\n",
      "Epoch [7754/10000] Train Loss: 0.007082 Val Loss: nan\n",
      "Epoch [7755/10000] Train Loss: 0.007062 Val Loss: nan\n",
      "Epoch [7756/10000] Train Loss: 0.007147 Val Loss: nan\n",
      "Epoch [7757/10000] Train Loss: 0.007109 Val Loss: nan\n",
      "Epoch [7758/10000] Train Loss: 0.007002 Val Loss: nan\n",
      "Epoch [7759/10000] Train Loss: 0.007038 Val Loss: nan\n",
      "Epoch [7760/10000] Train Loss: 0.007036 Val Loss: nan\n",
      "Epoch [7761/10000] Train Loss: 0.006978 Val Loss: nan\n",
      "Epoch [7762/10000] Train Loss: 0.006974 Val Loss: nan\n",
      "Epoch [7763/10000] Train Loss: 0.007098 Val Loss: nan\n",
      "Epoch [7764/10000] Train Loss: 0.007124 Val Loss: nan\n",
      "Epoch [7765/10000] Train Loss: 0.006976 Val Loss: nan\n",
      "Epoch [7766/10000] Train Loss: 0.006984 Val Loss: nan\n",
      "Epoch [7767/10000] Train Loss: 0.007016 Val Loss: nan\n",
      "Epoch [7768/10000] Train Loss: 0.007086 Val Loss: nan\n",
      "Epoch [7769/10000] Train Loss: 0.007106 Val Loss: nan\n",
      "Epoch [7770/10000] Train Loss: 0.007227 Val Loss: nan\n",
      "Epoch [7771/10000] Train Loss: 0.006978 Val Loss: nan\n",
      "Epoch [7772/10000] Train Loss: 0.007021 Val Loss: nan\n",
      "Epoch [7773/10000] Train Loss: 0.007187 Val Loss: nan\n",
      "Epoch [7774/10000] Train Loss: 0.007058 Val Loss: nan\n",
      "Epoch [7775/10000] Train Loss: 0.007008 Val Loss: nan\n",
      "Epoch [7776/10000] Train Loss: 0.006998 Val Loss: nan\n",
      "Epoch [7777/10000] Train Loss: 0.007179 Val Loss: nan\n",
      "Epoch [7778/10000] Train Loss: 0.007005 Val Loss: nan\n",
      "Epoch [7779/10000] Train Loss: 0.006997 Val Loss: nan\n",
      "Epoch [7780/10000] Train Loss: 0.007183 Val Loss: nan\n",
      "Epoch [7781/10000] Train Loss: 0.007026 Val Loss: nan\n",
      "Epoch [7782/10000] Train Loss: 0.007116 Val Loss: nan\n",
      "Epoch [7783/10000] Train Loss: 0.007002 Val Loss: nan\n",
      "Epoch [7784/10000] Train Loss: 0.007020 Val Loss: nan\n",
      "Epoch [7785/10000] Train Loss: 0.007090 Val Loss: nan\n",
      "Epoch [7786/10000] Train Loss: 0.007030 Val Loss: nan\n",
      "Epoch [7787/10000] Train Loss: 0.007147 Val Loss: nan\n",
      "Epoch [7788/10000] Train Loss: 0.006984 Val Loss: nan\n",
      "Epoch [7789/10000] Train Loss: 0.006932 Val Loss: nan\n",
      "Epoch [7790/10000] Train Loss: 0.007007 Val Loss: nan\n",
      "Epoch [7791/10000] Train Loss: 0.006993 Val Loss: nan\n",
      "Epoch [7792/10000] Train Loss: 0.006984 Val Loss: nan\n",
      "Epoch [7793/10000] Train Loss: 0.007018 Val Loss: nan\n",
      "Epoch [7794/10000] Train Loss: 0.007101 Val Loss: nan\n",
      "Epoch [7795/10000] Train Loss: 0.007078 Val Loss: nan\n",
      "Epoch [7796/10000] Train Loss: 0.007127 Val Loss: nan\n",
      "Epoch [7797/10000] Train Loss: 0.007059 Val Loss: nan\n",
      "Epoch [7798/10000] Train Loss: 0.007028 Val Loss: nan\n",
      "Epoch [7799/10000] Train Loss: 0.007047 Val Loss: nan\n",
      "Epoch [7800/10000] Train Loss: 0.007131 Val Loss: nan\n",
      "Epoch [7801/10000] Train Loss: 0.007061 Val Loss: nan\n",
      "Epoch [7802/10000] Train Loss: 0.007174 Val Loss: nan\n",
      "Epoch [7803/10000] Train Loss: 0.007085 Val Loss: nan\n",
      "Epoch [7804/10000] Train Loss: 0.007091 Val Loss: nan\n",
      "Epoch [7805/10000] Train Loss: 0.007119 Val Loss: nan\n",
      "Epoch [7806/10000] Train Loss: 0.007065 Val Loss: nan\n",
      "Epoch [7807/10000] Train Loss: 0.007158 Val Loss: nan\n",
      "Epoch [7808/10000] Train Loss: 0.006965 Val Loss: nan\n",
      "Epoch [7809/10000] Train Loss: 0.007064 Val Loss: nan\n",
      "Epoch [7810/10000] Train Loss: 0.006958 Val Loss: nan\n",
      "Epoch [7811/10000] Train Loss: 0.007062 Val Loss: nan\n",
      "Epoch [7812/10000] Train Loss: 0.007245 Val Loss: nan\n",
      "Epoch [7813/10000] Train Loss: 0.007018 Val Loss: nan\n",
      "Epoch [7814/10000] Train Loss: 0.007274 Val Loss: nan\n",
      "Epoch [7815/10000] Train Loss: 0.007019 Val Loss: nan\n",
      "Epoch [7816/10000] Train Loss: 0.006972 Val Loss: nan\n",
      "Epoch [7817/10000] Train Loss: 0.007019 Val Loss: nan\n",
      "Epoch [7818/10000] Train Loss: 0.007303 Val Loss: nan\n",
      "Epoch [7819/10000] Train Loss: 0.007048 Val Loss: nan\n",
      "Epoch [7820/10000] Train Loss: 0.006943 Val Loss: nan\n",
      "Epoch [7821/10000] Train Loss: 0.007179 Val Loss: nan\n",
      "Epoch [7822/10000] Train Loss: 0.007092 Val Loss: nan\n",
      "Epoch [7823/10000] Train Loss: 0.007159 Val Loss: nan\n",
      "Epoch [7824/10000] Train Loss: 0.006996 Val Loss: nan\n",
      "Epoch [7825/10000] Train Loss: 0.007066 Val Loss: nan\n",
      "Epoch [7826/10000] Train Loss: 0.007033 Val Loss: nan\n",
      "Epoch [7827/10000] Train Loss: 0.007087 Val Loss: nan\n",
      "Epoch [7828/10000] Train Loss: 0.006982 Val Loss: nan\n",
      "Epoch [7829/10000] Train Loss: 0.007160 Val Loss: nan\n",
      "Epoch [7830/10000] Train Loss: 0.006986 Val Loss: nan\n",
      "Epoch [7831/10000] Train Loss: 0.007117 Val Loss: nan\n",
      "Epoch [7832/10000] Train Loss: 0.007030 Val Loss: nan\n",
      "Epoch [7833/10000] Train Loss: 0.007122 Val Loss: nan\n",
      "Epoch [7834/10000] Train Loss: 0.007261 Val Loss: nan\n",
      "Epoch [7835/10000] Train Loss: 0.007169 Val Loss: nan\n",
      "Epoch [7836/10000] Train Loss: 0.006964 Val Loss: nan\n",
      "Epoch [7837/10000] Train Loss: 0.006958 Val Loss: nan\n",
      "Epoch [7838/10000] Train Loss: 0.007006 Val Loss: nan\n",
      "Epoch [7839/10000] Train Loss: 0.006964 Val Loss: nan\n",
      "Epoch [7840/10000] Train Loss: 0.007159 Val Loss: nan\n",
      "Epoch [7841/10000] Train Loss: 0.007153 Val Loss: nan\n",
      "Epoch [7842/10000] Train Loss: 0.007048 Val Loss: nan\n",
      "Epoch [7843/10000] Train Loss: 0.007218 Val Loss: nan\n",
      "Epoch [7844/10000] Train Loss: 0.007089 Val Loss: nan\n",
      "Epoch [7845/10000] Train Loss: 0.006979 Val Loss: nan\n",
      "Epoch [7846/10000] Train Loss: 0.007071 Val Loss: nan\n",
      "Epoch [7847/10000] Train Loss: 0.007074 Val Loss: nan\n",
      "Epoch [7848/10000] Train Loss: 0.007066 Val Loss: nan\n",
      "Epoch [7849/10000] Train Loss: 0.007135 Val Loss: nan\n",
      "Epoch [7850/10000] Train Loss: 0.007209 Val Loss: nan\n",
      "Epoch [7851/10000] Train Loss: 0.007016 Val Loss: nan\n",
      "Epoch [7852/10000] Train Loss: 0.006986 Val Loss: nan\n",
      "Epoch [7853/10000] Train Loss: 0.007310 Val Loss: nan\n",
      "Epoch [7854/10000] Train Loss: 0.006953 Val Loss: nan\n",
      "Epoch [7855/10000] Train Loss: 0.007181 Val Loss: nan\n",
      "Epoch [7856/10000] Train Loss: 0.007087 Val Loss: nan\n",
      "Epoch [7857/10000] Train Loss: 0.007259 Val Loss: nan\n",
      "Epoch [7858/10000] Train Loss: 0.007077 Val Loss: nan\n",
      "Epoch [7859/10000] Train Loss: 0.007042 Val Loss: nan\n",
      "Epoch [7860/10000] Train Loss: 0.007082 Val Loss: nan\n",
      "Epoch [7861/10000] Train Loss: 0.006991 Val Loss: nan\n",
      "Epoch [7862/10000] Train Loss: 0.006957 Val Loss: nan\n",
      "Epoch [7863/10000] Train Loss: 0.007138 Val Loss: nan\n",
      "Epoch [7864/10000] Train Loss: 0.007085 Val Loss: nan\n",
      "Epoch [7865/10000] Train Loss: 0.006984 Val Loss: nan\n",
      "Epoch [7866/10000] Train Loss: 0.007025 Val Loss: nan\n",
      "Epoch [7867/10000] Train Loss: 0.007098 Val Loss: nan\n",
      "Epoch [7868/10000] Train Loss: 0.006995 Val Loss: nan\n",
      "Epoch [7869/10000] Train Loss: 0.007064 Val Loss: nan\n",
      "Epoch [7870/10000] Train Loss: 0.007101 Val Loss: nan\n",
      "Epoch [7871/10000] Train Loss: 0.006997 Val Loss: nan\n",
      "Epoch [7872/10000] Train Loss: 0.006991 Val Loss: nan\n",
      "Epoch [7873/10000] Train Loss: 0.006945 Val Loss: nan\n",
      "Epoch [7874/10000] Train Loss: 0.006977 Val Loss: nan\n",
      "Epoch [7875/10000] Train Loss: 0.007135 Val Loss: nan\n",
      "Epoch [7876/10000] Train Loss: 0.007074 Val Loss: nan\n",
      "Epoch [7877/10000] Train Loss: 0.007157 Val Loss: nan\n",
      "Epoch [7878/10000] Train Loss: 0.007069 Val Loss: nan\n",
      "Epoch [7879/10000] Train Loss: 0.007038 Val Loss: nan\n",
      "Epoch [7880/10000] Train Loss: 0.006998 Val Loss: nan\n",
      "Epoch [7881/10000] Train Loss: 0.006996 Val Loss: nan\n",
      "Epoch [7882/10000] Train Loss: 0.007006 Val Loss: nan\n",
      "Epoch [7883/10000] Train Loss: 0.007041 Val Loss: nan\n",
      "Epoch [7884/10000] Train Loss: 0.007087 Val Loss: nan\n",
      "Epoch [7885/10000] Train Loss: 0.007069 Val Loss: nan\n",
      "Epoch [7886/10000] Train Loss: 0.006977 Val Loss: nan\n",
      "Epoch [7887/10000] Train Loss: 0.007010 Val Loss: nan\n",
      "Epoch [7888/10000] Train Loss: 0.007060 Val Loss: nan\n",
      "Epoch [7889/10000] Train Loss: 0.006936 Val Loss: nan\n",
      "Epoch [7890/10000] Train Loss: 0.007068 Val Loss: nan\n",
      "Epoch [7891/10000] Train Loss: 0.007201 Val Loss: nan\n",
      "Epoch [7892/10000] Train Loss: 0.007036 Val Loss: nan\n",
      "Epoch [7893/10000] Train Loss: 0.007185 Val Loss: nan\n",
      "Epoch [7894/10000] Train Loss: 0.007067 Val Loss: nan\n",
      "Epoch [7895/10000] Train Loss: 0.007038 Val Loss: nan\n",
      "Epoch [7896/10000] Train Loss: 0.007045 Val Loss: nan\n",
      "Epoch [7897/10000] Train Loss: 0.007278 Val Loss: nan\n",
      "Epoch [7898/10000] Train Loss: 0.006961 Val Loss: nan\n",
      "Epoch [7899/10000] Train Loss: 0.007081 Val Loss: nan\n",
      "Epoch [7900/10000] Train Loss: 0.007030 Val Loss: nan\n",
      "Epoch [7901/10000] Train Loss: 0.007000 Val Loss: nan\n",
      "Epoch [7902/10000] Train Loss: 0.007084 Val Loss: nan\n",
      "Epoch [7903/10000] Train Loss: 0.006975 Val Loss: nan\n",
      "Epoch [7904/10000] Train Loss: 0.007001 Val Loss: nan\n",
      "Epoch [7905/10000] Train Loss: 0.007093 Val Loss: nan\n",
      "Epoch [7906/10000] Train Loss: 0.007085 Val Loss: nan\n",
      "Epoch [7907/10000] Train Loss: 0.007002 Val Loss: nan\n",
      "Epoch [7908/10000] Train Loss: 0.006974 Val Loss: nan\n",
      "Epoch [7909/10000] Train Loss: 0.006991 Val Loss: nan\n",
      "Epoch [7910/10000] Train Loss: 0.007077 Val Loss: nan\n",
      "Epoch [7911/10000] Train Loss: 0.007176 Val Loss: nan\n",
      "Epoch [7912/10000] Train Loss: 0.007090 Val Loss: nan\n",
      "Epoch [7913/10000] Train Loss: 0.007107 Val Loss: nan\n",
      "Epoch [7914/10000] Train Loss: 0.007242 Val Loss: nan\n",
      "Epoch [7915/10000] Train Loss: 0.007336 Val Loss: nan\n",
      "Epoch [7916/10000] Train Loss: 0.007234 Val Loss: nan\n",
      "Epoch [7917/10000] Train Loss: 0.007284 Val Loss: nan\n",
      "Epoch [7918/10000] Train Loss: 0.007179 Val Loss: nan\n",
      "Epoch [7919/10000] Train Loss: 0.006969 Val Loss: nan\n",
      "Epoch [7920/10000] Train Loss: 0.007062 Val Loss: nan\n",
      "Epoch [7921/10000] Train Loss: 0.006939 Val Loss: nan\n",
      "Epoch [7922/10000] Train Loss: 0.006968 Val Loss: nan\n",
      "Epoch [7923/10000] Train Loss: 0.007129 Val Loss: nan\n",
      "Epoch [7924/10000] Train Loss: 0.007152 Val Loss: nan\n",
      "Epoch [7925/10000] Train Loss: 0.007219 Val Loss: nan\n",
      "Epoch [7926/10000] Train Loss: 0.007038 Val Loss: nan\n",
      "Epoch [7927/10000] Train Loss: 0.007025 Val Loss: nan\n",
      "Epoch [7928/10000] Train Loss: 0.007230 Val Loss: nan\n",
      "Epoch [7929/10000] Train Loss: 0.006956 Val Loss: nan\n",
      "Epoch [7930/10000] Train Loss: 0.006959 Val Loss: nan\n",
      "Epoch [7931/10000] Train Loss: 0.006988 Val Loss: nan\n",
      "Epoch [7932/10000] Train Loss: 0.007022 Val Loss: nan\n",
      "Epoch [7933/10000] Train Loss: 0.006963 Val Loss: nan\n",
      "Epoch [7934/10000] Train Loss: 0.006967 Val Loss: nan\n",
      "Epoch [7935/10000] Train Loss: 0.007017 Val Loss: nan\n",
      "Epoch [7936/10000] Train Loss: 0.007057 Val Loss: nan\n",
      "Epoch [7937/10000] Train Loss: 0.007122 Val Loss: nan\n",
      "Epoch [7938/10000] Train Loss: 0.007057 Val Loss: nan\n",
      "Epoch [7939/10000] Train Loss: 0.007218 Val Loss: nan\n",
      "Epoch [7940/10000] Train Loss: 0.007101 Val Loss: nan\n",
      "Epoch [7941/10000] Train Loss: 0.007104 Val Loss: nan\n",
      "Epoch [7942/10000] Train Loss: 0.007166 Val Loss: nan\n",
      "Epoch [7943/10000] Train Loss: 0.007154 Val Loss: nan\n",
      "Epoch [7944/10000] Train Loss: 0.006967 Val Loss: nan\n",
      "Epoch [7945/10000] Train Loss: 0.007200 Val Loss: nan\n",
      "Epoch [7946/10000] Train Loss: 0.007282 Val Loss: nan\n",
      "Epoch [7947/10000] Train Loss: 0.007083 Val Loss: nan\n",
      "Epoch [7948/10000] Train Loss: 0.007014 Val Loss: nan\n",
      "Epoch [7949/10000] Train Loss: 0.007103 Val Loss: nan\n",
      "Epoch [7950/10000] Train Loss: 0.007048 Val Loss: nan\n",
      "Epoch [7951/10000] Train Loss: 0.007132 Val Loss: nan\n",
      "Epoch [7952/10000] Train Loss: 0.007012 Val Loss: nan\n",
      "Epoch [7953/10000] Train Loss: 0.007241 Val Loss: nan\n",
      "Epoch [7954/10000] Train Loss: 0.007059 Val Loss: nan\n",
      "Epoch [7955/10000] Train Loss: 0.007000 Val Loss: nan\n",
      "Epoch [7956/10000] Train Loss: 0.007046 Val Loss: nan\n",
      "Epoch [7957/10000] Train Loss: 0.007135 Val Loss: nan\n",
      "Epoch [7958/10000] Train Loss: 0.007021 Val Loss: nan\n",
      "Epoch [7959/10000] Train Loss: 0.007089 Val Loss: nan\n",
      "Epoch [7960/10000] Train Loss: 0.007119 Val Loss: nan\n",
      "Epoch [7961/10000] Train Loss: 0.007052 Val Loss: nan\n",
      "Epoch [7962/10000] Train Loss: 0.007048 Val Loss: nan\n",
      "Epoch [7963/10000] Train Loss: 0.007177 Val Loss: nan\n",
      "Epoch [7964/10000] Train Loss: 0.007033 Val Loss: nan\n",
      "Epoch [7965/10000] Train Loss: 0.007016 Val Loss: nan\n",
      "Epoch [7966/10000] Train Loss: 0.007000 Val Loss: nan\n",
      "Epoch [7967/10000] Train Loss: 0.007079 Val Loss: nan\n",
      "Epoch [7968/10000] Train Loss: 0.006943 Val Loss: nan\n",
      "Epoch [7969/10000] Train Loss: 0.007130 Val Loss: nan\n",
      "Epoch [7970/10000] Train Loss: 0.007114 Val Loss: nan\n",
      "Epoch [7971/10000] Train Loss: 0.007008 Val Loss: nan\n",
      "Epoch [7972/10000] Train Loss: 0.006940 Val Loss: nan\n",
      "Epoch [7973/10000] Train Loss: 0.006927 Val Loss: nan\n",
      "Epoch [7974/10000] Train Loss: 0.006979 Val Loss: nan\n",
      "Epoch [7975/10000] Train Loss: 0.007119 Val Loss: nan\n",
      "Epoch [7976/10000] Train Loss: 0.007059 Val Loss: nan\n",
      "Epoch [7977/10000] Train Loss: 0.007025 Val Loss: nan\n",
      "Epoch [7978/10000] Train Loss: 0.007223 Val Loss: nan\n",
      "Epoch [7979/10000] Train Loss: 0.007190 Val Loss: nan\n",
      "Epoch [7980/10000] Train Loss: 0.007106 Val Loss: nan\n",
      "Epoch [7981/10000] Train Loss: 0.007037 Val Loss: nan\n",
      "Epoch [7982/10000] Train Loss: 0.007123 Val Loss: nan\n",
      "Epoch [7983/10000] Train Loss: 0.007264 Val Loss: nan\n",
      "Epoch [7984/10000] Train Loss: 0.007071 Val Loss: nan\n",
      "Epoch [7985/10000] Train Loss: 0.007033 Val Loss: nan\n",
      "Epoch [7986/10000] Train Loss: 0.007139 Val Loss: nan\n",
      "Epoch [7987/10000] Train Loss: 0.007052 Val Loss: nan\n",
      "Epoch [7988/10000] Train Loss: 0.006978 Val Loss: nan\n",
      "Epoch [7989/10000] Train Loss: 0.007053 Val Loss: nan\n",
      "Epoch [7990/10000] Train Loss: 0.006964 Val Loss: nan\n",
      "Epoch [7991/10000] Train Loss: 0.007014 Val Loss: nan\n",
      "Epoch [7992/10000] Train Loss: 0.007055 Val Loss: nan\n",
      "Epoch [7993/10000] Train Loss: 0.006940 Val Loss: nan\n",
      "Epoch [7994/10000] Train Loss: 0.006967 Val Loss: nan\n",
      "Epoch [7995/10000] Train Loss: 0.006943 Val Loss: nan\n",
      "Epoch [7996/10000] Train Loss: 0.007049 Val Loss: nan\n",
      "Epoch [7997/10000] Train Loss: 0.007165 Val Loss: nan\n",
      "Epoch [7998/10000] Train Loss: 0.007163 Val Loss: nan\n",
      "Epoch [7999/10000] Train Loss: 0.007094 Val Loss: nan\n",
      "Epoch [8000/10000] Train Loss: 0.007220 Val Loss: nan\n",
      "Epoch [8001/10000] Train Loss: 0.006958 Val Loss: nan\n",
      "Epoch [8002/10000] Train Loss: 0.006916 Val Loss: nan\n",
      "Epoch [8003/10000] Train Loss: 0.006955 Val Loss: nan\n",
      "Epoch [8004/10000] Train Loss: 0.007074 Val Loss: nan\n",
      "Epoch [8005/10000] Train Loss: 0.007230 Val Loss: nan\n",
      "Epoch [8006/10000] Train Loss: 0.007068 Val Loss: nan\n",
      "Epoch [8007/10000] Train Loss: 0.006995 Val Loss: nan\n",
      "Epoch [8008/10000] Train Loss: 0.006993 Val Loss: nan\n",
      "Epoch [8009/10000] Train Loss: 0.007067 Val Loss: nan\n",
      "Epoch [8010/10000] Train Loss: 0.006960 Val Loss: nan\n",
      "Epoch [8011/10000] Train Loss: 0.007088 Val Loss: nan\n",
      "Epoch [8012/10000] Train Loss: 0.007038 Val Loss: nan\n",
      "Epoch [8013/10000] Train Loss: 0.007076 Val Loss: nan\n",
      "Epoch [8014/10000] Train Loss: 0.007288 Val Loss: nan\n",
      "Epoch [8015/10000] Train Loss: 0.006965 Val Loss: nan\n",
      "Epoch [8016/10000] Train Loss: 0.007058 Val Loss: nan\n",
      "Epoch [8017/10000] Train Loss: 0.007044 Val Loss: nan\n",
      "Epoch [8018/10000] Train Loss: 0.006972 Val Loss: nan\n",
      "Epoch [8019/10000] Train Loss: 0.007196 Val Loss: nan\n",
      "Epoch [8020/10000] Train Loss: 0.007035 Val Loss: nan\n",
      "Epoch [8021/10000] Train Loss: 0.006936 Val Loss: nan\n",
      "Epoch [8022/10000] Train Loss: 0.007065 Val Loss: nan\n",
      "Epoch [8023/10000] Train Loss: 0.006981 Val Loss: nan\n",
      "Epoch [8024/10000] Train Loss: 0.006957 Val Loss: nan\n",
      "Epoch [8025/10000] Train Loss: 0.007081 Val Loss: nan\n",
      "Epoch [8026/10000] Train Loss: 0.007087 Val Loss: nan\n",
      "Epoch [8027/10000] Train Loss: 0.007153 Val Loss: nan\n",
      "Epoch [8028/10000] Train Loss: 0.006967 Val Loss: nan\n",
      "Epoch [8029/10000] Train Loss: 0.006993 Val Loss: nan\n",
      "Epoch [8030/10000] Train Loss: 0.006975 Val Loss: nan\n",
      "Epoch [8031/10000] Train Loss: 0.007109 Val Loss: nan\n",
      "Epoch [8032/10000] Train Loss: 0.007197 Val Loss: nan\n",
      "Epoch [8033/10000] Train Loss: 0.007133 Val Loss: nan\n",
      "Epoch [8034/10000] Train Loss: 0.007005 Val Loss: nan\n",
      "Epoch [8035/10000] Train Loss: 0.007068 Val Loss: nan\n",
      "Epoch [8036/10000] Train Loss: 0.006944 Val Loss: nan\n",
      "Epoch [8037/10000] Train Loss: 0.007044 Val Loss: nan\n",
      "Epoch [8038/10000] Train Loss: 0.006980 Val Loss: nan\n",
      "Epoch [8039/10000] Train Loss: 0.007004 Val Loss: nan\n",
      "Epoch [8040/10000] Train Loss: 0.007014 Val Loss: nan\n",
      "Epoch [8041/10000] Train Loss: 0.007079 Val Loss: nan\n",
      "Epoch [8042/10000] Train Loss: 0.006945 Val Loss: nan\n",
      "Epoch [8043/10000] Train Loss: 0.007060 Val Loss: nan\n",
      "Epoch [8044/10000] Train Loss: 0.006904 Val Loss: nan\n",
      "Epoch [8045/10000] Train Loss: 0.006938 Val Loss: nan\n",
      "Epoch [8046/10000] Train Loss: 0.006932 Val Loss: nan\n",
      "Epoch [8047/10000] Train Loss: 0.007025 Val Loss: nan\n",
      "Epoch [8048/10000] Train Loss: 0.007083 Val Loss: nan\n",
      "Epoch [8049/10000] Train Loss: 0.007044 Val Loss: nan\n",
      "Epoch [8050/10000] Train Loss: 0.007066 Val Loss: nan\n",
      "Epoch [8051/10000] Train Loss: 0.006945 Val Loss: nan\n",
      "Epoch [8052/10000] Train Loss: 0.006939 Val Loss: nan\n",
      "Epoch [8053/10000] Train Loss: 0.007224 Val Loss: nan\n",
      "Epoch [8054/10000] Train Loss: 0.007086 Val Loss: nan\n",
      "Epoch [8055/10000] Train Loss: 0.007147 Val Loss: nan\n",
      "Epoch [8056/10000] Train Loss: 0.007182 Val Loss: nan\n",
      "Epoch [8057/10000] Train Loss: 0.007119 Val Loss: nan\n",
      "Epoch [8058/10000] Train Loss: 0.007127 Val Loss: nan\n",
      "Epoch [8059/10000] Train Loss: 0.006993 Val Loss: nan\n",
      "Epoch [8060/10000] Train Loss: 0.007081 Val Loss: nan\n",
      "Epoch [8061/10000] Train Loss: 0.007166 Val Loss: nan\n",
      "Epoch [8062/10000] Train Loss: 0.007093 Val Loss: nan\n",
      "Epoch [8063/10000] Train Loss: 0.007012 Val Loss: nan\n",
      "Epoch [8064/10000] Train Loss: 0.007041 Val Loss: nan\n",
      "Epoch [8065/10000] Train Loss: 0.007075 Val Loss: nan\n",
      "Epoch [8066/10000] Train Loss: 0.007089 Val Loss: nan\n",
      "Epoch [8067/10000] Train Loss: 0.006934 Val Loss: nan\n",
      "Epoch [8068/10000] Train Loss: 0.006981 Val Loss: nan\n",
      "Epoch [8069/10000] Train Loss: 0.007031 Val Loss: nan\n",
      "Epoch [8070/10000] Train Loss: 0.006955 Val Loss: nan\n",
      "Epoch [8071/10000] Train Loss: 0.007043 Val Loss: nan\n",
      "Epoch [8072/10000] Train Loss: 0.007162 Val Loss: nan\n",
      "Epoch [8073/10000] Train Loss: 0.007008 Val Loss: nan\n",
      "Epoch [8074/10000] Train Loss: 0.007155 Val Loss: nan\n",
      "Epoch [8075/10000] Train Loss: 0.007023 Val Loss: nan\n",
      "Epoch [8076/10000] Train Loss: 0.007177 Val Loss: nan\n",
      "Epoch [8077/10000] Train Loss: 0.007051 Val Loss: nan\n",
      "Epoch [8078/10000] Train Loss: 0.007054 Val Loss: nan\n",
      "Epoch [8079/10000] Train Loss: 0.006950 Val Loss: nan\n",
      "Epoch [8080/10000] Train Loss: 0.006932 Val Loss: nan\n",
      "Epoch [8081/10000] Train Loss: 0.006962 Val Loss: nan\n",
      "Epoch [8082/10000] Train Loss: 0.006990 Val Loss: nan\n",
      "Epoch [8083/10000] Train Loss: 0.007239 Val Loss: nan\n",
      "Epoch [8084/10000] Train Loss: 0.007130 Val Loss: nan\n",
      "Epoch [8085/10000] Train Loss: 0.007141 Val Loss: nan\n",
      "Epoch [8086/10000] Train Loss: 0.007402 Val Loss: nan\n",
      "Epoch [8087/10000] Train Loss: 0.007291 Val Loss: nan\n",
      "Epoch [8088/10000] Train Loss: 0.007126 Val Loss: nan\n",
      "Epoch [8089/10000] Train Loss: 0.006974 Val Loss: nan\n",
      "Epoch [8090/10000] Train Loss: 0.006973 Val Loss: nan\n",
      "Epoch [8091/10000] Train Loss: 0.007063 Val Loss: nan\n",
      "Epoch [8092/10000] Train Loss: 0.006962 Val Loss: nan\n",
      "Epoch [8093/10000] Train Loss: 0.006990 Val Loss: nan\n",
      "Epoch [8094/10000] Train Loss: 0.006918 Val Loss: nan\n",
      "Epoch [8095/10000] Train Loss: 0.007441 Val Loss: nan\n",
      "Epoch [8096/10000] Train Loss: 0.006962 Val Loss: nan\n",
      "Epoch [8097/10000] Train Loss: 0.007046 Val Loss: nan\n",
      "Epoch [8098/10000] Train Loss: 0.007057 Val Loss: nan\n",
      "Epoch [8099/10000] Train Loss: 0.007063 Val Loss: nan\n",
      "Epoch [8100/10000] Train Loss: 0.007019 Val Loss: nan\n",
      "Epoch [8101/10000] Train Loss: 0.006971 Val Loss: nan\n",
      "Epoch [8102/10000] Train Loss: 0.007016 Val Loss: nan\n",
      "Epoch [8103/10000] Train Loss: 0.006971 Val Loss: nan\n",
      "Epoch [8104/10000] Train Loss: 0.006994 Val Loss: nan\n",
      "Epoch [8105/10000] Train Loss: 0.007093 Val Loss: nan\n",
      "Epoch [8106/10000] Train Loss: 0.007008 Val Loss: nan\n",
      "Epoch [8107/10000] Train Loss: 0.007043 Val Loss: nan\n",
      "Epoch [8108/10000] Train Loss: 0.006922 Val Loss: nan\n",
      "Epoch [8109/10000] Train Loss: 0.006912 Val Loss: nan\n",
      "Epoch [8110/10000] Train Loss: 0.006905 Val Loss: nan\n",
      "Epoch [8111/10000] Train Loss: 0.006959 Val Loss: nan\n",
      "Epoch [8112/10000] Train Loss: 0.006998 Val Loss: nan\n",
      "Epoch [8113/10000] Train Loss: 0.007138 Val Loss: nan\n",
      "Epoch [8114/10000] Train Loss: 0.006997 Val Loss: nan\n",
      "Epoch [8115/10000] Train Loss: 0.006927 Val Loss: nan\n",
      "Epoch [8116/10000] Train Loss: 0.006930 Val Loss: nan\n",
      "Epoch [8117/10000] Train Loss: 0.006960 Val Loss: nan\n",
      "Epoch [8118/10000] Train Loss: 0.006912 Val Loss: nan\n",
      "Epoch [8119/10000] Train Loss: 0.007048 Val Loss: nan\n",
      "Epoch [8120/10000] Train Loss: 0.006894 Val Loss: nan\n",
      "Epoch [8121/10000] Train Loss: 0.006952 Val Loss: nan\n",
      "Epoch [8122/10000] Train Loss: 0.007036 Val Loss: nan\n",
      "Epoch [8123/10000] Train Loss: 0.006923 Val Loss: nan\n",
      "Epoch [8124/10000] Train Loss: 0.007019 Val Loss: nan\n",
      "Epoch [8125/10000] Train Loss: 0.007037 Val Loss: nan\n",
      "Epoch [8126/10000] Train Loss: 0.006946 Val Loss: nan\n",
      "Epoch [8127/10000] Train Loss: 0.007059 Val Loss: nan\n",
      "Epoch [8128/10000] Train Loss: 0.007041 Val Loss: nan\n",
      "Epoch [8129/10000] Train Loss: 0.006942 Val Loss: nan\n",
      "Epoch [8130/10000] Train Loss: 0.007070 Val Loss: nan\n",
      "Epoch [8131/10000] Train Loss: 0.006956 Val Loss: nan\n",
      "Epoch [8132/10000] Train Loss: 0.007086 Val Loss: nan\n",
      "Epoch [8133/10000] Train Loss: 0.007242 Val Loss: nan\n",
      "Epoch [8134/10000] Train Loss: 0.006991 Val Loss: nan\n",
      "Epoch [8135/10000] Train Loss: 0.006984 Val Loss: nan\n",
      "Epoch [8136/10000] Train Loss: 0.006998 Val Loss: nan\n",
      "Epoch [8137/10000] Train Loss: 0.007334 Val Loss: nan\n",
      "Epoch [8138/10000] Train Loss: 0.007084 Val Loss: nan\n",
      "Epoch [8139/10000] Train Loss: 0.006937 Val Loss: nan\n",
      "Epoch [8140/10000] Train Loss: 0.007007 Val Loss: nan\n",
      "Epoch [8141/10000] Train Loss: 0.006953 Val Loss: nan\n",
      "Epoch [8142/10000] Train Loss: 0.007066 Val Loss: nan\n",
      "Epoch [8143/10000] Train Loss: 0.006940 Val Loss: nan\n",
      "Epoch [8144/10000] Train Loss: 0.007027 Val Loss: nan\n",
      "Epoch [8145/10000] Train Loss: 0.007226 Val Loss: nan\n",
      "Epoch [8146/10000] Train Loss: 0.006923 Val Loss: nan\n",
      "Epoch [8147/10000] Train Loss: 0.007134 Val Loss: nan\n",
      "Epoch [8148/10000] Train Loss: 0.006991 Val Loss: nan\n",
      "Epoch [8149/10000] Train Loss: 0.007158 Val Loss: nan\n",
      "Epoch [8150/10000] Train Loss: 0.006942 Val Loss: nan\n",
      "Epoch [8151/10000] Train Loss: 0.006920 Val Loss: nan\n",
      "Epoch [8152/10000] Train Loss: 0.007207 Val Loss: nan\n",
      "Epoch [8153/10000] Train Loss: 0.007176 Val Loss: nan\n",
      "Epoch [8154/10000] Train Loss: 0.007046 Val Loss: nan\n",
      "Epoch [8155/10000] Train Loss: 0.006961 Val Loss: nan\n",
      "Epoch [8156/10000] Train Loss: 0.007051 Val Loss: nan\n",
      "Epoch [8157/10000] Train Loss: 0.007055 Val Loss: nan\n",
      "Epoch [8158/10000] Train Loss: 0.006955 Val Loss: nan\n",
      "Epoch [8159/10000] Train Loss: 0.007019 Val Loss: nan\n",
      "Epoch [8160/10000] Train Loss: 0.006979 Val Loss: nan\n",
      "Epoch [8161/10000] Train Loss: 0.006970 Val Loss: nan\n",
      "Epoch [8162/10000] Train Loss: 0.007076 Val Loss: nan\n",
      "Epoch [8163/10000] Train Loss: 0.007169 Val Loss: nan\n",
      "Epoch [8164/10000] Train Loss: 0.007048 Val Loss: nan\n",
      "Epoch [8165/10000] Train Loss: 0.007057 Val Loss: nan\n",
      "Epoch [8166/10000] Train Loss: 0.007017 Val Loss: nan\n",
      "Epoch [8167/10000] Train Loss: 0.007058 Val Loss: nan\n",
      "Epoch [8168/10000] Train Loss: 0.007010 Val Loss: nan\n",
      "Epoch [8169/10000] Train Loss: 0.006982 Val Loss: nan\n",
      "Epoch [8170/10000] Train Loss: 0.007049 Val Loss: nan\n",
      "Epoch [8171/10000] Train Loss: 0.007109 Val Loss: nan\n",
      "Epoch [8172/10000] Train Loss: 0.007068 Val Loss: nan\n",
      "Epoch [8173/10000] Train Loss: 0.006981 Val Loss: nan\n",
      "Epoch [8174/10000] Train Loss: 0.006977 Val Loss: nan\n",
      "Epoch [8175/10000] Train Loss: 0.007164 Val Loss: nan\n",
      "Epoch [8176/10000] Train Loss: 0.007105 Val Loss: nan\n",
      "Epoch [8177/10000] Train Loss: 0.006942 Val Loss: nan\n",
      "Epoch [8178/10000] Train Loss: 0.006946 Val Loss: nan\n",
      "Epoch [8179/10000] Train Loss: 0.007058 Val Loss: nan\n",
      "Epoch [8180/10000] Train Loss: 0.007130 Val Loss: nan\n",
      "Epoch [8181/10000] Train Loss: 0.007154 Val Loss: nan\n",
      "Epoch [8182/10000] Train Loss: 0.006966 Val Loss: nan\n",
      "Epoch [8183/10000] Train Loss: 0.007002 Val Loss: nan\n",
      "Epoch [8184/10000] Train Loss: 0.006996 Val Loss: nan\n",
      "Epoch [8185/10000] Train Loss: 0.007077 Val Loss: nan\n",
      "Epoch [8186/10000] Train Loss: 0.007026 Val Loss: nan\n",
      "Epoch [8187/10000] Train Loss: 0.007191 Val Loss: nan\n",
      "Epoch [8188/10000] Train Loss: 0.007042 Val Loss: nan\n",
      "Epoch [8189/10000] Train Loss: 0.006898 Val Loss: nan\n",
      "Epoch [8190/10000] Train Loss: 0.007029 Val Loss: nan\n",
      "Epoch [8191/10000] Train Loss: 0.007283 Val Loss: nan\n",
      "Epoch [8192/10000] Train Loss: 0.007064 Val Loss: nan\n",
      "Epoch [8193/10000] Train Loss: 0.006952 Val Loss: nan\n",
      "Epoch [8194/10000] Train Loss: 0.007030 Val Loss: nan\n",
      "Epoch [8195/10000] Train Loss: 0.007046 Val Loss: nan\n",
      "Epoch [8196/10000] Train Loss: 0.007108 Val Loss: nan\n",
      "Epoch [8197/10000] Train Loss: 0.006977 Val Loss: nan\n",
      "Epoch [8198/10000] Train Loss: 0.007316 Val Loss: nan\n",
      "Epoch [8199/10000] Train Loss: 0.007189 Val Loss: nan\n",
      "Epoch [8200/10000] Train Loss: 0.007103 Val Loss: nan\n",
      "Epoch [8201/10000] Train Loss: 0.007064 Val Loss: nan\n",
      "Epoch [8202/10000] Train Loss: 0.007034 Val Loss: nan\n",
      "Epoch [8203/10000] Train Loss: 0.006939 Val Loss: nan\n",
      "Epoch [8204/10000] Train Loss: 0.006946 Val Loss: nan\n",
      "Epoch [8205/10000] Train Loss: 0.007155 Val Loss: nan\n",
      "Epoch [8206/10000] Train Loss: 0.007186 Val Loss: nan\n",
      "Epoch [8207/10000] Train Loss: 0.006963 Val Loss: nan\n",
      "Epoch [8208/10000] Train Loss: 0.006940 Val Loss: nan\n",
      "Epoch [8209/10000] Train Loss: 0.007039 Val Loss: nan\n",
      "Epoch [8210/10000] Train Loss: 0.007111 Val Loss: nan\n",
      "Epoch [8211/10000] Train Loss: 0.006946 Val Loss: nan\n",
      "Epoch [8212/10000] Train Loss: 0.007119 Val Loss: nan\n",
      "Epoch [8213/10000] Train Loss: 0.007093 Val Loss: nan\n",
      "Epoch [8214/10000] Train Loss: 0.006973 Val Loss: nan\n",
      "Epoch [8215/10000] Train Loss: 0.006937 Val Loss: nan\n",
      "Epoch [8216/10000] Train Loss: 0.006943 Val Loss: nan\n",
      "Epoch [8217/10000] Train Loss: 0.007103 Val Loss: nan\n",
      "Epoch [8218/10000] Train Loss: 0.006937 Val Loss: nan\n",
      "Epoch [8219/10000] Train Loss: 0.007014 Val Loss: nan\n",
      "Epoch [8220/10000] Train Loss: 0.006947 Val Loss: nan\n",
      "Epoch [8221/10000] Train Loss: 0.007040 Val Loss: nan\n",
      "Epoch [8222/10000] Train Loss: 0.007015 Val Loss: nan\n",
      "Epoch [8223/10000] Train Loss: 0.007059 Val Loss: nan\n",
      "Epoch [8224/10000] Train Loss: 0.006938 Val Loss: nan\n",
      "Epoch [8225/10000] Train Loss: 0.006934 Val Loss: nan\n",
      "Epoch [8226/10000] Train Loss: 0.006903 Val Loss: nan\n",
      "Epoch [8227/10000] Train Loss: 0.006988 Val Loss: nan\n",
      "Epoch [8228/10000] Train Loss: 0.007040 Val Loss: nan\n",
      "Epoch [8229/10000] Train Loss: 0.006962 Val Loss: nan\n",
      "Epoch [8230/10000] Train Loss: 0.007034 Val Loss: nan\n",
      "Epoch [8231/10000] Train Loss: 0.007077 Val Loss: nan\n",
      "Epoch [8232/10000] Train Loss: 0.007133 Val Loss: nan\n",
      "Epoch [8233/10000] Train Loss: 0.007115 Val Loss: nan\n",
      "Epoch [8234/10000] Train Loss: 0.007007 Val Loss: nan\n",
      "Epoch [8235/10000] Train Loss: 0.006948 Val Loss: nan\n",
      "Epoch [8236/10000] Train Loss: 0.006925 Val Loss: nan\n",
      "Epoch [8237/10000] Train Loss: 0.007075 Val Loss: nan\n",
      "Epoch [8238/10000] Train Loss: 0.007182 Val Loss: nan\n",
      "Epoch [8239/10000] Train Loss: 0.006984 Val Loss: nan\n",
      "Epoch [8240/10000] Train Loss: 0.007338 Val Loss: nan\n",
      "Epoch [8241/10000] Train Loss: 0.007109 Val Loss: nan\n",
      "Epoch [8242/10000] Train Loss: 0.006932 Val Loss: nan\n",
      "Epoch [8243/10000] Train Loss: 0.006911 Val Loss: nan\n",
      "Epoch [8244/10000] Train Loss: 0.006969 Val Loss: nan\n",
      "Epoch [8245/10000] Train Loss: 0.006921 Val Loss: nan\n",
      "Epoch [8246/10000] Train Loss: 0.007065 Val Loss: nan\n",
      "Epoch [8247/10000] Train Loss: 0.007001 Val Loss: nan\n",
      "Epoch [8248/10000] Train Loss: 0.007025 Val Loss: nan\n",
      "Epoch [8249/10000] Train Loss: 0.006891 Val Loss: nan\n",
      "Epoch [8250/10000] Train Loss: 0.006892 Val Loss: nan\n",
      "Epoch [8251/10000] Train Loss: 0.007305 Val Loss: nan\n",
      "Epoch [8252/10000] Train Loss: 0.007077 Val Loss: nan\n",
      "Epoch [8253/10000] Train Loss: 0.006987 Val Loss: nan\n",
      "Epoch [8254/10000] Train Loss: 0.007021 Val Loss: nan\n",
      "Epoch [8255/10000] Train Loss: 0.007294 Val Loss: nan\n",
      "Epoch [8256/10000] Train Loss: 0.007237 Val Loss: nan\n",
      "Epoch [8257/10000] Train Loss: 0.006955 Val Loss: nan\n",
      "Epoch [8258/10000] Train Loss: 0.007048 Val Loss: nan\n",
      "Epoch [8259/10000] Train Loss: 0.007031 Val Loss: nan\n",
      "Epoch [8260/10000] Train Loss: 0.006947 Val Loss: nan\n",
      "Epoch [8261/10000] Train Loss: 0.006916 Val Loss: nan\n",
      "Epoch [8262/10000] Train Loss: 0.006994 Val Loss: nan\n",
      "Epoch [8263/10000] Train Loss: 0.007060 Val Loss: nan\n",
      "Epoch [8264/10000] Train Loss: 0.007137 Val Loss: nan\n",
      "Epoch [8265/10000] Train Loss: 0.007207 Val Loss: nan\n",
      "Epoch [8266/10000] Train Loss: 0.006929 Val Loss: nan\n",
      "Epoch [8267/10000] Train Loss: 0.007025 Val Loss: nan\n",
      "Epoch [8268/10000] Train Loss: 0.007029 Val Loss: nan\n",
      "Epoch [8269/10000] Train Loss: 0.006929 Val Loss: nan\n",
      "Epoch [8270/10000] Train Loss: 0.006940 Val Loss: nan\n",
      "Epoch [8271/10000] Train Loss: 0.006906 Val Loss: nan\n",
      "Epoch [8272/10000] Train Loss: 0.006933 Val Loss: nan\n",
      "Epoch [8273/10000] Train Loss: 0.006925 Val Loss: nan\n",
      "Epoch [8274/10000] Train Loss: 0.007134 Val Loss: nan\n",
      "Epoch [8275/10000] Train Loss: 0.007023 Val Loss: nan\n",
      "Epoch [8276/10000] Train Loss: 0.007018 Val Loss: nan\n",
      "Epoch [8277/10000] Train Loss: 0.007043 Val Loss: nan\n",
      "Epoch [8278/10000] Train Loss: 0.007033 Val Loss: nan\n",
      "Epoch [8279/10000] Train Loss: 0.006966 Val Loss: nan\n",
      "Epoch [8280/10000] Train Loss: 0.006983 Val Loss: nan\n",
      "Epoch [8281/10000] Train Loss: 0.006891 Val Loss: nan\n",
      "Epoch [8282/10000] Train Loss: 0.007146 Val Loss: nan\n",
      "Epoch [8283/10000] Train Loss: 0.006930 Val Loss: nan\n",
      "Epoch [8284/10000] Train Loss: 0.006893 Val Loss: nan\n",
      "Epoch [8285/10000] Train Loss: 0.006916 Val Loss: nan\n",
      "Epoch [8286/10000] Train Loss: 0.007052 Val Loss: nan\n",
      "Epoch [8287/10000] Train Loss: 0.007160 Val Loss: nan\n",
      "Epoch [8288/10000] Train Loss: 0.007053 Val Loss: nan\n",
      "Epoch [8289/10000] Train Loss: 0.006913 Val Loss: nan\n",
      "Epoch [8290/10000] Train Loss: 0.006982 Val Loss: nan\n",
      "Epoch [8291/10000] Train Loss: 0.006958 Val Loss: nan\n",
      "Epoch [8292/10000] Train Loss: 0.006901 Val Loss: nan\n",
      "Epoch [8293/10000] Train Loss: 0.006946 Val Loss: nan\n",
      "Epoch [8294/10000] Train Loss: 0.007082 Val Loss: nan\n",
      "Epoch [8295/10000] Train Loss: 0.006912 Val Loss: nan\n",
      "Epoch [8296/10000] Train Loss: 0.006918 Val Loss: nan\n",
      "Epoch [8297/10000] Train Loss: 0.006978 Val Loss: nan\n",
      "Epoch [8298/10000] Train Loss: 0.006953 Val Loss: nan\n",
      "Epoch [8299/10000] Train Loss: 0.006987 Val Loss: nan\n",
      "Epoch [8300/10000] Train Loss: 0.006952 Val Loss: nan\n",
      "Epoch [8301/10000] Train Loss: 0.007123 Val Loss: nan\n",
      "Epoch [8302/10000] Train Loss: 0.007011 Val Loss: nan\n",
      "Epoch [8303/10000] Train Loss: 0.006978 Val Loss: nan\n",
      "Epoch [8304/10000] Train Loss: 0.006996 Val Loss: nan\n",
      "Epoch [8305/10000] Train Loss: 0.006953 Val Loss: nan\n",
      "Epoch [8306/10000] Train Loss: 0.007012 Val Loss: nan\n",
      "Epoch [8307/10000] Train Loss: 0.006976 Val Loss: nan\n",
      "Epoch [8308/10000] Train Loss: 0.006973 Val Loss: nan\n",
      "Epoch [8309/10000] Train Loss: 0.006981 Val Loss: nan\n",
      "Epoch [8310/10000] Train Loss: 0.006957 Val Loss: nan\n",
      "Epoch [8311/10000] Train Loss: 0.006951 Val Loss: nan\n",
      "Epoch [8312/10000] Train Loss: 0.006913 Val Loss: nan\n",
      "Epoch [8313/10000] Train Loss: 0.006911 Val Loss: nan\n",
      "Epoch [8314/10000] Train Loss: 0.006962 Val Loss: nan\n",
      "Epoch [8315/10000] Train Loss: 0.007261 Val Loss: nan\n",
      "Epoch [8316/10000] Train Loss: 0.007169 Val Loss: nan\n",
      "Epoch [8317/10000] Train Loss: 0.006956 Val Loss: nan\n",
      "Epoch [8318/10000] Train Loss: 0.006942 Val Loss: nan\n",
      "Epoch [8319/10000] Train Loss: 0.007022 Val Loss: nan\n",
      "Epoch [8320/10000] Train Loss: 0.007077 Val Loss: nan\n",
      "Epoch [8321/10000] Train Loss: 0.007103 Val Loss: nan\n",
      "Epoch [8322/10000] Train Loss: 0.007064 Val Loss: nan\n",
      "Epoch [8323/10000] Train Loss: 0.006935 Val Loss: nan\n",
      "Epoch [8324/10000] Train Loss: 0.007040 Val Loss: nan\n",
      "Epoch [8325/10000] Train Loss: 0.006923 Val Loss: nan\n",
      "Epoch [8326/10000] Train Loss: 0.007044 Val Loss: nan\n",
      "Epoch [8327/10000] Train Loss: 0.007074 Val Loss: nan\n",
      "Epoch [8328/10000] Train Loss: 0.006951 Val Loss: nan\n",
      "Epoch [8329/10000] Train Loss: 0.006915 Val Loss: nan\n",
      "Epoch [8330/10000] Train Loss: 0.006992 Val Loss: nan\n",
      "Epoch [8331/10000] Train Loss: 0.006997 Val Loss: nan\n",
      "Epoch [8332/10000] Train Loss: 0.007053 Val Loss: nan\n",
      "Epoch [8333/10000] Train Loss: 0.007040 Val Loss: nan\n",
      "Epoch [8334/10000] Train Loss: 0.007062 Val Loss: nan\n",
      "Epoch [8335/10000] Train Loss: 0.006938 Val Loss: nan\n",
      "Epoch [8336/10000] Train Loss: 0.007036 Val Loss: nan\n",
      "Epoch [8337/10000] Train Loss: 0.006977 Val Loss: nan\n",
      "Epoch [8338/10000] Train Loss: 0.007163 Val Loss: nan\n",
      "Epoch [8339/10000] Train Loss: 0.007163 Val Loss: nan\n",
      "Epoch [8340/10000] Train Loss: 0.007112 Val Loss: nan\n",
      "Epoch [8341/10000] Train Loss: 0.006957 Val Loss: nan\n",
      "Epoch [8342/10000] Train Loss: 0.007038 Val Loss: nan\n",
      "Epoch [8343/10000] Train Loss: 0.007227 Val Loss: nan\n",
      "Epoch [8344/10000] Train Loss: 0.007182 Val Loss: nan\n",
      "Epoch [8345/10000] Train Loss: 0.007046 Val Loss: nan\n",
      "Epoch [8346/10000] Train Loss: 0.006981 Val Loss: nan\n",
      "Epoch [8347/10000] Train Loss: 0.007065 Val Loss: nan\n",
      "Epoch [8348/10000] Train Loss: 0.006928 Val Loss: nan\n",
      "Epoch [8349/10000] Train Loss: 0.007058 Val Loss: nan\n",
      "Epoch [8350/10000] Train Loss: 0.006890 Val Loss: nan\n",
      "Epoch [8351/10000] Train Loss: 0.006935 Val Loss: nan\n",
      "Epoch [8352/10000] Train Loss: 0.007074 Val Loss: nan\n",
      "Epoch [8353/10000] Train Loss: 0.007118 Val Loss: nan\n",
      "Epoch [8354/10000] Train Loss: 0.006944 Val Loss: nan\n",
      "Epoch [8355/10000] Train Loss: 0.006930 Val Loss: nan\n",
      "Epoch [8356/10000] Train Loss: 0.006931 Val Loss: nan\n",
      "Epoch [8357/10000] Train Loss: 0.007012 Val Loss: nan\n",
      "Epoch [8358/10000] Train Loss: 0.006895 Val Loss: nan\n",
      "Epoch [8359/10000] Train Loss: 0.006888 Val Loss: nan\n",
      "Epoch [8360/10000] Train Loss: 0.006934 Val Loss: nan\n",
      "Epoch [8361/10000] Train Loss: 0.007165 Val Loss: nan\n",
      "Epoch [8362/10000] Train Loss: 0.007008 Val Loss: nan\n",
      "Epoch [8363/10000] Train Loss: 0.006922 Val Loss: nan\n",
      "Epoch [8364/10000] Train Loss: 0.007163 Val Loss: nan\n",
      "Epoch [8365/10000] Train Loss: 0.007066 Val Loss: nan\n",
      "Epoch [8366/10000] Train Loss: 0.007077 Val Loss: nan\n",
      "Epoch [8367/10000] Train Loss: 0.007019 Val Loss: nan\n",
      "Epoch [8368/10000] Train Loss: 0.007037 Val Loss: nan\n",
      "Epoch [8369/10000] Train Loss: 0.006937 Val Loss: nan\n",
      "Epoch [8370/10000] Train Loss: 0.006982 Val Loss: nan\n",
      "Epoch [8371/10000] Train Loss: 0.006965 Val Loss: nan\n",
      "Epoch [8372/10000] Train Loss: 0.007063 Val Loss: nan\n",
      "Epoch [8373/10000] Train Loss: 0.007109 Val Loss: nan\n",
      "Epoch [8374/10000] Train Loss: 0.006973 Val Loss: nan\n",
      "Epoch [8375/10000] Train Loss: 0.007275 Val Loss: nan\n",
      "Epoch [8376/10000] Train Loss: 0.007350 Val Loss: nan\n",
      "Epoch [8377/10000] Train Loss: 0.007141 Val Loss: nan\n",
      "Epoch [8378/10000] Train Loss: 0.006972 Val Loss: nan\n",
      "Epoch [8379/10000] Train Loss: 0.007202 Val Loss: nan\n",
      "Epoch [8380/10000] Train Loss: 0.007173 Val Loss: nan\n",
      "Epoch [8381/10000] Train Loss: 0.007099 Val Loss: nan\n",
      "Epoch [8382/10000] Train Loss: 0.006943 Val Loss: nan\n",
      "Epoch [8383/10000] Train Loss: 0.007119 Val Loss: nan\n",
      "Epoch [8384/10000] Train Loss: 0.006929 Val Loss: nan\n",
      "Epoch [8385/10000] Train Loss: 0.007075 Val Loss: nan\n",
      "Epoch [8386/10000] Train Loss: 0.006933 Val Loss: nan\n",
      "Epoch [8387/10000] Train Loss: 0.006984 Val Loss: nan\n",
      "Epoch [8388/10000] Train Loss: 0.006978 Val Loss: nan\n",
      "Epoch [8389/10000] Train Loss: 0.007136 Val Loss: nan\n",
      "Epoch [8390/10000] Train Loss: 0.007178 Val Loss: nan\n",
      "Epoch [8391/10000] Train Loss: 0.007065 Val Loss: nan\n",
      "Epoch [8392/10000] Train Loss: 0.006970 Val Loss: nan\n",
      "Epoch [8393/10000] Train Loss: 0.006956 Val Loss: nan\n",
      "Epoch [8394/10000] Train Loss: 0.007030 Val Loss: nan\n",
      "Epoch [8395/10000] Train Loss: 0.006924 Val Loss: nan\n",
      "Epoch [8396/10000] Train Loss: 0.006876 Val Loss: nan\n",
      "Epoch [8397/10000] Train Loss: 0.006896 Val Loss: nan\n",
      "Epoch [8398/10000] Train Loss: 0.007106 Val Loss: nan\n",
      "Epoch [8399/10000] Train Loss: 0.006986 Val Loss: nan\n",
      "Epoch [8400/10000] Train Loss: 0.006964 Val Loss: nan\n",
      "Epoch [8401/10000] Train Loss: 0.006963 Val Loss: nan\n",
      "Epoch [8402/10000] Train Loss: 0.007023 Val Loss: nan\n",
      "Epoch [8403/10000] Train Loss: 0.006954 Val Loss: nan\n",
      "Epoch [8404/10000] Train Loss: 0.007092 Val Loss: nan\n",
      "Epoch [8405/10000] Train Loss: 0.007022 Val Loss: nan\n",
      "Epoch [8406/10000] Train Loss: 0.006935 Val Loss: nan\n",
      "Epoch [8407/10000] Train Loss: 0.007084 Val Loss: nan\n",
      "Epoch [8408/10000] Train Loss: 0.006873 Val Loss: nan\n",
      "Epoch [8409/10000] Train Loss: 0.007132 Val Loss: nan\n",
      "Epoch [8410/10000] Train Loss: 0.007148 Val Loss: nan\n",
      "Epoch [8411/10000] Train Loss: 0.006966 Val Loss: nan\n",
      "Epoch [8412/10000] Train Loss: 0.006912 Val Loss: nan\n",
      "Epoch [8413/10000] Train Loss: 0.006906 Val Loss: nan\n",
      "Epoch [8414/10000] Train Loss: 0.006955 Val Loss: nan\n",
      "Epoch [8415/10000] Train Loss: 0.007083 Val Loss: nan\n",
      "Epoch [8416/10000] Train Loss: 0.007312 Val Loss: nan\n",
      "Epoch [8417/10000] Train Loss: 0.006980 Val Loss: nan\n",
      "Epoch [8418/10000] Train Loss: 0.007197 Val Loss: nan\n",
      "Epoch [8419/10000] Train Loss: 0.006959 Val Loss: nan\n",
      "Epoch [8420/10000] Train Loss: 0.006977 Val Loss: nan\n",
      "Epoch [8421/10000] Train Loss: 0.007016 Val Loss: nan\n",
      "Epoch [8422/10000] Train Loss: 0.007123 Val Loss: nan\n",
      "Epoch [8423/10000] Train Loss: 0.007257 Val Loss: nan\n",
      "Epoch [8424/10000] Train Loss: 0.007108 Val Loss: nan\n",
      "Epoch [8425/10000] Train Loss: 0.006929 Val Loss: nan\n",
      "Epoch [8426/10000] Train Loss: 0.007103 Val Loss: nan\n",
      "Epoch [8427/10000] Train Loss: 0.006895 Val Loss: nan\n",
      "Epoch [8428/10000] Train Loss: 0.007046 Val Loss: nan\n",
      "Epoch [8429/10000] Train Loss: 0.006932 Val Loss: nan\n",
      "Epoch [8430/10000] Train Loss: 0.006992 Val Loss: nan\n",
      "Epoch [8431/10000] Train Loss: 0.007124 Val Loss: nan\n",
      "Epoch [8432/10000] Train Loss: 0.006934 Val Loss: nan\n",
      "Epoch [8433/10000] Train Loss: 0.007117 Val Loss: nan\n",
      "Epoch [8434/10000] Train Loss: 0.007090 Val Loss: nan\n",
      "Epoch [8435/10000] Train Loss: 0.006900 Val Loss: nan\n",
      "Epoch [8436/10000] Train Loss: 0.007003 Val Loss: nan\n",
      "Epoch [8437/10000] Train Loss: 0.006887 Val Loss: nan\n",
      "Epoch [8438/10000] Train Loss: 0.007017 Val Loss: nan\n",
      "Epoch [8439/10000] Train Loss: 0.007054 Val Loss: nan\n",
      "Epoch [8440/10000] Train Loss: 0.007085 Val Loss: nan\n",
      "Epoch [8441/10000] Train Loss: 0.006919 Val Loss: nan\n",
      "Epoch [8442/10000] Train Loss: 0.007352 Val Loss: nan\n",
      "Epoch [8443/10000] Train Loss: 0.007028 Val Loss: nan\n",
      "Epoch [8444/10000] Train Loss: 0.007145 Val Loss: nan\n",
      "Epoch [8445/10000] Train Loss: 0.006955 Val Loss: nan\n",
      "Epoch [8446/10000] Train Loss: 0.006993 Val Loss: nan\n",
      "Epoch [8447/10000] Train Loss: 0.006894 Val Loss: nan\n",
      "Epoch [8448/10000] Train Loss: 0.006912 Val Loss: nan\n",
      "Epoch [8449/10000] Train Loss: 0.007111 Val Loss: nan\n",
      "Epoch [8450/10000] Train Loss: 0.007017 Val Loss: nan\n",
      "Epoch [8451/10000] Train Loss: 0.006884 Val Loss: nan\n",
      "Epoch [8452/10000] Train Loss: 0.006886 Val Loss: nan\n",
      "Epoch [8453/10000] Train Loss: 0.006976 Val Loss: nan\n",
      "Epoch [8454/10000] Train Loss: 0.006865 Val Loss: nan\n",
      "Epoch [8455/10000] Train Loss: 0.007006 Val Loss: nan\n",
      "Epoch [8456/10000] Train Loss: 0.006912 Val Loss: nan\n",
      "Epoch [8457/10000] Train Loss: 0.007027 Val Loss: nan\n",
      "Epoch [8458/10000] Train Loss: 0.006946 Val Loss: nan\n",
      "Epoch [8459/10000] Train Loss: 0.006990 Val Loss: nan\n",
      "Epoch [8460/10000] Train Loss: 0.007149 Val Loss: nan\n",
      "Epoch [8461/10000] Train Loss: 0.007045 Val Loss: nan\n",
      "Epoch [8462/10000] Train Loss: 0.006913 Val Loss: nan\n",
      "Epoch [8463/10000] Train Loss: 0.006914 Val Loss: nan\n",
      "Epoch [8464/10000] Train Loss: 0.006906 Val Loss: nan\n",
      "Epoch [8465/10000] Train Loss: 0.007071 Val Loss: nan\n",
      "Epoch [8466/10000] Train Loss: 0.007196 Val Loss: nan\n",
      "Epoch [8467/10000] Train Loss: 0.007019 Val Loss: nan\n",
      "Epoch [8468/10000] Train Loss: 0.006970 Val Loss: nan\n",
      "Epoch [8469/10000] Train Loss: 0.007180 Val Loss: nan\n",
      "Epoch [8470/10000] Train Loss: 0.007066 Val Loss: nan\n",
      "Epoch [8471/10000] Train Loss: 0.006910 Val Loss: nan\n",
      "Epoch [8472/10000] Train Loss: 0.007118 Val Loss: nan\n",
      "Epoch [8473/10000] Train Loss: 0.006913 Val Loss: nan\n",
      "Epoch [8474/10000] Train Loss: 0.007004 Val Loss: nan\n",
      "Epoch [8475/10000] Train Loss: 0.006851 Val Loss: nan\n",
      "Epoch [8476/10000] Train Loss: 0.007015 Val Loss: nan\n",
      "Epoch [8477/10000] Train Loss: 0.006866 Val Loss: nan\n",
      "Epoch [8478/10000] Train Loss: 0.006849 Val Loss: nan\n",
      "Epoch [8479/10000] Train Loss: 0.006889 Val Loss: nan\n",
      "Epoch [8480/10000] Train Loss: 0.007039 Val Loss: nan\n",
      "Epoch [8481/10000] Train Loss: 0.006919 Val Loss: nan\n",
      "Epoch [8482/10000] Train Loss: 0.007012 Val Loss: nan\n",
      "Epoch [8483/10000] Train Loss: 0.006987 Val Loss: nan\n",
      "Epoch [8484/10000] Train Loss: 0.007010 Val Loss: nan\n",
      "Epoch [8485/10000] Train Loss: 0.007054 Val Loss: nan\n",
      "Epoch [8486/10000] Train Loss: 0.007071 Val Loss: nan\n",
      "Epoch [8487/10000] Train Loss: 0.007043 Val Loss: nan\n",
      "Epoch [8488/10000] Train Loss: 0.006976 Val Loss: nan\n",
      "Epoch [8489/10000] Train Loss: 0.006960 Val Loss: nan\n",
      "Epoch [8490/10000] Train Loss: 0.006965 Val Loss: nan\n",
      "Epoch [8491/10000] Train Loss: 0.006909 Val Loss: nan\n",
      "Epoch [8492/10000] Train Loss: 0.007037 Val Loss: nan\n",
      "Epoch [8493/10000] Train Loss: 0.007105 Val Loss: nan\n",
      "Epoch [8494/10000] Train Loss: 0.007101 Val Loss: nan\n",
      "Epoch [8495/10000] Train Loss: 0.007008 Val Loss: nan\n",
      "Epoch [8496/10000] Train Loss: 0.006917 Val Loss: nan\n",
      "Epoch [8497/10000] Train Loss: 0.006926 Val Loss: nan\n",
      "Epoch [8498/10000] Train Loss: 0.006902 Val Loss: nan\n",
      "Epoch [8499/10000] Train Loss: 0.007032 Val Loss: nan\n",
      "Epoch [8500/10000] Train Loss: 0.007088 Val Loss: nan\n",
      "Epoch [8501/10000] Train Loss: 0.006928 Val Loss: nan\n",
      "Epoch [8502/10000] Train Loss: 0.006952 Val Loss: nan\n",
      "Epoch [8503/10000] Train Loss: 0.006922 Val Loss: nan\n",
      "Epoch [8504/10000] Train Loss: 0.006960 Val Loss: nan\n",
      "Epoch [8505/10000] Train Loss: 0.006977 Val Loss: nan\n",
      "Epoch [8506/10000] Train Loss: 0.006875 Val Loss: nan\n",
      "Epoch [8507/10000] Train Loss: 0.006947 Val Loss: nan\n",
      "Epoch [8508/10000] Train Loss: 0.007011 Val Loss: nan\n",
      "Epoch [8509/10000] Train Loss: 0.007049 Val Loss: nan\n",
      "Epoch [8510/10000] Train Loss: 0.006875 Val Loss: nan\n",
      "Epoch [8511/10000] Train Loss: 0.006928 Val Loss: nan\n",
      "Epoch [8512/10000] Train Loss: 0.006948 Val Loss: nan\n",
      "Epoch [8513/10000] Train Loss: 0.007056 Val Loss: nan\n",
      "Epoch [8514/10000] Train Loss: 0.006944 Val Loss: nan\n",
      "Epoch [8515/10000] Train Loss: 0.006986 Val Loss: nan\n",
      "Epoch [8516/10000] Train Loss: 0.007291 Val Loss: nan\n",
      "Epoch [8517/10000] Train Loss: 0.006990 Val Loss: nan\n",
      "Epoch [8518/10000] Train Loss: 0.006921 Val Loss: nan\n",
      "Epoch [8519/10000] Train Loss: 0.006891 Val Loss: nan\n",
      "Epoch [8520/10000] Train Loss: 0.007068 Val Loss: nan\n",
      "Epoch [8521/10000] Train Loss: 0.006882 Val Loss: nan\n",
      "Epoch [8522/10000] Train Loss: 0.007001 Val Loss: nan\n",
      "Epoch [8523/10000] Train Loss: 0.006859 Val Loss: nan\n",
      "Epoch [8524/10000] Train Loss: 0.007089 Val Loss: nan\n",
      "Epoch [8525/10000] Train Loss: 0.006860 Val Loss: nan\n",
      "Epoch [8526/10000] Train Loss: 0.006851 Val Loss: nan\n",
      "Epoch [8527/10000] Train Loss: 0.006955 Val Loss: nan\n",
      "Epoch [8528/10000] Train Loss: 0.006933 Val Loss: nan\n",
      "Epoch [8529/10000] Train Loss: 0.007073 Val Loss: nan\n",
      "Epoch [8530/10000] Train Loss: 0.006902 Val Loss: nan\n",
      "Epoch [8531/10000] Train Loss: 0.006862 Val Loss: nan\n",
      "Epoch [8532/10000] Train Loss: 0.006870 Val Loss: nan\n",
      "Epoch [8533/10000] Train Loss: 0.007077 Val Loss: nan\n",
      "Epoch [8534/10000] Train Loss: 0.006921 Val Loss: nan\n",
      "Epoch [8535/10000] Train Loss: 0.006890 Val Loss: nan\n",
      "Epoch [8536/10000] Train Loss: 0.006972 Val Loss: nan\n",
      "Epoch [8537/10000] Train Loss: 0.006995 Val Loss: nan\n",
      "Epoch [8538/10000] Train Loss: 0.007004 Val Loss: nan\n",
      "Epoch [8539/10000] Train Loss: 0.007059 Val Loss: nan\n",
      "Epoch [8540/10000] Train Loss: 0.007010 Val Loss: nan\n",
      "Epoch [8541/10000] Train Loss: 0.006942 Val Loss: nan\n",
      "Epoch [8542/10000] Train Loss: 0.007042 Val Loss: nan\n",
      "Epoch [8543/10000] Train Loss: 0.006863 Val Loss: nan\n",
      "Epoch [8544/10000] Train Loss: 0.006856 Val Loss: nan\n",
      "Epoch [8545/10000] Train Loss: 0.006973 Val Loss: nan\n",
      "Epoch [8546/10000] Train Loss: 0.006913 Val Loss: nan\n",
      "Epoch [8547/10000] Train Loss: 0.006918 Val Loss: nan\n",
      "Epoch [8548/10000] Train Loss: 0.006876 Val Loss: nan\n",
      "Epoch [8549/10000] Train Loss: 0.007078 Val Loss: nan\n",
      "Epoch [8550/10000] Train Loss: 0.007027 Val Loss: nan\n",
      "Epoch [8551/10000] Train Loss: 0.007064 Val Loss: nan\n",
      "Epoch [8552/10000] Train Loss: 0.007016 Val Loss: nan\n",
      "Epoch [8553/10000] Train Loss: 0.006889 Val Loss: nan\n",
      "Epoch [8554/10000] Train Loss: 0.006996 Val Loss: nan\n",
      "Epoch [8555/10000] Train Loss: 0.007167 Val Loss: nan\n",
      "Epoch [8556/10000] Train Loss: 0.006922 Val Loss: nan\n",
      "Epoch [8557/10000] Train Loss: 0.007078 Val Loss: nan\n",
      "Epoch [8558/10000] Train Loss: 0.006997 Val Loss: nan\n",
      "Epoch [8559/10000] Train Loss: 0.007104 Val Loss: nan\n",
      "Epoch [8560/10000] Train Loss: 0.006963 Val Loss: nan\n",
      "Epoch [8561/10000] Train Loss: 0.006959 Val Loss: nan\n",
      "Epoch [8562/10000] Train Loss: 0.006915 Val Loss: nan\n",
      "Epoch [8563/10000] Train Loss: 0.006898 Val Loss: nan\n",
      "Epoch [8564/10000] Train Loss: 0.006962 Val Loss: nan\n",
      "Epoch [8565/10000] Train Loss: 0.006886 Val Loss: nan\n",
      "Epoch [8566/10000] Train Loss: 0.007116 Val Loss: nan\n",
      "Epoch [8567/10000] Train Loss: 0.006901 Val Loss: nan\n",
      "Epoch [8568/10000] Train Loss: 0.006895 Val Loss: nan\n",
      "Epoch [8569/10000] Train Loss: 0.006881 Val Loss: nan\n",
      "Epoch [8570/10000] Train Loss: 0.006854 Val Loss: nan\n",
      "Epoch [8571/10000] Train Loss: 0.007007 Val Loss: nan\n",
      "Epoch [8572/10000] Train Loss: 0.006990 Val Loss: nan\n",
      "Epoch [8573/10000] Train Loss: 0.006997 Val Loss: nan\n",
      "Epoch [8574/10000] Train Loss: 0.006862 Val Loss: nan\n",
      "Epoch [8575/10000] Train Loss: 0.006995 Val Loss: nan\n",
      "Epoch [8576/10000] Train Loss: 0.007057 Val Loss: nan\n",
      "Epoch [8577/10000] Train Loss: 0.007062 Val Loss: nan\n",
      "Epoch [8578/10000] Train Loss: 0.006911 Val Loss: nan\n",
      "Epoch [8579/10000] Train Loss: 0.006884 Val Loss: nan\n",
      "Epoch [8580/10000] Train Loss: 0.007008 Val Loss: nan\n",
      "Epoch [8581/10000] Train Loss: 0.006908 Val Loss: nan\n",
      "Epoch [8582/10000] Train Loss: 0.006933 Val Loss: nan\n",
      "Epoch [8583/10000] Train Loss: 0.007080 Val Loss: nan\n",
      "Epoch [8584/10000] Train Loss: 0.006953 Val Loss: nan\n",
      "Epoch [8585/10000] Train Loss: 0.006944 Val Loss: nan\n",
      "Epoch [8586/10000] Train Loss: 0.006930 Val Loss: nan\n",
      "Epoch [8587/10000] Train Loss: 0.007102 Val Loss: nan\n",
      "Epoch [8588/10000] Train Loss: 0.007018 Val Loss: nan\n",
      "Epoch [8589/10000] Train Loss: 0.007043 Val Loss: nan\n",
      "Epoch [8590/10000] Train Loss: 0.006984 Val Loss: nan\n",
      "Epoch [8591/10000] Train Loss: 0.006999 Val Loss: nan\n",
      "Epoch [8592/10000] Train Loss: 0.006881 Val Loss: nan\n",
      "Epoch [8593/10000] Train Loss: 0.007117 Val Loss: nan\n",
      "Epoch [8594/10000] Train Loss: 0.006896 Val Loss: nan\n",
      "Epoch [8595/10000] Train Loss: 0.007033 Val Loss: nan\n",
      "Epoch [8596/10000] Train Loss: 0.006969 Val Loss: nan\n",
      "Epoch [8597/10000] Train Loss: 0.007207 Val Loss: nan\n",
      "Epoch [8598/10000] Train Loss: 0.006996 Val Loss: nan\n",
      "Epoch [8599/10000] Train Loss: 0.006859 Val Loss: nan\n",
      "Epoch [8600/10000] Train Loss: 0.006960 Val Loss: nan\n",
      "Epoch [8601/10000] Train Loss: 0.007013 Val Loss: nan\n",
      "Epoch [8602/10000] Train Loss: 0.006923 Val Loss: nan\n",
      "Epoch [8603/10000] Train Loss: 0.007027 Val Loss: nan\n",
      "Epoch [8604/10000] Train Loss: 0.006898 Val Loss: nan\n",
      "Epoch [8605/10000] Train Loss: 0.006904 Val Loss: nan\n",
      "Epoch [8606/10000] Train Loss: 0.006917 Val Loss: nan\n",
      "Epoch [8607/10000] Train Loss: 0.006881 Val Loss: nan\n",
      "Epoch [8608/10000] Train Loss: 0.007085 Val Loss: nan\n",
      "Epoch [8609/10000] Train Loss: 0.007029 Val Loss: nan\n",
      "Epoch [8610/10000] Train Loss: 0.006967 Val Loss: nan\n",
      "Epoch [8611/10000] Train Loss: 0.007148 Val Loss: nan\n",
      "Epoch [8612/10000] Train Loss: 0.006975 Val Loss: nan\n",
      "Epoch [8613/10000] Train Loss: 0.007113 Val Loss: nan\n",
      "Epoch [8614/10000] Train Loss: 0.006916 Val Loss: nan\n",
      "Epoch [8615/10000] Train Loss: 0.007076 Val Loss: nan\n",
      "Epoch [8616/10000] Train Loss: 0.007012 Val Loss: nan\n",
      "Epoch [8617/10000] Train Loss: 0.007001 Val Loss: nan\n",
      "Epoch [8618/10000] Train Loss: 0.006843 Val Loss: nan\n",
      "Epoch [8619/10000] Train Loss: 0.006935 Val Loss: nan\n",
      "Epoch [8620/10000] Train Loss: 0.006934 Val Loss: nan\n",
      "Epoch [8621/10000] Train Loss: 0.006899 Val Loss: nan\n",
      "Epoch [8622/10000] Train Loss: 0.006987 Val Loss: nan\n",
      "Epoch [8623/10000] Train Loss: 0.007058 Val Loss: nan\n",
      "Epoch [8624/10000] Train Loss: 0.007072 Val Loss: nan\n",
      "Epoch [8625/10000] Train Loss: 0.006969 Val Loss: nan\n",
      "Epoch [8626/10000] Train Loss: 0.007026 Val Loss: nan\n",
      "Epoch [8627/10000] Train Loss: 0.007050 Val Loss: nan\n",
      "Epoch [8628/10000] Train Loss: 0.007153 Val Loss: nan\n",
      "Epoch [8629/10000] Train Loss: 0.007123 Val Loss: nan\n",
      "Epoch [8630/10000] Train Loss: 0.006983 Val Loss: nan\n",
      "Epoch [8631/10000] Train Loss: 0.006908 Val Loss: nan\n",
      "Epoch [8632/10000] Train Loss: 0.006995 Val Loss: nan\n",
      "Epoch [8633/10000] Train Loss: 0.007040 Val Loss: nan\n",
      "Epoch [8634/10000] Train Loss: 0.007063 Val Loss: nan\n",
      "Epoch [8635/10000] Train Loss: 0.007107 Val Loss: nan\n",
      "Epoch [8636/10000] Train Loss: 0.006953 Val Loss: nan\n",
      "Epoch [8637/10000] Train Loss: 0.006861 Val Loss: nan\n",
      "Epoch [8638/10000] Train Loss: 0.006967 Val Loss: nan\n",
      "Epoch [8639/10000] Train Loss: 0.006938 Val Loss: nan\n",
      "Epoch [8640/10000] Train Loss: 0.007229 Val Loss: nan\n",
      "Epoch [8641/10000] Train Loss: 0.007141 Val Loss: nan\n",
      "Epoch [8642/10000] Train Loss: 0.007037 Val Loss: nan\n",
      "Epoch [8643/10000] Train Loss: 0.006911 Val Loss: nan\n",
      "Epoch [8644/10000] Train Loss: 0.007004 Val Loss: nan\n",
      "Epoch [8645/10000] Train Loss: 0.006886 Val Loss: nan\n",
      "Epoch [8646/10000] Train Loss: 0.006887 Val Loss: nan\n",
      "Epoch [8647/10000] Train Loss: 0.006962 Val Loss: nan\n",
      "Epoch [8648/10000] Train Loss: 0.007032 Val Loss: nan\n",
      "Epoch [8649/10000] Train Loss: 0.007048 Val Loss: nan\n",
      "Epoch [8650/10000] Train Loss: 0.006894 Val Loss: nan\n",
      "Epoch [8651/10000] Train Loss: 0.006991 Val Loss: nan\n",
      "Epoch [8652/10000] Train Loss: 0.006954 Val Loss: nan\n",
      "Epoch [8653/10000] Train Loss: 0.006870 Val Loss: nan\n",
      "Epoch [8654/10000] Train Loss: 0.006905 Val Loss: nan\n",
      "Epoch [8655/10000] Train Loss: 0.006921 Val Loss: nan\n",
      "Epoch [8656/10000] Train Loss: 0.007033 Val Loss: nan\n",
      "Epoch [8657/10000] Train Loss: 0.007000 Val Loss: nan\n",
      "Epoch [8658/10000] Train Loss: 0.006934 Val Loss: nan\n",
      "Epoch [8659/10000] Train Loss: 0.007204 Val Loss: nan\n",
      "Epoch [8660/10000] Train Loss: 0.006983 Val Loss: nan\n",
      "Epoch [8661/10000] Train Loss: 0.006900 Val Loss: nan\n",
      "Epoch [8662/10000] Train Loss: 0.007065 Val Loss: nan\n",
      "Epoch [8663/10000] Train Loss: 0.007015 Val Loss: nan\n",
      "Epoch [8664/10000] Train Loss: 0.006966 Val Loss: nan\n",
      "Epoch [8665/10000] Train Loss: 0.006943 Val Loss: nan\n",
      "Epoch [8666/10000] Train Loss: 0.006844 Val Loss: nan\n",
      "Epoch [8667/10000] Train Loss: 0.006894 Val Loss: nan\n",
      "Epoch [8668/10000] Train Loss: 0.006956 Val Loss: nan\n",
      "Epoch [8669/10000] Train Loss: 0.007014 Val Loss: nan\n",
      "Epoch [8670/10000] Train Loss: 0.007086 Val Loss: nan\n",
      "Epoch [8671/10000] Train Loss: 0.006914 Val Loss: nan\n",
      "Epoch [8672/10000] Train Loss: 0.006915 Val Loss: nan\n",
      "Epoch [8673/10000] Train Loss: 0.007183 Val Loss: nan\n",
      "Epoch [8674/10000] Train Loss: 0.006876 Val Loss: nan\n",
      "Epoch [8675/10000] Train Loss: 0.007100 Val Loss: nan\n",
      "Epoch [8676/10000] Train Loss: 0.006950 Val Loss: nan\n",
      "Epoch [8677/10000] Train Loss: 0.006981 Val Loss: nan\n",
      "Epoch [8678/10000] Train Loss: 0.006947 Val Loss: nan\n",
      "Epoch [8679/10000] Train Loss: 0.006844 Val Loss: nan\n",
      "Epoch [8680/10000] Train Loss: 0.006871 Val Loss: nan\n",
      "Epoch [8681/10000] Train Loss: 0.006864 Val Loss: nan\n",
      "Epoch [8682/10000] Train Loss: 0.006880 Val Loss: nan\n",
      "Epoch [8683/10000] Train Loss: 0.006886 Val Loss: nan\n",
      "Epoch [8684/10000] Train Loss: 0.006870 Val Loss: nan\n",
      "Epoch [8685/10000] Train Loss: 0.007129 Val Loss: nan\n",
      "Epoch [8686/10000] Train Loss: 0.007083 Val Loss: nan\n",
      "Epoch [8687/10000] Train Loss: 0.006872 Val Loss: nan\n",
      "Epoch [8688/10000] Train Loss: 0.006857 Val Loss: nan\n",
      "Epoch [8689/10000] Train Loss: 0.007066 Val Loss: nan\n",
      "Epoch [8690/10000] Train Loss: 0.006869 Val Loss: nan\n",
      "Epoch [8691/10000] Train Loss: 0.007249 Val Loss: nan\n",
      "Epoch [8692/10000] Train Loss: 0.007032 Val Loss: nan\n",
      "Epoch [8693/10000] Train Loss: 0.007040 Val Loss: nan\n",
      "Epoch [8694/10000] Train Loss: 0.007113 Val Loss: nan\n",
      "Epoch [8695/10000] Train Loss: 0.006992 Val Loss: nan\n",
      "Epoch [8696/10000] Train Loss: 0.007074 Val Loss: nan\n",
      "Epoch [8697/10000] Train Loss: 0.006869 Val Loss: nan\n",
      "Epoch [8698/10000] Train Loss: 0.007008 Val Loss: nan\n",
      "Epoch [8699/10000] Train Loss: 0.006956 Val Loss: nan\n",
      "Epoch [8700/10000] Train Loss: 0.007022 Val Loss: nan\n",
      "Epoch [8701/10000] Train Loss: 0.006988 Val Loss: nan\n",
      "Epoch [8702/10000] Train Loss: 0.006938 Val Loss: nan\n",
      "Epoch [8703/10000] Train Loss: 0.006861 Val Loss: nan\n",
      "Epoch [8704/10000] Train Loss: 0.006948 Val Loss: nan\n",
      "Epoch [8705/10000] Train Loss: 0.006863 Val Loss: nan\n",
      "Epoch [8706/10000] Train Loss: 0.006953 Val Loss: nan\n",
      "Epoch [8707/10000] Train Loss: 0.006952 Val Loss: nan\n",
      "Epoch [8708/10000] Train Loss: 0.007209 Val Loss: nan\n",
      "Epoch [8709/10000] Train Loss: 0.006953 Val Loss: nan\n",
      "Epoch [8710/10000] Train Loss: 0.006965 Val Loss: nan\n",
      "Epoch [8711/10000] Train Loss: 0.006845 Val Loss: nan\n",
      "Epoch [8712/10000] Train Loss: 0.006956 Val Loss: nan\n",
      "Epoch [8713/10000] Train Loss: 0.006926 Val Loss: nan\n",
      "Epoch [8714/10000] Train Loss: 0.006883 Val Loss: nan\n",
      "Epoch [8715/10000] Train Loss: 0.006960 Val Loss: nan\n",
      "Epoch [8716/10000] Train Loss: 0.006943 Val Loss: nan\n",
      "Epoch [8717/10000] Train Loss: 0.007071 Val Loss: nan\n",
      "Epoch [8718/10000] Train Loss: 0.007030 Val Loss: nan\n",
      "Epoch [8719/10000] Train Loss: 0.006884 Val Loss: nan\n",
      "Epoch [8720/10000] Train Loss: 0.006964 Val Loss: nan\n",
      "Epoch [8721/10000] Train Loss: 0.006910 Val Loss: nan\n",
      "Epoch [8722/10000] Train Loss: 0.007072 Val Loss: nan\n",
      "Epoch [8723/10000] Train Loss: 0.006999 Val Loss: nan\n",
      "Epoch [8724/10000] Train Loss: 0.006868 Val Loss: nan\n",
      "Epoch [8725/10000] Train Loss: 0.006833 Val Loss: nan\n",
      "Epoch [8726/10000] Train Loss: 0.006865 Val Loss: nan\n",
      "Epoch [8727/10000] Train Loss: 0.007025 Val Loss: nan\n",
      "Epoch [8728/10000] Train Loss: 0.007103 Val Loss: nan\n",
      "Epoch [8729/10000] Train Loss: 0.006936 Val Loss: nan\n",
      "Epoch [8730/10000] Train Loss: 0.006893 Val Loss: nan\n",
      "Epoch [8731/10000] Train Loss: 0.006845 Val Loss: nan\n",
      "Epoch [8732/10000] Train Loss: 0.007027 Val Loss: nan\n",
      "Epoch [8733/10000] Train Loss: 0.007080 Val Loss: nan\n",
      "Epoch [8734/10000] Train Loss: 0.006840 Val Loss: nan\n",
      "Epoch [8735/10000] Train Loss: 0.006877 Val Loss: nan\n",
      "Epoch [8736/10000] Train Loss: 0.006952 Val Loss: nan\n",
      "Epoch [8737/10000] Train Loss: 0.007014 Val Loss: nan\n",
      "Epoch [8738/10000] Train Loss: 0.007044 Val Loss: nan\n",
      "Epoch [8739/10000] Train Loss: 0.006888 Val Loss: nan\n",
      "Epoch [8740/10000] Train Loss: 0.007114 Val Loss: nan\n",
      "Epoch [8741/10000] Train Loss: 0.006879 Val Loss: nan\n",
      "Epoch [8742/10000] Train Loss: 0.006866 Val Loss: nan\n",
      "Epoch [8743/10000] Train Loss: 0.006960 Val Loss: nan\n",
      "Epoch [8744/10000] Train Loss: 0.007107 Val Loss: nan\n",
      "Epoch [8745/10000] Train Loss: 0.007145 Val Loss: nan\n",
      "Epoch [8746/10000] Train Loss: 0.007016 Val Loss: nan\n",
      "Epoch [8747/10000] Train Loss: 0.006909 Val Loss: nan\n",
      "Epoch [8748/10000] Train Loss: 0.006881 Val Loss: nan\n",
      "Epoch [8749/10000] Train Loss: 0.006928 Val Loss: nan\n",
      "Epoch [8750/10000] Train Loss: 0.006897 Val Loss: nan\n",
      "Epoch [8751/10000] Train Loss: 0.006971 Val Loss: nan\n",
      "Epoch [8752/10000] Train Loss: 0.006865 Val Loss: nan\n",
      "Epoch [8753/10000] Train Loss: 0.006859 Val Loss: nan\n",
      "Epoch [8754/10000] Train Loss: 0.006886 Val Loss: nan\n",
      "Epoch [8755/10000] Train Loss: 0.006955 Val Loss: nan\n",
      "Epoch [8756/10000] Train Loss: 0.006840 Val Loss: nan\n",
      "Epoch [8757/10000] Train Loss: 0.006940 Val Loss: nan\n",
      "Epoch [8758/10000] Train Loss: 0.007107 Val Loss: nan\n",
      "Epoch [8759/10000] Train Loss: 0.006917 Val Loss: nan\n",
      "Epoch [8760/10000] Train Loss: 0.006977 Val Loss: nan\n",
      "Epoch [8761/10000] Train Loss: 0.006983 Val Loss: nan\n",
      "Epoch [8762/10000] Train Loss: 0.007110 Val Loss: nan\n",
      "Epoch [8763/10000] Train Loss: 0.007004 Val Loss: nan\n",
      "Epoch [8764/10000] Train Loss: 0.006877 Val Loss: nan\n",
      "Epoch [8765/10000] Train Loss: 0.007018 Val Loss: nan\n",
      "Epoch [8766/10000] Train Loss: 0.007034 Val Loss: nan\n",
      "Epoch [8767/10000] Train Loss: 0.006987 Val Loss: nan\n",
      "Epoch [8768/10000] Train Loss: 0.006846 Val Loss: nan\n",
      "Epoch [8769/10000] Train Loss: 0.007090 Val Loss: nan\n",
      "Epoch [8770/10000] Train Loss: 0.007029 Val Loss: nan\n",
      "Epoch [8771/10000] Train Loss: 0.006888 Val Loss: nan\n",
      "Epoch [8772/10000] Train Loss: 0.006974 Val Loss: nan\n",
      "Epoch [8773/10000] Train Loss: 0.006886 Val Loss: nan\n",
      "Epoch [8774/10000] Train Loss: 0.006923 Val Loss: nan\n",
      "Epoch [8775/10000] Train Loss: 0.006892 Val Loss: nan\n",
      "Epoch [8776/10000] Train Loss: 0.006917 Val Loss: nan\n",
      "Epoch [8777/10000] Train Loss: 0.006900 Val Loss: nan\n",
      "Epoch [8778/10000] Train Loss: 0.007161 Val Loss: nan\n",
      "Epoch [8779/10000] Train Loss: 0.006930 Val Loss: nan\n",
      "Epoch [8780/10000] Train Loss: 0.006963 Val Loss: nan\n",
      "Epoch [8781/10000] Train Loss: 0.006899 Val Loss: nan\n",
      "Epoch [8782/10000] Train Loss: 0.007072 Val Loss: nan\n",
      "Epoch [8783/10000] Train Loss: 0.007058 Val Loss: nan\n",
      "Epoch [8784/10000] Train Loss: 0.007017 Val Loss: nan\n",
      "Epoch [8785/10000] Train Loss: 0.007289 Val Loss: nan\n",
      "Epoch [8786/10000] Train Loss: 0.006968 Val Loss: nan\n",
      "Epoch [8787/10000] Train Loss: 0.006935 Val Loss: nan\n",
      "Epoch [8788/10000] Train Loss: 0.006838 Val Loss: nan\n",
      "Epoch [8789/10000] Train Loss: 0.006881 Val Loss: nan\n",
      "Epoch [8790/10000] Train Loss: 0.006979 Val Loss: nan\n",
      "Epoch [8791/10000] Train Loss: 0.006982 Val Loss: nan\n",
      "Epoch [8792/10000] Train Loss: 0.006988 Val Loss: nan\n",
      "Epoch [8793/10000] Train Loss: 0.006920 Val Loss: nan\n",
      "Epoch [8794/10000] Train Loss: 0.006884 Val Loss: nan\n",
      "Epoch [8795/10000] Train Loss: 0.006994 Val Loss: nan\n",
      "Epoch [8796/10000] Train Loss: 0.006912 Val Loss: nan\n",
      "Epoch [8797/10000] Train Loss: 0.006797 Val Loss: nan\n",
      "Epoch [8798/10000] Train Loss: 0.006861 Val Loss: nan\n",
      "Epoch [8799/10000] Train Loss: 0.006853 Val Loss: nan\n",
      "Epoch [8800/10000] Train Loss: 0.006840 Val Loss: nan\n",
      "Epoch [8801/10000] Train Loss: 0.006890 Val Loss: nan\n",
      "Epoch [8802/10000] Train Loss: 0.006907 Val Loss: nan\n",
      "Epoch [8803/10000] Train Loss: 0.007138 Val Loss: nan\n",
      "Epoch [8804/10000] Train Loss: 0.006899 Val Loss: nan\n",
      "Epoch [8805/10000] Train Loss: 0.006880 Val Loss: nan\n",
      "Epoch [8806/10000] Train Loss: 0.006982 Val Loss: nan\n",
      "Epoch [8807/10000] Train Loss: 0.006972 Val Loss: nan\n",
      "Epoch [8808/10000] Train Loss: 0.006859 Val Loss: nan\n",
      "Epoch [8809/10000] Train Loss: 0.006974 Val Loss: nan\n",
      "Epoch [8810/10000] Train Loss: 0.006864 Val Loss: nan\n",
      "Epoch [8811/10000] Train Loss: 0.007091 Val Loss: nan\n",
      "Epoch [8812/10000] Train Loss: 0.006994 Val Loss: nan\n",
      "Epoch [8813/10000] Train Loss: 0.006909 Val Loss: nan\n",
      "Epoch [8814/10000] Train Loss: 0.006836 Val Loss: nan\n",
      "Epoch [8815/10000] Train Loss: 0.006858 Val Loss: nan\n",
      "Epoch [8816/10000] Train Loss: 0.007127 Val Loss: nan\n",
      "Epoch [8817/10000] Train Loss: 0.007005 Val Loss: nan\n",
      "Epoch [8818/10000] Train Loss: 0.007036 Val Loss: nan\n",
      "Epoch [8819/10000] Train Loss: 0.006908 Val Loss: nan\n",
      "Epoch [8820/10000] Train Loss: 0.007088 Val Loss: nan\n",
      "Epoch [8821/10000] Train Loss: 0.006978 Val Loss: nan\n",
      "Epoch [8822/10000] Train Loss: 0.006911 Val Loss: nan\n",
      "Epoch [8823/10000] Train Loss: 0.007032 Val Loss: nan\n",
      "Epoch [8824/10000] Train Loss: 0.006990 Val Loss: nan\n",
      "Epoch [8825/10000] Train Loss: 0.006896 Val Loss: nan\n",
      "Epoch [8826/10000] Train Loss: 0.006910 Val Loss: nan\n",
      "Epoch [8827/10000] Train Loss: 0.007095 Val Loss: nan\n",
      "Epoch [8828/10000] Train Loss: 0.006888 Val Loss: nan\n",
      "Epoch [8829/10000] Train Loss: 0.006999 Val Loss: nan\n",
      "Epoch [8830/10000] Train Loss: 0.006842 Val Loss: nan\n",
      "Epoch [8831/10000] Train Loss: 0.007041 Val Loss: nan\n",
      "Epoch [8832/10000] Train Loss: 0.006987 Val Loss: nan\n",
      "Epoch [8833/10000] Train Loss: 0.006950 Val Loss: nan\n",
      "Epoch [8834/10000] Train Loss: 0.006947 Val Loss: nan\n",
      "Epoch [8835/10000] Train Loss: 0.006850 Val Loss: nan\n",
      "Epoch [8836/10000] Train Loss: 0.007057 Val Loss: nan\n",
      "Epoch [8837/10000] Train Loss: 0.006876 Val Loss: nan\n",
      "Epoch [8838/10000] Train Loss: 0.007006 Val Loss: nan\n",
      "Epoch [8839/10000] Train Loss: 0.006948 Val Loss: nan\n",
      "Epoch [8840/10000] Train Loss: 0.006852 Val Loss: nan\n",
      "Epoch [8841/10000] Train Loss: 0.006826 Val Loss: nan\n",
      "Epoch [8842/10000] Train Loss: 0.007075 Val Loss: nan\n",
      "Epoch [8843/10000] Train Loss: 0.006947 Val Loss: nan\n",
      "Epoch [8844/10000] Train Loss: 0.006948 Val Loss: nan\n",
      "Epoch [8845/10000] Train Loss: 0.006999 Val Loss: nan\n",
      "Epoch [8846/10000] Train Loss: 0.006938 Val Loss: nan\n",
      "Epoch [8847/10000] Train Loss: 0.006972 Val Loss: nan\n",
      "Epoch [8848/10000] Train Loss: 0.006956 Val Loss: nan\n",
      "Epoch [8849/10000] Train Loss: 0.007107 Val Loss: nan\n",
      "Epoch [8850/10000] Train Loss: 0.006911 Val Loss: nan\n",
      "Epoch [8851/10000] Train Loss: 0.006826 Val Loss: nan\n",
      "Epoch [8852/10000] Train Loss: 0.006964 Val Loss: nan\n",
      "Epoch [8853/10000] Train Loss: 0.006874 Val Loss: nan\n",
      "Epoch [8854/10000] Train Loss: 0.006956 Val Loss: nan\n",
      "Epoch [8855/10000] Train Loss: 0.007014 Val Loss: nan\n",
      "Epoch [8856/10000] Train Loss: 0.006918 Val Loss: nan\n",
      "Epoch [8857/10000] Train Loss: 0.006989 Val Loss: nan\n",
      "Epoch [8858/10000] Train Loss: 0.006982 Val Loss: nan\n",
      "Epoch [8859/10000] Train Loss: 0.006934 Val Loss: nan\n",
      "Epoch [8860/10000] Train Loss: 0.006961 Val Loss: nan\n",
      "Epoch [8861/10000] Train Loss: 0.006890 Val Loss: nan\n",
      "Epoch [8862/10000] Train Loss: 0.007052 Val Loss: nan\n",
      "Epoch [8863/10000] Train Loss: 0.006864 Val Loss: nan\n",
      "Epoch [8864/10000] Train Loss: 0.006861 Val Loss: nan\n",
      "Epoch [8865/10000] Train Loss: 0.006852 Val Loss: nan\n",
      "Epoch [8866/10000] Train Loss: 0.006843 Val Loss: nan\n",
      "Epoch [8867/10000] Train Loss: 0.006952 Val Loss: nan\n",
      "Epoch [8868/10000] Train Loss: 0.006980 Val Loss: nan\n",
      "Epoch [8869/10000] Train Loss: 0.006919 Val Loss: nan\n",
      "Epoch [8870/10000] Train Loss: 0.007051 Val Loss: nan\n",
      "Epoch [8871/10000] Train Loss: 0.006877 Val Loss: nan\n",
      "Epoch [8872/10000] Train Loss: 0.006826 Val Loss: nan\n",
      "Epoch [8873/10000] Train Loss: 0.006942 Val Loss: nan\n",
      "Epoch [8874/10000] Train Loss: 0.007000 Val Loss: nan\n",
      "Epoch [8875/10000] Train Loss: 0.006880 Val Loss: nan\n",
      "Epoch [8876/10000] Train Loss: 0.006970 Val Loss: nan\n",
      "Epoch [8877/10000] Train Loss: 0.006956 Val Loss: nan\n",
      "Epoch [8878/10000] Train Loss: 0.006793 Val Loss: nan\n",
      "Epoch [8879/10000] Train Loss: 0.007077 Val Loss: nan\n",
      "Epoch [8880/10000] Train Loss: 0.006957 Val Loss: nan\n",
      "Epoch [8881/10000] Train Loss: 0.006966 Val Loss: nan\n",
      "Epoch [8882/10000] Train Loss: 0.007070 Val Loss: nan\n",
      "Epoch [8883/10000] Train Loss: 0.006980 Val Loss: nan\n",
      "Epoch [8884/10000] Train Loss: 0.006936 Val Loss: nan\n",
      "Epoch [8885/10000] Train Loss: 0.006891 Val Loss: nan\n",
      "Epoch [8886/10000] Train Loss: 0.006979 Val Loss: nan\n",
      "Epoch [8887/10000] Train Loss: 0.006884 Val Loss: nan\n",
      "Epoch [8888/10000] Train Loss: 0.006869 Val Loss: nan\n",
      "Epoch [8889/10000] Train Loss: 0.006901 Val Loss: nan\n",
      "Epoch [8890/10000] Train Loss: 0.007162 Val Loss: nan\n",
      "Epoch [8891/10000] Train Loss: 0.006947 Val Loss: nan\n",
      "Epoch [8892/10000] Train Loss: 0.006855 Val Loss: nan\n",
      "Epoch [8893/10000] Train Loss: 0.006951 Val Loss: nan\n",
      "Epoch [8894/10000] Train Loss: 0.006836 Val Loss: nan\n",
      "Epoch [8895/10000] Train Loss: 0.006964 Val Loss: nan\n",
      "Epoch [8896/10000] Train Loss: 0.006995 Val Loss: nan\n",
      "Epoch [8897/10000] Train Loss: 0.006912 Val Loss: nan\n",
      "Epoch [8898/10000] Train Loss: 0.006958 Val Loss: nan\n",
      "Epoch [8899/10000] Train Loss: 0.006904 Val Loss: nan\n",
      "Epoch [8900/10000] Train Loss: 0.006972 Val Loss: nan\n",
      "Epoch [8901/10000] Train Loss: 0.007083 Val Loss: nan\n",
      "Epoch [8902/10000] Train Loss: 0.006912 Val Loss: nan\n",
      "Epoch [8903/10000] Train Loss: 0.006921 Val Loss: nan\n",
      "Epoch [8904/10000] Train Loss: 0.006865 Val Loss: nan\n",
      "Epoch [8905/10000] Train Loss: 0.006910 Val Loss: nan\n",
      "Epoch [8906/10000] Train Loss: 0.006948 Val Loss: nan\n",
      "Epoch [8907/10000] Train Loss: 0.006819 Val Loss: nan\n",
      "Epoch [8908/10000] Train Loss: 0.006890 Val Loss: nan\n",
      "Epoch [8909/10000] Train Loss: 0.007105 Val Loss: nan\n",
      "Epoch [8910/10000] Train Loss: 0.006862 Val Loss: nan\n",
      "Epoch [8911/10000] Train Loss: 0.007105 Val Loss: nan\n",
      "Epoch [8912/10000] Train Loss: 0.006854 Val Loss: nan\n",
      "Epoch [8913/10000] Train Loss: 0.006800 Val Loss: nan\n",
      "Epoch [8914/10000] Train Loss: 0.007014 Val Loss: nan\n",
      "Epoch [8915/10000] Train Loss: 0.006930 Val Loss: nan\n",
      "Epoch [8916/10000] Train Loss: 0.006900 Val Loss: nan\n",
      "Epoch [8917/10000] Train Loss: 0.006865 Val Loss: nan\n",
      "Epoch [8918/10000] Train Loss: 0.006963 Val Loss: nan\n",
      "Epoch [8919/10000] Train Loss: 0.006969 Val Loss: nan\n",
      "Epoch [8920/10000] Train Loss: 0.006949 Val Loss: nan\n",
      "Epoch [8921/10000] Train Loss: 0.007014 Val Loss: nan\n",
      "Epoch [8922/10000] Train Loss: 0.007148 Val Loss: nan\n",
      "Epoch [8923/10000] Train Loss: 0.006978 Val Loss: nan\n",
      "Epoch [8924/10000] Train Loss: 0.006991 Val Loss: nan\n",
      "Epoch [8925/10000] Train Loss: 0.006838 Val Loss: nan\n",
      "Epoch [8926/10000] Train Loss: 0.007028 Val Loss: nan\n",
      "Epoch [8927/10000] Train Loss: 0.006954 Val Loss: nan\n",
      "Epoch [8928/10000] Train Loss: 0.007034 Val Loss: nan\n",
      "Epoch [8929/10000] Train Loss: 0.006998 Val Loss: nan\n",
      "Epoch [8930/10000] Train Loss: 0.006934 Val Loss: nan\n",
      "Epoch [8931/10000] Train Loss: 0.006914 Val Loss: nan\n",
      "Epoch [8932/10000] Train Loss: 0.006958 Val Loss: nan\n",
      "Epoch [8933/10000] Train Loss: 0.006876 Val Loss: nan\n",
      "Epoch [8934/10000] Train Loss: 0.006965 Val Loss: nan\n",
      "Epoch [8935/10000] Train Loss: 0.006987 Val Loss: nan\n",
      "Epoch [8936/10000] Train Loss: 0.007080 Val Loss: nan\n",
      "Epoch [8937/10000] Train Loss: 0.006952 Val Loss: nan\n",
      "Epoch [8938/10000] Train Loss: 0.007142 Val Loss: nan\n",
      "Epoch [8939/10000] Train Loss: 0.006949 Val Loss: nan\n",
      "Epoch [8940/10000] Train Loss: 0.006838 Val Loss: nan\n",
      "Epoch [8941/10000] Train Loss: 0.006947 Val Loss: nan\n",
      "Epoch [8942/10000] Train Loss: 0.006831 Val Loss: nan\n",
      "Epoch [8943/10000] Train Loss: 0.007072 Val Loss: nan\n",
      "Epoch [8944/10000] Train Loss: 0.006868 Val Loss: nan\n",
      "Epoch [8945/10000] Train Loss: 0.007048 Val Loss: nan\n",
      "Epoch [8946/10000] Train Loss: 0.006895 Val Loss: nan\n",
      "Epoch [8947/10000] Train Loss: 0.006930 Val Loss: nan\n",
      "Epoch [8948/10000] Train Loss: 0.006959 Val Loss: nan\n",
      "Epoch [8949/10000] Train Loss: 0.007099 Val Loss: nan\n",
      "Epoch [8950/10000] Train Loss: 0.006963 Val Loss: nan\n",
      "Epoch [8951/10000] Train Loss: 0.006989 Val Loss: nan\n",
      "Epoch [8952/10000] Train Loss: 0.007020 Val Loss: nan\n",
      "Epoch [8953/10000] Train Loss: 0.007166 Val Loss: nan\n",
      "Epoch [8954/10000] Train Loss: 0.006915 Val Loss: nan\n",
      "Epoch [8955/10000] Train Loss: 0.006980 Val Loss: nan\n",
      "Epoch [8956/10000] Train Loss: 0.006959 Val Loss: nan\n",
      "Epoch [8957/10000] Train Loss: 0.006991 Val Loss: nan\n",
      "Epoch [8958/10000] Train Loss: 0.007135 Val Loss: nan\n",
      "Epoch [8959/10000] Train Loss: 0.006893 Val Loss: nan\n",
      "Epoch [8960/10000] Train Loss: 0.006910 Val Loss: nan\n",
      "Epoch [8961/10000] Train Loss: 0.007049 Val Loss: nan\n",
      "Epoch [8962/10000] Train Loss: 0.006925 Val Loss: nan\n",
      "Epoch [8963/10000] Train Loss: 0.006924 Val Loss: nan\n",
      "Epoch [8964/10000] Train Loss: 0.006976 Val Loss: nan\n",
      "Epoch [8965/10000] Train Loss: 0.006934 Val Loss: nan\n",
      "Epoch [8966/10000] Train Loss: 0.006953 Val Loss: nan\n",
      "Epoch [8967/10000] Train Loss: 0.007016 Val Loss: nan\n",
      "Epoch [8968/10000] Train Loss: 0.006869 Val Loss: nan\n",
      "Epoch [8969/10000] Train Loss: 0.006877 Val Loss: nan\n",
      "Epoch [8970/10000] Train Loss: 0.006972 Val Loss: nan\n",
      "Epoch [8971/10000] Train Loss: 0.006885 Val Loss: nan\n",
      "Epoch [8972/10000] Train Loss: 0.007069 Val Loss: nan\n",
      "Epoch [8973/10000] Train Loss: 0.006883 Val Loss: nan\n",
      "Epoch [8974/10000] Train Loss: 0.006885 Val Loss: nan\n",
      "Epoch [8975/10000] Train Loss: 0.006875 Val Loss: nan\n",
      "Epoch [8976/10000] Train Loss: 0.007063 Val Loss: nan\n",
      "Epoch [8977/10000] Train Loss: 0.006889 Val Loss: nan\n",
      "Epoch [8978/10000] Train Loss: 0.007194 Val Loss: nan\n",
      "Epoch [8979/10000] Train Loss: 0.006952 Val Loss: nan\n",
      "Epoch [8980/10000] Train Loss: 0.006944 Val Loss: nan\n",
      "Epoch [8981/10000] Train Loss: 0.007087 Val Loss: nan\n",
      "Epoch [8982/10000] Train Loss: 0.007035 Val Loss: nan\n",
      "Epoch [8983/10000] Train Loss: 0.006897 Val Loss: nan\n",
      "Epoch [8984/10000] Train Loss: 0.006857 Val Loss: nan\n",
      "Epoch [8985/10000] Train Loss: 0.006981 Val Loss: nan\n",
      "Epoch [8986/10000] Train Loss: 0.006890 Val Loss: nan\n",
      "Epoch [8987/10000] Train Loss: 0.007140 Val Loss: nan\n",
      "Epoch [8988/10000] Train Loss: 0.006884 Val Loss: nan\n",
      "Epoch [8989/10000] Train Loss: 0.006956 Val Loss: nan\n",
      "Epoch [8990/10000] Train Loss: 0.006846 Val Loss: nan\n",
      "Epoch [8991/10000] Train Loss: 0.006881 Val Loss: nan\n",
      "Epoch [8992/10000] Train Loss: 0.006843 Val Loss: nan\n",
      "Epoch [8993/10000] Train Loss: 0.006927 Val Loss: nan\n",
      "Epoch [8994/10000] Train Loss: 0.007011 Val Loss: nan\n",
      "Epoch [8995/10000] Train Loss: 0.006918 Val Loss: nan\n",
      "Epoch [8996/10000] Train Loss: 0.006869 Val Loss: nan\n",
      "Epoch [8997/10000] Train Loss: 0.006836 Val Loss: nan\n",
      "Epoch [8998/10000] Train Loss: 0.006903 Val Loss: nan\n",
      "Epoch [8999/10000] Train Loss: 0.006939 Val Loss: nan\n",
      "Epoch [9000/10000] Train Loss: 0.007019 Val Loss: nan\n",
      "Epoch [9001/10000] Train Loss: 0.006842 Val Loss: nan\n",
      "Epoch [9002/10000] Train Loss: 0.006868 Val Loss: nan\n",
      "Epoch [9003/10000] Train Loss: 0.007068 Val Loss: nan\n",
      "Epoch [9004/10000] Train Loss: 0.006840 Val Loss: nan\n",
      "Epoch [9005/10000] Train Loss: 0.006979 Val Loss: nan\n",
      "Epoch [9006/10000] Train Loss: 0.006848 Val Loss: nan\n",
      "Epoch [9007/10000] Train Loss: 0.006829 Val Loss: nan\n",
      "Epoch [9008/10000] Train Loss: 0.006856 Val Loss: nan\n",
      "Epoch [9009/10000] Train Loss: 0.006922 Val Loss: nan\n",
      "Epoch [9010/10000] Train Loss: 0.006941 Val Loss: nan\n",
      "Epoch [9011/10000] Train Loss: 0.006915 Val Loss: nan\n",
      "Epoch [9012/10000] Train Loss: 0.006814 Val Loss: nan\n",
      "Epoch [9013/10000] Train Loss: 0.006996 Val Loss: nan\n",
      "Epoch [9014/10000] Train Loss: 0.006815 Val Loss: nan\n",
      "Epoch [9015/10000] Train Loss: 0.006825 Val Loss: nan\n",
      "Epoch [9016/10000] Train Loss: 0.006894 Val Loss: nan\n",
      "Epoch [9017/10000] Train Loss: 0.006831 Val Loss: nan\n",
      "Epoch [9018/10000] Train Loss: 0.006940 Val Loss: nan\n",
      "Epoch [9019/10000] Train Loss: 0.006797 Val Loss: nan\n",
      "Epoch [9020/10000] Train Loss: 0.006814 Val Loss: nan\n",
      "Epoch [9021/10000] Train Loss: 0.006799 Val Loss: nan\n",
      "Epoch [9022/10000] Train Loss: 0.006840 Val Loss: nan\n",
      "Epoch [9023/10000] Train Loss: 0.006801 Val Loss: nan\n",
      "Epoch [9024/10000] Train Loss: 0.006868 Val Loss: nan\n",
      "Epoch [9025/10000] Train Loss: 0.006872 Val Loss: nan\n",
      "Epoch [9026/10000] Train Loss: 0.007064 Val Loss: nan\n",
      "Epoch [9027/10000] Train Loss: 0.007006 Val Loss: nan\n",
      "Epoch [9028/10000] Train Loss: 0.006884 Val Loss: nan\n",
      "Epoch [9029/10000] Train Loss: 0.006979 Val Loss: nan\n",
      "Epoch [9030/10000] Train Loss: 0.006936 Val Loss: nan\n",
      "Epoch [9031/10000] Train Loss: 0.006845 Val Loss: nan\n",
      "Epoch [9032/10000] Train Loss: 0.006836 Val Loss: nan\n",
      "Epoch [9033/10000] Train Loss: 0.006954 Val Loss: nan\n",
      "Epoch [9034/10000] Train Loss: 0.006957 Val Loss: nan\n",
      "Epoch [9035/10000] Train Loss: 0.007035 Val Loss: nan\n",
      "Epoch [9036/10000] Train Loss: 0.006975 Val Loss: nan\n",
      "Epoch [9037/10000] Train Loss: 0.006970 Val Loss: nan\n",
      "Epoch [9038/10000] Train Loss: 0.006814 Val Loss: nan\n",
      "Epoch [9039/10000] Train Loss: 0.006853 Val Loss: nan\n",
      "Epoch [9040/10000] Train Loss: 0.006846 Val Loss: nan\n",
      "Epoch [9041/10000] Train Loss: 0.006933 Val Loss: nan\n",
      "Epoch [9042/10000] Train Loss: 0.007003 Val Loss: nan\n",
      "Epoch [9043/10000] Train Loss: 0.007049 Val Loss: nan\n",
      "Epoch [9044/10000] Train Loss: 0.006902 Val Loss: nan\n",
      "Epoch [9045/10000] Train Loss: 0.006851 Val Loss: nan\n",
      "Epoch [9046/10000] Train Loss: 0.007038 Val Loss: nan\n",
      "Epoch [9047/10000] Train Loss: 0.006982 Val Loss: nan\n",
      "Epoch [9048/10000] Train Loss: 0.006899 Val Loss: nan\n",
      "Epoch [9049/10000] Train Loss: 0.006985 Val Loss: nan\n",
      "Epoch [9050/10000] Train Loss: 0.006910 Val Loss: nan\n",
      "Epoch [9051/10000] Train Loss: 0.006968 Val Loss: nan\n",
      "Epoch [9052/10000] Train Loss: 0.006950 Val Loss: nan\n",
      "Epoch [9053/10000] Train Loss: 0.006912 Val Loss: nan\n",
      "Epoch [9054/10000] Train Loss: 0.006937 Val Loss: nan\n",
      "Epoch [9055/10000] Train Loss: 0.006860 Val Loss: nan\n",
      "Epoch [9056/10000] Train Loss: 0.006861 Val Loss: nan\n",
      "Epoch [9057/10000] Train Loss: 0.006808 Val Loss: nan\n",
      "Epoch [9058/10000] Train Loss: 0.007151 Val Loss: nan\n",
      "Epoch [9059/10000] Train Loss: 0.006838 Val Loss: nan\n",
      "Epoch [9060/10000] Train Loss: 0.006837 Val Loss: nan\n",
      "Epoch [9061/10000] Train Loss: 0.006799 Val Loss: nan\n",
      "Epoch [9062/10000] Train Loss: 0.007012 Val Loss: nan\n",
      "Epoch [9063/10000] Train Loss: 0.006849 Val Loss: nan\n",
      "Epoch [9064/10000] Train Loss: 0.006980 Val Loss: nan\n",
      "Epoch [9065/10000] Train Loss: 0.007060 Val Loss: nan\n",
      "Epoch [9066/10000] Train Loss: 0.006846 Val Loss: nan\n",
      "Epoch [9067/10000] Train Loss: 0.006851 Val Loss: nan\n",
      "Epoch [9068/10000] Train Loss: 0.006981 Val Loss: nan\n",
      "Epoch [9069/10000] Train Loss: 0.006799 Val Loss: nan\n",
      "Epoch [9070/10000] Train Loss: 0.006897 Val Loss: nan\n",
      "Epoch [9071/10000] Train Loss: 0.007038 Val Loss: nan\n",
      "Epoch [9072/10000] Train Loss: 0.006901 Val Loss: nan\n",
      "Epoch [9073/10000] Train Loss: 0.006986 Val Loss: nan\n",
      "Epoch [9074/10000] Train Loss: 0.006925 Val Loss: nan\n",
      "Epoch [9075/10000] Train Loss: 0.006947 Val Loss: nan\n",
      "Epoch [9076/10000] Train Loss: 0.006898 Val Loss: nan\n",
      "Epoch [9077/10000] Train Loss: 0.006915 Val Loss: nan\n",
      "Epoch [9078/10000] Train Loss: 0.006845 Val Loss: nan\n",
      "Epoch [9079/10000] Train Loss: 0.006990 Val Loss: nan\n",
      "Epoch [9080/10000] Train Loss: 0.006815 Val Loss: nan\n",
      "Epoch [9081/10000] Train Loss: 0.006959 Val Loss: nan\n",
      "Epoch [9082/10000] Train Loss: 0.006818 Val Loss: nan\n",
      "Epoch [9083/10000] Train Loss: 0.007017 Val Loss: nan\n",
      "Epoch [9084/10000] Train Loss: 0.006826 Val Loss: nan\n",
      "Epoch [9085/10000] Train Loss: 0.006973 Val Loss: nan\n",
      "Epoch [9086/10000] Train Loss: 0.006811 Val Loss: nan\n",
      "Epoch [9087/10000] Train Loss: 0.007051 Val Loss: nan\n",
      "Epoch [9088/10000] Train Loss: 0.006856 Val Loss: nan\n",
      "Epoch [9089/10000] Train Loss: 0.006802 Val Loss: nan\n",
      "Epoch [9090/10000] Train Loss: 0.006924 Val Loss: nan\n",
      "Epoch [9091/10000] Train Loss: 0.006920 Val Loss: nan\n",
      "Epoch [9092/10000] Train Loss: 0.006914 Val Loss: nan\n",
      "Epoch [9093/10000] Train Loss: 0.006810 Val Loss: nan\n",
      "Epoch [9094/10000] Train Loss: 0.006903 Val Loss: nan\n",
      "Epoch [9095/10000] Train Loss: 0.007066 Val Loss: nan\n",
      "Epoch [9096/10000] Train Loss: 0.006824 Val Loss: nan\n",
      "Epoch [9097/10000] Train Loss: 0.006909 Val Loss: nan\n",
      "Epoch [9098/10000] Train Loss: 0.007005 Val Loss: nan\n",
      "Epoch [9099/10000] Train Loss: 0.007124 Val Loss: nan\n",
      "Epoch [9100/10000] Train Loss: 0.006935 Val Loss: nan\n",
      "Epoch [9101/10000] Train Loss: 0.006842 Val Loss: nan\n",
      "Epoch [9102/10000] Train Loss: 0.006847 Val Loss: nan\n",
      "Epoch [9103/10000] Train Loss: 0.006918 Val Loss: nan\n",
      "Epoch [9104/10000] Train Loss: 0.006885 Val Loss: nan\n",
      "Epoch [9105/10000] Train Loss: 0.006815 Val Loss: nan\n",
      "Epoch [9106/10000] Train Loss: 0.007052 Val Loss: nan\n",
      "Epoch [9107/10000] Train Loss: 0.006784 Val Loss: nan\n",
      "Epoch [9108/10000] Train Loss: 0.006985 Val Loss: nan\n",
      "Epoch [9109/10000] Train Loss: 0.006911 Val Loss: nan\n",
      "Epoch [9110/10000] Train Loss: 0.006868 Val Loss: nan\n",
      "Epoch [9111/10000] Train Loss: 0.006831 Val Loss: nan\n",
      "Epoch [9112/10000] Train Loss: 0.006858 Val Loss: nan\n",
      "Epoch [9113/10000] Train Loss: 0.006825 Val Loss: nan\n",
      "Epoch [9114/10000] Train Loss: 0.006789 Val Loss: nan\n",
      "Epoch [9115/10000] Train Loss: 0.007022 Val Loss: nan\n",
      "Epoch [9116/10000] Train Loss: 0.006830 Val Loss: nan\n",
      "Epoch [9117/10000] Train Loss: 0.006853 Val Loss: nan\n",
      "Epoch [9118/10000] Train Loss: 0.006954 Val Loss: nan\n",
      "Epoch [9119/10000] Train Loss: 0.006995 Val Loss: nan\n",
      "Epoch [9120/10000] Train Loss: 0.006981 Val Loss: nan\n",
      "Epoch [9121/10000] Train Loss: 0.006916 Val Loss: nan\n",
      "Epoch [9122/10000] Train Loss: 0.006838 Val Loss: nan\n",
      "Epoch [9123/10000] Train Loss: 0.006799 Val Loss: nan\n",
      "Epoch [9124/10000] Train Loss: 0.006844 Val Loss: nan\n",
      "Epoch [9125/10000] Train Loss: 0.006889 Val Loss: nan\n",
      "Epoch [9126/10000] Train Loss: 0.006791 Val Loss: nan\n",
      "Epoch [9127/10000] Train Loss: 0.006795 Val Loss: nan\n",
      "Epoch [9128/10000] Train Loss: 0.006961 Val Loss: nan\n",
      "Epoch [9129/10000] Train Loss: 0.006911 Val Loss: nan\n",
      "Epoch [9130/10000] Train Loss: 0.006819 Val Loss: nan\n",
      "Epoch [9131/10000] Train Loss: 0.006860 Val Loss: nan\n",
      "Epoch [9132/10000] Train Loss: 0.006993 Val Loss: nan\n",
      "Epoch [9133/10000] Train Loss: 0.006829 Val Loss: nan\n",
      "Epoch [9134/10000] Train Loss: 0.006950 Val Loss: nan\n",
      "Epoch [9135/10000] Train Loss: 0.007187 Val Loss: nan\n",
      "Epoch [9136/10000] Train Loss: 0.007073 Val Loss: nan\n",
      "Epoch [9137/10000] Train Loss: 0.006955 Val Loss: nan\n",
      "Epoch [9138/10000] Train Loss: 0.006926 Val Loss: nan\n",
      "Epoch [9139/10000] Train Loss: 0.006959 Val Loss: nan\n",
      "Epoch [9140/10000] Train Loss: 0.006934 Val Loss: nan\n",
      "Epoch [9141/10000] Train Loss: 0.007004 Val Loss: nan\n",
      "Epoch [9142/10000] Train Loss: 0.006841 Val Loss: nan\n",
      "Epoch [9143/10000] Train Loss: 0.006829 Val Loss: nan\n",
      "Epoch [9144/10000] Train Loss: 0.006900 Val Loss: nan\n",
      "Epoch [9145/10000] Train Loss: 0.006800 Val Loss: nan\n",
      "Epoch [9146/10000] Train Loss: 0.006912 Val Loss: nan\n",
      "Epoch [9147/10000] Train Loss: 0.006877 Val Loss: nan\n",
      "Epoch [9148/10000] Train Loss: 0.007052 Val Loss: nan\n",
      "Epoch [9149/10000] Train Loss: 0.006912 Val Loss: nan\n",
      "Epoch [9150/10000] Train Loss: 0.006968 Val Loss: nan\n",
      "Epoch [9151/10000] Train Loss: 0.006866 Val Loss: nan\n",
      "Epoch [9152/10000] Train Loss: 0.007065 Val Loss: nan\n",
      "Epoch [9153/10000] Train Loss: 0.007013 Val Loss: nan\n",
      "Epoch [9154/10000] Train Loss: 0.006862 Val Loss: nan\n",
      "Epoch [9155/10000] Train Loss: 0.006938 Val Loss: nan\n",
      "Epoch [9156/10000] Train Loss: 0.006945 Val Loss: nan\n",
      "Epoch [9157/10000] Train Loss: 0.006874 Val Loss: nan\n",
      "Epoch [9158/10000] Train Loss: 0.006999 Val Loss: nan\n",
      "Epoch [9159/10000] Train Loss: 0.006971 Val Loss: nan\n",
      "Epoch [9160/10000] Train Loss: 0.006935 Val Loss: nan\n",
      "Epoch [9161/10000] Train Loss: 0.006918 Val Loss: nan\n",
      "Epoch [9162/10000] Train Loss: 0.006870 Val Loss: nan\n",
      "Epoch [9163/10000] Train Loss: 0.006900 Val Loss: nan\n",
      "Epoch [9164/10000] Train Loss: 0.006923 Val Loss: nan\n",
      "Epoch [9165/10000] Train Loss: 0.006884 Val Loss: nan\n",
      "Epoch [9166/10000] Train Loss: 0.007025 Val Loss: nan\n",
      "Epoch [9167/10000] Train Loss: 0.006884 Val Loss: nan\n",
      "Epoch [9168/10000] Train Loss: 0.006951 Val Loss: nan\n",
      "Epoch [9169/10000] Train Loss: 0.006824 Val Loss: nan\n",
      "Epoch [9170/10000] Train Loss: 0.006814 Val Loss: nan\n",
      "Epoch [9171/10000] Train Loss: 0.006826 Val Loss: nan\n",
      "Epoch [9172/10000] Train Loss: 0.006998 Val Loss: nan\n",
      "Epoch [9173/10000] Train Loss: 0.006875 Val Loss: nan\n",
      "Epoch [9174/10000] Train Loss: 0.007120 Val Loss: nan\n",
      "Epoch [9175/10000] Train Loss: 0.007054 Val Loss: nan\n",
      "Epoch [9176/10000] Train Loss: 0.007171 Val Loss: nan\n",
      "Epoch [9177/10000] Train Loss: 0.007048 Val Loss: nan\n",
      "Epoch [9178/10000] Train Loss: 0.006898 Val Loss: nan\n",
      "Epoch [9179/10000] Train Loss: 0.006951 Val Loss: nan\n",
      "Epoch [9180/10000] Train Loss: 0.006859 Val Loss: nan\n",
      "Epoch [9181/10000] Train Loss: 0.006860 Val Loss: nan\n",
      "Epoch [9182/10000] Train Loss: 0.006838 Val Loss: nan\n",
      "Epoch [9183/10000] Train Loss: 0.006818 Val Loss: nan\n",
      "Epoch [9184/10000] Train Loss: 0.006848 Val Loss: nan\n",
      "Epoch [9185/10000] Train Loss: 0.006861 Val Loss: nan\n",
      "Epoch [9186/10000] Train Loss: 0.006925 Val Loss: nan\n",
      "Epoch [9187/10000] Train Loss: 0.006832 Val Loss: nan\n",
      "Epoch [9188/10000] Train Loss: 0.006951 Val Loss: nan\n",
      "Epoch [9189/10000] Train Loss: 0.006835 Val Loss: nan\n",
      "Epoch [9190/10000] Train Loss: 0.006867 Val Loss: nan\n",
      "Epoch [9191/10000] Train Loss: 0.006832 Val Loss: nan\n",
      "Epoch [9192/10000] Train Loss: 0.006887 Val Loss: nan\n",
      "Epoch [9193/10000] Train Loss: 0.006833 Val Loss: nan\n",
      "Epoch [9194/10000] Train Loss: 0.006793 Val Loss: nan\n",
      "Epoch [9195/10000] Train Loss: 0.006944 Val Loss: nan\n",
      "Epoch [9196/10000] Train Loss: 0.006919 Val Loss: nan\n",
      "Epoch [9197/10000] Train Loss: 0.006863 Val Loss: nan\n",
      "Epoch [9198/10000] Train Loss: 0.006841 Val Loss: nan\n",
      "Epoch [9199/10000] Train Loss: 0.006931 Val Loss: nan\n",
      "Epoch [9200/10000] Train Loss: 0.007231 Val Loss: nan\n",
      "Epoch [9201/10000] Train Loss: 0.006964 Val Loss: nan\n",
      "Epoch [9202/10000] Train Loss: 0.006882 Val Loss: nan\n",
      "Epoch [9203/10000] Train Loss: 0.006890 Val Loss: nan\n",
      "Epoch [9204/10000] Train Loss: 0.006878 Val Loss: nan\n",
      "Epoch [9205/10000] Train Loss: 0.006802 Val Loss: nan\n",
      "Epoch [9206/10000] Train Loss: 0.006825 Val Loss: nan\n",
      "Epoch [9207/10000] Train Loss: 0.006824 Val Loss: nan\n",
      "Epoch [9208/10000] Train Loss: 0.006868 Val Loss: nan\n",
      "Epoch [9209/10000] Train Loss: 0.006885 Val Loss: nan\n",
      "Epoch [9210/10000] Train Loss: 0.006991 Val Loss: nan\n",
      "Epoch [9211/10000] Train Loss: 0.006893 Val Loss: nan\n",
      "Epoch [9212/10000] Train Loss: 0.007075 Val Loss: nan\n",
      "Epoch [9213/10000] Train Loss: 0.006912 Val Loss: nan\n",
      "Epoch [9214/10000] Train Loss: 0.006789 Val Loss: nan\n",
      "Epoch [9215/10000] Train Loss: 0.007037 Val Loss: nan\n",
      "Epoch [9216/10000] Train Loss: 0.006900 Val Loss: nan\n",
      "Epoch [9217/10000] Train Loss: 0.006934 Val Loss: nan\n",
      "Epoch [9218/10000] Train Loss: 0.007014 Val Loss: nan\n",
      "Epoch [9219/10000] Train Loss: 0.006925 Val Loss: nan\n",
      "Epoch [9220/10000] Train Loss: 0.006877 Val Loss: nan\n",
      "Epoch [9221/10000] Train Loss: 0.006928 Val Loss: nan\n",
      "Epoch [9222/10000] Train Loss: 0.006850 Val Loss: nan\n",
      "Epoch [9223/10000] Train Loss: 0.006793 Val Loss: nan\n",
      "Epoch [9224/10000] Train Loss: 0.006824 Val Loss: nan\n",
      "Epoch [9225/10000] Train Loss: 0.007151 Val Loss: nan\n",
      "Epoch [9226/10000] Train Loss: 0.006801 Val Loss: nan\n",
      "Epoch [9227/10000] Train Loss: 0.006982 Val Loss: nan\n",
      "Epoch [9228/10000] Train Loss: 0.006822 Val Loss: nan\n",
      "Epoch [9229/10000] Train Loss: 0.006951 Val Loss: nan\n",
      "Epoch [9230/10000] Train Loss: 0.006817 Val Loss: nan\n",
      "Epoch [9231/10000] Train Loss: 0.007012 Val Loss: nan\n",
      "Epoch [9232/10000] Train Loss: 0.006898 Val Loss: nan\n",
      "Epoch [9233/10000] Train Loss: 0.006808 Val Loss: nan\n",
      "Epoch [9234/10000] Train Loss: 0.006861 Val Loss: nan\n",
      "Epoch [9235/10000] Train Loss: 0.006874 Val Loss: nan\n",
      "Epoch [9236/10000] Train Loss: 0.006890 Val Loss: nan\n",
      "Epoch [9237/10000] Train Loss: 0.006806 Val Loss: nan\n",
      "Epoch [9238/10000] Train Loss: 0.006855 Val Loss: nan\n",
      "Epoch [9239/10000] Train Loss: 0.007101 Val Loss: nan\n",
      "Epoch [9240/10000] Train Loss: 0.006920 Val Loss: nan\n",
      "Epoch [9241/10000] Train Loss: 0.006928 Val Loss: nan\n",
      "Epoch [9242/10000] Train Loss: 0.007049 Val Loss: nan\n",
      "Epoch [9243/10000] Train Loss: 0.006950 Val Loss: nan\n",
      "Epoch [9244/10000] Train Loss: 0.006869 Val Loss: nan\n",
      "Epoch [9245/10000] Train Loss: 0.006846 Val Loss: nan\n",
      "Epoch [9246/10000] Train Loss: 0.006892 Val Loss: nan\n",
      "Epoch [9247/10000] Train Loss: 0.006852 Val Loss: nan\n",
      "Epoch [9248/10000] Train Loss: 0.006845 Val Loss: nan\n",
      "Epoch [9249/10000] Train Loss: 0.006954 Val Loss: nan\n",
      "Epoch [9250/10000] Train Loss: 0.006830 Val Loss: nan\n",
      "Epoch [9251/10000] Train Loss: 0.006860 Val Loss: nan\n",
      "Epoch [9252/10000] Train Loss: 0.006786 Val Loss: nan\n",
      "Epoch [9253/10000] Train Loss: 0.006799 Val Loss: nan\n",
      "Epoch [9254/10000] Train Loss: 0.007010 Val Loss: nan\n",
      "Epoch [9255/10000] Train Loss: 0.006859 Val Loss: nan\n",
      "Epoch [9256/10000] Train Loss: 0.006918 Val Loss: nan\n",
      "Epoch [9257/10000] Train Loss: 0.006800 Val Loss: nan\n",
      "Epoch [9258/10000] Train Loss: 0.006896 Val Loss: nan\n",
      "Epoch [9259/10000] Train Loss: 0.006936 Val Loss: nan\n",
      "Epoch [9260/10000] Train Loss: 0.006869 Val Loss: nan\n",
      "Epoch [9261/10000] Train Loss: 0.007004 Val Loss: nan\n",
      "Epoch [9262/10000] Train Loss: 0.006880 Val Loss: nan\n",
      "Epoch [9263/10000] Train Loss: 0.006860 Val Loss: nan\n",
      "Epoch [9264/10000] Train Loss: 0.006965 Val Loss: nan\n",
      "Epoch [9265/10000] Train Loss: 0.007026 Val Loss: nan\n",
      "Epoch [9266/10000] Train Loss: 0.006872 Val Loss: nan\n",
      "Epoch [9267/10000] Train Loss: 0.006954 Val Loss: nan\n",
      "Epoch [9268/10000] Train Loss: 0.006937 Val Loss: nan\n",
      "Epoch [9269/10000] Train Loss: 0.006951 Val Loss: nan\n",
      "Epoch [9270/10000] Train Loss: 0.006871 Val Loss: nan\n",
      "Epoch [9271/10000] Train Loss: 0.007048 Val Loss: nan\n",
      "Epoch [9272/10000] Train Loss: 0.007143 Val Loss: nan\n",
      "Epoch [9273/10000] Train Loss: 0.006990 Val Loss: nan\n",
      "Epoch [9274/10000] Train Loss: 0.007018 Val Loss: nan\n",
      "Epoch [9275/10000] Train Loss: 0.006825 Val Loss: nan\n",
      "Epoch [9276/10000] Train Loss: 0.006796 Val Loss: nan\n",
      "Epoch [9277/10000] Train Loss: 0.007060 Val Loss: nan\n",
      "Epoch [9278/10000] Train Loss: 0.006932 Val Loss: nan\n",
      "Epoch [9279/10000] Train Loss: 0.006867 Val Loss: nan\n",
      "Epoch [9280/10000] Train Loss: 0.007007 Val Loss: nan\n",
      "Epoch [9281/10000] Train Loss: 0.006999 Val Loss: nan\n",
      "Epoch [9282/10000] Train Loss: 0.006857 Val Loss: nan\n",
      "Epoch [9283/10000] Train Loss: 0.007180 Val Loss: nan\n",
      "Epoch [9284/10000] Train Loss: 0.006905 Val Loss: nan\n",
      "Epoch [9285/10000] Train Loss: 0.006941 Val Loss: nan\n",
      "Epoch [9286/10000] Train Loss: 0.006863 Val Loss: nan\n",
      "Epoch [9287/10000] Train Loss: 0.006830 Val Loss: nan\n",
      "Epoch [9288/10000] Train Loss: 0.006947 Val Loss: nan\n",
      "Epoch [9289/10000] Train Loss: 0.006907 Val Loss: nan\n",
      "Epoch [9290/10000] Train Loss: 0.006892 Val Loss: nan\n",
      "Epoch [9291/10000] Train Loss: 0.006903 Val Loss: nan\n",
      "Epoch [9292/10000] Train Loss: 0.007068 Val Loss: nan\n",
      "Epoch [9293/10000] Train Loss: 0.006770 Val Loss: nan\n",
      "Epoch [9294/10000] Train Loss: 0.006935 Val Loss: nan\n",
      "Epoch [9295/10000] Train Loss: 0.006815 Val Loss: nan\n",
      "Epoch [9296/10000] Train Loss: 0.006867 Val Loss: nan\n",
      "Epoch [9297/10000] Train Loss: 0.006786 Val Loss: nan\n",
      "Epoch [9298/10000] Train Loss: 0.007178 Val Loss: nan\n",
      "Epoch [9299/10000] Train Loss: 0.006870 Val Loss: nan\n",
      "Epoch [9300/10000] Train Loss: 0.006878 Val Loss: nan\n",
      "Epoch [9301/10000] Train Loss: 0.006920 Val Loss: nan\n",
      "Epoch [9302/10000] Train Loss: 0.006964 Val Loss: nan\n",
      "Epoch [9303/10000] Train Loss: 0.006895 Val Loss: nan\n",
      "Epoch [9304/10000] Train Loss: 0.006933 Val Loss: nan\n",
      "Epoch [9305/10000] Train Loss: 0.006979 Val Loss: nan\n",
      "Epoch [9306/10000] Train Loss: 0.006793 Val Loss: nan\n",
      "Epoch [9307/10000] Train Loss: 0.006887 Val Loss: nan\n",
      "Epoch [9308/10000] Train Loss: 0.006765 Val Loss: nan\n",
      "Epoch [9309/10000] Train Loss: 0.006788 Val Loss: nan\n",
      "Epoch [9310/10000] Train Loss: 0.006868 Val Loss: nan\n",
      "Epoch [9311/10000] Train Loss: 0.006862 Val Loss: nan\n",
      "Epoch [9312/10000] Train Loss: 0.006918 Val Loss: nan\n",
      "Epoch [9313/10000] Train Loss: 0.007039 Val Loss: nan\n",
      "Epoch [9314/10000] Train Loss: 0.006875 Val Loss: nan\n",
      "Epoch [9315/10000] Train Loss: 0.006957 Val Loss: nan\n",
      "Epoch [9316/10000] Train Loss: 0.006794 Val Loss: nan\n",
      "Epoch [9317/10000] Train Loss: 0.006927 Val Loss: nan\n",
      "Epoch [9318/10000] Train Loss: 0.006829 Val Loss: nan\n",
      "Epoch [9319/10000] Train Loss: 0.006896 Val Loss: nan\n",
      "Epoch [9320/10000] Train Loss: 0.006928 Val Loss: nan\n",
      "Epoch [9321/10000] Train Loss: 0.006822 Val Loss: nan\n",
      "Epoch [9322/10000] Train Loss: 0.006906 Val Loss: nan\n",
      "Epoch [9323/10000] Train Loss: 0.006834 Val Loss: nan\n",
      "Epoch [9324/10000] Train Loss: 0.006888 Val Loss: nan\n",
      "Epoch [9325/10000] Train Loss: 0.006909 Val Loss: nan\n",
      "Epoch [9326/10000] Train Loss: 0.006844 Val Loss: nan\n",
      "Epoch [9327/10000] Train Loss: 0.006804 Val Loss: nan\n",
      "Epoch [9328/10000] Train Loss: 0.007096 Val Loss: nan\n",
      "Epoch [9329/10000] Train Loss: 0.006787 Val Loss: nan\n",
      "Epoch [9330/10000] Train Loss: 0.007034 Val Loss: nan\n",
      "Epoch [9331/10000] Train Loss: 0.006943 Val Loss: nan\n",
      "Epoch [9332/10000] Train Loss: 0.006823 Val Loss: nan\n",
      "Epoch [9333/10000] Train Loss: 0.006898 Val Loss: nan\n",
      "Epoch [9334/10000] Train Loss: 0.006789 Val Loss: nan\n",
      "Epoch [9335/10000] Train Loss: 0.006796 Val Loss: nan\n",
      "Epoch [9336/10000] Train Loss: 0.006894 Val Loss: nan\n",
      "Epoch [9337/10000] Train Loss: 0.006957 Val Loss: nan\n",
      "Epoch [9338/10000] Train Loss: 0.006986 Val Loss: nan\n",
      "Epoch [9339/10000] Train Loss: 0.007082 Val Loss: nan\n",
      "Epoch [9340/10000] Train Loss: 0.007156 Val Loss: nan\n",
      "Epoch [9341/10000] Train Loss: 0.006918 Val Loss: nan\n",
      "Epoch [9342/10000] Train Loss: 0.006832 Val Loss: nan\n",
      "Epoch [9343/10000] Train Loss: 0.007011 Val Loss: nan\n",
      "Epoch [9344/10000] Train Loss: 0.006880 Val Loss: nan\n",
      "Epoch [9345/10000] Train Loss: 0.006841 Val Loss: nan\n",
      "Epoch [9346/10000] Train Loss: 0.006887 Val Loss: nan\n",
      "Epoch [9347/10000] Train Loss: 0.006827 Val Loss: nan\n",
      "Epoch [9348/10000] Train Loss: 0.006900 Val Loss: nan\n",
      "Epoch [9349/10000] Train Loss: 0.007109 Val Loss: nan\n",
      "Epoch [9350/10000] Train Loss: 0.007019 Val Loss: nan\n",
      "Epoch [9351/10000] Train Loss: 0.006994 Val Loss: nan\n",
      "Epoch [9352/10000] Train Loss: 0.006997 Val Loss: nan\n",
      "Epoch [9353/10000] Train Loss: 0.006799 Val Loss: nan\n",
      "Epoch [9354/10000] Train Loss: 0.007128 Val Loss: nan\n",
      "Epoch [9355/10000] Train Loss: 0.006789 Val Loss: nan\n",
      "Epoch [9356/10000] Train Loss: 0.006777 Val Loss: nan\n",
      "Epoch [9357/10000] Train Loss: 0.006907 Val Loss: nan\n",
      "Epoch [9358/10000] Train Loss: 0.006946 Val Loss: nan\n",
      "Epoch [9359/10000] Train Loss: 0.006839 Val Loss: nan\n",
      "Epoch [9360/10000] Train Loss: 0.006764 Val Loss: nan\n",
      "Epoch [9361/10000] Train Loss: 0.006837 Val Loss: nan\n",
      "Epoch [9362/10000] Train Loss: 0.006886 Val Loss: nan\n",
      "Epoch [9363/10000] Train Loss: 0.006788 Val Loss: nan\n",
      "Epoch [9364/10000] Train Loss: 0.006846 Val Loss: nan\n",
      "Epoch [9365/10000] Train Loss: 0.006920 Val Loss: nan\n",
      "Epoch [9366/10000] Train Loss: 0.006826 Val Loss: nan\n",
      "Epoch [9367/10000] Train Loss: 0.006804 Val Loss: nan\n",
      "Epoch [9368/10000] Train Loss: 0.006795 Val Loss: nan\n",
      "Epoch [9369/10000] Train Loss: 0.006746 Val Loss: nan\n",
      "Epoch [9370/10000] Train Loss: 0.006828 Val Loss: nan\n",
      "Epoch [9371/10000] Train Loss: 0.007008 Val Loss: nan\n",
      "Epoch [9372/10000] Train Loss: 0.006893 Val Loss: nan\n",
      "Epoch [9373/10000] Train Loss: 0.006823 Val Loss: nan\n",
      "Epoch [9374/10000] Train Loss: 0.006907 Val Loss: nan\n",
      "Epoch [9375/10000] Train Loss: 0.006951 Val Loss: nan\n",
      "Epoch [9376/10000] Train Loss: 0.007165 Val Loss: nan\n",
      "Epoch [9377/10000] Train Loss: 0.007012 Val Loss: nan\n",
      "Epoch [9378/10000] Train Loss: 0.006913 Val Loss: nan\n",
      "Epoch [9379/10000] Train Loss: 0.006784 Val Loss: nan\n",
      "Epoch [9380/10000] Train Loss: 0.006958 Val Loss: nan\n",
      "Epoch [9381/10000] Train Loss: 0.006969 Val Loss: nan\n",
      "Epoch [9382/10000] Train Loss: 0.006859 Val Loss: nan\n",
      "Epoch [9383/10000] Train Loss: 0.006901 Val Loss: nan\n",
      "Epoch [9384/10000] Train Loss: 0.006753 Val Loss: nan\n",
      "Epoch [9385/10000] Train Loss: 0.006842 Val Loss: nan\n",
      "Epoch [9386/10000] Train Loss: 0.006901 Val Loss: nan\n",
      "Epoch [9387/10000] Train Loss: 0.006952 Val Loss: nan\n",
      "Epoch [9388/10000] Train Loss: 0.006886 Val Loss: nan\n",
      "Epoch [9389/10000] Train Loss: 0.006809 Val Loss: nan\n",
      "Epoch [9390/10000] Train Loss: 0.006857 Val Loss: nan\n",
      "Epoch [9391/10000] Train Loss: 0.006762 Val Loss: nan\n",
      "Epoch [9392/10000] Train Loss: 0.006783 Val Loss: nan\n",
      "Epoch [9393/10000] Train Loss: 0.006856 Val Loss: nan\n",
      "Epoch [9394/10000] Train Loss: 0.007014 Val Loss: nan\n",
      "Epoch [9395/10000] Train Loss: 0.006976 Val Loss: nan\n",
      "Epoch [9396/10000] Train Loss: 0.006878 Val Loss: nan\n",
      "Epoch [9397/10000] Train Loss: 0.006990 Val Loss: nan\n",
      "Epoch [9398/10000] Train Loss: 0.006814 Val Loss: nan\n",
      "Epoch [9399/10000] Train Loss: 0.006813 Val Loss: nan\n",
      "Epoch [9400/10000] Train Loss: 0.006762 Val Loss: nan\n",
      "Epoch [9401/10000] Train Loss: 0.006901 Val Loss: nan\n",
      "Epoch [9402/10000] Train Loss: 0.007105 Val Loss: nan\n",
      "Epoch [9403/10000] Train Loss: 0.006953 Val Loss: nan\n",
      "Epoch [9404/10000] Train Loss: 0.006834 Val Loss: nan\n",
      "Epoch [9405/10000] Train Loss: 0.006774 Val Loss: nan\n",
      "Epoch [9406/10000] Train Loss: 0.007066 Val Loss: nan\n",
      "Epoch [9407/10000] Train Loss: 0.006924 Val Loss: nan\n",
      "Epoch [9408/10000] Train Loss: 0.006940 Val Loss: nan\n",
      "Epoch [9409/10000] Train Loss: 0.006929 Val Loss: nan\n",
      "Epoch [9410/10000] Train Loss: 0.006755 Val Loss: nan\n",
      "Epoch [9411/10000] Train Loss: 0.006757 Val Loss: nan\n",
      "Epoch [9412/10000] Train Loss: 0.006970 Val Loss: nan\n",
      "Epoch [9413/10000] Train Loss: 0.007131 Val Loss: nan\n",
      "Epoch [9414/10000] Train Loss: 0.006952 Val Loss: nan\n",
      "Epoch [9415/10000] Train Loss: 0.006989 Val Loss: nan\n",
      "Epoch [9416/10000] Train Loss: 0.006958 Val Loss: nan\n",
      "Epoch [9417/10000] Train Loss: 0.006956 Val Loss: nan\n",
      "Epoch [9418/10000] Train Loss: 0.006777 Val Loss: nan\n",
      "Epoch [9419/10000] Train Loss: 0.006863 Val Loss: nan\n",
      "Epoch [9420/10000] Train Loss: 0.006847 Val Loss: nan\n",
      "Epoch [9421/10000] Train Loss: 0.006757 Val Loss: nan\n",
      "Epoch [9422/10000] Train Loss: 0.007090 Val Loss: nan\n",
      "Epoch [9423/10000] Train Loss: 0.007025 Val Loss: nan\n",
      "Epoch [9424/10000] Train Loss: 0.006814 Val Loss: nan\n",
      "Epoch [9425/10000] Train Loss: 0.006976 Val Loss: nan\n",
      "Epoch [9426/10000] Train Loss: 0.006715 Val Loss: nan\n",
      "Epoch [9427/10000] Train Loss: 0.006765 Val Loss: nan\n",
      "Epoch [9428/10000] Train Loss: 0.006900 Val Loss: nan\n",
      "Epoch [9429/10000] Train Loss: 0.006876 Val Loss: nan\n",
      "Epoch [9430/10000] Train Loss: 0.007053 Val Loss: nan\n",
      "Epoch [9431/10000] Train Loss: 0.006873 Val Loss: nan\n",
      "Epoch [9432/10000] Train Loss: 0.006790 Val Loss: nan\n",
      "Epoch [9433/10000] Train Loss: 0.006971 Val Loss: nan\n",
      "Epoch [9434/10000] Train Loss: 0.006804 Val Loss: nan\n",
      "Epoch [9435/10000] Train Loss: 0.006948 Val Loss: nan\n",
      "Epoch [9436/10000] Train Loss: 0.006978 Val Loss: nan\n",
      "Epoch [9437/10000] Train Loss: 0.006903 Val Loss: nan\n",
      "Epoch [9438/10000] Train Loss: 0.006782 Val Loss: nan\n",
      "Epoch [9439/10000] Train Loss: 0.006898 Val Loss: nan\n",
      "Epoch [9440/10000] Train Loss: 0.006902 Val Loss: nan\n",
      "Epoch [9441/10000] Train Loss: 0.006761 Val Loss: nan\n",
      "Epoch [9442/10000] Train Loss: 0.006801 Val Loss: nan\n",
      "Epoch [9443/10000] Train Loss: 0.007147 Val Loss: nan\n",
      "Epoch [9444/10000] Train Loss: 0.007044 Val Loss: nan\n",
      "Epoch [9445/10000] Train Loss: 0.006915 Val Loss: nan\n",
      "Epoch [9446/10000] Train Loss: 0.006808 Val Loss: nan\n",
      "Epoch [9447/10000] Train Loss: 0.006772 Val Loss: nan\n",
      "Epoch [9448/10000] Train Loss: 0.006764 Val Loss: nan\n",
      "Epoch [9449/10000] Train Loss: 0.006751 Val Loss: nan\n",
      "Epoch [9450/10000] Train Loss: 0.006815 Val Loss: nan\n",
      "Epoch [9451/10000] Train Loss: 0.006853 Val Loss: nan\n",
      "Epoch [9452/10000] Train Loss: 0.006894 Val Loss: nan\n",
      "Epoch [9453/10000] Train Loss: 0.006785 Val Loss: nan\n",
      "Epoch [9454/10000] Train Loss: 0.007013 Val Loss: nan\n",
      "Epoch [9455/10000] Train Loss: 0.006859 Val Loss: nan\n",
      "Epoch [9456/10000] Train Loss: 0.006762 Val Loss: nan\n",
      "Epoch [9457/10000] Train Loss: 0.006927 Val Loss: nan\n",
      "Epoch [9458/10000] Train Loss: 0.007013 Val Loss: nan\n",
      "Epoch [9459/10000] Train Loss: 0.006794 Val Loss: nan\n",
      "Epoch [9460/10000] Train Loss: 0.006956 Val Loss: nan\n",
      "Epoch [9461/10000] Train Loss: 0.006786 Val Loss: nan\n",
      "Epoch [9462/10000] Train Loss: 0.006867 Val Loss: nan\n",
      "Epoch [9463/10000] Train Loss: 0.006889 Val Loss: nan\n",
      "Epoch [9464/10000] Train Loss: 0.006824 Val Loss: nan\n",
      "Epoch [9465/10000] Train Loss: 0.006913 Val Loss: nan\n",
      "Epoch [9466/10000] Train Loss: 0.006745 Val Loss: nan\n",
      "Epoch [9467/10000] Train Loss: 0.006920 Val Loss: nan\n",
      "Epoch [9468/10000] Train Loss: 0.007047 Val Loss: nan\n",
      "Epoch [9469/10000] Train Loss: 0.006878 Val Loss: nan\n",
      "Epoch [9470/10000] Train Loss: 0.006890 Val Loss: nan\n",
      "Epoch [9471/10000] Train Loss: 0.006895 Val Loss: nan\n",
      "Epoch [9472/10000] Train Loss: 0.007038 Val Loss: nan\n",
      "Epoch [9473/10000] Train Loss: 0.006814 Val Loss: nan\n",
      "Epoch [9474/10000] Train Loss: 0.006844 Val Loss: nan\n",
      "Epoch [9475/10000] Train Loss: 0.006817 Val Loss: nan\n",
      "Epoch [9476/10000] Train Loss: 0.006892 Val Loss: nan\n",
      "Epoch [9477/10000] Train Loss: 0.006786 Val Loss: nan\n",
      "Epoch [9478/10000] Train Loss: 0.006763 Val Loss: nan\n",
      "Epoch [9479/10000] Train Loss: 0.006750 Val Loss: nan\n",
      "Epoch [9480/10000] Train Loss: 0.006810 Val Loss: nan\n",
      "Epoch [9481/10000] Train Loss: 0.006822 Val Loss: nan\n",
      "Epoch [9482/10000] Train Loss: 0.006791 Val Loss: nan\n",
      "Epoch [9483/10000] Train Loss: 0.006934 Val Loss: nan\n",
      "Epoch [9484/10000] Train Loss: 0.006849 Val Loss: nan\n",
      "Epoch [9485/10000] Train Loss: 0.006840 Val Loss: nan\n",
      "Epoch [9486/10000] Train Loss: 0.007017 Val Loss: nan\n",
      "Epoch [9487/10000] Train Loss: 0.006816 Val Loss: nan\n",
      "Epoch [9488/10000] Train Loss: 0.006977 Val Loss: nan\n",
      "Epoch [9489/10000] Train Loss: 0.006812 Val Loss: nan\n",
      "Epoch [9490/10000] Train Loss: 0.006850 Val Loss: nan\n",
      "Epoch [9491/10000] Train Loss: 0.006876 Val Loss: nan\n",
      "Epoch [9492/10000] Train Loss: 0.006789 Val Loss: nan\n",
      "Epoch [9493/10000] Train Loss: 0.006766 Val Loss: nan\n",
      "Epoch [9494/10000] Train Loss: 0.006888 Val Loss: nan\n",
      "Epoch [9495/10000] Train Loss: 0.006843 Val Loss: nan\n",
      "Epoch [9496/10000] Train Loss: 0.006839 Val Loss: nan\n",
      "Epoch [9497/10000] Train Loss: 0.006759 Val Loss: nan\n",
      "Epoch [9498/10000] Train Loss: 0.006884 Val Loss: nan\n",
      "Epoch [9499/10000] Train Loss: 0.006860 Val Loss: nan\n",
      "Epoch [9500/10000] Train Loss: 0.006841 Val Loss: nan\n",
      "Epoch [9501/10000] Train Loss: 0.006792 Val Loss: nan\n",
      "Epoch [9502/10000] Train Loss: 0.007008 Val Loss: nan\n",
      "Epoch [9503/10000] Train Loss: 0.006977 Val Loss: nan\n",
      "Epoch [9504/10000] Train Loss: 0.006994 Val Loss: nan\n",
      "Epoch [9505/10000] Train Loss: 0.006860 Val Loss: nan\n",
      "Epoch [9506/10000] Train Loss: 0.006856 Val Loss: nan\n",
      "Epoch [9507/10000] Train Loss: 0.006738 Val Loss: nan\n",
      "Epoch [9508/10000] Train Loss: 0.006935 Val Loss: nan\n",
      "Epoch [9509/10000] Train Loss: 0.006817 Val Loss: nan\n",
      "Epoch [9510/10000] Train Loss: 0.006828 Val Loss: nan\n",
      "Epoch [9511/10000] Train Loss: 0.006877 Val Loss: nan\n",
      "Epoch [9512/10000] Train Loss: 0.006925 Val Loss: nan\n",
      "Epoch [9513/10000] Train Loss: 0.006774 Val Loss: nan\n",
      "Epoch [9514/10000] Train Loss: 0.006751 Val Loss: nan\n",
      "Epoch [9515/10000] Train Loss: 0.006966 Val Loss: nan\n",
      "Epoch [9516/10000] Train Loss: 0.006869 Val Loss: nan\n",
      "Epoch [9517/10000] Train Loss: 0.006947 Val Loss: nan\n",
      "Epoch [9518/10000] Train Loss: 0.006956 Val Loss: nan\n",
      "Epoch [9519/10000] Train Loss: 0.006926 Val Loss: nan\n",
      "Epoch [9520/10000] Train Loss: 0.006788 Val Loss: nan\n",
      "Epoch [9521/10000] Train Loss: 0.006892 Val Loss: nan\n",
      "Epoch [9522/10000] Train Loss: 0.006773 Val Loss: nan\n",
      "Epoch [9523/10000] Train Loss: 0.006985 Val Loss: nan\n",
      "Epoch [9524/10000] Train Loss: 0.006848 Val Loss: nan\n",
      "Epoch [9525/10000] Train Loss: 0.006991 Val Loss: nan\n",
      "Epoch [9526/10000] Train Loss: 0.006836 Val Loss: nan\n",
      "Epoch [9527/10000] Train Loss: 0.006871 Val Loss: nan\n",
      "Epoch [9528/10000] Train Loss: 0.006841 Val Loss: nan\n",
      "Epoch [9529/10000] Train Loss: 0.006795 Val Loss: nan\n",
      "Epoch [9530/10000] Train Loss: 0.006778 Val Loss: nan\n",
      "Epoch [9531/10000] Train Loss: 0.007066 Val Loss: nan\n",
      "Epoch [9532/10000] Train Loss: 0.006794 Val Loss: nan\n",
      "Epoch [9533/10000] Train Loss: 0.007053 Val Loss: nan\n",
      "Epoch [9534/10000] Train Loss: 0.006807 Val Loss: nan\n",
      "Epoch [9535/10000] Train Loss: 0.006951 Val Loss: nan\n",
      "Epoch [9536/10000] Train Loss: 0.007035 Val Loss: nan\n",
      "Epoch [9537/10000] Train Loss: 0.006956 Val Loss: nan\n",
      "Epoch [9538/10000] Train Loss: 0.006889 Val Loss: nan\n",
      "Epoch [9539/10000] Train Loss: 0.006792 Val Loss: nan\n",
      "Epoch [9540/10000] Train Loss: 0.006977 Val Loss: nan\n",
      "Epoch [9541/10000] Train Loss: 0.007103 Val Loss: nan\n",
      "Epoch [9542/10000] Train Loss: 0.007087 Val Loss: nan\n",
      "Epoch [9543/10000] Train Loss: 0.007044 Val Loss: nan\n",
      "Epoch [9544/10000] Train Loss: 0.006889 Val Loss: nan\n",
      "Epoch [9545/10000] Train Loss: 0.006870 Val Loss: nan\n",
      "Epoch [9546/10000] Train Loss: 0.007154 Val Loss: nan\n",
      "Epoch [9547/10000] Train Loss: 0.007034 Val Loss: nan\n",
      "Epoch [9548/10000] Train Loss: 0.006726 Val Loss: nan\n",
      "Epoch [9549/10000] Train Loss: 0.006824 Val Loss: nan\n",
      "Epoch [9550/10000] Train Loss: 0.006820 Val Loss: nan\n",
      "Epoch [9551/10000] Train Loss: 0.006928 Val Loss: nan\n",
      "Epoch [9552/10000] Train Loss: 0.006794 Val Loss: nan\n",
      "Epoch [9553/10000] Train Loss: 0.006833 Val Loss: nan\n",
      "Epoch [9554/10000] Train Loss: 0.007000 Val Loss: nan\n",
      "Epoch [9555/10000] Train Loss: 0.006946 Val Loss: nan\n",
      "Epoch [9556/10000] Train Loss: 0.006978 Val Loss: nan\n",
      "Epoch [9557/10000] Train Loss: 0.006933 Val Loss: nan\n",
      "Epoch [9558/10000] Train Loss: 0.006777 Val Loss: nan\n",
      "Epoch [9559/10000] Train Loss: 0.006787 Val Loss: nan\n",
      "Epoch [9560/10000] Train Loss: 0.006889 Val Loss: nan\n",
      "Epoch [9561/10000] Train Loss: 0.006872 Val Loss: nan\n",
      "Epoch [9562/10000] Train Loss: 0.006903 Val Loss: nan\n",
      "Epoch [9563/10000] Train Loss: 0.006786 Val Loss: nan\n",
      "Epoch [9564/10000] Train Loss: 0.006759 Val Loss: nan\n",
      "Epoch [9565/10000] Train Loss: 0.006764 Val Loss: nan\n",
      "Epoch [9566/10000] Train Loss: 0.006988 Val Loss: nan\n",
      "Epoch [9567/10000] Train Loss: 0.006848 Val Loss: nan\n",
      "Epoch [9568/10000] Train Loss: 0.006938 Val Loss: nan\n",
      "Epoch [9569/10000] Train Loss: 0.006847 Val Loss: nan\n",
      "Epoch [9570/10000] Train Loss: 0.006842 Val Loss: nan\n",
      "Epoch [9571/10000] Train Loss: 0.006955 Val Loss: nan\n",
      "Epoch [9572/10000] Train Loss: 0.007133 Val Loss: nan\n",
      "Epoch [9573/10000] Train Loss: 0.006847 Val Loss: nan\n",
      "Epoch [9574/10000] Train Loss: 0.006924 Val Loss: nan\n",
      "Epoch [9575/10000] Train Loss: 0.006979 Val Loss: nan\n",
      "Epoch [9576/10000] Train Loss: 0.006774 Val Loss: nan\n",
      "Epoch [9577/10000] Train Loss: 0.006803 Val Loss: nan\n",
      "Epoch [9578/10000] Train Loss: 0.006792 Val Loss: nan\n",
      "Epoch [9579/10000] Train Loss: 0.006762 Val Loss: nan\n",
      "Epoch [9580/10000] Train Loss: 0.006769 Val Loss: nan\n",
      "Epoch [9581/10000] Train Loss: 0.006887 Val Loss: nan\n",
      "Epoch [9582/10000] Train Loss: 0.006904 Val Loss: nan\n",
      "Epoch [9583/10000] Train Loss: 0.006846 Val Loss: nan\n",
      "Epoch [9584/10000] Train Loss: 0.006763 Val Loss: nan\n",
      "Epoch [9585/10000] Train Loss: 0.006780 Val Loss: nan\n",
      "Epoch [9586/10000] Train Loss: 0.007022 Val Loss: nan\n",
      "Epoch [9587/10000] Train Loss: 0.006740 Val Loss: nan\n",
      "Epoch [9588/10000] Train Loss: 0.006845 Val Loss: nan\n",
      "Epoch [9589/10000] Train Loss: 0.006870 Val Loss: nan\n",
      "Epoch [9590/10000] Train Loss: 0.006838 Val Loss: nan\n",
      "Epoch [9591/10000] Train Loss: 0.006746 Val Loss: nan\n",
      "Epoch [9592/10000] Train Loss: 0.006887 Val Loss: nan\n",
      "Epoch [9593/10000] Train Loss: 0.006854 Val Loss: nan\n",
      "Epoch [9594/10000] Train Loss: 0.006743 Val Loss: nan\n",
      "Epoch [9595/10000] Train Loss: 0.006825 Val Loss: nan\n",
      "Epoch [9596/10000] Train Loss: 0.006838 Val Loss: nan\n",
      "Epoch [9597/10000] Train Loss: 0.006918 Val Loss: nan\n",
      "Epoch [9598/10000] Train Loss: 0.007024 Val Loss: nan\n",
      "Epoch [9599/10000] Train Loss: 0.006825 Val Loss: nan\n",
      "Epoch [9600/10000] Train Loss: 0.006770 Val Loss: nan\n",
      "Epoch [9601/10000] Train Loss: 0.006989 Val Loss: nan\n",
      "Epoch [9602/10000] Train Loss: 0.006764 Val Loss: nan\n",
      "Epoch [9603/10000] Train Loss: 0.006810 Val Loss: nan\n",
      "Epoch [9604/10000] Train Loss: 0.006808 Val Loss: nan\n",
      "Epoch [9605/10000] Train Loss: 0.006725 Val Loss: nan\n",
      "Epoch [9606/10000] Train Loss: 0.006733 Val Loss: nan\n",
      "Epoch [9607/10000] Train Loss: 0.006828 Val Loss: nan\n",
      "Epoch [9608/10000] Train Loss: 0.006885 Val Loss: nan\n",
      "Epoch [9609/10000] Train Loss: 0.006756 Val Loss: nan\n",
      "Epoch [9610/10000] Train Loss: 0.006833 Val Loss: nan\n",
      "Epoch [9611/10000] Train Loss: 0.006807 Val Loss: nan\n",
      "Epoch [9612/10000] Train Loss: 0.006938 Val Loss: nan\n",
      "Epoch [9613/10000] Train Loss: 0.006846 Val Loss: nan\n",
      "Epoch [9614/10000] Train Loss: 0.006884 Val Loss: nan\n",
      "Epoch [9615/10000] Train Loss: 0.006973 Val Loss: nan\n",
      "Epoch [9616/10000] Train Loss: 0.006991 Val Loss: nan\n",
      "Epoch [9617/10000] Train Loss: 0.007137 Val Loss: nan\n",
      "Epoch [9618/10000] Train Loss: 0.006751 Val Loss: nan\n",
      "Epoch [9619/10000] Train Loss: 0.006882 Val Loss: nan\n",
      "Epoch [9620/10000] Train Loss: 0.006815 Val Loss: nan\n",
      "Epoch [9621/10000] Train Loss: 0.006768 Val Loss: nan\n",
      "Epoch [9622/10000] Train Loss: 0.006907 Val Loss: nan\n",
      "Epoch [9623/10000] Train Loss: 0.006849 Val Loss: nan\n",
      "Epoch [9624/10000] Train Loss: 0.006853 Val Loss: nan\n",
      "Epoch [9625/10000] Train Loss: 0.006778 Val Loss: nan\n",
      "Epoch [9626/10000] Train Loss: 0.007035 Val Loss: nan\n",
      "Epoch [9627/10000] Train Loss: 0.006851 Val Loss: nan\n",
      "Epoch [9628/10000] Train Loss: 0.006900 Val Loss: nan\n",
      "Epoch [9629/10000] Train Loss: 0.006813 Val Loss: nan\n",
      "Epoch [9630/10000] Train Loss: 0.006904 Val Loss: nan\n",
      "Epoch [9631/10000] Train Loss: 0.006755 Val Loss: nan\n",
      "Epoch [9632/10000] Train Loss: 0.006759 Val Loss: nan\n",
      "Epoch [9633/10000] Train Loss: 0.006730 Val Loss: nan\n",
      "Epoch [9634/10000] Train Loss: 0.006871 Val Loss: nan\n",
      "Epoch [9635/10000] Train Loss: 0.006881 Val Loss: nan\n",
      "Epoch [9636/10000] Train Loss: 0.006916 Val Loss: nan\n",
      "Epoch [9637/10000] Train Loss: 0.006776 Val Loss: nan\n",
      "Epoch [9638/10000] Train Loss: 0.006746 Val Loss: nan\n",
      "Epoch [9639/10000] Train Loss: 0.007110 Val Loss: nan\n",
      "Epoch [9640/10000] Train Loss: 0.006939 Val Loss: nan\n",
      "Epoch [9641/10000] Train Loss: 0.006838 Val Loss: nan\n",
      "Epoch [9642/10000] Train Loss: 0.006987 Val Loss: nan\n",
      "Epoch [9643/10000] Train Loss: 0.007027 Val Loss: nan\n",
      "Epoch [9644/10000] Train Loss: 0.006844 Val Loss: nan\n",
      "Epoch [9645/10000] Train Loss: 0.006760 Val Loss: nan\n",
      "Epoch [9646/10000] Train Loss: 0.006884 Val Loss: nan\n",
      "Epoch [9647/10000] Train Loss: 0.007028 Val Loss: nan\n",
      "Epoch [9648/10000] Train Loss: 0.006972 Val Loss: nan\n",
      "Epoch [9649/10000] Train Loss: 0.006905 Val Loss: nan\n",
      "Epoch [9650/10000] Train Loss: 0.006760 Val Loss: nan\n",
      "Epoch [9651/10000] Train Loss: 0.006825 Val Loss: nan\n",
      "Epoch [9652/10000] Train Loss: 0.006844 Val Loss: nan\n",
      "Epoch [9653/10000] Train Loss: 0.007076 Val Loss: nan\n",
      "Epoch [9654/10000] Train Loss: 0.006904 Val Loss: nan\n",
      "Epoch [9655/10000] Train Loss: 0.006764 Val Loss: nan\n",
      "Epoch [9656/10000] Train Loss: 0.006829 Val Loss: nan\n",
      "Epoch [9657/10000] Train Loss: 0.006885 Val Loss: nan\n",
      "Epoch [9658/10000] Train Loss: 0.006896 Val Loss: nan\n",
      "Epoch [9659/10000] Train Loss: 0.006852 Val Loss: nan\n",
      "Epoch [9660/10000] Train Loss: 0.006836 Val Loss: nan\n",
      "Epoch [9661/10000] Train Loss: 0.006741 Val Loss: nan\n",
      "Epoch [9662/10000] Train Loss: 0.006982 Val Loss: nan\n",
      "Epoch [9663/10000] Train Loss: 0.006952 Val Loss: nan\n",
      "Epoch [9664/10000] Train Loss: 0.006853 Val Loss: nan\n",
      "Epoch [9665/10000] Train Loss: 0.006874 Val Loss: nan\n",
      "Epoch [9666/10000] Train Loss: 0.006741 Val Loss: nan\n",
      "Epoch [9667/10000] Train Loss: 0.006733 Val Loss: nan\n",
      "Epoch [9668/10000] Train Loss: 0.006992 Val Loss: nan\n",
      "Epoch [9669/10000] Train Loss: 0.006863 Val Loss: nan\n",
      "Epoch [9670/10000] Train Loss: 0.006740 Val Loss: nan\n",
      "Epoch [9671/10000] Train Loss: 0.006923 Val Loss: nan\n",
      "Epoch [9672/10000] Train Loss: 0.006877 Val Loss: nan\n",
      "Epoch [9673/10000] Train Loss: 0.006748 Val Loss: nan\n",
      "Epoch [9674/10000] Train Loss: 0.006873 Val Loss: nan\n",
      "Epoch [9675/10000] Train Loss: 0.007079 Val Loss: nan\n",
      "Epoch [9676/10000] Train Loss: 0.006939 Val Loss: nan\n",
      "Epoch [9677/10000] Train Loss: 0.006925 Val Loss: nan\n",
      "Epoch [9678/10000] Train Loss: 0.006998 Val Loss: nan\n",
      "Epoch [9679/10000] Train Loss: 0.006766 Val Loss: nan\n",
      "Epoch [9680/10000] Train Loss: 0.006862 Val Loss: nan\n",
      "Epoch [9681/10000] Train Loss: 0.006844 Val Loss: nan\n",
      "Epoch [9682/10000] Train Loss: 0.006729 Val Loss: nan\n",
      "Epoch [9683/10000] Train Loss: 0.006710 Val Loss: nan\n",
      "Epoch [9684/10000] Train Loss: 0.006771 Val Loss: nan\n",
      "Epoch [9685/10000] Train Loss: 0.007107 Val Loss: nan\n",
      "Epoch [9686/10000] Train Loss: 0.006950 Val Loss: nan\n",
      "Epoch [9687/10000] Train Loss: 0.006932 Val Loss: nan\n",
      "Epoch [9688/10000] Train Loss: 0.006957 Val Loss: nan\n",
      "Epoch [9689/10000] Train Loss: 0.006838 Val Loss: nan\n",
      "Epoch [9690/10000] Train Loss: 0.006873 Val Loss: nan\n",
      "Epoch [9691/10000] Train Loss: 0.006728 Val Loss: nan\n",
      "Epoch [9692/10000] Train Loss: 0.006770 Val Loss: nan\n",
      "Epoch [9693/10000] Train Loss: 0.006802 Val Loss: nan\n",
      "Epoch [9694/10000] Train Loss: 0.006936 Val Loss: nan\n",
      "Epoch [9695/10000] Train Loss: 0.006956 Val Loss: nan\n",
      "Epoch [9696/10000] Train Loss: 0.006848 Val Loss: nan\n",
      "Epoch [9697/10000] Train Loss: 0.006735 Val Loss: nan\n",
      "Epoch [9698/10000] Train Loss: 0.006756 Val Loss: nan\n",
      "Epoch [9699/10000] Train Loss: 0.006819 Val Loss: nan\n",
      "Epoch [9700/10000] Train Loss: 0.006797 Val Loss: nan\n",
      "Epoch [9701/10000] Train Loss: 0.006903 Val Loss: nan\n",
      "Epoch [9702/10000] Train Loss: 0.006864 Val Loss: nan\n",
      "Epoch [9703/10000] Train Loss: 0.006894 Val Loss: nan\n",
      "Epoch [9704/10000] Train Loss: 0.007001 Val Loss: nan\n",
      "Epoch [9705/10000] Train Loss: 0.007074 Val Loss: nan\n",
      "Epoch [9706/10000] Train Loss: 0.006856 Val Loss: nan\n",
      "Epoch [9707/10000] Train Loss: 0.006830 Val Loss: nan\n",
      "Epoch [9708/10000] Train Loss: 0.006975 Val Loss: nan\n",
      "Epoch [9709/10000] Train Loss: 0.006791 Val Loss: nan\n",
      "Epoch [9710/10000] Train Loss: 0.006747 Val Loss: nan\n",
      "Epoch [9711/10000] Train Loss: 0.006757 Val Loss: nan\n",
      "Epoch [9712/10000] Train Loss: 0.006824 Val Loss: nan\n",
      "Epoch [9713/10000] Train Loss: 0.006776 Val Loss: nan\n",
      "Epoch [9714/10000] Train Loss: 0.006885 Val Loss: nan\n",
      "Epoch [9715/10000] Train Loss: 0.007020 Val Loss: nan\n",
      "Epoch [9716/10000] Train Loss: 0.006780 Val Loss: nan\n",
      "Epoch [9717/10000] Train Loss: 0.006773 Val Loss: nan\n",
      "Epoch [9718/10000] Train Loss: 0.006915 Val Loss: nan\n",
      "Epoch [9719/10000] Train Loss: 0.006875 Val Loss: nan\n",
      "Epoch [9720/10000] Train Loss: 0.006834 Val Loss: nan\n",
      "Epoch [9721/10000] Train Loss: 0.007003 Val Loss: nan\n",
      "Epoch [9722/10000] Train Loss: 0.006817 Val Loss: nan\n",
      "Epoch [9723/10000] Train Loss: 0.006973 Val Loss: nan\n",
      "Epoch [9724/10000] Train Loss: 0.006781 Val Loss: nan\n",
      "Epoch [9725/10000] Train Loss: 0.006922 Val Loss: nan\n",
      "Epoch [9726/10000] Train Loss: 0.006905 Val Loss: nan\n",
      "Epoch [9727/10000] Train Loss: 0.006835 Val Loss: nan\n",
      "Epoch [9728/10000] Train Loss: 0.006808 Val Loss: nan\n",
      "Epoch [9729/10000] Train Loss: 0.006858 Val Loss: nan\n",
      "Epoch [9730/10000] Train Loss: 0.007054 Val Loss: nan\n",
      "Epoch [9731/10000] Train Loss: 0.006875 Val Loss: nan\n",
      "Epoch [9732/10000] Train Loss: 0.006839 Val Loss: nan\n",
      "Epoch [9733/10000] Train Loss: 0.006908 Val Loss: nan\n",
      "Epoch [9734/10000] Train Loss: 0.006939 Val Loss: nan\n",
      "Epoch [9735/10000] Train Loss: 0.006790 Val Loss: nan\n",
      "Epoch [9736/10000] Train Loss: 0.006796 Val Loss: nan\n",
      "Epoch [9737/10000] Train Loss: 0.006835 Val Loss: nan\n",
      "Epoch [9738/10000] Train Loss: 0.006756 Val Loss: nan\n",
      "Epoch [9739/10000] Train Loss: 0.006899 Val Loss: nan\n",
      "Epoch [9740/10000] Train Loss: 0.006788 Val Loss: nan\n",
      "Epoch [9741/10000] Train Loss: 0.006831 Val Loss: nan\n",
      "Epoch [9742/10000] Train Loss: 0.006785 Val Loss: nan\n",
      "Epoch [9743/10000] Train Loss: 0.006819 Val Loss: nan\n",
      "Epoch [9744/10000] Train Loss: 0.006859 Val Loss: nan\n",
      "Epoch [9745/10000] Train Loss: 0.006762 Val Loss: nan\n",
      "Epoch [9746/10000] Train Loss: 0.006895 Val Loss: nan\n",
      "Epoch [9747/10000] Train Loss: 0.006841 Val Loss: nan\n",
      "Epoch [9748/10000] Train Loss: 0.006815 Val Loss: nan\n",
      "Epoch [9749/10000] Train Loss: 0.006834 Val Loss: nan\n",
      "Epoch [9750/10000] Train Loss: 0.006885 Val Loss: nan\n",
      "Epoch [9751/10000] Train Loss: 0.006928 Val Loss: nan\n",
      "Epoch [9752/10000] Train Loss: 0.006956 Val Loss: nan\n",
      "Epoch [9753/10000] Train Loss: 0.006832 Val Loss: nan\n",
      "Epoch [9754/10000] Train Loss: 0.006809 Val Loss: nan\n",
      "Epoch [9755/10000] Train Loss: 0.006857 Val Loss: nan\n",
      "Epoch [9756/10000] Train Loss: 0.006768 Val Loss: nan\n",
      "Epoch [9757/10000] Train Loss: 0.006716 Val Loss: nan\n",
      "Epoch [9758/10000] Train Loss: 0.006757 Val Loss: nan\n",
      "Epoch [9759/10000] Train Loss: 0.006769 Val Loss: nan\n",
      "Epoch [9760/10000] Train Loss: 0.006792 Val Loss: nan\n",
      "Epoch [9761/10000] Train Loss: 0.006769 Val Loss: nan\n",
      "Epoch [9762/10000] Train Loss: 0.006852 Val Loss: nan\n",
      "Epoch [9763/10000] Train Loss: 0.006898 Val Loss: nan\n",
      "Epoch [9764/10000] Train Loss: 0.006828 Val Loss: nan\n",
      "Epoch [9765/10000] Train Loss: 0.006806 Val Loss: nan\n",
      "Epoch [9766/10000] Train Loss: 0.007135 Val Loss: nan\n",
      "Epoch [9767/10000] Train Loss: 0.006954 Val Loss: nan\n",
      "Epoch [9768/10000] Train Loss: 0.006805 Val Loss: nan\n",
      "Epoch [9769/10000] Train Loss: 0.007000 Val Loss: nan\n",
      "Epoch [9770/10000] Train Loss: 0.006810 Val Loss: nan\n",
      "Epoch [9771/10000] Train Loss: 0.006887 Val Loss: nan\n",
      "Epoch [9772/10000] Train Loss: 0.006831 Val Loss: nan\n",
      "Epoch [9773/10000] Train Loss: 0.006913 Val Loss: nan\n",
      "Epoch [9774/10000] Train Loss: 0.006752 Val Loss: nan\n",
      "Epoch [9775/10000] Train Loss: 0.006777 Val Loss: nan\n",
      "Epoch [9776/10000] Train Loss: 0.006814 Val Loss: nan\n",
      "Epoch [9777/10000] Train Loss: 0.006838 Val Loss: nan\n",
      "Epoch [9778/10000] Train Loss: 0.006900 Val Loss: nan\n",
      "Epoch [9779/10000] Train Loss: 0.007215 Val Loss: nan\n",
      "Epoch [9780/10000] Train Loss: 0.006759 Val Loss: nan\n",
      "Epoch [9781/10000] Train Loss: 0.006859 Val Loss: nan\n",
      "Epoch [9782/10000] Train Loss: 0.006893 Val Loss: nan\n",
      "Epoch [9783/10000] Train Loss: 0.006741 Val Loss: nan\n",
      "Epoch [9784/10000] Train Loss: 0.006759 Val Loss: nan\n",
      "Epoch [9785/10000] Train Loss: 0.006899 Val Loss: nan\n",
      "Epoch [9786/10000] Train Loss: 0.006838 Val Loss: nan\n",
      "Epoch [9787/10000] Train Loss: 0.006836 Val Loss: nan\n",
      "Epoch [9788/10000] Train Loss: 0.006836 Val Loss: nan\n",
      "Epoch [9789/10000] Train Loss: 0.006709 Val Loss: nan\n",
      "Epoch [9790/10000] Train Loss: 0.006992 Val Loss: nan\n",
      "Epoch [9791/10000] Train Loss: 0.006846 Val Loss: nan\n",
      "Epoch [9792/10000] Train Loss: 0.006890 Val Loss: nan\n",
      "Epoch [9793/10000] Train Loss: 0.006857 Val Loss: nan\n",
      "Epoch [9794/10000] Train Loss: 0.006865 Val Loss: nan\n",
      "Epoch [9795/10000] Train Loss: 0.006738 Val Loss: nan\n",
      "Epoch [9796/10000] Train Loss: 0.006712 Val Loss: nan\n",
      "Epoch [9797/10000] Train Loss: 0.006765 Val Loss: nan\n",
      "Epoch [9798/10000] Train Loss: 0.006969 Val Loss: nan\n",
      "Epoch [9799/10000] Train Loss: 0.006870 Val Loss: nan\n",
      "Epoch [9800/10000] Train Loss: 0.006882 Val Loss: nan\n",
      "Epoch [9801/10000] Train Loss: 0.006940 Val Loss: nan\n",
      "Epoch [9802/10000] Train Loss: 0.006982 Val Loss: nan\n",
      "Epoch [9803/10000] Train Loss: 0.006904 Val Loss: nan\n",
      "Epoch [9804/10000] Train Loss: 0.006745 Val Loss: nan\n",
      "Epoch [9805/10000] Train Loss: 0.006838 Val Loss: nan\n",
      "Epoch [9806/10000] Train Loss: 0.006844 Val Loss: nan\n",
      "Epoch [9807/10000] Train Loss: 0.006726 Val Loss: nan\n",
      "Epoch [9808/10000] Train Loss: 0.006813 Val Loss: nan\n",
      "Epoch [9809/10000] Train Loss: 0.006798 Val Loss: nan\n",
      "Epoch [9810/10000] Train Loss: 0.006864 Val Loss: nan\n",
      "Epoch [9811/10000] Train Loss: 0.006799 Val Loss: nan\n",
      "Epoch [9812/10000] Train Loss: 0.006757 Val Loss: nan\n",
      "Epoch [9813/10000] Train Loss: 0.006713 Val Loss: nan\n",
      "Epoch [9814/10000] Train Loss: 0.007046 Val Loss: nan\n",
      "Epoch [9815/10000] Train Loss: 0.006910 Val Loss: nan\n",
      "Epoch [9816/10000] Train Loss: 0.007027 Val Loss: nan\n",
      "Epoch [9817/10000] Train Loss: 0.006783 Val Loss: nan\n",
      "Epoch [9818/10000] Train Loss: 0.006729 Val Loss: nan\n",
      "Epoch [9819/10000] Train Loss: 0.007163 Val Loss: nan\n",
      "Epoch [9820/10000] Train Loss: 0.006921 Val Loss: nan\n",
      "Epoch [9821/10000] Train Loss: 0.006927 Val Loss: nan\n",
      "Epoch [9822/10000] Train Loss: 0.006875 Val Loss: nan\n",
      "Epoch [9823/10000] Train Loss: 0.006773 Val Loss: nan\n",
      "Epoch [9824/10000] Train Loss: 0.006839 Val Loss: nan\n",
      "Epoch [9825/10000] Train Loss: 0.006856 Val Loss: nan\n",
      "Epoch [9826/10000] Train Loss: 0.006753 Val Loss: nan\n",
      "Epoch [9827/10000] Train Loss: 0.006731 Val Loss: nan\n",
      "Epoch [9828/10000] Train Loss: 0.006873 Val Loss: nan\n",
      "Epoch [9829/10000] Train Loss: 0.006704 Val Loss: nan\n",
      "Epoch [9830/10000] Train Loss: 0.006738 Val Loss: nan\n",
      "Epoch [9831/10000] Train Loss: 0.006957 Val Loss: nan\n",
      "Epoch [9832/10000] Train Loss: 0.006808 Val Loss: nan\n",
      "Epoch [9833/10000] Train Loss: 0.006871 Val Loss: nan\n",
      "Epoch [9834/10000] Train Loss: 0.007089 Val Loss: nan\n",
      "Epoch [9835/10000] Train Loss: 0.006998 Val Loss: nan\n",
      "Epoch [9836/10000] Train Loss: 0.006862 Val Loss: nan\n",
      "Epoch [9837/10000] Train Loss: 0.006944 Val Loss: nan\n",
      "Epoch [9838/10000] Train Loss: 0.006818 Val Loss: nan\n",
      "Epoch [9839/10000] Train Loss: 0.006850 Val Loss: nan\n",
      "Epoch [9840/10000] Train Loss: 0.006797 Val Loss: nan\n",
      "Epoch [9841/10000] Train Loss: 0.007017 Val Loss: nan\n",
      "Epoch [9842/10000] Train Loss: 0.006882 Val Loss: nan\n",
      "Epoch [9843/10000] Train Loss: 0.006783 Val Loss: nan\n",
      "Epoch [9844/10000] Train Loss: 0.006884 Val Loss: nan\n",
      "Epoch [9845/10000] Train Loss: 0.006706 Val Loss: nan\n",
      "Epoch [9846/10000] Train Loss: 0.006743 Val Loss: nan\n",
      "Epoch [9847/10000] Train Loss: 0.006961 Val Loss: nan\n",
      "Epoch [9848/10000] Train Loss: 0.006961 Val Loss: nan\n",
      "Epoch [9849/10000] Train Loss: 0.006913 Val Loss: nan\n",
      "Epoch [9850/10000] Train Loss: 0.006816 Val Loss: nan\n",
      "Epoch [9851/10000] Train Loss: 0.006759 Val Loss: nan\n",
      "Epoch [9852/10000] Train Loss: 0.006835 Val Loss: nan\n",
      "Epoch [9853/10000] Train Loss: 0.006708 Val Loss: nan\n",
      "Epoch [9854/10000] Train Loss: 0.006859 Val Loss: nan\n",
      "Epoch [9855/10000] Train Loss: 0.006710 Val Loss: nan\n",
      "Epoch [9856/10000] Train Loss: 0.006790 Val Loss: nan\n",
      "Epoch [9857/10000] Train Loss: 0.006812 Val Loss: nan\n",
      "Epoch [9858/10000] Train Loss: 0.006794 Val Loss: nan\n",
      "Epoch [9859/10000] Train Loss: 0.006891 Val Loss: nan\n",
      "Epoch [9860/10000] Train Loss: 0.006981 Val Loss: nan\n",
      "Epoch [9861/10000] Train Loss: 0.006862 Val Loss: nan\n",
      "Epoch [9862/10000] Train Loss: 0.006842 Val Loss: nan\n",
      "Epoch [9863/10000] Train Loss: 0.006797 Val Loss: nan\n",
      "Epoch [9864/10000] Train Loss: 0.006971 Val Loss: nan\n",
      "Epoch [9865/10000] Train Loss: 0.006879 Val Loss: nan\n",
      "Epoch [9866/10000] Train Loss: 0.006731 Val Loss: nan\n",
      "Epoch [9867/10000] Train Loss: 0.006802 Val Loss: nan\n",
      "Epoch [9868/10000] Train Loss: 0.006876 Val Loss: nan\n",
      "Epoch [9869/10000] Train Loss: 0.006780 Val Loss: nan\n",
      "Epoch [9870/10000] Train Loss: 0.006820 Val Loss: nan\n",
      "Epoch [9871/10000] Train Loss: 0.006747 Val Loss: nan\n",
      "Epoch [9872/10000] Train Loss: 0.007003 Val Loss: nan\n",
      "Epoch [9873/10000] Train Loss: 0.006904 Val Loss: nan\n",
      "Epoch [9874/10000] Train Loss: 0.006862 Val Loss: nan\n",
      "Epoch [9875/10000] Train Loss: 0.006778 Val Loss: nan\n",
      "Epoch [9876/10000] Train Loss: 0.006818 Val Loss: nan\n",
      "Epoch [9877/10000] Train Loss: 0.006804 Val Loss: nan\n",
      "Epoch [9878/10000] Train Loss: 0.006940 Val Loss: nan\n",
      "Epoch [9879/10000] Train Loss: 0.006933 Val Loss: nan\n",
      "Epoch [9880/10000] Train Loss: 0.006887 Val Loss: nan\n",
      "Epoch [9881/10000] Train Loss: 0.006738 Val Loss: nan\n",
      "Epoch [9882/10000] Train Loss: 0.006722 Val Loss: nan\n",
      "Epoch [9883/10000] Train Loss: 0.007046 Val Loss: nan\n",
      "Epoch [9884/10000] Train Loss: 0.006804 Val Loss: nan\n",
      "Epoch [9885/10000] Train Loss: 0.006784 Val Loss: nan\n",
      "Epoch [9886/10000] Train Loss: 0.006730 Val Loss: nan\n",
      "Epoch [9887/10000] Train Loss: 0.007023 Val Loss: nan\n",
      "Epoch [9888/10000] Train Loss: 0.006867 Val Loss: nan\n",
      "Epoch [9889/10000] Train Loss: 0.006932 Val Loss: nan\n",
      "Epoch [9890/10000] Train Loss: 0.006851 Val Loss: nan\n",
      "Epoch [9891/10000] Train Loss: 0.006903 Val Loss: nan\n",
      "Epoch [9892/10000] Train Loss: 0.006843 Val Loss: nan\n",
      "Epoch [9893/10000] Train Loss: 0.006820 Val Loss: nan\n",
      "Epoch [9894/10000] Train Loss: 0.006940 Val Loss: nan\n",
      "Epoch [9895/10000] Train Loss: 0.006796 Val Loss: nan\n",
      "Epoch [9896/10000] Train Loss: 0.006740 Val Loss: nan\n",
      "Epoch [9897/10000] Train Loss: 0.006794 Val Loss: nan\n",
      "Epoch [9898/10000] Train Loss: 0.006956 Val Loss: nan\n",
      "Epoch [9899/10000] Train Loss: 0.006776 Val Loss: nan\n",
      "Epoch [9900/10000] Train Loss: 0.006701 Val Loss: nan\n",
      "Epoch [9901/10000] Train Loss: 0.006808 Val Loss: nan\n",
      "Epoch [9902/10000] Train Loss: 0.006767 Val Loss: nan\n",
      "Epoch [9903/10000] Train Loss: 0.006902 Val Loss: nan\n",
      "Epoch [9904/10000] Train Loss: 0.007023 Val Loss: nan\n",
      "Epoch [9905/10000] Train Loss: 0.006795 Val Loss: nan\n",
      "Epoch [9906/10000] Train Loss: 0.006742 Val Loss: nan\n",
      "Epoch [9907/10000] Train Loss: 0.006892 Val Loss: nan\n",
      "Epoch [9908/10000] Train Loss: 0.006869 Val Loss: nan\n",
      "Epoch [9909/10000] Train Loss: 0.006730 Val Loss: nan\n",
      "Epoch [9910/10000] Train Loss: 0.006726 Val Loss: nan\n",
      "Epoch [9911/10000] Train Loss: 0.006811 Val Loss: nan\n",
      "Epoch [9912/10000] Train Loss: 0.006768 Val Loss: nan\n",
      "Epoch [9913/10000] Train Loss: 0.006891 Val Loss: nan\n",
      "Epoch [9914/10000] Train Loss: 0.006868 Val Loss: nan\n",
      "Epoch [9915/10000] Train Loss: 0.006873 Val Loss: nan\n",
      "Epoch [9916/10000] Train Loss: 0.006878 Val Loss: nan\n",
      "Epoch [9917/10000] Train Loss: 0.006884 Val Loss: nan\n",
      "Epoch [9918/10000] Train Loss: 0.006924 Val Loss: nan\n",
      "Epoch [9919/10000] Train Loss: 0.006711 Val Loss: nan\n",
      "Epoch [9920/10000] Train Loss: 0.006956 Val Loss: nan\n",
      "Epoch [9921/10000] Train Loss: 0.006799 Val Loss: nan\n",
      "Epoch [9922/10000] Train Loss: 0.006753 Val Loss: nan\n",
      "Epoch [9923/10000] Train Loss: 0.006840 Val Loss: nan\n",
      "Epoch [9924/10000] Train Loss: 0.006854 Val Loss: nan\n",
      "Epoch [9925/10000] Train Loss: 0.006735 Val Loss: nan\n",
      "Epoch [9926/10000] Train Loss: 0.006892 Val Loss: nan\n",
      "Epoch [9927/10000] Train Loss: 0.006834 Val Loss: nan\n",
      "Epoch [9928/10000] Train Loss: 0.006847 Val Loss: nan\n",
      "Epoch [9929/10000] Train Loss: 0.006696 Val Loss: nan\n",
      "Epoch [9930/10000] Train Loss: 0.006693 Val Loss: nan\n",
      "Epoch [9931/10000] Train Loss: 0.006789 Val Loss: nan\n",
      "Epoch [9932/10000] Train Loss: 0.006771 Val Loss: nan\n",
      "Epoch [9933/10000] Train Loss: 0.006898 Val Loss: nan\n",
      "Epoch [9934/10000] Train Loss: 0.006860 Val Loss: nan\n",
      "Epoch [9935/10000] Train Loss: 0.006810 Val Loss: nan\n",
      "Epoch [9936/10000] Train Loss: 0.006829 Val Loss: nan\n",
      "Epoch [9937/10000] Train Loss: 0.007052 Val Loss: nan\n",
      "Epoch [9938/10000] Train Loss: 0.006888 Val Loss: nan\n",
      "Epoch [9939/10000] Train Loss: 0.006772 Val Loss: nan\n",
      "Epoch [9940/10000] Train Loss: 0.006776 Val Loss: nan\n",
      "Epoch [9941/10000] Train Loss: 0.006879 Val Loss: nan\n",
      "Epoch [9942/10000] Train Loss: 0.006993 Val Loss: nan\n",
      "Epoch [9943/10000] Train Loss: 0.006882 Val Loss: nan\n",
      "Epoch [9944/10000] Train Loss: 0.006737 Val Loss: nan\n",
      "Epoch [9945/10000] Train Loss: 0.006851 Val Loss: nan\n",
      "Epoch [9946/10000] Train Loss: 0.007020 Val Loss: nan\n",
      "Epoch [9947/10000] Train Loss: 0.006936 Val Loss: nan\n",
      "Epoch [9948/10000] Train Loss: 0.006819 Val Loss: nan\n",
      "Epoch [9949/10000] Train Loss: 0.006810 Val Loss: nan\n",
      "Epoch [9950/10000] Train Loss: 0.006974 Val Loss: nan\n",
      "Epoch [9951/10000] Train Loss: 0.006786 Val Loss: nan\n",
      "Epoch [9952/10000] Train Loss: 0.006837 Val Loss: nan\n",
      "Epoch [9953/10000] Train Loss: 0.006842 Val Loss: nan\n",
      "Epoch [9954/10000] Train Loss: 0.006860 Val Loss: nan\n",
      "Epoch [9955/10000] Train Loss: 0.006754 Val Loss: nan\n",
      "Epoch [9956/10000] Train Loss: 0.006827 Val Loss: nan\n",
      "Epoch [9957/10000] Train Loss: 0.006765 Val Loss: nan\n",
      "Epoch [9958/10000] Train Loss: 0.006934 Val Loss: nan\n",
      "Epoch [9959/10000] Train Loss: 0.006829 Val Loss: nan\n",
      "Epoch [9960/10000] Train Loss: 0.006740 Val Loss: nan\n",
      "Epoch [9961/10000] Train Loss: 0.006845 Val Loss: nan\n",
      "Epoch [9962/10000] Train Loss: 0.006752 Val Loss: nan\n",
      "Epoch [9963/10000] Train Loss: 0.006713 Val Loss: nan\n",
      "Epoch [9964/10000] Train Loss: 0.006721 Val Loss: nan\n",
      "Epoch [9965/10000] Train Loss: 0.006921 Val Loss: nan\n",
      "Epoch [9966/10000] Train Loss: 0.006971 Val Loss: nan\n",
      "Epoch [9967/10000] Train Loss: 0.006762 Val Loss: nan\n",
      "Epoch [9968/10000] Train Loss: 0.006730 Val Loss: nan\n",
      "Epoch [9969/10000] Train Loss: 0.006771 Val Loss: nan\n",
      "Epoch [9970/10000] Train Loss: 0.006700 Val Loss: nan\n",
      "Epoch [9971/10000] Train Loss: 0.007089 Val Loss: nan\n",
      "Epoch [9972/10000] Train Loss: 0.006721 Val Loss: nan\n",
      "Epoch [9973/10000] Train Loss: 0.006910 Val Loss: nan\n",
      "Epoch [9974/10000] Train Loss: 0.006846 Val Loss: nan\n",
      "Epoch [9975/10000] Train Loss: 0.006788 Val Loss: nan\n",
      "Epoch [9976/10000] Train Loss: 0.006758 Val Loss: nan\n",
      "Epoch [9977/10000] Train Loss: 0.006717 Val Loss: nan\n",
      "Epoch [9978/10000] Train Loss: 0.006759 Val Loss: nan\n",
      "Epoch [9979/10000] Train Loss: 0.006714 Val Loss: nan\n",
      "Epoch [9980/10000] Train Loss: 0.006731 Val Loss: nan\n",
      "Epoch [9981/10000] Train Loss: 0.006918 Val Loss: nan\n",
      "Epoch [9982/10000] Train Loss: 0.006856 Val Loss: nan\n",
      "Epoch [9983/10000] Train Loss: 0.006841 Val Loss: nan\n",
      "Epoch [9984/10000] Train Loss: 0.006749 Val Loss: nan\n",
      "Epoch [9985/10000] Train Loss: 0.006715 Val Loss: nan\n",
      "Epoch [9986/10000] Train Loss: 0.006748 Val Loss: nan\n",
      "Epoch [9987/10000] Train Loss: 0.006876 Val Loss: nan\n",
      "Epoch [9988/10000] Train Loss: 0.006819 Val Loss: nan\n",
      "Epoch [9989/10000] Train Loss: 0.006830 Val Loss: nan\n",
      "Epoch [9990/10000] Train Loss: 0.007004 Val Loss: nan\n",
      "Epoch [9991/10000] Train Loss: 0.006891 Val Loss: nan\n",
      "Epoch [9992/10000] Train Loss: 0.006768 Val Loss: nan\n",
      "Epoch [9993/10000] Train Loss: 0.006737 Val Loss: nan\n",
      "Epoch [9994/10000] Train Loss: 0.006763 Val Loss: nan\n",
      "Epoch [9995/10000] Train Loss: 0.006874 Val Loss: nan\n",
      "Epoch [9996/10000] Train Loss: 0.006941 Val Loss: nan\n",
      "Epoch [9997/10000] Train Loss: 0.006701 Val Loss: nan\n",
      "Epoch [9998/10000] Train Loss: 0.006888 Val Loss: nan\n",
      "Epoch [9999/10000] Train Loss: 0.006836 Val Loss: nan\n",
      "Epoch [10000/10000] Train Loss: 0.006856 Val Loss: nan\n"
     ]
    }
   ],
   "source": [
    "model, history = train(\n",
    "    model,\n",
    "    train_loader=train_dl,\n",
    "    val_loader=None,\n",
    "    epochs=EPOCHS,\n",
    "    patience=PATIENCE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8069c9c1",
   "metadata": {},
   "source": [
    "# Compare with textures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f21295e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralSH()\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7630277",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_sh = true_sh\n",
    "with torch.no_grad():\n",
    "    pred_sh = model(torch.tensor(pos_grid, dtype=torch.float32), torch.tensor(light_angle, dtype=torch.float32))\n",
    "pred_sh = pred_sh.numpy()\n",
    "assert true_sh.shape == pred_sh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19f6e0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_sh_tex = SH3ToTex(true_sh)\n",
    "pred_sh_tex = SH3ToTex(pred_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c0ec055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that nearest sampling yields the same result as loading a raw texture\n",
    "raw_sh_tex = LoadTextures()\n",
    "for field in fields(SHTextures):\n",
    "    raw = getattr(raw_sh_tex, field.name).data\n",
    "    true = getattr(true_sh_tex, field.name).data\n",
    "    assert np.all(raw == true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abfc89a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.44759566..4.867305].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..4.4375].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.35373098..5.373789].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..9.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.41433808..1.6929016].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..1.5625].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.3510786..2.7279384].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..2.875].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.297393..4.572666].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..4.4375].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for field in fields(SHTextures):\n",
    "    pred = getattr(pred_sh_tex, field.name)\n",
    "    true = getattr(true_sh_tex, field.name)\n",
    "\n",
    "    fig, axes = plt.subplots(5, 2, figsize=(8, 6))\n",
    "    axes.flatten()\n",
    "    for row in range(5):\n",
    "        # axes[row].set_title(field.name)\n",
    "        axes[row, 0].axis(\"off\")\n",
    "        axes[row, 1].axis(\"off\")\n",
    "\n",
    "        axes[row, 0].imshow(pred.data[row])\n",
    "        axes[row, 1].imshow(true.data[row])\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"img/{field.name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c43749a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
