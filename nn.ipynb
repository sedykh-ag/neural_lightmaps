{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4db5a8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "39777b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable constants\n",
    "L_POS = 2\n",
    "L_ANGLE = 3\n",
    "\n",
    "MLP_HIDDEN_LAYER_WIDTH = 512\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 10000\n",
    "PATIENCE = 0\n",
    "\n",
    "# fixed constants\n",
    "SH_FLOAT_COUNT = 27\n",
    "\n",
    "PROBES_DIM_X = 50\n",
    "PROBES_DIM_Y = 5\n",
    "PROBES_DIM_Z = 5\n",
    "PROBES_COUNT = PROBES_DIM_X * PROBES_DIM_Y * PROBES_DIM_Z\n",
    "\n",
    "ENCODED_POS_DIM = 3 * L_POS * 2\n",
    "ENCODED_ANGLE_DIM = 1 * L_ANGLE * 2\n",
    "\n",
    "INPUT_DIM = PROBES_COUNT + ENCODED_POS_DIM + ENCODED_ANGLE_DIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "28963ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralSH(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralSH, self).__init__()\n",
    "        self.probe_features = nn.Parameter(torch.rand(PROBES_COUNT), requires_grad=True)\n",
    "\n",
    "        self.hidden_layer = nn.Linear(INPUT_DIM, MLP_HIDDEN_LAYER_WIDTH)\n",
    "        self.hidden_layer2 = nn.Linear(MLP_HIDDEN_LAYER_WIDTH, MLP_HIDDEN_LAYER_WIDTH)\n",
    "        self.hidden_layer3 = nn.Linear(MLP_HIDDEN_LAYER_WIDTH, MLP_HIDDEN_LAYER_WIDTH)\n",
    "\n",
    "        self.output_layer = nn.Linear(MLP_HIDDEN_LAYER_WIDTH, SH_FLOAT_COUNT)\n",
    "\n",
    "    def trigonometric_encoding(self, x: torch.Tensor, L: int):\n",
    "        assert x.ndim == 2\n",
    "        y = []\n",
    "        for i in range(L):\n",
    "            s = torch.sin(2**i * torch.pi * x)\n",
    "            c = torch.cos(2**i * torch.pi * x)\n",
    "            y.append(s)\n",
    "            y.append(c)\n",
    "        y = torch.cat(y, dim=1)\n",
    "        return y\n",
    "\n",
    "    def forward(self, pos, angle):\n",
    "        assert pos.ndim == 2\n",
    "        assert angle.ndim == 2\n",
    "\n",
    "        pos = pos.view(-1, 3)\n",
    "        angle = angle.view(-1, 1)\n",
    "\n",
    "        assert pos.shape[0] == angle.shape[0]\n",
    "        batch_size = pos.shape[0]\n",
    "\n",
    "        pos_enc = self.trigonometric_encoding(pos, L=L_POS)\n",
    "        angle_enc = self.trigonometric_encoding(angle, L=L_ANGLE)\n",
    "\n",
    "        x = torch.cat([pos_enc, angle_enc, self.probe_features.repeat(batch_size, 1)], dim=1)\n",
    "        x = torch.sigmoid(self.hidden_layer(x))\n",
    "        x = torch.sigmoid(self.hidden_layer2(x))\n",
    "        x = torch.sigmoid(self.hidden_layer3(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "805dd4de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1250, 3)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xx, yy, zz = np.meshgrid(\n",
    "    np.linspace(0.0 + 1/(2*PROBES_DIM_X), 1.0 - 1/(2*PROBES_DIM_X), PROBES_DIM_X),\n",
    "    np.linspace(0.0 + 1/(2*PROBES_DIM_Y), 1.0 - 1/(2*PROBES_DIM_Y), PROBES_DIM_Y),\n",
    "    np.linspace(0.0 + 1/(2*PROBES_DIM_Z), 1.0 - 1/(2*PROBES_DIM_Z), PROBES_DIM_Z),\n",
    "    indexing=\"ij\"\n",
    ")\n",
    "pos_grid = np.stack([xx, yy, zz], axis=-1).reshape(-1, 3, order=\"F\") # ensures texture-like (x=fastest, y=middle, z=slowest) ordering\n",
    "pos_grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3cd4ce85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01, 0.1 , 0.1 ],\n",
       "       [0.03, 0.1 , 0.1 ],\n",
       "       [0.05, 0.1 , 0.1 ],\n",
       "       ...,\n",
       "       [0.95, 0.9 , 0.9 ],\n",
       "       [0.97, 0.9 , 0.9 ],\n",
       "       [0.99, 0.9 , 0.9 ]], shape=(1250, 3))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d2181dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1250, 1)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "light_angle = np.zeros((pos_grid.shape[0], 1), dtype=float)\n",
    "light_angle.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c9faf1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, fields\n",
    "from pathlib import Path\n",
    "\n",
    "from texture_sampler import Texture, load_texture_by_name, sample_uv\n",
    "\n",
    "DATA_DIR = Path(\"LightmapsData/\")\n",
    "\n",
    "@dataclass\n",
    "class SHTextures:\n",
    "    AmbientVector: Texture\n",
    "\n",
    "    SHCoefficients0Red: Texture\n",
    "    SHCoefficients0Green: Texture\n",
    "    SHCoefficients0Blue: Texture\n",
    "\n",
    "    SHCoefficients1Red: Texture\n",
    "    SHCoefficients1Green: Texture\n",
    "    SHCoefficients1Blue: Texture\n",
    "\n",
    "def LoadTextures():\n",
    "    return SHTextures(\n",
    "        AmbientVector=load_texture_by_name(DATA_DIR, \"AmbientVector\"),\n",
    "\n",
    "        SHCoefficients0Red=load_texture_by_name(DATA_DIR, \"SHCoefficients_0\"),\n",
    "        SHCoefficients0Green=load_texture_by_name(DATA_DIR, \"SHCoefficients_2\"),\n",
    "        SHCoefficients0Blue=load_texture_by_name(DATA_DIR, \"SHCoefficients_4\"),\n",
    "\n",
    "        SHCoefficients1Red=load_texture_by_name(DATA_DIR, \"SHCoefficients_1\"),\n",
    "        SHCoefficients1Green=load_texture_by_name(DATA_DIR, \"SHCoefficients_3\"),\n",
    "        SHCoefficients1Blue=load_texture_by_name(DATA_DIR, \"SHCoefficients_5\"),\n",
    "    )\n",
    "\n",
    "def Texture3DSample(tex: Texture, uvw: np.ndarray, method: str = \"nearest\"):\n",
    "    uvw = uvw.reshape(-1, 3)\n",
    "    _, _, _, channels = tex.data.shape\n",
    "    results = []\n",
    "    for p in uvw:\n",
    "        color = sample_uv(tex, p[0], p[1], p[2], method=method)\n",
    "        color = color.reshape(-1, channels)\n",
    "        results.append(color)\n",
    "    results = np.concatenate(results, axis=0)\n",
    "    return results\n",
    "\n",
    "def GetRawSH3(BrickTextureUVs: np.ndarray):\n",
    "    tex = LoadTextures()\n",
    "\n",
    "    AmbientVector = Texture3DSample(tex.AmbientVector, BrickTextureUVs)\n",
    "\n",
    "    SHCoefficients0Red = Texture3DSample(tex.SHCoefficients0Red, BrickTextureUVs)\n",
    "    SHCoefficients0Green = Texture3DSample(tex.SHCoefficients0Green, BrickTextureUVs)\n",
    "    SHCoefficients0Blue = Texture3DSample(tex.SHCoefficients0Blue, BrickTextureUVs)\n",
    "\n",
    "    SHCoefficients1Red = Texture3DSample(tex.SHCoefficients1Red, BrickTextureUVs)\n",
    "    SHCoefficients1Green = Texture3DSample(tex.SHCoefficients1Green, BrickTextureUVs)\n",
    "    SHCoefficients1Blue = Texture3DSample(tex.SHCoefficients1Blue, BrickTextureUVs)\n",
    "\n",
    "    IrradianceSH = np.concatenate([\n",
    "        AmbientVector[:, 0:1], # .x\n",
    "        SHCoefficients0Red[:],\n",
    "        SHCoefficients1Red[:],\n",
    "        AmbientVector[:, 1:2], # .y\n",
    "        SHCoefficients0Green[:],\n",
    "        SHCoefficients1Green[:],\n",
    "        AmbientVector[:, 2:3], # .z\n",
    "        SHCoefficients0Blue[:],\n",
    "        SHCoefficients1Blue[:],\n",
    "    ], axis=1)\n",
    "\n",
    "    return IrradianceSH\n",
    "\n",
    "def SH3ToTex(sh: np.ndarray):\n",
    "    assert sh.ndim == 2\n",
    "    assert sh.shape[1] == 27\n",
    "\n",
    "    textures = LoadTextures()\n",
    "\n",
    "    textures.AmbientVector.data = np.concatenate([sh[:, 0:1], sh[:, 9:10], sh[:, 18:19]], axis=1)\n",
    "    textures.SHCoefficients0Red.data = sh[:, 1:5]\n",
    "    textures.SHCoefficients1Red.data = sh[:, 5:9]\n",
    "    textures.SHCoefficients0Green.data = sh[:, 10:14]\n",
    "    textures.SHCoefficients1Green.data = sh[:, 14:18]\n",
    "    textures.SHCoefficients0Blue.data = sh[:, 19:23]\n",
    "    textures.SHCoefficients1Blue.data = sh[:, 23:27]\n",
    "\n",
    "    for field in fields(SHTextures):\n",
    "        tex = getattr(textures, field.name)\n",
    "        # this assumes correct texture-like ordering of tex.data,\n",
    "        # where width changes the fastest, then height and depth is the slowest\n",
    "        tex.data = tex.data.reshape(\n",
    "            tex.meta.depth, tex.meta.height, tex.meta.width, -1\n",
    "        )\n",
    "\n",
    "    return textures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "64b82db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1250, 27)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_sh = GetRawSH3(pos_grid)\n",
    "true_sh.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bdca41",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "25fdc848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "class SHDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            positions_data: np.ndarray,\n",
    "            light_angles_data: np.ndarray,\n",
    "            spherical_harmonics_data: np.ndarray\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.positions_data = torch.FloatTensor(positions_data)\n",
    "        self.light_angles_data = torch.FloatTensor(light_angles_data)\n",
    "        self.spherical_harmonics_data = torch.FloatTensor(spherical_harmonics_data)\n",
    "        assert spherical_harmonics_data.shape[1] == SH_FLOAT_COUNT\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        source = self.positions_data[index], self.light_angles_data[index]\n",
    "        target = self.spherical_harmonics_data[index]\n",
    "        return source, target\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.positions_data) == len(self.light_angles_data)\n",
    "        return len(self.positions_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "05cd883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SHDataset(pos_grid, light_angle, true_sh)\n",
    "# train_ds, test_ds = random_split(dataset, [1.0, 0.0], torch.Generator().manual_seed(42))\n",
    "\n",
    "# train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# test_dl = DataLoader(test_ds, batch_size=len(test_ds))\n",
    "\n",
    "train_ds = dataset\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "028d206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, train_loader: DataLoader, val_loader: DataLoader,\n",
    "          epochs=100, lr=1e-3, patience=10, device=\"cuda\"):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "    history = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for source, target in train_loader:\n",
    "            pos, angle = source\n",
    "            pos = pos.to(device)\n",
    "            angle = angle.to(device)\n",
    "            target = target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(pos, angle)\n",
    "            loss = criterion(outputs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "\n",
    "        # Validation\n",
    "        avg_val_loss = np.nan\n",
    "        if val_loader:\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            with torch.no_grad():\n",
    "                for source, target in val_loader:\n",
    "                    pos, angle = source\n",
    "                    pos = pos.to(device)\n",
    "                    angle = angle.to(device)\n",
    "                    target = target.to(device)\n",
    "                    outputs = model(pos, angle)\n",
    "                    val_loss = criterion(outputs, target)\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "            avg_val_loss = np.mean(val_losses)\n",
    "\n",
    "        history[\"train_loss\"].append(avg_train_loss)\n",
    "        history[\"val_loss\"].append(avg_val_loss)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] \"\n",
    "              f\"Train Loss: {avg_train_loss:.6f} \"\n",
    "              f\"Val Loss: {avg_val_loss:.6f}\")\n",
    "\n",
    "        if (patience != 0) and val_loader: # early stopping based on validation loss\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                epochs_no_improve = 0\n",
    "                torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "                    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "                    break\n",
    "\n",
    "    if (patience == 0) or (val_loader is None): # save model every epoch\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05656aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = NeuralSH()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65167ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10000] Train Loss: 0.064900 Val Loss: nan\n",
      "Epoch [2/10000] Train Loss: 0.034060 Val Loss: nan\n",
      "Epoch [3/10000] Train Loss: 0.030026 Val Loss: nan\n",
      "Epoch [4/10000] Train Loss: 0.028579 Val Loss: nan\n",
      "Epoch [5/10000] Train Loss: 0.028334 Val Loss: nan\n",
      "Epoch [6/10000] Train Loss: 0.027948 Val Loss: nan\n",
      "Epoch [7/10000] Train Loss: 0.027732 Val Loss: nan\n",
      "Epoch [8/10000] Train Loss: 0.027865 Val Loss: nan\n",
      "Epoch [9/10000] Train Loss: 0.027787 Val Loss: nan\n",
      "Epoch [10/10000] Train Loss: 0.027875 Val Loss: nan\n",
      "Epoch [11/10000] Train Loss: 0.027706 Val Loss: nan\n",
      "Epoch [12/10000] Train Loss: 0.027650 Val Loss: nan\n",
      "Epoch [13/10000] Train Loss: 0.027770 Val Loss: nan\n",
      "Epoch [14/10000] Train Loss: 0.027663 Val Loss: nan\n",
      "Epoch [15/10000] Train Loss: 0.027525 Val Loss: nan\n",
      "Epoch [16/10000] Train Loss: 0.027922 Val Loss: nan\n",
      "Epoch [17/10000] Train Loss: 0.028114 Val Loss: nan\n",
      "Epoch [18/10000] Train Loss: 0.027800 Val Loss: nan\n",
      "Epoch [19/10000] Train Loss: 0.027806 Val Loss: nan\n",
      "Epoch [20/10000] Train Loss: 0.027761 Val Loss: nan\n",
      "Epoch [21/10000] Train Loss: 0.027857 Val Loss: nan\n",
      "Epoch [22/10000] Train Loss: 0.027565 Val Loss: nan\n",
      "Epoch [23/10000] Train Loss: 0.027543 Val Loss: nan\n",
      "Epoch [24/10000] Train Loss: 0.027455 Val Loss: nan\n",
      "Epoch [25/10000] Train Loss: 0.027105 Val Loss: nan\n",
      "Epoch [26/10000] Train Loss: 0.027144 Val Loss: nan\n",
      "Epoch [27/10000] Train Loss: 0.027631 Val Loss: nan\n",
      "Epoch [28/10000] Train Loss: 0.027570 Val Loss: nan\n",
      "Epoch [29/10000] Train Loss: 0.026910 Val Loss: nan\n",
      "Epoch [30/10000] Train Loss: 0.026644 Val Loss: nan\n",
      "Epoch [31/10000] Train Loss: 0.026613 Val Loss: nan\n",
      "Epoch [32/10000] Train Loss: 0.026800 Val Loss: nan\n",
      "Epoch [33/10000] Train Loss: 0.026981 Val Loss: nan\n",
      "Epoch [34/10000] Train Loss: 0.027095 Val Loss: nan\n",
      "Epoch [35/10000] Train Loss: 0.026628 Val Loss: nan\n",
      "Epoch [36/10000] Train Loss: 0.027033 Val Loss: nan\n",
      "Epoch [37/10000] Train Loss: 0.026785 Val Loss: nan\n",
      "Epoch [38/10000] Train Loss: 0.026164 Val Loss: nan\n",
      "Epoch [39/10000] Train Loss: 0.026505 Val Loss: nan\n",
      "Epoch [40/10000] Train Loss: 0.027174 Val Loss: nan\n",
      "Epoch [41/10000] Train Loss: 0.026535 Val Loss: nan\n",
      "Epoch [42/10000] Train Loss: 0.025952 Val Loss: nan\n",
      "Epoch [43/10000] Train Loss: 0.026360 Val Loss: nan\n",
      "Epoch [44/10000] Train Loss: 0.026527 Val Loss: nan\n",
      "Epoch [45/10000] Train Loss: 0.026259 Val Loss: nan\n",
      "Epoch [46/10000] Train Loss: 0.026191 Val Loss: nan\n",
      "Epoch [47/10000] Train Loss: 0.025888 Val Loss: nan\n",
      "Epoch [48/10000] Train Loss: 0.026179 Val Loss: nan\n",
      "Epoch [49/10000] Train Loss: 0.025613 Val Loss: nan\n",
      "Epoch [50/10000] Train Loss: 0.026159 Val Loss: nan\n",
      "Epoch [51/10000] Train Loss: 0.026099 Val Loss: nan\n",
      "Epoch [52/10000] Train Loss: 0.026169 Val Loss: nan\n",
      "Epoch [53/10000] Train Loss: 0.026043 Val Loss: nan\n",
      "Epoch [54/10000] Train Loss: 0.025897 Val Loss: nan\n",
      "Epoch [55/10000] Train Loss: 0.025747 Val Loss: nan\n",
      "Epoch [56/10000] Train Loss: 0.025213 Val Loss: nan\n",
      "Epoch [57/10000] Train Loss: 0.026175 Val Loss: nan\n",
      "Epoch [58/10000] Train Loss: 0.026138 Val Loss: nan\n",
      "Epoch [59/10000] Train Loss: 0.025988 Val Loss: nan\n",
      "Epoch [60/10000] Train Loss: 0.026654 Val Loss: nan\n",
      "Epoch [61/10000] Train Loss: 0.025470 Val Loss: nan\n",
      "Epoch [62/10000] Train Loss: 0.024610 Val Loss: nan\n",
      "Epoch [63/10000] Train Loss: 0.024953 Val Loss: nan\n",
      "Epoch [64/10000] Train Loss: 0.025068 Val Loss: nan\n",
      "Epoch [65/10000] Train Loss: 0.024973 Val Loss: nan\n",
      "Epoch [66/10000] Train Loss: 0.024446 Val Loss: nan\n",
      "Epoch [67/10000] Train Loss: 0.024474 Val Loss: nan\n",
      "Epoch [68/10000] Train Loss: 0.024736 Val Loss: nan\n",
      "Epoch [69/10000] Train Loss: 0.024024 Val Loss: nan\n",
      "Epoch [70/10000] Train Loss: 0.024002 Val Loss: nan\n",
      "Epoch [71/10000] Train Loss: 0.023526 Val Loss: nan\n",
      "Epoch [72/10000] Train Loss: 0.023928 Val Loss: nan\n",
      "Epoch [73/10000] Train Loss: 0.024096 Val Loss: nan\n",
      "Epoch [74/10000] Train Loss: 0.024210 Val Loss: nan\n",
      "Epoch [75/10000] Train Loss: 0.023649 Val Loss: nan\n",
      "Epoch [76/10000] Train Loss: 0.023628 Val Loss: nan\n",
      "Epoch [77/10000] Train Loss: 0.023584 Val Loss: nan\n",
      "Epoch [78/10000] Train Loss: 0.023462 Val Loss: nan\n",
      "Epoch [79/10000] Train Loss: 0.023474 Val Loss: nan\n",
      "Epoch [80/10000] Train Loss: 0.023028 Val Loss: nan\n",
      "Epoch [81/10000] Train Loss: 0.023513 Val Loss: nan\n",
      "Epoch [82/10000] Train Loss: 0.024104 Val Loss: nan\n",
      "Epoch [83/10000] Train Loss: 0.024536 Val Loss: nan\n",
      "Epoch [84/10000] Train Loss: 0.024126 Val Loss: nan\n",
      "Epoch [85/10000] Train Loss: 0.023830 Val Loss: nan\n",
      "Epoch [86/10000] Train Loss: 0.023290 Val Loss: nan\n",
      "Epoch [87/10000] Train Loss: 0.023464 Val Loss: nan\n",
      "Epoch [88/10000] Train Loss: 0.022936 Val Loss: nan\n",
      "Epoch [89/10000] Train Loss: 0.023329 Val Loss: nan\n",
      "Epoch [90/10000] Train Loss: 0.023666 Val Loss: nan\n",
      "Epoch [91/10000] Train Loss: 0.022904 Val Loss: nan\n",
      "Epoch [92/10000] Train Loss: 0.023091 Val Loss: nan\n",
      "Epoch [93/10000] Train Loss: 0.023136 Val Loss: nan\n",
      "Epoch [94/10000] Train Loss: 0.022913 Val Loss: nan\n",
      "Epoch [95/10000] Train Loss: 0.023262 Val Loss: nan\n",
      "Epoch [96/10000] Train Loss: 0.022671 Val Loss: nan\n",
      "Epoch [97/10000] Train Loss: 0.022797 Val Loss: nan\n",
      "Epoch [98/10000] Train Loss: 0.022393 Val Loss: nan\n",
      "Epoch [99/10000] Train Loss: 0.022752 Val Loss: nan\n",
      "Epoch [100/10000] Train Loss: 0.022528 Val Loss: nan\n",
      "Epoch [101/10000] Train Loss: 0.022965 Val Loss: nan\n",
      "Epoch [102/10000] Train Loss: 0.024281 Val Loss: nan\n",
      "Epoch [103/10000] Train Loss: 0.023460 Val Loss: nan\n",
      "Epoch [104/10000] Train Loss: 0.023547 Val Loss: nan\n",
      "Epoch [105/10000] Train Loss: 0.022286 Val Loss: nan\n",
      "Epoch [106/10000] Train Loss: 0.022258 Val Loss: nan\n",
      "Epoch [107/10000] Train Loss: 0.022406 Val Loss: nan\n",
      "Epoch [108/10000] Train Loss: 0.022196 Val Loss: nan\n",
      "Epoch [109/10000] Train Loss: 0.022045 Val Loss: nan\n",
      "Epoch [110/10000] Train Loss: 0.022328 Val Loss: nan\n",
      "Epoch [111/10000] Train Loss: 0.022263 Val Loss: nan\n",
      "Epoch [112/10000] Train Loss: 0.021680 Val Loss: nan\n",
      "Epoch [113/10000] Train Loss: 0.022055 Val Loss: nan\n",
      "Epoch [114/10000] Train Loss: 0.021455 Val Loss: nan\n",
      "Epoch [115/10000] Train Loss: 0.021233 Val Loss: nan\n",
      "Epoch [116/10000] Train Loss: 0.021961 Val Loss: nan\n",
      "Epoch [117/10000] Train Loss: 0.022740 Val Loss: nan\n",
      "Epoch [118/10000] Train Loss: 0.022314 Val Loss: nan\n",
      "Epoch [119/10000] Train Loss: 0.021810 Val Loss: nan\n",
      "Epoch [120/10000] Train Loss: 0.022253 Val Loss: nan\n",
      "Epoch [121/10000] Train Loss: 0.021402 Val Loss: nan\n",
      "Epoch [122/10000] Train Loss: 0.021349 Val Loss: nan\n",
      "Epoch [123/10000] Train Loss: 0.021517 Val Loss: nan\n",
      "Epoch [124/10000] Train Loss: 0.021723 Val Loss: nan\n",
      "Epoch [125/10000] Train Loss: 0.021525 Val Loss: nan\n",
      "Epoch [126/10000] Train Loss: 0.021378 Val Loss: nan\n",
      "Epoch [127/10000] Train Loss: 0.021272 Val Loss: nan\n",
      "Epoch [128/10000] Train Loss: 0.021572 Val Loss: nan\n",
      "Epoch [129/10000] Train Loss: 0.021006 Val Loss: nan\n",
      "Epoch [130/10000] Train Loss: 0.020449 Val Loss: nan\n",
      "Epoch [131/10000] Train Loss: 0.021328 Val Loss: nan\n",
      "Epoch [132/10000] Train Loss: 0.020492 Val Loss: nan\n",
      "Epoch [133/10000] Train Loss: 0.020146 Val Loss: nan\n",
      "Epoch [134/10000] Train Loss: 0.019876 Val Loss: nan\n",
      "Epoch [135/10000] Train Loss: 0.020280 Val Loss: nan\n",
      "Epoch [136/10000] Train Loss: 0.021110 Val Loss: nan\n",
      "Epoch [137/10000] Train Loss: 0.020884 Val Loss: nan\n",
      "Epoch [138/10000] Train Loss: 0.020452 Val Loss: nan\n",
      "Epoch [139/10000] Train Loss: 0.021371 Val Loss: nan\n",
      "Epoch [140/10000] Train Loss: 0.019665 Val Loss: nan\n",
      "Epoch [141/10000] Train Loss: 0.019474 Val Loss: nan\n",
      "Epoch [142/10000] Train Loss: 0.019186 Val Loss: nan\n",
      "Epoch [143/10000] Train Loss: 0.019679 Val Loss: nan\n",
      "Epoch [144/10000] Train Loss: 0.018757 Val Loss: nan\n",
      "Epoch [145/10000] Train Loss: 0.019004 Val Loss: nan\n",
      "Epoch [146/10000] Train Loss: 0.018582 Val Loss: nan\n",
      "Epoch [147/10000] Train Loss: 0.019041 Val Loss: nan\n",
      "Epoch [148/10000] Train Loss: 0.019118 Val Loss: nan\n",
      "Epoch [149/10000] Train Loss: 0.018777 Val Loss: nan\n",
      "Epoch [150/10000] Train Loss: 0.018109 Val Loss: nan\n",
      "Epoch [151/10000] Train Loss: 0.019369 Val Loss: nan\n",
      "Epoch [152/10000] Train Loss: 0.019334 Val Loss: nan\n",
      "Epoch [153/10000] Train Loss: 0.017755 Val Loss: nan\n",
      "Epoch [154/10000] Train Loss: 0.017420 Val Loss: nan\n",
      "Epoch [155/10000] Train Loss: 0.018602 Val Loss: nan\n",
      "Epoch [156/10000] Train Loss: 0.017997 Val Loss: nan\n",
      "Epoch [157/10000] Train Loss: 0.017188 Val Loss: nan\n",
      "Epoch [158/10000] Train Loss: 0.017001 Val Loss: nan\n",
      "Epoch [159/10000] Train Loss: 0.017763 Val Loss: nan\n",
      "Epoch [160/10000] Train Loss: 0.017965 Val Loss: nan\n",
      "Epoch [161/10000] Train Loss: 0.017958 Val Loss: nan\n",
      "Epoch [162/10000] Train Loss: 0.017969 Val Loss: nan\n",
      "Epoch [163/10000] Train Loss: 0.016830 Val Loss: nan\n",
      "Epoch [164/10000] Train Loss: 0.017951 Val Loss: nan\n",
      "Epoch [165/10000] Train Loss: 0.016970 Val Loss: nan\n",
      "Epoch [166/10000] Train Loss: 0.016448 Val Loss: nan\n",
      "Epoch [167/10000] Train Loss: 0.016510 Val Loss: nan\n",
      "Epoch [168/10000] Train Loss: 0.015895 Val Loss: nan\n",
      "Epoch [169/10000] Train Loss: 0.015729 Val Loss: nan\n",
      "Epoch [170/10000] Train Loss: 0.015389 Val Loss: nan\n",
      "Epoch [171/10000] Train Loss: 0.016714 Val Loss: nan\n",
      "Epoch [172/10000] Train Loss: 0.015650 Val Loss: nan\n",
      "Epoch [173/10000] Train Loss: 0.015994 Val Loss: nan\n",
      "Epoch [174/10000] Train Loss: 0.016188 Val Loss: nan\n",
      "Epoch [175/10000] Train Loss: 0.016087 Val Loss: nan\n",
      "Epoch [176/10000] Train Loss: 0.015594 Val Loss: nan\n",
      "Epoch [177/10000] Train Loss: 0.016653 Val Loss: nan\n",
      "Epoch [178/10000] Train Loss: 0.015316 Val Loss: nan\n",
      "Epoch [179/10000] Train Loss: 0.015787 Val Loss: nan\n",
      "Epoch [180/10000] Train Loss: 0.015596 Val Loss: nan\n",
      "Epoch [181/10000] Train Loss: 0.014915 Val Loss: nan\n",
      "Epoch [182/10000] Train Loss: 0.015792 Val Loss: nan\n",
      "Epoch [183/10000] Train Loss: 0.015014 Val Loss: nan\n",
      "Epoch [184/10000] Train Loss: 0.014040 Val Loss: nan\n",
      "Epoch [185/10000] Train Loss: 0.013660 Val Loss: nan\n",
      "Epoch [186/10000] Train Loss: 0.014205 Val Loss: nan\n",
      "Epoch [187/10000] Train Loss: 0.014004 Val Loss: nan\n",
      "Epoch [188/10000] Train Loss: 0.015137 Val Loss: nan\n",
      "Epoch [189/10000] Train Loss: 0.014208 Val Loss: nan\n",
      "Epoch [190/10000] Train Loss: 0.014210 Val Loss: nan\n",
      "Epoch [191/10000] Train Loss: 0.013809 Val Loss: nan\n",
      "Epoch [192/10000] Train Loss: 0.013836 Val Loss: nan\n",
      "Epoch [193/10000] Train Loss: 0.013464 Val Loss: nan\n",
      "Epoch [194/10000] Train Loss: 0.013124 Val Loss: nan\n",
      "Epoch [195/10000] Train Loss: 0.012967 Val Loss: nan\n",
      "Epoch [196/10000] Train Loss: 0.012385 Val Loss: nan\n",
      "Epoch [197/10000] Train Loss: 0.012237 Val Loss: nan\n",
      "Epoch [198/10000] Train Loss: 0.013300 Val Loss: nan\n",
      "Epoch [199/10000] Train Loss: 0.012407 Val Loss: nan\n",
      "Epoch [200/10000] Train Loss: 0.011954 Val Loss: nan\n",
      "Epoch [201/10000] Train Loss: 0.012443 Val Loss: nan\n",
      "Epoch [202/10000] Train Loss: 0.012003 Val Loss: nan\n",
      "Epoch [203/10000] Train Loss: 0.012311 Val Loss: nan\n",
      "Epoch [204/10000] Train Loss: 0.011948 Val Loss: nan\n",
      "Epoch [205/10000] Train Loss: 0.012739 Val Loss: nan\n",
      "Epoch [206/10000] Train Loss: 0.011391 Val Loss: nan\n",
      "Epoch [207/10000] Train Loss: 0.012250 Val Loss: nan\n",
      "Epoch [208/10000] Train Loss: 0.011766 Val Loss: nan\n",
      "Epoch [209/10000] Train Loss: 0.012322 Val Loss: nan\n",
      "Epoch [210/10000] Train Loss: 0.011177 Val Loss: nan\n",
      "Epoch [211/10000] Train Loss: 0.012533 Val Loss: nan\n",
      "Epoch [212/10000] Train Loss: 0.013964 Val Loss: nan\n",
      "Epoch [213/10000] Train Loss: 0.013295 Val Loss: nan\n",
      "Epoch [214/10000] Train Loss: 0.011587 Val Loss: nan\n",
      "Epoch [215/10000] Train Loss: 0.010658 Val Loss: nan\n",
      "Epoch [216/10000] Train Loss: 0.010475 Val Loss: nan\n",
      "Epoch [217/10000] Train Loss: 0.010139 Val Loss: nan\n",
      "Epoch [218/10000] Train Loss: 0.010847 Val Loss: nan\n",
      "Epoch [219/10000] Train Loss: 0.010008 Val Loss: nan\n",
      "Epoch [220/10000] Train Loss: 0.010250 Val Loss: nan\n",
      "Epoch [221/10000] Train Loss: 0.010125 Val Loss: nan\n",
      "Epoch [222/10000] Train Loss: 0.011190 Val Loss: nan\n",
      "Epoch [223/10000] Train Loss: 0.010576 Val Loss: nan\n",
      "Epoch [224/10000] Train Loss: 0.009675 Val Loss: nan\n",
      "Epoch [225/10000] Train Loss: 0.009711 Val Loss: nan\n",
      "Epoch [226/10000] Train Loss: 0.010128 Val Loss: nan\n",
      "Epoch [227/10000] Train Loss: 0.009953 Val Loss: nan\n",
      "Epoch [228/10000] Train Loss: 0.010375 Val Loss: nan\n",
      "Epoch [229/10000] Train Loss: 0.009989 Val Loss: nan\n",
      "Epoch [230/10000] Train Loss: 0.009509 Val Loss: nan\n",
      "Epoch [231/10000] Train Loss: 0.009203 Val Loss: nan\n",
      "Epoch [232/10000] Train Loss: 0.009271 Val Loss: nan\n",
      "Epoch [233/10000] Train Loss: 0.009749 Val Loss: nan\n",
      "Epoch [234/10000] Train Loss: 0.009888 Val Loss: nan\n",
      "Epoch [235/10000] Train Loss: 0.009163 Val Loss: nan\n",
      "Epoch [236/10000] Train Loss: 0.009323 Val Loss: nan\n",
      "Epoch [237/10000] Train Loss: 0.009013 Val Loss: nan\n",
      "Epoch [238/10000] Train Loss: 0.009305 Val Loss: nan\n",
      "Epoch [239/10000] Train Loss: 0.009607 Val Loss: nan\n",
      "Epoch [240/10000] Train Loss: 0.008816 Val Loss: nan\n",
      "Epoch [241/10000] Train Loss: 0.008535 Val Loss: nan\n",
      "Epoch [242/10000] Train Loss: 0.008995 Val Loss: nan\n",
      "Epoch [243/10000] Train Loss: 0.008596 Val Loss: nan\n",
      "Epoch [244/10000] Train Loss: 0.008678 Val Loss: nan\n",
      "Epoch [245/10000] Train Loss: 0.008279 Val Loss: nan\n",
      "Epoch [246/10000] Train Loss: 0.008270 Val Loss: nan\n",
      "Epoch [247/10000] Train Loss: 0.008083 Val Loss: nan\n",
      "Epoch [248/10000] Train Loss: 0.008370 Val Loss: nan\n",
      "Epoch [249/10000] Train Loss: 0.009257 Val Loss: nan\n",
      "Epoch [250/10000] Train Loss: 0.009574 Val Loss: nan\n",
      "Epoch [251/10000] Train Loss: 0.008153 Val Loss: nan\n",
      "Epoch [252/10000] Train Loss: 0.008567 Val Loss: nan\n",
      "Epoch [253/10000] Train Loss: 0.008235 Val Loss: nan\n",
      "Epoch [254/10000] Train Loss: 0.008299 Val Loss: nan\n",
      "Epoch [255/10000] Train Loss: 0.008371 Val Loss: nan\n",
      "Epoch [256/10000] Train Loss: 0.008564 Val Loss: nan\n",
      "Epoch [257/10000] Train Loss: 0.008326 Val Loss: nan\n",
      "Epoch [258/10000] Train Loss: 0.008273 Val Loss: nan\n",
      "Epoch [259/10000] Train Loss: 0.008666 Val Loss: nan\n",
      "Epoch [260/10000] Train Loss: 0.009178 Val Loss: nan\n",
      "Epoch [261/10000] Train Loss: 0.007924 Val Loss: nan\n",
      "Epoch [262/10000] Train Loss: 0.008234 Val Loss: nan\n",
      "Epoch [263/10000] Train Loss: 0.008468 Val Loss: nan\n",
      "Epoch [264/10000] Train Loss: 0.009369 Val Loss: nan\n",
      "Epoch [265/10000] Train Loss: 0.008923 Val Loss: nan\n",
      "Epoch [266/10000] Train Loss: 0.008203 Val Loss: nan\n",
      "Epoch [267/10000] Train Loss: 0.008745 Val Loss: nan\n",
      "Epoch [268/10000] Train Loss: 0.009140 Val Loss: nan\n",
      "Epoch [269/10000] Train Loss: 0.008203 Val Loss: nan\n",
      "Epoch [270/10000] Train Loss: 0.007320 Val Loss: nan\n",
      "Epoch [271/10000] Train Loss: 0.007689 Val Loss: nan\n",
      "Epoch [272/10000] Train Loss: 0.007898 Val Loss: nan\n",
      "Epoch [273/10000] Train Loss: 0.007196 Val Loss: nan\n",
      "Epoch [274/10000] Train Loss: 0.009096 Val Loss: nan\n",
      "Epoch [275/10000] Train Loss: 0.009180 Val Loss: nan\n",
      "Epoch [276/10000] Train Loss: 0.008831 Val Loss: nan\n",
      "Epoch [277/10000] Train Loss: 0.008653 Val Loss: nan\n",
      "Epoch [278/10000] Train Loss: 0.006923 Val Loss: nan\n",
      "Epoch [279/10000] Train Loss: 0.006769 Val Loss: nan\n",
      "Epoch [280/10000] Train Loss: 0.008282 Val Loss: nan\n",
      "Epoch [281/10000] Train Loss: 0.008700 Val Loss: nan\n",
      "Epoch [282/10000] Train Loss: 0.007057 Val Loss: nan\n",
      "Epoch [283/10000] Train Loss: 0.007162 Val Loss: nan\n",
      "Epoch [284/10000] Train Loss: 0.007034 Val Loss: nan\n",
      "Epoch [285/10000] Train Loss: 0.007078 Val Loss: nan\n",
      "Epoch [286/10000] Train Loss: 0.007173 Val Loss: nan\n",
      "Epoch [287/10000] Train Loss: 0.007098 Val Loss: nan\n",
      "Epoch [288/10000] Train Loss: 0.006449 Val Loss: nan\n",
      "Epoch [289/10000] Train Loss: 0.006860 Val Loss: nan\n",
      "Epoch [290/10000] Train Loss: 0.007157 Val Loss: nan\n",
      "Epoch [291/10000] Train Loss: 0.006554 Val Loss: nan\n",
      "Epoch [292/10000] Train Loss: 0.007391 Val Loss: nan\n",
      "Epoch [293/10000] Train Loss: 0.007371 Val Loss: nan\n",
      "Epoch [294/10000] Train Loss: 0.007461 Val Loss: nan\n",
      "Epoch [295/10000] Train Loss: 0.006938 Val Loss: nan\n",
      "Epoch [296/10000] Train Loss: 0.007096 Val Loss: nan\n",
      "Epoch [297/10000] Train Loss: 0.006648 Val Loss: nan\n",
      "Epoch [298/10000] Train Loss: 0.005970 Val Loss: nan\n",
      "Epoch [299/10000] Train Loss: 0.005975 Val Loss: nan\n",
      "Epoch [300/10000] Train Loss: 0.006008 Val Loss: nan\n",
      "Epoch [301/10000] Train Loss: 0.006066 Val Loss: nan\n",
      "Epoch [302/10000] Train Loss: 0.006392 Val Loss: nan\n",
      "Epoch [303/10000] Train Loss: 0.005811 Val Loss: nan\n",
      "Epoch [304/10000] Train Loss: 0.006189 Val Loss: nan\n",
      "Epoch [305/10000] Train Loss: 0.006105 Val Loss: nan\n",
      "Epoch [306/10000] Train Loss: 0.006051 Val Loss: nan\n",
      "Epoch [307/10000] Train Loss: 0.007242 Val Loss: nan\n",
      "Epoch [308/10000] Train Loss: 0.007204 Val Loss: nan\n",
      "Epoch [309/10000] Train Loss: 0.006371 Val Loss: nan\n",
      "Epoch [310/10000] Train Loss: 0.006685 Val Loss: nan\n",
      "Epoch [311/10000] Train Loss: 0.006219 Val Loss: nan\n",
      "Epoch [312/10000] Train Loss: 0.005861 Val Loss: nan\n",
      "Epoch [313/10000] Train Loss: 0.006265 Val Loss: nan\n",
      "Epoch [314/10000] Train Loss: 0.006156 Val Loss: nan\n",
      "Epoch [315/10000] Train Loss: 0.006818 Val Loss: nan\n",
      "Epoch [316/10000] Train Loss: 0.006025 Val Loss: nan\n",
      "Epoch [317/10000] Train Loss: 0.006209 Val Loss: nan\n",
      "Epoch [318/10000] Train Loss: 0.005695 Val Loss: nan\n",
      "Epoch [319/10000] Train Loss: 0.005847 Val Loss: nan\n",
      "Epoch [320/10000] Train Loss: 0.006100 Val Loss: nan\n",
      "Epoch [321/10000] Train Loss: 0.006082 Val Loss: nan\n",
      "Epoch [322/10000] Train Loss: 0.005986 Val Loss: nan\n",
      "Epoch [323/10000] Train Loss: 0.005795 Val Loss: nan\n",
      "Epoch [324/10000] Train Loss: 0.005707 Val Loss: nan\n",
      "Epoch [325/10000] Train Loss: 0.005675 Val Loss: nan\n",
      "Epoch [326/10000] Train Loss: 0.006221 Val Loss: nan\n",
      "Epoch [327/10000] Train Loss: 0.005755 Val Loss: nan\n",
      "Epoch [328/10000] Train Loss: 0.005746 Val Loss: nan\n",
      "Epoch [329/10000] Train Loss: 0.005617 Val Loss: nan\n",
      "Epoch [330/10000] Train Loss: 0.005535 Val Loss: nan\n",
      "Epoch [331/10000] Train Loss: 0.005371 Val Loss: nan\n",
      "Epoch [332/10000] Train Loss: 0.005297 Val Loss: nan\n",
      "Epoch [333/10000] Train Loss: 0.005758 Val Loss: nan\n",
      "Epoch [334/10000] Train Loss: 0.005981 Val Loss: nan\n",
      "Epoch [335/10000] Train Loss: 0.005279 Val Loss: nan\n",
      "Epoch [336/10000] Train Loss: 0.006126 Val Loss: nan\n",
      "Epoch [337/10000] Train Loss: 0.006125 Val Loss: nan\n",
      "Epoch [338/10000] Train Loss: 0.006080 Val Loss: nan\n",
      "Epoch [339/10000] Train Loss: 0.005333 Val Loss: nan\n",
      "Epoch [340/10000] Train Loss: 0.005589 Val Loss: nan\n",
      "Epoch [341/10000] Train Loss: 0.005246 Val Loss: nan\n",
      "Epoch [342/10000] Train Loss: 0.005069 Val Loss: nan\n",
      "Epoch [343/10000] Train Loss: 0.005227 Val Loss: nan\n",
      "Epoch [344/10000] Train Loss: 0.005069 Val Loss: nan\n",
      "Epoch [345/10000] Train Loss: 0.004980 Val Loss: nan\n",
      "Epoch [346/10000] Train Loss: 0.005299 Val Loss: nan\n",
      "Epoch [347/10000] Train Loss: 0.005774 Val Loss: nan\n",
      "Epoch [348/10000] Train Loss: 0.006348 Val Loss: nan\n",
      "Epoch [349/10000] Train Loss: 0.006165 Val Loss: nan\n",
      "Epoch [350/10000] Train Loss: 0.004943 Val Loss: nan\n",
      "Epoch [351/10000] Train Loss: 0.005433 Val Loss: nan\n",
      "Epoch [352/10000] Train Loss: 0.005994 Val Loss: nan\n",
      "Epoch [353/10000] Train Loss: 0.004862 Val Loss: nan\n",
      "Epoch [354/10000] Train Loss: 0.006325 Val Loss: nan\n",
      "Epoch [355/10000] Train Loss: 0.005898 Val Loss: nan\n",
      "Epoch [356/10000] Train Loss: 0.005086 Val Loss: nan\n",
      "Epoch [357/10000] Train Loss: 0.005179 Val Loss: nan\n",
      "Epoch [358/10000] Train Loss: 0.005264 Val Loss: nan\n",
      "Epoch [359/10000] Train Loss: 0.004903 Val Loss: nan\n",
      "Epoch [360/10000] Train Loss: 0.004943 Val Loss: nan\n",
      "Epoch [361/10000] Train Loss: 0.004927 Val Loss: nan\n",
      "Epoch [362/10000] Train Loss: 0.004789 Val Loss: nan\n",
      "Epoch [363/10000] Train Loss: 0.004565 Val Loss: nan\n",
      "Epoch [364/10000] Train Loss: 0.005764 Val Loss: nan\n",
      "Epoch [365/10000] Train Loss: 0.004487 Val Loss: nan\n",
      "Epoch [366/10000] Train Loss: 0.005146 Val Loss: nan\n",
      "Epoch [367/10000] Train Loss: 0.004997 Val Loss: nan\n",
      "Epoch [368/10000] Train Loss: 0.006096 Val Loss: nan\n",
      "Epoch [369/10000] Train Loss: 0.004822 Val Loss: nan\n",
      "Epoch [370/10000] Train Loss: 0.004748 Val Loss: nan\n",
      "Epoch [371/10000] Train Loss: 0.005694 Val Loss: nan\n",
      "Epoch [372/10000] Train Loss: 0.004954 Val Loss: nan\n",
      "Epoch [373/10000] Train Loss: 0.005131 Val Loss: nan\n",
      "Epoch [374/10000] Train Loss: 0.004736 Val Loss: nan\n",
      "Epoch [375/10000] Train Loss: 0.004736 Val Loss: nan\n",
      "Epoch [376/10000] Train Loss: 0.004595 Val Loss: nan\n",
      "Epoch [377/10000] Train Loss: 0.005341 Val Loss: nan\n",
      "Epoch [378/10000] Train Loss: 0.005284 Val Loss: nan\n",
      "Epoch [379/10000] Train Loss: 0.004630 Val Loss: nan\n",
      "Epoch [380/10000] Train Loss: 0.004548 Val Loss: nan\n",
      "Epoch [381/10000] Train Loss: 0.004900 Val Loss: nan\n",
      "Epoch [382/10000] Train Loss: 0.004694 Val Loss: nan\n",
      "Epoch [383/10000] Train Loss: 0.004480 Val Loss: nan\n",
      "Epoch [384/10000] Train Loss: 0.004540 Val Loss: nan\n",
      "Epoch [385/10000] Train Loss: 0.004744 Val Loss: nan\n",
      "Epoch [386/10000] Train Loss: 0.004628 Val Loss: nan\n",
      "Epoch [387/10000] Train Loss: 0.004536 Val Loss: nan\n",
      "Epoch [388/10000] Train Loss: 0.004832 Val Loss: nan\n",
      "Epoch [389/10000] Train Loss: 0.004796 Val Loss: nan\n",
      "Epoch [390/10000] Train Loss: 0.004376 Val Loss: nan\n",
      "Epoch [391/10000] Train Loss: 0.004579 Val Loss: nan\n",
      "Epoch [392/10000] Train Loss: 0.004650 Val Loss: nan\n",
      "Epoch [393/10000] Train Loss: 0.004565 Val Loss: nan\n",
      "Epoch [394/10000] Train Loss: 0.004214 Val Loss: nan\n",
      "Epoch [395/10000] Train Loss: 0.004183 Val Loss: nan\n",
      "Epoch [396/10000] Train Loss: 0.004534 Val Loss: nan\n",
      "Epoch [397/10000] Train Loss: 0.004681 Val Loss: nan\n",
      "Epoch [398/10000] Train Loss: 0.004844 Val Loss: nan\n",
      "Epoch [399/10000] Train Loss: 0.004880 Val Loss: nan\n",
      "Epoch [400/10000] Train Loss: 0.004836 Val Loss: nan\n",
      "Epoch [401/10000] Train Loss: 0.004663 Val Loss: nan\n",
      "Epoch [402/10000] Train Loss: 0.004934 Val Loss: nan\n",
      "Epoch [403/10000] Train Loss: 0.004143 Val Loss: nan\n",
      "Epoch [404/10000] Train Loss: 0.004716 Val Loss: nan\n",
      "Epoch [405/10000] Train Loss: 0.004058 Val Loss: nan\n",
      "Epoch [406/10000] Train Loss: 0.004392 Val Loss: nan\n",
      "Epoch [407/10000] Train Loss: 0.004077 Val Loss: nan\n",
      "Epoch [408/10000] Train Loss: 0.003950 Val Loss: nan\n",
      "Epoch [409/10000] Train Loss: 0.004185 Val Loss: nan\n",
      "Epoch [410/10000] Train Loss: 0.004073 Val Loss: nan\n",
      "Epoch [411/10000] Train Loss: 0.004059 Val Loss: nan\n",
      "Epoch [412/10000] Train Loss: 0.004100 Val Loss: nan\n",
      "Epoch [413/10000] Train Loss: 0.005187 Val Loss: nan\n",
      "Epoch [414/10000] Train Loss: 0.004529 Val Loss: nan\n",
      "Epoch [415/10000] Train Loss: 0.004086 Val Loss: nan\n",
      "Epoch [416/10000] Train Loss: 0.005096 Val Loss: nan\n",
      "Epoch [417/10000] Train Loss: 0.004754 Val Loss: nan\n",
      "Epoch [418/10000] Train Loss: 0.004485 Val Loss: nan\n",
      "Epoch [419/10000] Train Loss: 0.004142 Val Loss: nan\n",
      "Epoch [420/10000] Train Loss: 0.003836 Val Loss: nan\n",
      "Epoch [421/10000] Train Loss: 0.004096 Val Loss: nan\n",
      "Epoch [422/10000] Train Loss: 0.004121 Val Loss: nan\n",
      "Epoch [423/10000] Train Loss: 0.004688 Val Loss: nan\n",
      "Epoch [424/10000] Train Loss: 0.004339 Val Loss: nan\n",
      "Epoch [425/10000] Train Loss: 0.003985 Val Loss: nan\n",
      "Epoch [426/10000] Train Loss: 0.003944 Val Loss: nan\n",
      "Epoch [427/10000] Train Loss: 0.003934 Val Loss: nan\n",
      "Epoch [428/10000] Train Loss: 0.003742 Val Loss: nan\n",
      "Epoch [429/10000] Train Loss: 0.003996 Val Loss: nan\n",
      "Epoch [430/10000] Train Loss: 0.003964 Val Loss: nan\n",
      "Epoch [431/10000] Train Loss: 0.003783 Val Loss: nan\n",
      "Epoch [432/10000] Train Loss: 0.003963 Val Loss: nan\n",
      "Epoch [433/10000] Train Loss: 0.003813 Val Loss: nan\n",
      "Epoch [434/10000] Train Loss: 0.003717 Val Loss: nan\n",
      "Epoch [435/10000] Train Loss: 0.003986 Val Loss: nan\n",
      "Epoch [436/10000] Train Loss: 0.004218 Val Loss: nan\n",
      "Epoch [437/10000] Train Loss: 0.005136 Val Loss: nan\n",
      "Epoch [438/10000] Train Loss: 0.005236 Val Loss: nan\n",
      "Epoch [439/10000] Train Loss: 0.003941 Val Loss: nan\n",
      "Epoch [440/10000] Train Loss: 0.003707 Val Loss: nan\n",
      "Epoch [441/10000] Train Loss: 0.003933 Val Loss: nan\n",
      "Epoch [442/10000] Train Loss: 0.003822 Val Loss: nan\n",
      "Epoch [443/10000] Train Loss: 0.004266 Val Loss: nan\n",
      "Epoch [444/10000] Train Loss: 0.003442 Val Loss: nan\n",
      "Epoch [445/10000] Train Loss: 0.003690 Val Loss: nan\n",
      "Epoch [446/10000] Train Loss: 0.003579 Val Loss: nan\n",
      "Epoch [447/10000] Train Loss: 0.003669 Val Loss: nan\n",
      "Epoch [448/10000] Train Loss: 0.004252 Val Loss: nan\n",
      "Epoch [449/10000] Train Loss: 0.003723 Val Loss: nan\n",
      "Epoch [450/10000] Train Loss: 0.003689 Val Loss: nan\n",
      "Epoch [451/10000] Train Loss: 0.003685 Val Loss: nan\n",
      "Epoch [452/10000] Train Loss: 0.003811 Val Loss: nan\n",
      "Epoch [453/10000] Train Loss: 0.003987 Val Loss: nan\n",
      "Epoch [454/10000] Train Loss: 0.003725 Val Loss: nan\n",
      "Epoch [455/10000] Train Loss: 0.003952 Val Loss: nan\n",
      "Epoch [456/10000] Train Loss: 0.003758 Val Loss: nan\n",
      "Epoch [457/10000] Train Loss: 0.003548 Val Loss: nan\n",
      "Epoch [458/10000] Train Loss: 0.003693 Val Loss: nan\n",
      "Epoch [459/10000] Train Loss: 0.003760 Val Loss: nan\n",
      "Epoch [460/10000] Train Loss: 0.003970 Val Loss: nan\n",
      "Epoch [461/10000] Train Loss: 0.003600 Val Loss: nan\n",
      "Epoch [462/10000] Train Loss: 0.003490 Val Loss: nan\n",
      "Epoch [463/10000] Train Loss: 0.003779 Val Loss: nan\n",
      "Epoch [464/10000] Train Loss: 0.003799 Val Loss: nan\n",
      "Epoch [465/10000] Train Loss: 0.003602 Val Loss: nan\n",
      "Epoch [466/10000] Train Loss: 0.003730 Val Loss: nan\n",
      "Epoch [467/10000] Train Loss: 0.003855 Val Loss: nan\n",
      "Epoch [468/10000] Train Loss: 0.003749 Val Loss: nan\n",
      "Epoch [469/10000] Train Loss: 0.003461 Val Loss: nan\n",
      "Epoch [470/10000] Train Loss: 0.003812 Val Loss: nan\n",
      "Epoch [471/10000] Train Loss: 0.003976 Val Loss: nan\n",
      "Epoch [472/10000] Train Loss: 0.003570 Val Loss: nan\n",
      "Epoch [473/10000] Train Loss: 0.003145 Val Loss: nan\n",
      "Epoch [474/10000] Train Loss: 0.003211 Val Loss: nan\n",
      "Epoch [475/10000] Train Loss: 0.003634 Val Loss: nan\n",
      "Epoch [476/10000] Train Loss: 0.003400 Val Loss: nan\n",
      "Epoch [477/10000] Train Loss: 0.003284 Val Loss: nan\n",
      "Epoch [478/10000] Train Loss: 0.003153 Val Loss: nan\n",
      "Epoch [479/10000] Train Loss: 0.003212 Val Loss: nan\n",
      "Epoch [480/10000] Train Loss: 0.003713 Val Loss: nan\n",
      "Epoch [481/10000] Train Loss: 0.003767 Val Loss: nan\n",
      "Epoch [482/10000] Train Loss: 0.003555 Val Loss: nan\n",
      "Epoch [483/10000] Train Loss: 0.003570 Val Loss: nan\n",
      "Epoch [484/10000] Train Loss: 0.003863 Val Loss: nan\n",
      "Epoch [485/10000] Train Loss: 0.003571 Val Loss: nan\n",
      "Epoch [486/10000] Train Loss: 0.003359 Val Loss: nan\n",
      "Epoch [487/10000] Train Loss: 0.003175 Val Loss: nan\n",
      "Epoch [488/10000] Train Loss: 0.003105 Val Loss: nan\n",
      "Epoch [489/10000] Train Loss: 0.003209 Val Loss: nan\n",
      "Epoch [490/10000] Train Loss: 0.003188 Val Loss: nan\n",
      "Epoch [491/10000] Train Loss: 0.003205 Val Loss: nan\n",
      "Epoch [492/10000] Train Loss: 0.003074 Val Loss: nan\n",
      "Epoch [493/10000] Train Loss: 0.003290 Val Loss: nan\n",
      "Epoch [494/10000] Train Loss: 0.003665 Val Loss: nan\n",
      "Epoch [495/10000] Train Loss: 0.003323 Val Loss: nan\n",
      "Epoch [496/10000] Train Loss: 0.004231 Val Loss: nan\n",
      "Epoch [497/10000] Train Loss: 0.004708 Val Loss: nan\n",
      "Epoch [498/10000] Train Loss: 0.003368 Val Loss: nan\n",
      "Epoch [499/10000] Train Loss: 0.003175 Val Loss: nan\n",
      "Epoch [500/10000] Train Loss: 0.003183 Val Loss: nan\n",
      "Epoch [501/10000] Train Loss: 0.003294 Val Loss: nan\n",
      "Epoch [502/10000] Train Loss: 0.002818 Val Loss: nan\n",
      "Epoch [503/10000] Train Loss: 0.002922 Val Loss: nan\n",
      "Epoch [504/10000] Train Loss: 0.002876 Val Loss: nan\n",
      "Epoch [505/10000] Train Loss: 0.002935 Val Loss: nan\n",
      "Epoch [506/10000] Train Loss: 0.003244 Val Loss: nan\n",
      "Epoch [507/10000] Train Loss: 0.003563 Val Loss: nan\n",
      "Epoch [508/10000] Train Loss: 0.003395 Val Loss: nan\n",
      "Epoch [509/10000] Train Loss: 0.002959 Val Loss: nan\n",
      "Epoch [510/10000] Train Loss: 0.003210 Val Loss: nan\n",
      "Epoch [511/10000] Train Loss: 0.003062 Val Loss: nan\n",
      "Epoch [512/10000] Train Loss: 0.003209 Val Loss: nan\n",
      "Epoch [513/10000] Train Loss: 0.003476 Val Loss: nan\n",
      "Epoch [514/10000] Train Loss: 0.003155 Val Loss: nan\n",
      "Epoch [515/10000] Train Loss: 0.003262 Val Loss: nan\n",
      "Epoch [516/10000] Train Loss: 0.003243 Val Loss: nan\n",
      "Epoch [517/10000] Train Loss: 0.003011 Val Loss: nan\n",
      "Epoch [518/10000] Train Loss: 0.003081 Val Loss: nan\n",
      "Epoch [519/10000] Train Loss: 0.002975 Val Loss: nan\n",
      "Epoch [520/10000] Train Loss: 0.002910 Val Loss: nan\n",
      "Epoch [521/10000] Train Loss: 0.003084 Val Loss: nan\n",
      "Epoch [522/10000] Train Loss: 0.003469 Val Loss: nan\n",
      "Epoch [523/10000] Train Loss: 0.003017 Val Loss: nan\n",
      "Epoch [524/10000] Train Loss: 0.003480 Val Loss: nan\n",
      "Epoch [525/10000] Train Loss: 0.003090 Val Loss: nan\n",
      "Epoch [526/10000] Train Loss: 0.003085 Val Loss: nan\n",
      "Epoch [527/10000] Train Loss: 0.003031 Val Loss: nan\n",
      "Epoch [528/10000] Train Loss: 0.003142 Val Loss: nan\n",
      "Epoch [529/10000] Train Loss: 0.002837 Val Loss: nan\n",
      "Epoch [530/10000] Train Loss: 0.002914 Val Loss: nan\n",
      "Epoch [531/10000] Train Loss: 0.003281 Val Loss: nan\n",
      "Epoch [532/10000] Train Loss: 0.003152 Val Loss: nan\n",
      "Epoch [533/10000] Train Loss: 0.002725 Val Loss: nan\n",
      "Epoch [534/10000] Train Loss: 0.002893 Val Loss: nan\n",
      "Epoch [535/10000] Train Loss: 0.002792 Val Loss: nan\n",
      "Epoch [536/10000] Train Loss: 0.003051 Val Loss: nan\n",
      "Epoch [537/10000] Train Loss: 0.002749 Val Loss: nan\n",
      "Epoch [538/10000] Train Loss: 0.002854 Val Loss: nan\n",
      "Epoch [539/10000] Train Loss: 0.003135 Val Loss: nan\n",
      "Epoch [540/10000] Train Loss: 0.002904 Val Loss: nan\n",
      "Epoch [541/10000] Train Loss: 0.002688 Val Loss: nan\n",
      "Epoch [542/10000] Train Loss: 0.002729 Val Loss: nan\n",
      "Epoch [543/10000] Train Loss: 0.002804 Val Loss: nan\n",
      "Epoch [544/10000] Train Loss: 0.002727 Val Loss: nan\n",
      "Epoch [545/10000] Train Loss: 0.002669 Val Loss: nan\n",
      "Epoch [546/10000] Train Loss: 0.002521 Val Loss: nan\n",
      "Epoch [547/10000] Train Loss: 0.002459 Val Loss: nan\n",
      "Epoch [548/10000] Train Loss: 0.002663 Val Loss: nan\n",
      "Epoch [549/10000] Train Loss: 0.002543 Val Loss: nan\n",
      "Epoch [550/10000] Train Loss: 0.002835 Val Loss: nan\n",
      "Epoch [551/10000] Train Loss: 0.002780 Val Loss: nan\n",
      "Epoch [552/10000] Train Loss: 0.003286 Val Loss: nan\n",
      "Epoch [553/10000] Train Loss: 0.002985 Val Loss: nan\n",
      "Epoch [554/10000] Train Loss: 0.003056 Val Loss: nan\n",
      "Epoch [555/10000] Train Loss: 0.002657 Val Loss: nan\n",
      "Epoch [556/10000] Train Loss: 0.002408 Val Loss: nan\n",
      "Epoch [557/10000] Train Loss: 0.002732 Val Loss: nan\n",
      "Epoch [558/10000] Train Loss: 0.002780 Val Loss: nan\n",
      "Epoch [559/10000] Train Loss: 0.003061 Val Loss: nan\n",
      "Epoch [560/10000] Train Loss: 0.002784 Val Loss: nan\n",
      "Epoch [561/10000] Train Loss: 0.002673 Val Loss: nan\n",
      "Epoch [562/10000] Train Loss: 0.003205 Val Loss: nan\n",
      "Epoch [563/10000] Train Loss: 0.003102 Val Loss: nan\n",
      "Epoch [564/10000] Train Loss: 0.002940 Val Loss: nan\n",
      "Epoch [565/10000] Train Loss: 0.002435 Val Loss: nan\n",
      "Epoch [566/10000] Train Loss: 0.002877 Val Loss: nan\n",
      "Epoch [567/10000] Train Loss: 0.002830 Val Loss: nan\n",
      "Epoch [568/10000] Train Loss: 0.002539 Val Loss: nan\n",
      "Epoch [569/10000] Train Loss: 0.002663 Val Loss: nan\n",
      "Epoch [570/10000] Train Loss: 0.002677 Val Loss: nan\n",
      "Epoch [571/10000] Train Loss: 0.002507 Val Loss: nan\n",
      "Epoch [572/10000] Train Loss: 0.003169 Val Loss: nan\n",
      "Epoch [573/10000] Train Loss: 0.002758 Val Loss: nan\n",
      "Epoch [574/10000] Train Loss: 0.002626 Val Loss: nan\n",
      "Epoch [575/10000] Train Loss: 0.002610 Val Loss: nan\n",
      "Epoch [576/10000] Train Loss: 0.002348 Val Loss: nan\n",
      "Epoch [577/10000] Train Loss: 0.002660 Val Loss: nan\n",
      "Epoch [578/10000] Train Loss: 0.002750 Val Loss: nan\n",
      "Epoch [579/10000] Train Loss: 0.002624 Val Loss: nan\n",
      "Epoch [580/10000] Train Loss: 0.002310 Val Loss: nan\n",
      "Epoch [581/10000] Train Loss: 0.003068 Val Loss: nan\n",
      "Epoch [582/10000] Train Loss: 0.003176 Val Loss: nan\n",
      "Epoch [583/10000] Train Loss: 0.003057 Val Loss: nan\n",
      "Epoch [584/10000] Train Loss: 0.002774 Val Loss: nan\n",
      "Epoch [585/10000] Train Loss: 0.002696 Val Loss: nan\n",
      "Epoch [586/10000] Train Loss: 0.002902 Val Loss: nan\n",
      "Epoch [587/10000] Train Loss: 0.002605 Val Loss: nan\n",
      "Epoch [588/10000] Train Loss: 0.002834 Val Loss: nan\n",
      "Epoch [589/10000] Train Loss: 0.002763 Val Loss: nan\n",
      "Epoch [590/10000] Train Loss: 0.002436 Val Loss: nan\n",
      "Epoch [591/10000] Train Loss: 0.002540 Val Loss: nan\n",
      "Epoch [592/10000] Train Loss: 0.002595 Val Loss: nan\n",
      "Epoch [593/10000] Train Loss: 0.002447 Val Loss: nan\n",
      "Epoch [594/10000] Train Loss: 0.002444 Val Loss: nan\n",
      "Epoch [595/10000] Train Loss: 0.002541 Val Loss: nan\n",
      "Epoch [596/10000] Train Loss: 0.002343 Val Loss: nan\n",
      "Epoch [597/10000] Train Loss: 0.002530 Val Loss: nan\n",
      "Epoch [598/10000] Train Loss: 0.002908 Val Loss: nan\n",
      "Epoch [599/10000] Train Loss: 0.002532 Val Loss: nan\n",
      "Epoch [600/10000] Train Loss: 0.002505 Val Loss: nan\n",
      "Epoch [601/10000] Train Loss: 0.002485 Val Loss: nan\n",
      "Epoch [602/10000] Train Loss: 0.002560 Val Loss: nan\n",
      "Epoch [603/10000] Train Loss: 0.002464 Val Loss: nan\n",
      "Epoch [604/10000] Train Loss: 0.002286 Val Loss: nan\n",
      "Epoch [605/10000] Train Loss: 0.002346 Val Loss: nan\n",
      "Epoch [606/10000] Train Loss: 0.002255 Val Loss: nan\n",
      "Epoch [607/10000] Train Loss: 0.002294 Val Loss: nan\n",
      "Epoch [608/10000] Train Loss: 0.002233 Val Loss: nan\n",
      "Epoch [609/10000] Train Loss: 0.002461 Val Loss: nan\n",
      "Epoch [610/10000] Train Loss: 0.002429 Val Loss: nan\n",
      "Epoch [611/10000] Train Loss: 0.002202 Val Loss: nan\n",
      "Epoch [612/10000] Train Loss: 0.002377 Val Loss: nan\n",
      "Epoch [613/10000] Train Loss: 0.002167 Val Loss: nan\n",
      "Epoch [614/10000] Train Loss: 0.002303 Val Loss: nan\n",
      "Epoch [615/10000] Train Loss: 0.002396 Val Loss: nan\n",
      "Epoch [616/10000] Train Loss: 0.002149 Val Loss: nan\n",
      "Epoch [617/10000] Train Loss: 0.002893 Val Loss: nan\n",
      "Epoch [618/10000] Train Loss: 0.002811 Val Loss: nan\n",
      "Epoch [619/10000] Train Loss: 0.002527 Val Loss: nan\n",
      "Epoch [620/10000] Train Loss: 0.002422 Val Loss: nan\n",
      "Epoch [621/10000] Train Loss: 0.002726 Val Loss: nan\n",
      "Epoch [622/10000] Train Loss: 0.002646 Val Loss: nan\n",
      "Epoch [623/10000] Train Loss: 0.002342 Val Loss: nan\n",
      "Epoch [624/10000] Train Loss: 0.002247 Val Loss: nan\n",
      "Epoch [625/10000] Train Loss: 0.002213 Val Loss: nan\n",
      "Epoch [626/10000] Train Loss: 0.002383 Val Loss: nan\n",
      "Epoch [627/10000] Train Loss: 0.002194 Val Loss: nan\n",
      "Epoch [628/10000] Train Loss: 0.002275 Val Loss: nan\n",
      "Epoch [629/10000] Train Loss: 0.002216 Val Loss: nan\n",
      "Epoch [630/10000] Train Loss: 0.002569 Val Loss: nan\n",
      "Epoch [631/10000] Train Loss: 0.002162 Val Loss: nan\n",
      "Epoch [632/10000] Train Loss: 0.002313 Val Loss: nan\n",
      "Epoch [633/10000] Train Loss: 0.002125 Val Loss: nan\n",
      "Epoch [634/10000] Train Loss: 0.002073 Val Loss: nan\n",
      "Epoch [635/10000] Train Loss: 0.002132 Val Loss: nan\n",
      "Epoch [636/10000] Train Loss: 0.002218 Val Loss: nan\n",
      "Epoch [637/10000] Train Loss: 0.002065 Val Loss: nan\n",
      "Epoch [638/10000] Train Loss: 0.002491 Val Loss: nan\n",
      "Epoch [639/10000] Train Loss: 0.002122 Val Loss: nan\n",
      "Epoch [640/10000] Train Loss: 0.002160 Val Loss: nan\n",
      "Epoch [641/10000] Train Loss: 0.001920 Val Loss: nan\n",
      "Epoch [642/10000] Train Loss: 0.002206 Val Loss: nan\n",
      "Epoch [643/10000] Train Loss: 0.002049 Val Loss: nan\n",
      "Epoch [644/10000] Train Loss: 0.002230 Val Loss: nan\n",
      "Epoch [645/10000] Train Loss: 0.002249 Val Loss: nan\n",
      "Epoch [646/10000] Train Loss: 0.002551 Val Loss: nan\n",
      "Epoch [647/10000] Train Loss: 0.002477 Val Loss: nan\n",
      "Epoch [648/10000] Train Loss: 0.002621 Val Loss: nan\n",
      "Epoch [649/10000] Train Loss: 0.002437 Val Loss: nan\n",
      "Epoch [650/10000] Train Loss: 0.002255 Val Loss: nan\n",
      "Epoch [651/10000] Train Loss: 0.002072 Val Loss: nan\n",
      "Epoch [652/10000] Train Loss: 0.002064 Val Loss: nan\n",
      "Epoch [653/10000] Train Loss: 0.002044 Val Loss: nan\n",
      "Epoch [654/10000] Train Loss: 0.001929 Val Loss: nan\n",
      "Epoch [655/10000] Train Loss: 0.002191 Val Loss: nan\n",
      "Epoch [656/10000] Train Loss: 0.002450 Val Loss: nan\n",
      "Epoch [657/10000] Train Loss: 0.002002 Val Loss: nan\n",
      "Epoch [658/10000] Train Loss: 0.002031 Val Loss: nan\n",
      "Epoch [659/10000] Train Loss: 0.002026 Val Loss: nan\n",
      "Epoch [660/10000] Train Loss: 0.001856 Val Loss: nan\n",
      "Epoch [661/10000] Train Loss: 0.002062 Val Loss: nan\n",
      "Epoch [662/10000] Train Loss: 0.002097 Val Loss: nan\n",
      "Epoch [663/10000] Train Loss: 0.002064 Val Loss: nan\n",
      "Epoch [664/10000] Train Loss: 0.002081 Val Loss: nan\n",
      "Epoch [665/10000] Train Loss: 0.002298 Val Loss: nan\n",
      "Epoch [666/10000] Train Loss: 0.002349 Val Loss: nan\n",
      "Epoch [667/10000] Train Loss: 0.002724 Val Loss: nan\n",
      "Epoch [668/10000] Train Loss: 0.002203 Val Loss: nan\n",
      "Epoch [669/10000] Train Loss: 0.002254 Val Loss: nan\n",
      "Epoch [670/10000] Train Loss: 0.002059 Val Loss: nan\n",
      "Epoch [671/10000] Train Loss: 0.002028 Val Loss: nan\n",
      "Epoch [672/10000] Train Loss: 0.002342 Val Loss: nan\n",
      "Epoch [673/10000] Train Loss: 0.002551 Val Loss: nan\n",
      "Epoch [674/10000] Train Loss: 0.002115 Val Loss: nan\n",
      "Epoch [675/10000] Train Loss: 0.001882 Val Loss: nan\n",
      "Epoch [676/10000] Train Loss: 0.001817 Val Loss: nan\n",
      "Epoch [677/10000] Train Loss: 0.001981 Val Loss: nan\n",
      "Epoch [678/10000] Train Loss: 0.002314 Val Loss: nan\n",
      "Epoch [679/10000] Train Loss: 0.002328 Val Loss: nan\n",
      "Epoch [680/10000] Train Loss: 0.002108 Val Loss: nan\n",
      "Epoch [681/10000] Train Loss: 0.002202 Val Loss: nan\n",
      "Epoch [682/10000] Train Loss: 0.001987 Val Loss: nan\n",
      "Epoch [683/10000] Train Loss: 0.001994 Val Loss: nan\n",
      "Epoch [684/10000] Train Loss: 0.001886 Val Loss: nan\n",
      "Epoch [685/10000] Train Loss: 0.001909 Val Loss: nan\n",
      "Epoch [686/10000] Train Loss: 0.001672 Val Loss: nan\n",
      "Epoch [687/10000] Train Loss: 0.001844 Val Loss: nan\n",
      "Epoch [688/10000] Train Loss: 0.001866 Val Loss: nan\n",
      "Epoch [689/10000] Train Loss: 0.001796 Val Loss: nan\n",
      "Epoch [690/10000] Train Loss: 0.002148 Val Loss: nan\n",
      "Epoch [691/10000] Train Loss: 0.002091 Val Loss: nan\n",
      "Epoch [692/10000] Train Loss: 0.001958 Val Loss: nan\n",
      "Epoch [693/10000] Train Loss: 0.002041 Val Loss: nan\n",
      "Epoch [694/10000] Train Loss: 0.002097 Val Loss: nan\n",
      "Epoch [695/10000] Train Loss: 0.001972 Val Loss: nan\n",
      "Epoch [696/10000] Train Loss: 0.002176 Val Loss: nan\n",
      "Epoch [697/10000] Train Loss: 0.002072 Val Loss: nan\n",
      "Epoch [698/10000] Train Loss: 0.002021 Val Loss: nan\n",
      "Epoch [699/10000] Train Loss: 0.002106 Val Loss: nan\n",
      "Epoch [700/10000] Train Loss: 0.002268 Val Loss: nan\n",
      "Epoch [701/10000] Train Loss: 0.001909 Val Loss: nan\n",
      "Epoch [702/10000] Train Loss: 0.001740 Val Loss: nan\n",
      "Epoch [703/10000] Train Loss: 0.002082 Val Loss: nan\n",
      "Epoch [704/10000] Train Loss: 0.001967 Val Loss: nan\n",
      "Epoch [705/10000] Train Loss: 0.001765 Val Loss: nan\n",
      "Epoch [706/10000] Train Loss: 0.001746 Val Loss: nan\n",
      "Epoch [707/10000] Train Loss: 0.001863 Val Loss: nan\n",
      "Epoch [708/10000] Train Loss: 0.002146 Val Loss: nan\n",
      "Epoch [709/10000] Train Loss: 0.002098 Val Loss: nan\n",
      "Epoch [710/10000] Train Loss: 0.001790 Val Loss: nan\n",
      "Epoch [711/10000] Train Loss: 0.002083 Val Loss: nan\n",
      "Epoch [712/10000] Train Loss: 0.002268 Val Loss: nan\n",
      "Epoch [713/10000] Train Loss: 0.002467 Val Loss: nan\n",
      "Epoch [714/10000] Train Loss: 0.001844 Val Loss: nan\n",
      "Epoch [715/10000] Train Loss: 0.001652 Val Loss: nan\n",
      "Epoch [716/10000] Train Loss: 0.001605 Val Loss: nan\n",
      "Epoch [717/10000] Train Loss: 0.001646 Val Loss: nan\n",
      "Epoch [718/10000] Train Loss: 0.001693 Val Loss: nan\n",
      "Epoch [719/10000] Train Loss: 0.001580 Val Loss: nan\n",
      "Epoch [720/10000] Train Loss: 0.001934 Val Loss: nan\n",
      "Epoch [721/10000] Train Loss: 0.002005 Val Loss: nan\n",
      "Epoch [722/10000] Train Loss: 0.001888 Val Loss: nan\n",
      "Epoch [723/10000] Train Loss: 0.001875 Val Loss: nan\n",
      "Epoch [724/10000] Train Loss: 0.002091 Val Loss: nan\n",
      "Epoch [725/10000] Train Loss: 0.001800 Val Loss: nan\n",
      "Epoch [726/10000] Train Loss: 0.001797 Val Loss: nan\n",
      "Epoch [727/10000] Train Loss: 0.001596 Val Loss: nan\n",
      "Epoch [728/10000] Train Loss: 0.001761 Val Loss: nan\n",
      "Epoch [729/10000] Train Loss: 0.001701 Val Loss: nan\n",
      "Epoch [730/10000] Train Loss: 0.001846 Val Loss: nan\n",
      "Epoch [731/10000] Train Loss: 0.001989 Val Loss: nan\n",
      "Epoch [732/10000] Train Loss: 0.001996 Val Loss: nan\n",
      "Epoch [733/10000] Train Loss: 0.001437 Val Loss: nan\n",
      "Epoch [734/10000] Train Loss: 0.001549 Val Loss: nan\n",
      "Epoch [735/10000] Train Loss: 0.001505 Val Loss: nan\n",
      "Epoch [736/10000] Train Loss: 0.001745 Val Loss: nan\n",
      "Epoch [737/10000] Train Loss: 0.001542 Val Loss: nan\n",
      "Epoch [738/10000] Train Loss: 0.001758 Val Loss: nan\n",
      "Epoch [739/10000] Train Loss: 0.001965 Val Loss: nan\n",
      "Epoch [740/10000] Train Loss: 0.001761 Val Loss: nan\n",
      "Epoch [741/10000] Train Loss: 0.001551 Val Loss: nan\n",
      "Epoch [742/10000] Train Loss: 0.001467 Val Loss: nan\n",
      "Epoch [743/10000] Train Loss: 0.001489 Val Loss: nan\n",
      "Epoch [744/10000] Train Loss: 0.001505 Val Loss: nan\n",
      "Epoch [745/10000] Train Loss: 0.001619 Val Loss: nan\n",
      "Epoch [746/10000] Train Loss: 0.001474 Val Loss: nan\n",
      "Epoch [747/10000] Train Loss: 0.001466 Val Loss: nan\n",
      "Epoch [748/10000] Train Loss: 0.001624 Val Loss: nan\n",
      "Epoch [749/10000] Train Loss: 0.001473 Val Loss: nan\n",
      "Epoch [750/10000] Train Loss: 0.001585 Val Loss: nan\n",
      "Epoch [751/10000] Train Loss: 0.001567 Val Loss: nan\n",
      "Epoch [752/10000] Train Loss: 0.001406 Val Loss: nan\n",
      "Epoch [753/10000] Train Loss: 0.001356 Val Loss: nan\n",
      "Epoch [754/10000] Train Loss: 0.001415 Val Loss: nan\n",
      "Epoch [755/10000] Train Loss: 0.001340 Val Loss: nan\n",
      "Epoch [756/10000] Train Loss: 0.001446 Val Loss: nan\n",
      "Epoch [757/10000] Train Loss: 0.001452 Val Loss: nan\n",
      "Epoch [758/10000] Train Loss: 0.001662 Val Loss: nan\n",
      "Epoch [759/10000] Train Loss: 0.001731 Val Loss: nan\n",
      "Epoch [760/10000] Train Loss: 0.001366 Val Loss: nan\n",
      "Epoch [761/10000] Train Loss: 0.001468 Val Loss: nan\n",
      "Epoch [762/10000] Train Loss: 0.001363 Val Loss: nan\n",
      "Epoch [763/10000] Train Loss: 0.001426 Val Loss: nan\n",
      "Epoch [764/10000] Train Loss: 0.001578 Val Loss: nan\n",
      "Epoch [765/10000] Train Loss: 0.001685 Val Loss: nan\n",
      "Epoch [766/10000] Train Loss: 0.001791 Val Loss: nan\n",
      "Epoch [767/10000] Train Loss: 0.001676 Val Loss: nan\n",
      "Epoch [768/10000] Train Loss: 0.001616 Val Loss: nan\n",
      "Epoch [769/10000] Train Loss: 0.001325 Val Loss: nan\n",
      "Epoch [770/10000] Train Loss: 0.001340 Val Loss: nan\n",
      "Epoch [771/10000] Train Loss: 0.001235 Val Loss: nan\n",
      "Epoch [772/10000] Train Loss: 0.001313 Val Loss: nan\n",
      "Epoch [773/10000] Train Loss: 0.001284 Val Loss: nan\n",
      "Epoch [774/10000] Train Loss: 0.001423 Val Loss: nan\n",
      "Epoch [775/10000] Train Loss: 0.001498 Val Loss: nan\n",
      "Epoch [776/10000] Train Loss: 0.001615 Val Loss: nan\n",
      "Epoch [777/10000] Train Loss: 0.001649 Val Loss: nan\n",
      "Epoch [778/10000] Train Loss: 0.001901 Val Loss: nan\n",
      "Epoch [779/10000] Train Loss: 0.001534 Val Loss: nan\n",
      "Epoch [780/10000] Train Loss: 0.001628 Val Loss: nan\n",
      "Epoch [781/10000] Train Loss: 0.001665 Val Loss: nan\n",
      "Epoch [782/10000] Train Loss: 0.001360 Val Loss: nan\n",
      "Epoch [783/10000] Train Loss: 0.001401 Val Loss: nan\n",
      "Epoch [784/10000] Train Loss: 0.001384 Val Loss: nan\n",
      "Epoch [785/10000] Train Loss: 0.001227 Val Loss: nan\n",
      "Epoch [786/10000] Train Loss: 0.001238 Val Loss: nan\n",
      "Epoch [787/10000] Train Loss: 0.001266 Val Loss: nan\n",
      "Epoch [788/10000] Train Loss: 0.001380 Val Loss: nan\n",
      "Epoch [789/10000] Train Loss: 0.001390 Val Loss: nan\n",
      "Epoch [790/10000] Train Loss: 0.001396 Val Loss: nan\n",
      "Epoch [791/10000] Train Loss: 0.001425 Val Loss: nan\n",
      "Epoch [792/10000] Train Loss: 0.001282 Val Loss: nan\n",
      "Epoch [793/10000] Train Loss: 0.001394 Val Loss: nan\n",
      "Epoch [794/10000] Train Loss: 0.001198 Val Loss: nan\n",
      "Epoch [795/10000] Train Loss: 0.001407 Val Loss: nan\n",
      "Epoch [796/10000] Train Loss: 0.001439 Val Loss: nan\n",
      "Epoch [797/10000] Train Loss: 0.001624 Val Loss: nan\n",
      "Epoch [798/10000] Train Loss: 0.001612 Val Loss: nan\n",
      "Epoch [799/10000] Train Loss: 0.001325 Val Loss: nan\n",
      "Epoch [800/10000] Train Loss: 0.001317 Val Loss: nan\n",
      "Epoch [801/10000] Train Loss: 0.001197 Val Loss: nan\n",
      "Epoch [802/10000] Train Loss: 0.001174 Val Loss: nan\n",
      "Epoch [803/10000] Train Loss: 0.001340 Val Loss: nan\n",
      "Epoch [804/10000] Train Loss: 0.001621 Val Loss: nan\n",
      "Epoch [805/10000] Train Loss: 0.001595 Val Loss: nan\n",
      "Epoch [806/10000] Train Loss: 0.001591 Val Loss: nan\n",
      "Epoch [807/10000] Train Loss: 0.001323 Val Loss: nan\n",
      "Epoch [808/10000] Train Loss: 0.001266 Val Loss: nan\n",
      "Epoch [809/10000] Train Loss: 0.001255 Val Loss: nan\n",
      "Epoch [810/10000] Train Loss: 0.001288 Val Loss: nan\n",
      "Epoch [811/10000] Train Loss: 0.001345 Val Loss: nan\n",
      "Epoch [812/10000] Train Loss: 0.001258 Val Loss: nan\n",
      "Epoch [813/10000] Train Loss: 0.001671 Val Loss: nan\n",
      "Epoch [814/10000] Train Loss: 0.001609 Val Loss: nan\n",
      "Epoch [815/10000] Train Loss: 0.001364 Val Loss: nan\n",
      "Epoch [816/10000] Train Loss: 0.001535 Val Loss: nan\n",
      "Epoch [817/10000] Train Loss: 0.001529 Val Loss: nan\n",
      "Epoch [818/10000] Train Loss: 0.001308 Val Loss: nan\n",
      "Epoch [819/10000] Train Loss: 0.001403 Val Loss: nan\n",
      "Epoch [820/10000] Train Loss: 0.001122 Val Loss: nan\n",
      "Epoch [821/10000] Train Loss: 0.001171 Val Loss: nan\n",
      "Epoch [822/10000] Train Loss: 0.001141 Val Loss: nan\n",
      "Epoch [823/10000] Train Loss: 0.001265 Val Loss: nan\n",
      "Epoch [824/10000] Train Loss: 0.001238 Val Loss: nan\n",
      "Epoch [825/10000] Train Loss: 0.001498 Val Loss: nan\n",
      "Epoch [826/10000] Train Loss: 0.001384 Val Loss: nan\n",
      "Epoch [827/10000] Train Loss: 0.001460 Val Loss: nan\n",
      "Epoch [828/10000] Train Loss: 0.001401 Val Loss: nan\n",
      "Epoch [829/10000] Train Loss: 0.001474 Val Loss: nan\n",
      "Epoch [830/10000] Train Loss: 0.001497 Val Loss: nan\n",
      "Epoch [831/10000] Train Loss: 0.001184 Val Loss: nan\n",
      "Epoch [832/10000] Train Loss: 0.001203 Val Loss: nan\n",
      "Epoch [833/10000] Train Loss: 0.001187 Val Loss: nan\n",
      "Epoch [834/10000] Train Loss: 0.001341 Val Loss: nan\n",
      "Epoch [835/10000] Train Loss: 0.001341 Val Loss: nan\n",
      "Epoch [836/10000] Train Loss: 0.001369 Val Loss: nan\n",
      "Epoch [837/10000] Train Loss: 0.001091 Val Loss: nan\n",
      "Epoch [838/10000] Train Loss: 0.001054 Val Loss: nan\n",
      "Epoch [839/10000] Train Loss: 0.001016 Val Loss: nan\n",
      "Epoch [840/10000] Train Loss: 0.001123 Val Loss: nan\n",
      "Epoch [841/10000] Train Loss: 0.001202 Val Loss: nan\n",
      "Epoch [842/10000] Train Loss: 0.001194 Val Loss: nan\n",
      "Epoch [843/10000] Train Loss: 0.001262 Val Loss: nan\n",
      "Epoch [844/10000] Train Loss: 0.001262 Val Loss: nan\n",
      "Epoch [845/10000] Train Loss: 0.001275 Val Loss: nan\n",
      "Epoch [846/10000] Train Loss: 0.001416 Val Loss: nan\n",
      "Epoch [847/10000] Train Loss: 0.001385 Val Loss: nan\n",
      "Epoch [848/10000] Train Loss: 0.001475 Val Loss: nan\n",
      "Epoch [849/10000] Train Loss: 0.001681 Val Loss: nan\n",
      "Epoch [850/10000] Train Loss: 0.001252 Val Loss: nan\n",
      "Epoch [851/10000] Train Loss: 0.001526 Val Loss: nan\n",
      "Epoch [852/10000] Train Loss: 0.001010 Val Loss: nan\n",
      "Epoch [853/10000] Train Loss: 0.001090 Val Loss: nan\n",
      "Epoch [854/10000] Train Loss: 0.001138 Val Loss: nan\n",
      "Epoch [855/10000] Train Loss: 0.000993 Val Loss: nan\n",
      "Epoch [856/10000] Train Loss: 0.001114 Val Loss: nan\n",
      "Epoch [857/10000] Train Loss: 0.001168 Val Loss: nan\n",
      "Epoch [858/10000] Train Loss: 0.001250 Val Loss: nan\n",
      "Epoch [859/10000] Train Loss: 0.001470 Val Loss: nan\n",
      "Epoch [860/10000] Train Loss: 0.001794 Val Loss: nan\n",
      "Epoch [861/10000] Train Loss: 0.001629 Val Loss: nan\n",
      "Epoch [862/10000] Train Loss: 0.001400 Val Loss: nan\n",
      "Epoch [863/10000] Train Loss: 0.001503 Val Loss: nan\n",
      "Epoch [864/10000] Train Loss: 0.001585 Val Loss: nan\n",
      "Epoch [865/10000] Train Loss: 0.001203 Val Loss: nan\n",
      "Epoch [866/10000] Train Loss: 0.001019 Val Loss: nan\n",
      "Epoch [867/10000] Train Loss: 0.000984 Val Loss: nan\n",
      "Epoch [868/10000] Train Loss: 0.000902 Val Loss: nan\n",
      "Epoch [869/10000] Train Loss: 0.001011 Val Loss: nan\n",
      "Epoch [870/10000] Train Loss: 0.001021 Val Loss: nan\n",
      "Epoch [871/10000] Train Loss: 0.001100 Val Loss: nan\n",
      "Epoch [872/10000] Train Loss: 0.001019 Val Loss: nan\n",
      "Epoch [873/10000] Train Loss: 0.000935 Val Loss: nan\n",
      "Epoch [874/10000] Train Loss: 0.000977 Val Loss: nan\n",
      "Epoch [875/10000] Train Loss: 0.000899 Val Loss: nan\n",
      "Epoch [876/10000] Train Loss: 0.000878 Val Loss: nan\n",
      "Epoch [877/10000] Train Loss: 0.000872 Val Loss: nan\n",
      "Epoch [878/10000] Train Loss: 0.000884 Val Loss: nan\n",
      "Epoch [879/10000] Train Loss: 0.000860 Val Loss: nan\n",
      "Epoch [880/10000] Train Loss: 0.000915 Val Loss: nan\n",
      "Epoch [881/10000] Train Loss: 0.001004 Val Loss: nan\n",
      "Epoch [882/10000] Train Loss: 0.000954 Val Loss: nan\n",
      "Epoch [883/10000] Train Loss: 0.001048 Val Loss: nan\n",
      "Epoch [884/10000] Train Loss: 0.000865 Val Loss: nan\n",
      "Epoch [885/10000] Train Loss: 0.000878 Val Loss: nan\n",
      "Epoch [886/10000] Train Loss: 0.001012 Val Loss: nan\n",
      "Epoch [887/10000] Train Loss: 0.000891 Val Loss: nan\n",
      "Epoch [888/10000] Train Loss: 0.000950 Val Loss: nan\n",
      "Epoch [889/10000] Train Loss: 0.000879 Val Loss: nan\n",
      "Epoch [890/10000] Train Loss: 0.000967 Val Loss: nan\n",
      "Epoch [891/10000] Train Loss: 0.000904 Val Loss: nan\n",
      "Epoch [892/10000] Train Loss: 0.000978 Val Loss: nan\n",
      "Epoch [893/10000] Train Loss: 0.001085 Val Loss: nan\n",
      "Epoch [894/10000] Train Loss: 0.001186 Val Loss: nan\n",
      "Epoch [895/10000] Train Loss: 0.001127 Val Loss: nan\n",
      "Epoch [896/10000] Train Loss: 0.001101 Val Loss: nan\n",
      "Epoch [897/10000] Train Loss: 0.001132 Val Loss: nan\n",
      "Epoch [898/10000] Train Loss: 0.001122 Val Loss: nan\n",
      "Epoch [899/10000] Train Loss: 0.000945 Val Loss: nan\n",
      "Epoch [900/10000] Train Loss: 0.000942 Val Loss: nan\n",
      "Epoch [901/10000] Train Loss: 0.001005 Val Loss: nan\n",
      "Epoch [902/10000] Train Loss: 0.000902 Val Loss: nan\n",
      "Epoch [903/10000] Train Loss: 0.000838 Val Loss: nan\n",
      "Epoch [904/10000] Train Loss: 0.000808 Val Loss: nan\n",
      "Epoch [905/10000] Train Loss: 0.000790 Val Loss: nan\n",
      "Epoch [906/10000] Train Loss: 0.000793 Val Loss: nan\n",
      "Epoch [907/10000] Train Loss: 0.000785 Val Loss: nan\n",
      "Epoch [908/10000] Train Loss: 0.000865 Val Loss: nan\n",
      "Epoch [909/10000] Train Loss: 0.000801 Val Loss: nan\n",
      "Epoch [910/10000] Train Loss: 0.000805 Val Loss: nan\n",
      "Epoch [911/10000] Train Loss: 0.000916 Val Loss: nan\n",
      "Epoch [912/10000] Train Loss: 0.000874 Val Loss: nan\n",
      "Epoch [913/10000] Train Loss: 0.000849 Val Loss: nan\n",
      "Epoch [914/10000] Train Loss: 0.000805 Val Loss: nan\n",
      "Epoch [915/10000] Train Loss: 0.000843 Val Loss: nan\n",
      "Epoch [916/10000] Train Loss: 0.000955 Val Loss: nan\n",
      "Epoch [917/10000] Train Loss: 0.000821 Val Loss: nan\n",
      "Epoch [918/10000] Train Loss: 0.000784 Val Loss: nan\n",
      "Epoch [919/10000] Train Loss: 0.000782 Val Loss: nan\n",
      "Epoch [920/10000] Train Loss: 0.000870 Val Loss: nan\n",
      "Epoch [921/10000] Train Loss: 0.000965 Val Loss: nan\n",
      "Epoch [922/10000] Train Loss: 0.000857 Val Loss: nan\n",
      "Epoch [923/10000] Train Loss: 0.000810 Val Loss: nan\n",
      "Epoch [924/10000] Train Loss: 0.000852 Val Loss: nan\n",
      "Epoch [925/10000] Train Loss: 0.000812 Val Loss: nan\n",
      "Epoch [926/10000] Train Loss: 0.000796 Val Loss: nan\n",
      "Epoch [927/10000] Train Loss: 0.000896 Val Loss: nan\n",
      "Epoch [928/10000] Train Loss: 0.000883 Val Loss: nan\n",
      "Epoch [929/10000] Train Loss: 0.000985 Val Loss: nan\n",
      "Epoch [930/10000] Train Loss: 0.000856 Val Loss: nan\n",
      "Epoch [931/10000] Train Loss: 0.000813 Val Loss: nan\n",
      "Epoch [932/10000] Train Loss: 0.000816 Val Loss: nan\n",
      "Epoch [933/10000] Train Loss: 0.000834 Val Loss: nan\n",
      "Epoch [934/10000] Train Loss: 0.000853 Val Loss: nan\n",
      "Epoch [935/10000] Train Loss: 0.000967 Val Loss: nan\n",
      "Epoch [936/10000] Train Loss: 0.001043 Val Loss: nan\n",
      "Epoch [937/10000] Train Loss: 0.000843 Val Loss: nan\n",
      "Epoch [938/10000] Train Loss: 0.000955 Val Loss: nan\n",
      "Epoch [939/10000] Train Loss: 0.001014 Val Loss: nan\n",
      "Epoch [940/10000] Train Loss: 0.000865 Val Loss: nan\n",
      "Epoch [941/10000] Train Loss: 0.000821 Val Loss: nan\n",
      "Epoch [942/10000] Train Loss: 0.000801 Val Loss: nan\n",
      "Epoch [943/10000] Train Loss: 0.000775 Val Loss: nan\n",
      "Epoch [944/10000] Train Loss: 0.000802 Val Loss: nan\n",
      "Epoch [945/10000] Train Loss: 0.000766 Val Loss: nan\n",
      "Epoch [946/10000] Train Loss: 0.000767 Val Loss: nan\n",
      "Epoch [947/10000] Train Loss: 0.000758 Val Loss: nan\n",
      "Epoch [948/10000] Train Loss: 0.000788 Val Loss: nan\n",
      "Epoch [949/10000] Train Loss: 0.000788 Val Loss: nan\n",
      "Epoch [950/10000] Train Loss: 0.000819 Val Loss: nan\n",
      "Epoch [951/10000] Train Loss: 0.000842 Val Loss: nan\n",
      "Epoch [952/10000] Train Loss: 0.000897 Val Loss: nan\n",
      "Epoch [953/10000] Train Loss: 0.000797 Val Loss: nan\n",
      "Epoch [954/10000] Train Loss: 0.000744 Val Loss: nan\n",
      "Epoch [955/10000] Train Loss: 0.000733 Val Loss: nan\n",
      "Epoch [956/10000] Train Loss: 0.000695 Val Loss: nan\n",
      "Epoch [957/10000] Train Loss: 0.000693 Val Loss: nan\n",
      "Epoch [958/10000] Train Loss: 0.000675 Val Loss: nan\n",
      "Epoch [959/10000] Train Loss: 0.000649 Val Loss: nan\n",
      "Epoch [960/10000] Train Loss: 0.000653 Val Loss: nan\n",
      "Epoch [961/10000] Train Loss: 0.000642 Val Loss: nan\n",
      "Epoch [962/10000] Train Loss: 0.000665 Val Loss: nan\n",
      "Epoch [963/10000] Train Loss: 0.000621 Val Loss: nan\n",
      "Epoch [964/10000] Train Loss: 0.000626 Val Loss: nan\n",
      "Epoch [965/10000] Train Loss: 0.000660 Val Loss: nan\n",
      "Epoch [966/10000] Train Loss: 0.000671 Val Loss: nan\n",
      "Epoch [967/10000] Train Loss: 0.000716 Val Loss: nan\n",
      "Epoch [968/10000] Train Loss: 0.000815 Val Loss: nan\n",
      "Epoch [969/10000] Train Loss: 0.000931 Val Loss: nan\n",
      "Epoch [970/10000] Train Loss: 0.000887 Val Loss: nan\n",
      "Epoch [971/10000] Train Loss: 0.000892 Val Loss: nan\n",
      "Epoch [972/10000] Train Loss: 0.000846 Val Loss: nan\n",
      "Epoch [973/10000] Train Loss: 0.000765 Val Loss: nan\n",
      "Epoch [974/10000] Train Loss: 0.000658 Val Loss: nan\n",
      "Epoch [975/10000] Train Loss: 0.000639 Val Loss: nan\n",
      "Epoch [976/10000] Train Loss: 0.000646 Val Loss: nan\n",
      "Epoch [977/10000] Train Loss: 0.000695 Val Loss: nan\n",
      "Epoch [978/10000] Train Loss: 0.000662 Val Loss: nan\n",
      "Epoch [979/10000] Train Loss: 0.000594 Val Loss: nan\n",
      "Epoch [980/10000] Train Loss: 0.000655 Val Loss: nan\n",
      "Epoch [981/10000] Train Loss: 0.000630 Val Loss: nan\n",
      "Epoch [982/10000] Train Loss: 0.000620 Val Loss: nan\n",
      "Epoch [983/10000] Train Loss: 0.000620 Val Loss: nan\n",
      "Epoch [984/10000] Train Loss: 0.000649 Val Loss: nan\n",
      "Epoch [985/10000] Train Loss: 0.000611 Val Loss: nan\n",
      "Epoch [986/10000] Train Loss: 0.000621 Val Loss: nan\n",
      "Epoch [987/10000] Train Loss: 0.000593 Val Loss: nan\n",
      "Epoch [988/10000] Train Loss: 0.000573 Val Loss: nan\n",
      "Epoch [989/10000] Train Loss: 0.000590 Val Loss: nan\n",
      "Epoch [990/10000] Train Loss: 0.000617 Val Loss: nan\n",
      "Epoch [991/10000] Train Loss: 0.000630 Val Loss: nan\n",
      "Epoch [992/10000] Train Loss: 0.000661 Val Loss: nan\n",
      "Epoch [993/10000] Train Loss: 0.000656 Val Loss: nan\n",
      "Epoch [994/10000] Train Loss: 0.000885 Val Loss: nan\n",
      "Epoch [995/10000] Train Loss: 0.001066 Val Loss: nan\n",
      "Epoch [996/10000] Train Loss: 0.001100 Val Loss: nan\n",
      "Epoch [997/10000] Train Loss: 0.000893 Val Loss: nan\n",
      "Epoch [998/10000] Train Loss: 0.000700 Val Loss: nan\n",
      "Epoch [999/10000] Train Loss: 0.000621 Val Loss: nan\n",
      "Epoch [1000/10000] Train Loss: 0.000595 Val Loss: nan\n",
      "Epoch [1001/10000] Train Loss: 0.000569 Val Loss: nan\n",
      "Epoch [1002/10000] Train Loss: 0.000565 Val Loss: nan\n",
      "Epoch [1003/10000] Train Loss: 0.000563 Val Loss: nan\n",
      "Epoch [1004/10000] Train Loss: 0.000569 Val Loss: nan\n",
      "Epoch [1005/10000] Train Loss: 0.000547 Val Loss: nan\n",
      "Epoch [1006/10000] Train Loss: 0.000556 Val Loss: nan\n",
      "Epoch [1007/10000] Train Loss: 0.000559 Val Loss: nan\n",
      "Epoch [1008/10000] Train Loss: 0.000554 Val Loss: nan\n",
      "Epoch [1009/10000] Train Loss: 0.000528 Val Loss: nan\n",
      "Epoch [1010/10000] Train Loss: 0.000558 Val Loss: nan\n",
      "Epoch [1011/10000] Train Loss: 0.000594 Val Loss: nan\n",
      "Epoch [1012/10000] Train Loss: 0.000551 Val Loss: nan\n",
      "Epoch [1013/10000] Train Loss: 0.000551 Val Loss: nan\n",
      "Epoch [1014/10000] Train Loss: 0.000544 Val Loss: nan\n",
      "Epoch [1015/10000] Train Loss: 0.000512 Val Loss: nan\n",
      "Epoch [1016/10000] Train Loss: 0.000506 Val Loss: nan\n",
      "Epoch [1017/10000] Train Loss: 0.000509 Val Loss: nan\n",
      "Epoch [1018/10000] Train Loss: 0.000499 Val Loss: nan\n",
      "Epoch [1019/10000] Train Loss: 0.000526 Val Loss: nan\n",
      "Epoch [1020/10000] Train Loss: 0.000581 Val Loss: nan\n",
      "Epoch [1021/10000] Train Loss: 0.000531 Val Loss: nan\n",
      "Epoch [1022/10000] Train Loss: 0.000545 Val Loss: nan\n",
      "Epoch [1023/10000] Train Loss: 0.000536 Val Loss: nan\n",
      "Epoch [1024/10000] Train Loss: 0.000553 Val Loss: nan\n",
      "Epoch [1025/10000] Train Loss: 0.000536 Val Loss: nan\n",
      "Epoch [1026/10000] Train Loss: 0.000529 Val Loss: nan\n",
      "Epoch [1027/10000] Train Loss: 0.000548 Val Loss: nan\n",
      "Epoch [1028/10000] Train Loss: 0.000544 Val Loss: nan\n",
      "Epoch [1029/10000] Train Loss: 0.000556 Val Loss: nan\n",
      "Epoch [1030/10000] Train Loss: 0.000565 Val Loss: nan\n",
      "Epoch [1031/10000] Train Loss: 0.000562 Val Loss: nan\n",
      "Epoch [1032/10000] Train Loss: 0.000517 Val Loss: nan\n",
      "Epoch [1033/10000] Train Loss: 0.000510 Val Loss: nan\n",
      "Epoch [1034/10000] Train Loss: 0.000530 Val Loss: nan\n",
      "Epoch [1035/10000] Train Loss: 0.000494 Val Loss: nan\n",
      "Epoch [1036/10000] Train Loss: 0.000460 Val Loss: nan\n",
      "Epoch [1037/10000] Train Loss: 0.000472 Val Loss: nan\n",
      "Epoch [1038/10000] Train Loss: 0.000461 Val Loss: nan\n",
      "Epoch [1039/10000] Train Loss: 0.000501 Val Loss: nan\n",
      "Epoch [1040/10000] Train Loss: 0.000521 Val Loss: nan\n",
      "Epoch [1041/10000] Train Loss: 0.000527 Val Loss: nan\n",
      "Epoch [1042/10000] Train Loss: 0.000527 Val Loss: nan\n",
      "Epoch [1043/10000] Train Loss: 0.000522 Val Loss: nan\n",
      "Epoch [1044/10000] Train Loss: 0.000481 Val Loss: nan\n",
      "Epoch [1045/10000] Train Loss: 0.000465 Val Loss: nan\n",
      "Epoch [1046/10000] Train Loss: 0.000471 Val Loss: nan\n",
      "Epoch [1047/10000] Train Loss: 0.000472 Val Loss: nan\n",
      "Epoch [1048/10000] Train Loss: 0.000492 Val Loss: nan\n",
      "Epoch [1049/10000] Train Loss: 0.000511 Val Loss: nan\n",
      "Epoch [1050/10000] Train Loss: 0.000535 Val Loss: nan\n",
      "Epoch [1051/10000] Train Loss: 0.000506 Val Loss: nan\n",
      "Epoch [1052/10000] Train Loss: 0.000462 Val Loss: nan\n",
      "Epoch [1053/10000] Train Loss: 0.000468 Val Loss: nan\n",
      "Epoch [1054/10000] Train Loss: 0.000468 Val Loss: nan\n",
      "Epoch [1055/10000] Train Loss: 0.000482 Val Loss: nan\n",
      "Epoch [1056/10000] Train Loss: 0.000450 Val Loss: nan\n",
      "Epoch [1057/10000] Train Loss: 0.000441 Val Loss: nan\n",
      "Epoch [1058/10000] Train Loss: 0.000477 Val Loss: nan\n",
      "Epoch [1059/10000] Train Loss: 0.000508 Val Loss: nan\n",
      "Epoch [1060/10000] Train Loss: 0.000515 Val Loss: nan\n",
      "Epoch [1061/10000] Train Loss: 0.000499 Val Loss: nan\n",
      "Epoch [1062/10000] Train Loss: 0.000475 Val Loss: nan\n",
      "Epoch [1063/10000] Train Loss: 0.000498 Val Loss: nan\n",
      "Epoch [1064/10000] Train Loss: 0.000501 Val Loss: nan\n",
      "Epoch [1065/10000] Train Loss: 0.000481 Val Loss: nan\n",
      "Epoch [1066/10000] Train Loss: 0.000444 Val Loss: nan\n",
      "Epoch [1067/10000] Train Loss: 0.000450 Val Loss: nan\n",
      "Epoch [1068/10000] Train Loss: 0.000450 Val Loss: nan\n",
      "Epoch [1069/10000] Train Loss: 0.000464 Val Loss: nan\n",
      "Epoch [1070/10000] Train Loss: 0.000467 Val Loss: nan\n",
      "Epoch [1071/10000] Train Loss: 0.000530 Val Loss: nan\n",
      "Epoch [1072/10000] Train Loss: 0.000492 Val Loss: nan\n",
      "Epoch [1073/10000] Train Loss: 0.000487 Val Loss: nan\n",
      "Epoch [1074/10000] Train Loss: 0.000482 Val Loss: nan\n",
      "Epoch [1075/10000] Train Loss: 0.000459 Val Loss: nan\n",
      "Epoch [1076/10000] Train Loss: 0.000478 Val Loss: nan\n",
      "Epoch [1077/10000] Train Loss: 0.000487 Val Loss: nan\n",
      "Epoch [1078/10000] Train Loss: 0.000477 Val Loss: nan\n",
      "Epoch [1079/10000] Train Loss: 0.000425 Val Loss: nan\n",
      "Epoch [1080/10000] Train Loss: 0.000410 Val Loss: nan\n",
      "Epoch [1081/10000] Train Loss: 0.000441 Val Loss: nan\n",
      "Epoch [1082/10000] Train Loss: 0.000435 Val Loss: nan\n",
      "Epoch [1083/10000] Train Loss: 0.000423 Val Loss: nan\n",
      "Epoch [1084/10000] Train Loss: 0.000425 Val Loss: nan\n",
      "Epoch [1085/10000] Train Loss: 0.000421 Val Loss: nan\n",
      "Epoch [1086/10000] Train Loss: 0.000411 Val Loss: nan\n",
      "Epoch [1087/10000] Train Loss: 0.000418 Val Loss: nan\n",
      "Epoch [1088/10000] Train Loss: 0.000423 Val Loss: nan\n",
      "Epoch [1089/10000] Train Loss: 0.000403 Val Loss: nan\n",
      "Epoch [1090/10000] Train Loss: 0.000413 Val Loss: nan\n",
      "Epoch [1091/10000] Train Loss: 0.000392 Val Loss: nan\n",
      "Epoch [1092/10000] Train Loss: 0.000403 Val Loss: nan\n",
      "Epoch [1093/10000] Train Loss: 0.000421 Val Loss: nan\n",
      "Epoch [1094/10000] Train Loss: 0.000444 Val Loss: nan\n",
      "Epoch [1095/10000] Train Loss: 0.000421 Val Loss: nan\n",
      "Epoch [1096/10000] Train Loss: 0.000404 Val Loss: nan\n",
      "Epoch [1097/10000] Train Loss: 0.000427 Val Loss: nan\n",
      "Epoch [1098/10000] Train Loss: 0.000403 Val Loss: nan\n",
      "Epoch [1099/10000] Train Loss: 0.000423 Val Loss: nan\n",
      "Epoch [1100/10000] Train Loss: 0.000420 Val Loss: nan\n",
      "Epoch [1101/10000] Train Loss: 0.000431 Val Loss: nan\n",
      "Epoch [1102/10000] Train Loss: 0.000393 Val Loss: nan\n",
      "Epoch [1103/10000] Train Loss: 0.000434 Val Loss: nan\n",
      "Epoch [1104/10000] Train Loss: 0.000443 Val Loss: nan\n",
      "Epoch [1105/10000] Train Loss: 0.000425 Val Loss: nan\n",
      "Epoch [1106/10000] Train Loss: 0.000399 Val Loss: nan\n",
      "Epoch [1107/10000] Train Loss: 0.000394 Val Loss: nan\n",
      "Epoch [1108/10000] Train Loss: 0.000378 Val Loss: nan\n",
      "Epoch [1109/10000] Train Loss: 0.000372 Val Loss: nan\n",
      "Epoch [1110/10000] Train Loss: 0.000368 Val Loss: nan\n",
      "Epoch [1111/10000] Train Loss: 0.000364 Val Loss: nan\n",
      "Epoch [1112/10000] Train Loss: 0.000345 Val Loss: nan\n",
      "Epoch [1113/10000] Train Loss: 0.000350 Val Loss: nan\n",
      "Epoch [1114/10000] Train Loss: 0.000358 Val Loss: nan\n",
      "Epoch [1115/10000] Train Loss: 0.000385 Val Loss: nan\n",
      "Epoch [1116/10000] Train Loss: 0.000402 Val Loss: nan\n",
      "Epoch [1117/10000] Train Loss: 0.000410 Val Loss: nan\n",
      "Epoch [1118/10000] Train Loss: 0.000384 Val Loss: nan\n",
      "Epoch [1119/10000] Train Loss: 0.000360 Val Loss: nan\n",
      "Epoch [1120/10000] Train Loss: 0.000370 Val Loss: nan\n",
      "Epoch [1121/10000] Train Loss: 0.000355 Val Loss: nan\n",
      "Epoch [1122/10000] Train Loss: 0.000363 Val Loss: nan\n",
      "Epoch [1123/10000] Train Loss: 0.000369 Val Loss: nan\n",
      "Epoch [1124/10000] Train Loss: 0.000352 Val Loss: nan\n",
      "Epoch [1125/10000] Train Loss: 0.000365 Val Loss: nan\n",
      "Epoch [1126/10000] Train Loss: 0.000373 Val Loss: nan\n",
      "Epoch [1127/10000] Train Loss: 0.000415 Val Loss: nan\n",
      "Epoch [1128/10000] Train Loss: 0.000397 Val Loss: nan\n",
      "Epoch [1129/10000] Train Loss: 0.000386 Val Loss: nan\n",
      "Epoch [1130/10000] Train Loss: 0.000365 Val Loss: nan\n",
      "Epoch [1131/10000] Train Loss: 0.000342 Val Loss: nan\n",
      "Epoch [1132/10000] Train Loss: 0.000364 Val Loss: nan\n",
      "Epoch [1133/10000] Train Loss: 0.000347 Val Loss: nan\n",
      "Epoch [1134/10000] Train Loss: 0.000364 Val Loss: nan\n",
      "Epoch [1135/10000] Train Loss: 0.000349 Val Loss: nan\n",
      "Epoch [1136/10000] Train Loss: 0.000356 Val Loss: nan\n",
      "Epoch [1137/10000] Train Loss: 0.000364 Val Loss: nan\n",
      "Epoch [1138/10000] Train Loss: 0.000349 Val Loss: nan\n",
      "Epoch [1139/10000] Train Loss: 0.000342 Val Loss: nan\n",
      "Epoch [1140/10000] Train Loss: 0.000329 Val Loss: nan\n",
      "Epoch [1141/10000] Train Loss: 0.000347 Val Loss: nan\n",
      "Epoch [1142/10000] Train Loss: 0.000333 Val Loss: nan\n",
      "Epoch [1143/10000] Train Loss: 0.000315 Val Loss: nan\n",
      "Epoch [1144/10000] Train Loss: 0.000325 Val Loss: nan\n",
      "Epoch [1145/10000] Train Loss: 0.000324 Val Loss: nan\n",
      "Epoch [1146/10000] Train Loss: 0.000331 Val Loss: nan\n",
      "Epoch [1147/10000] Train Loss: 0.000331 Val Loss: nan\n",
      "Epoch [1148/10000] Train Loss: 0.000330 Val Loss: nan\n",
      "Epoch [1149/10000] Train Loss: 0.000350 Val Loss: nan\n",
      "Epoch [1150/10000] Train Loss: 0.000342 Val Loss: nan\n",
      "Epoch [1151/10000] Train Loss: 0.000324 Val Loss: nan\n",
      "Epoch [1152/10000] Train Loss: 0.000321 Val Loss: nan\n",
      "Epoch [1153/10000] Train Loss: 0.000310 Val Loss: nan\n",
      "Epoch [1154/10000] Train Loss: 0.000345 Val Loss: nan\n",
      "Epoch [1155/10000] Train Loss: 0.000347 Val Loss: nan\n",
      "Epoch [1156/10000] Train Loss: 0.000359 Val Loss: nan\n",
      "Epoch [1157/10000] Train Loss: 0.000324 Val Loss: nan\n",
      "Epoch [1158/10000] Train Loss: 0.000324 Val Loss: nan\n",
      "Epoch [1159/10000] Train Loss: 0.000355 Val Loss: nan\n",
      "Epoch [1160/10000] Train Loss: 0.000341 Val Loss: nan\n",
      "Epoch [1161/10000] Train Loss: 0.000356 Val Loss: nan\n",
      "Epoch [1162/10000] Train Loss: 0.000363 Val Loss: nan\n",
      "Epoch [1163/10000] Train Loss: 0.000358 Val Loss: nan\n",
      "Epoch [1164/10000] Train Loss: 0.000350 Val Loss: nan\n",
      "Epoch [1165/10000] Train Loss: 0.000352 Val Loss: nan\n",
      "Epoch [1166/10000] Train Loss: 0.000328 Val Loss: nan\n",
      "Epoch [1167/10000] Train Loss: 0.000350 Val Loss: nan\n",
      "Epoch [1168/10000] Train Loss: 0.000395 Val Loss: nan\n",
      "Epoch [1169/10000] Train Loss: 0.000453 Val Loss: nan\n",
      "Epoch [1170/10000] Train Loss: 0.000556 Val Loss: nan\n",
      "Epoch [1171/10000] Train Loss: 0.000439 Val Loss: nan\n",
      "Epoch [1172/10000] Train Loss: 0.000351 Val Loss: nan\n",
      "Epoch [1173/10000] Train Loss: 0.000371 Val Loss: nan\n",
      "Epoch [1174/10000] Train Loss: 0.000379 Val Loss: nan\n",
      "Epoch [1175/10000] Train Loss: 0.000358 Val Loss: nan\n",
      "Epoch [1176/10000] Train Loss: 0.000325 Val Loss: nan\n",
      "Epoch [1177/10000] Train Loss: 0.000308 Val Loss: nan\n",
      "Epoch [1178/10000] Train Loss: 0.000305 Val Loss: nan\n",
      "Epoch [1179/10000] Train Loss: 0.000313 Val Loss: nan\n",
      "Epoch [1180/10000] Train Loss: 0.000322 Val Loss: nan\n",
      "Epoch [1181/10000] Train Loss: 0.000325 Val Loss: nan\n",
      "Epoch [1182/10000] Train Loss: 0.000311 Val Loss: nan\n",
      "Epoch [1183/10000] Train Loss: 0.000329 Val Loss: nan\n",
      "Epoch [1184/10000] Train Loss: 0.000348 Val Loss: nan\n",
      "Epoch [1185/10000] Train Loss: 0.000346 Val Loss: nan\n",
      "Epoch [1186/10000] Train Loss: 0.000319 Val Loss: nan\n",
      "Epoch [1187/10000] Train Loss: 0.000308 Val Loss: nan\n",
      "Epoch [1188/10000] Train Loss: 0.000299 Val Loss: nan\n",
      "Epoch [1189/10000] Train Loss: 0.000297 Val Loss: nan\n",
      "Epoch [1190/10000] Train Loss: 0.000299 Val Loss: nan\n",
      "Epoch [1191/10000] Train Loss: 0.000311 Val Loss: nan\n",
      "Epoch [1192/10000] Train Loss: 0.000309 Val Loss: nan\n",
      "Epoch [1193/10000] Train Loss: 0.000289 Val Loss: nan\n",
      "Epoch [1194/10000] Train Loss: 0.000281 Val Loss: nan\n",
      "Epoch [1195/10000] Train Loss: 0.000299 Val Loss: nan\n",
      "Epoch [1196/10000] Train Loss: 0.000295 Val Loss: nan\n",
      "Epoch [1197/10000] Train Loss: 0.000294 Val Loss: nan\n",
      "Epoch [1198/10000] Train Loss: 0.000276 Val Loss: nan\n",
      "Epoch [1199/10000] Train Loss: 0.000295 Val Loss: nan\n",
      "Epoch [1200/10000] Train Loss: 0.000282 Val Loss: nan\n",
      "Epoch [1201/10000] Train Loss: 0.000318 Val Loss: nan\n",
      "Epoch [1202/10000] Train Loss: 0.000310 Val Loss: nan\n",
      "Epoch [1203/10000] Train Loss: 0.000312 Val Loss: nan\n",
      "Epoch [1204/10000] Train Loss: 0.000328 Val Loss: nan\n",
      "Epoch [1205/10000] Train Loss: 0.000354 Val Loss: nan\n",
      "Epoch [1206/10000] Train Loss: 0.000365 Val Loss: nan\n",
      "Epoch [1207/10000] Train Loss: 0.000331 Val Loss: nan\n",
      "Epoch [1208/10000] Train Loss: 0.000307 Val Loss: nan\n",
      "Epoch [1209/10000] Train Loss: 0.000290 Val Loss: nan\n",
      "Epoch [1210/10000] Train Loss: 0.000284 Val Loss: nan\n",
      "Epoch [1211/10000] Train Loss: 0.000294 Val Loss: nan\n",
      "Epoch [1212/10000] Train Loss: 0.000278 Val Loss: nan\n",
      "Epoch [1213/10000] Train Loss: 0.000271 Val Loss: nan\n",
      "Epoch [1214/10000] Train Loss: 0.000269 Val Loss: nan\n",
      "Epoch [1215/10000] Train Loss: 0.000253 Val Loss: nan\n",
      "Epoch [1216/10000] Train Loss: 0.000257 Val Loss: nan\n",
      "Epoch [1217/10000] Train Loss: 0.000252 Val Loss: nan\n",
      "Epoch [1218/10000] Train Loss: 0.000248 Val Loss: nan\n",
      "Epoch [1219/10000] Train Loss: 0.000278 Val Loss: nan\n",
      "Epoch [1220/10000] Train Loss: 0.000279 Val Loss: nan\n",
      "Epoch [1221/10000] Train Loss: 0.000260 Val Loss: nan\n",
      "Epoch [1222/10000] Train Loss: 0.000285 Val Loss: nan\n",
      "Epoch [1223/10000] Train Loss: 0.000293 Val Loss: nan\n",
      "Epoch [1224/10000] Train Loss: 0.000286 Val Loss: nan\n",
      "Epoch [1225/10000] Train Loss: 0.000272 Val Loss: nan\n",
      "Epoch [1226/10000] Train Loss: 0.000298 Val Loss: nan\n",
      "Epoch [1227/10000] Train Loss: 0.000322 Val Loss: nan\n",
      "Epoch [1228/10000] Train Loss: 0.000304 Val Loss: nan\n",
      "Epoch [1229/10000] Train Loss: 0.000430 Val Loss: nan\n",
      "Epoch [1230/10000] Train Loss: 0.000640 Val Loss: nan\n",
      "Epoch [1231/10000] Train Loss: 0.000528 Val Loss: nan\n",
      "Epoch [1232/10000] Train Loss: 0.000617 Val Loss: nan\n",
      "Epoch [1233/10000] Train Loss: 0.001784 Val Loss: nan\n",
      "Epoch [1234/10000] Train Loss: 0.001539 Val Loss: nan\n",
      "Epoch [1235/10000] Train Loss: 0.001965 Val Loss: nan\n",
      "Epoch [1236/10000] Train Loss: 0.001177 Val Loss: nan\n",
      "Epoch [1237/10000] Train Loss: 0.002322 Val Loss: nan\n",
      "Epoch [1238/10000] Train Loss: 0.001506 Val Loss: nan\n",
      "Epoch [1239/10000] Train Loss: 0.002285 Val Loss: nan\n",
      "Epoch [1240/10000] Train Loss: 0.001289 Val Loss: nan\n",
      "Epoch [1241/10000] Train Loss: 0.001258 Val Loss: nan\n",
      "Epoch [1242/10000] Train Loss: 0.000758 Val Loss: nan\n",
      "Epoch [1243/10000] Train Loss: 0.001062 Val Loss: nan\n",
      "Epoch [1244/10000] Train Loss: 0.000810 Val Loss: nan\n",
      "Epoch [1245/10000] Train Loss: 0.000642 Val Loss: nan\n",
      "Epoch [1246/10000] Train Loss: 0.000812 Val Loss: nan\n",
      "Epoch [1247/10000] Train Loss: 0.000768 Val Loss: nan\n",
      "Epoch [1248/10000] Train Loss: 0.001003 Val Loss: nan\n",
      "Epoch [1249/10000] Train Loss: 0.000865 Val Loss: nan\n",
      "Epoch [1250/10000] Train Loss: 0.000834 Val Loss: nan\n",
      "Epoch [1251/10000] Train Loss: 0.000717 Val Loss: nan\n",
      "Epoch [1252/10000] Train Loss: 0.000462 Val Loss: nan\n",
      "Epoch [1253/10000] Train Loss: 0.000364 Val Loss: nan\n",
      "Epoch [1254/10000] Train Loss: 0.000299 Val Loss: nan\n",
      "Epoch [1255/10000] Train Loss: 0.000262 Val Loss: nan\n",
      "Epoch [1256/10000] Train Loss: 0.000254 Val Loss: nan\n",
      "Epoch [1257/10000] Train Loss: 0.000245 Val Loss: nan\n",
      "Epoch [1258/10000] Train Loss: 0.000252 Val Loss: nan\n",
      "Epoch [1259/10000] Train Loss: 0.000239 Val Loss: nan\n",
      "Epoch [1260/10000] Train Loss: 0.000229 Val Loss: nan\n",
      "Epoch [1261/10000] Train Loss: 0.000227 Val Loss: nan\n",
      "Epoch [1262/10000] Train Loss: 0.000230 Val Loss: nan\n",
      "Epoch [1263/10000] Train Loss: 0.000220 Val Loss: nan\n",
      "Epoch [1264/10000] Train Loss: 0.000235 Val Loss: nan\n",
      "Epoch [1265/10000] Train Loss: 0.000231 Val Loss: nan\n",
      "Epoch [1266/10000] Train Loss: 0.000220 Val Loss: nan\n",
      "Epoch [1267/10000] Train Loss: 0.000214 Val Loss: nan\n",
      "Epoch [1268/10000] Train Loss: 0.000208 Val Loss: nan\n",
      "Epoch [1269/10000] Train Loss: 0.000214 Val Loss: nan\n",
      "Epoch [1270/10000] Train Loss: 0.000217 Val Loss: nan\n",
      "Epoch [1271/10000] Train Loss: 0.000210 Val Loss: nan\n",
      "Epoch [1272/10000] Train Loss: 0.000214 Val Loss: nan\n",
      "Epoch [1273/10000] Train Loss: 0.000210 Val Loss: nan\n",
      "Epoch [1274/10000] Train Loss: 0.000203 Val Loss: nan\n",
      "Epoch [1275/10000] Train Loss: 0.000207 Val Loss: nan\n",
      "Epoch [1276/10000] Train Loss: 0.000210 Val Loss: nan\n",
      "Epoch [1277/10000] Train Loss: 0.000210 Val Loss: nan\n",
      "Epoch [1278/10000] Train Loss: 0.000217 Val Loss: nan\n",
      "Epoch [1279/10000] Train Loss: 0.000221 Val Loss: nan\n",
      "Epoch [1280/10000] Train Loss: 0.000210 Val Loss: nan\n",
      "Epoch [1281/10000] Train Loss: 0.000217 Val Loss: nan\n",
      "Epoch [1282/10000] Train Loss: 0.000213 Val Loss: nan\n",
      "Epoch [1283/10000] Train Loss: 0.000210 Val Loss: nan\n",
      "Epoch [1284/10000] Train Loss: 0.000215 Val Loss: nan\n",
      "Epoch [1285/10000] Train Loss: 0.000214 Val Loss: nan\n",
      "Epoch [1286/10000] Train Loss: 0.000212 Val Loss: nan\n",
      "Epoch [1287/10000] Train Loss: 0.000204 Val Loss: nan\n",
      "Epoch [1288/10000] Train Loss: 0.000203 Val Loss: nan\n",
      "Epoch [1289/10000] Train Loss: 0.000196 Val Loss: nan\n",
      "Epoch [1290/10000] Train Loss: 0.000218 Val Loss: nan\n",
      "Epoch [1291/10000] Train Loss: 0.000223 Val Loss: nan\n",
      "Epoch [1292/10000] Train Loss: 0.000224 Val Loss: nan\n",
      "Epoch [1293/10000] Train Loss: 0.000225 Val Loss: nan\n",
      "Epoch [1294/10000] Train Loss: 0.000217 Val Loss: nan\n",
      "Epoch [1295/10000] Train Loss: 0.000211 Val Loss: nan\n",
      "Epoch [1296/10000] Train Loss: 0.000198 Val Loss: nan\n",
      "Epoch [1297/10000] Train Loss: 0.000207 Val Loss: nan\n",
      "Epoch [1298/10000] Train Loss: 0.000201 Val Loss: nan\n",
      "Epoch [1299/10000] Train Loss: 0.000196 Val Loss: nan\n",
      "Epoch [1300/10000] Train Loss: 0.000197 Val Loss: nan\n",
      "Epoch [1301/10000] Train Loss: 0.000198 Val Loss: nan\n",
      "Epoch [1302/10000] Train Loss: 0.000201 Val Loss: nan\n",
      "Epoch [1303/10000] Train Loss: 0.000196 Val Loss: nan\n",
      "Epoch [1304/10000] Train Loss: 0.000199 Val Loss: nan\n",
      "Epoch [1305/10000] Train Loss: 0.000206 Val Loss: nan\n",
      "Epoch [1306/10000] Train Loss: 0.000196 Val Loss: nan\n",
      "Epoch [1307/10000] Train Loss: 0.000196 Val Loss: nan\n",
      "Epoch [1308/10000] Train Loss: 0.000190 Val Loss: nan\n",
      "Epoch [1309/10000] Train Loss: 0.000191 Val Loss: nan\n",
      "Epoch [1310/10000] Train Loss: 0.000193 Val Loss: nan\n",
      "Epoch [1311/10000] Train Loss: 0.000183 Val Loss: nan\n",
      "Epoch [1312/10000] Train Loss: 0.000187 Val Loss: nan\n",
      "Epoch [1313/10000] Train Loss: 0.000181 Val Loss: nan\n",
      "Epoch [1314/10000] Train Loss: 0.000189 Val Loss: nan\n",
      "Epoch [1315/10000] Train Loss: 0.000196 Val Loss: nan\n",
      "Epoch [1316/10000] Train Loss: 0.000205 Val Loss: nan\n",
      "Epoch [1317/10000] Train Loss: 0.000194 Val Loss: nan\n",
      "Epoch [1318/10000] Train Loss: 0.000193 Val Loss: nan\n",
      "Epoch [1319/10000] Train Loss: 0.000195 Val Loss: nan\n",
      "Epoch [1320/10000] Train Loss: 0.000203 Val Loss: nan\n",
      "Epoch [1321/10000] Train Loss: 0.000194 Val Loss: nan\n",
      "Epoch [1322/10000] Train Loss: 0.000215 Val Loss: nan\n",
      "Epoch [1323/10000] Train Loss: 0.000207 Val Loss: nan\n",
      "Epoch [1324/10000] Train Loss: 0.000212 Val Loss: nan\n",
      "Epoch [1325/10000] Train Loss: 0.000214 Val Loss: nan\n",
      "Epoch [1326/10000] Train Loss: 0.000203 Val Loss: nan\n",
      "Epoch [1327/10000] Train Loss: 0.000194 Val Loss: nan\n",
      "Epoch [1328/10000] Train Loss: 0.000189 Val Loss: nan\n",
      "Epoch [1329/10000] Train Loss: 0.000190 Val Loss: nan\n",
      "Epoch [1330/10000] Train Loss: 0.000188 Val Loss: nan\n",
      "Epoch [1331/10000] Train Loss: 0.000196 Val Loss: nan\n",
      "Epoch [1332/10000] Train Loss: 0.000191 Val Loss: nan\n",
      "Epoch [1333/10000] Train Loss: 0.000199 Val Loss: nan\n",
      "Epoch [1334/10000] Train Loss: 0.000196 Val Loss: nan\n",
      "Epoch [1335/10000] Train Loss: 0.000187 Val Loss: nan\n",
      "Epoch [1336/10000] Train Loss: 0.000196 Val Loss: nan\n",
      "Epoch [1337/10000] Train Loss: 0.000198 Val Loss: nan\n",
      "Epoch [1338/10000] Train Loss: 0.000188 Val Loss: nan\n",
      "Epoch [1339/10000] Train Loss: 0.000197 Val Loss: nan\n",
      "Epoch [1340/10000] Train Loss: 0.000184 Val Loss: nan\n",
      "Epoch [1341/10000] Train Loss: 0.000184 Val Loss: nan\n",
      "Epoch [1342/10000] Train Loss: 0.000204 Val Loss: nan\n",
      "Epoch [1343/10000] Train Loss: 0.000193 Val Loss: nan\n",
      "Epoch [1344/10000] Train Loss: 0.000197 Val Loss: nan\n",
      "Epoch [1345/10000] Train Loss: 0.000200 Val Loss: nan\n",
      "Epoch [1346/10000] Train Loss: 0.000182 Val Loss: nan\n",
      "Epoch [1347/10000] Train Loss: 0.000183 Val Loss: nan\n",
      "Epoch [1348/10000] Train Loss: 0.000177 Val Loss: nan\n",
      "Epoch [1349/10000] Train Loss: 0.000177 Val Loss: nan\n",
      "Epoch [1350/10000] Train Loss: 0.000177 Val Loss: nan\n",
      "Epoch [1351/10000] Train Loss: 0.000184 Val Loss: nan\n",
      "Epoch [1352/10000] Train Loss: 0.000185 Val Loss: nan\n",
      "Epoch [1353/10000] Train Loss: 0.000196 Val Loss: nan\n",
      "Epoch [1354/10000] Train Loss: 0.000219 Val Loss: nan\n",
      "Epoch [1355/10000] Train Loss: 0.000201 Val Loss: nan\n",
      "Epoch [1356/10000] Train Loss: 0.000191 Val Loss: nan\n",
      "Epoch [1357/10000] Train Loss: 0.000192 Val Loss: nan\n",
      "Epoch [1358/10000] Train Loss: 0.000202 Val Loss: nan\n",
      "Epoch [1359/10000] Train Loss: 0.000201 Val Loss: nan\n",
      "Epoch [1360/10000] Train Loss: 0.000194 Val Loss: nan\n",
      "Epoch [1361/10000] Train Loss: 0.000191 Val Loss: nan\n",
      "Epoch [1362/10000] Train Loss: 0.000180 Val Loss: nan\n",
      "Epoch [1363/10000] Train Loss: 0.000178 Val Loss: nan\n",
      "Epoch [1364/10000] Train Loss: 0.000178 Val Loss: nan\n",
      "Epoch [1365/10000] Train Loss: 0.000175 Val Loss: nan\n",
      "Epoch [1366/10000] Train Loss: 0.000181 Val Loss: nan\n",
      "Epoch [1367/10000] Train Loss: 0.000181 Val Loss: nan\n",
      "Epoch [1368/10000] Train Loss: 0.000183 Val Loss: nan\n",
      "Epoch [1369/10000] Train Loss: 0.000178 Val Loss: nan\n",
      "Epoch [1370/10000] Train Loss: 0.000181 Val Loss: nan\n",
      "Epoch [1371/10000] Train Loss: 0.000169 Val Loss: nan\n",
      "Epoch [1372/10000] Train Loss: 0.000171 Val Loss: nan\n",
      "Epoch [1373/10000] Train Loss: 0.000176 Val Loss: nan\n",
      "Epoch [1374/10000] Train Loss: 0.000186 Val Loss: nan\n",
      "Epoch [1375/10000] Train Loss: 0.000176 Val Loss: nan\n",
      "Epoch [1376/10000] Train Loss: 0.000171 Val Loss: nan\n",
      "Epoch [1377/10000] Train Loss: 0.000181 Val Loss: nan\n",
      "Epoch [1378/10000] Train Loss: 0.000172 Val Loss: nan\n",
      "Epoch [1379/10000] Train Loss: 0.000169 Val Loss: nan\n",
      "Epoch [1380/10000] Train Loss: 0.000175 Val Loss: nan\n",
      "Epoch [1381/10000] Train Loss: 0.000175 Val Loss: nan\n",
      "Epoch [1382/10000] Train Loss: 0.000171 Val Loss: nan\n",
      "Epoch [1383/10000] Train Loss: 0.000176 Val Loss: nan\n",
      "Epoch [1384/10000] Train Loss: 0.000178 Val Loss: nan\n",
      "Epoch [1385/10000] Train Loss: 0.000189 Val Loss: nan\n",
      "Epoch [1386/10000] Train Loss: 0.000204 Val Loss: nan\n",
      "Epoch [1387/10000] Train Loss: 0.000206 Val Loss: nan\n",
      "Epoch [1388/10000] Train Loss: 0.000183 Val Loss: nan\n",
      "Epoch [1389/10000] Train Loss: 0.000166 Val Loss: nan\n",
      "Epoch [1390/10000] Train Loss: 0.000180 Val Loss: nan\n",
      "Epoch [1391/10000] Train Loss: 0.000194 Val Loss: nan\n",
      "Epoch [1392/10000] Train Loss: 0.000193 Val Loss: nan\n",
      "Epoch [1393/10000] Train Loss: 0.000182 Val Loss: nan\n",
      "Epoch [1394/10000] Train Loss: 0.000171 Val Loss: nan\n",
      "Epoch [1395/10000] Train Loss: 0.000168 Val Loss: nan\n",
      "Epoch [1396/10000] Train Loss: 0.000175 Val Loss: nan\n",
      "Epoch [1397/10000] Train Loss: 0.000191 Val Loss: nan\n",
      "Epoch [1398/10000] Train Loss: 0.000184 Val Loss: nan\n",
      "Epoch [1399/10000] Train Loss: 0.000186 Val Loss: nan\n",
      "Epoch [1400/10000] Train Loss: 0.000175 Val Loss: nan\n",
      "Epoch [1401/10000] Train Loss: 0.000173 Val Loss: nan\n",
      "Epoch [1402/10000] Train Loss: 0.000169 Val Loss: nan\n",
      "Epoch [1403/10000] Train Loss: 0.000173 Val Loss: nan\n",
      "Epoch [1404/10000] Train Loss: 0.000169 Val Loss: nan\n",
      "Epoch [1405/10000] Train Loss: 0.000189 Val Loss: nan\n",
      "Epoch [1406/10000] Train Loss: 0.000182 Val Loss: nan\n",
      "Epoch [1407/10000] Train Loss: 0.000181 Val Loss: nan\n",
      "Epoch [1408/10000] Train Loss: 0.000196 Val Loss: nan\n",
      "Epoch [1409/10000] Train Loss: 0.000182 Val Loss: nan\n",
      "Epoch [1410/10000] Train Loss: 0.000181 Val Loss: nan\n",
      "Epoch [1411/10000] Train Loss: 0.000181 Val Loss: nan\n",
      "Epoch [1412/10000] Train Loss: 0.000167 Val Loss: nan\n",
      "Epoch [1413/10000] Train Loss: 0.000153 Val Loss: nan\n",
      "Epoch [1414/10000] Train Loss: 0.000149 Val Loss: nan\n",
      "Epoch [1415/10000] Train Loss: 0.000156 Val Loss: nan\n",
      "Epoch [1416/10000] Train Loss: 0.000157 Val Loss: nan\n",
      "Epoch [1417/10000] Train Loss: 0.000187 Val Loss: nan\n",
      "Epoch [1418/10000] Train Loss: 0.000185 Val Loss: nan\n",
      "Epoch [1419/10000] Train Loss: 0.000172 Val Loss: nan\n",
      "Epoch [1420/10000] Train Loss: 0.000160 Val Loss: nan\n",
      "Epoch [1421/10000] Train Loss: 0.000177 Val Loss: nan\n",
      "Epoch [1422/10000] Train Loss: 0.000175 Val Loss: nan\n",
      "Epoch [1423/10000] Train Loss: 0.000191 Val Loss: nan\n",
      "Epoch [1424/10000] Train Loss: 0.000186 Val Loss: nan\n",
      "Epoch [1425/10000] Train Loss: 0.000173 Val Loss: nan\n",
      "Epoch [1426/10000] Train Loss: 0.000180 Val Loss: nan\n",
      "Epoch [1427/10000] Train Loss: 0.000182 Val Loss: nan\n",
      "Epoch [1428/10000] Train Loss: 0.000218 Val Loss: nan\n",
      "Epoch [1429/10000] Train Loss: 0.000214 Val Loss: nan\n",
      "Epoch [1430/10000] Train Loss: 0.000220 Val Loss: nan\n",
      "Epoch [1431/10000] Train Loss: 0.000205 Val Loss: nan\n",
      "Epoch [1432/10000] Train Loss: 0.000215 Val Loss: nan\n",
      "Epoch [1433/10000] Train Loss: 0.000299 Val Loss: nan\n",
      "Epoch [1434/10000] Train Loss: 0.000330 Val Loss: nan\n",
      "Epoch [1435/10000] Train Loss: 0.000365 Val Loss: nan\n",
      "Epoch [1436/10000] Train Loss: 0.000422 Val Loss: nan\n",
      "Epoch [1437/10000] Train Loss: 0.000266 Val Loss: nan\n",
      "Epoch [1438/10000] Train Loss: 0.000233 Val Loss: nan\n",
      "Epoch [1439/10000] Train Loss: 0.000251 Val Loss: nan\n",
      "Epoch [1440/10000] Train Loss: 0.000213 Val Loss: nan\n",
      "Epoch [1441/10000] Train Loss: 0.000174 Val Loss: nan\n",
      "Epoch [1442/10000] Train Loss: 0.000164 Val Loss: nan\n",
      "Epoch [1443/10000] Train Loss: 0.000157 Val Loss: nan\n",
      "Epoch [1444/10000] Train Loss: 0.000162 Val Loss: nan\n",
      "Epoch [1445/10000] Train Loss: 0.000167 Val Loss: nan\n",
      "Epoch [1446/10000] Train Loss: 0.000169 Val Loss: nan\n",
      "Epoch [1447/10000] Train Loss: 0.000161 Val Loss: nan\n",
      "Epoch [1448/10000] Train Loss: 0.000172 Val Loss: nan\n",
      "Epoch [1449/10000] Train Loss: 0.000168 Val Loss: nan\n",
      "Epoch [1450/10000] Train Loss: 0.000146 Val Loss: nan\n",
      "Epoch [1451/10000] Train Loss: 0.000143 Val Loss: nan\n",
      "Epoch [1452/10000] Train Loss: 0.000167 Val Loss: nan\n",
      "Epoch [1453/10000] Train Loss: 0.000177 Val Loss: nan\n",
      "Epoch [1454/10000] Train Loss: 0.000166 Val Loss: nan\n",
      "Epoch [1455/10000] Train Loss: 0.000182 Val Loss: nan\n",
      "Epoch [1456/10000] Train Loss: 0.000168 Val Loss: nan\n",
      "Epoch [1457/10000] Train Loss: 0.000174 Val Loss: nan\n",
      "Epoch [1458/10000] Train Loss: 0.000154 Val Loss: nan\n",
      "Epoch [1459/10000] Train Loss: 0.000151 Val Loss: nan\n",
      "Epoch [1460/10000] Train Loss: 0.000155 Val Loss: nan\n",
      "Epoch [1461/10000] Train Loss: 0.000152 Val Loss: nan\n",
      "Epoch [1462/10000] Train Loss: 0.000159 Val Loss: nan\n",
      "Epoch [1463/10000] Train Loss: 0.000153 Val Loss: nan\n",
      "Epoch [1464/10000] Train Loss: 0.000143 Val Loss: nan\n",
      "Epoch [1465/10000] Train Loss: 0.000146 Val Loss: nan\n",
      "Epoch [1466/10000] Train Loss: 0.000156 Val Loss: nan\n",
      "Epoch [1467/10000] Train Loss: 0.000147 Val Loss: nan\n",
      "Epoch [1468/10000] Train Loss: 0.000142 Val Loss: nan\n",
      "Epoch [1469/10000] Train Loss: 0.000141 Val Loss: nan\n",
      "Epoch [1470/10000] Train Loss: 0.000143 Val Loss: nan\n",
      "Epoch [1471/10000] Train Loss: 0.000142 Val Loss: nan\n",
      "Epoch [1472/10000] Train Loss: 0.000145 Val Loss: nan\n",
      "Epoch [1473/10000] Train Loss: 0.000143 Val Loss: nan\n",
      "Epoch [1474/10000] Train Loss: 0.000149 Val Loss: nan\n",
      "Epoch [1475/10000] Train Loss: 0.000164 Val Loss: nan\n",
      "Epoch [1476/10000] Train Loss: 0.000169 Val Loss: nan\n",
      "Epoch [1477/10000] Train Loss: 0.000166 Val Loss: nan\n",
      "Epoch [1478/10000] Train Loss: 0.000166 Val Loss: nan\n",
      "Epoch [1479/10000] Train Loss: 0.000160 Val Loss: nan\n",
      "Epoch [1480/10000] Train Loss: 0.000177 Val Loss: nan\n",
      "Epoch [1481/10000] Train Loss: 0.000169 Val Loss: nan\n",
      "Epoch [1482/10000] Train Loss: 0.000173 Val Loss: nan\n",
      "Epoch [1483/10000] Train Loss: 0.000191 Val Loss: nan\n",
      "Epoch [1484/10000] Train Loss: 0.000195 Val Loss: nan\n",
      "Epoch [1485/10000] Train Loss: 0.000198 Val Loss: nan\n",
      "Epoch [1486/10000] Train Loss: 0.000172 Val Loss: nan\n",
      "Epoch [1487/10000] Train Loss: 0.000156 Val Loss: nan\n",
      "Epoch [1488/10000] Train Loss: 0.000137 Val Loss: nan\n",
      "Epoch [1489/10000] Train Loss: 0.000144 Val Loss: nan\n",
      "Epoch [1490/10000] Train Loss: 0.000178 Val Loss: nan\n",
      "Epoch [1491/10000] Train Loss: 0.000239 Val Loss: nan\n",
      "Epoch [1492/10000] Train Loss: 0.000204 Val Loss: nan\n",
      "Epoch [1493/10000] Train Loss: 0.000172 Val Loss: nan\n",
      "Epoch [1494/10000] Train Loss: 0.000178 Val Loss: nan\n",
      "Epoch [1495/10000] Train Loss: 0.000173 Val Loss: nan\n",
      "Epoch [1496/10000] Train Loss: 0.000183 Val Loss: nan\n",
      "Epoch [1497/10000] Train Loss: 0.000199 Val Loss: nan\n",
      "Epoch [1498/10000] Train Loss: 0.000199 Val Loss: nan\n",
      "Epoch [1499/10000] Train Loss: 0.000185 Val Loss: nan\n",
      "Epoch [1500/10000] Train Loss: 0.000169 Val Loss: nan\n",
      "Epoch [1501/10000] Train Loss: 0.000140 Val Loss: nan\n",
      "Epoch [1502/10000] Train Loss: 0.000129 Val Loss: nan\n",
      "Epoch [1503/10000] Train Loss: 0.000139 Val Loss: nan\n",
      "Epoch [1504/10000] Train Loss: 0.000146 Val Loss: nan\n",
      "Epoch [1505/10000] Train Loss: 0.000155 Val Loss: nan\n",
      "Epoch [1506/10000] Train Loss: 0.000147 Val Loss: nan\n",
      "Epoch [1507/10000] Train Loss: 0.000158 Val Loss: nan\n",
      "Epoch [1508/10000] Train Loss: 0.000144 Val Loss: nan\n",
      "Epoch [1509/10000] Train Loss: 0.000145 Val Loss: nan\n",
      "Epoch [1510/10000] Train Loss: 0.000145 Val Loss: nan\n",
      "Epoch [1511/10000] Train Loss: 0.000141 Val Loss: nan\n",
      "Epoch [1512/10000] Train Loss: 0.000156 Val Loss: nan\n",
      "Epoch [1513/10000] Train Loss: 0.000195 Val Loss: nan\n",
      "Epoch [1514/10000] Train Loss: 0.000236 Val Loss: nan\n",
      "Epoch [1515/10000] Train Loss: 0.000247 Val Loss: nan\n",
      "Epoch [1516/10000] Train Loss: 0.000230 Val Loss: nan\n",
      "Epoch [1517/10000] Train Loss: 0.000196 Val Loss: nan\n",
      "Epoch [1518/10000] Train Loss: 0.000172 Val Loss: nan\n",
      "Epoch [1519/10000] Train Loss: 0.000148 Val Loss: nan\n",
      "Epoch [1520/10000] Train Loss: 0.000143 Val Loss: nan\n",
      "Epoch [1521/10000] Train Loss: 0.000156 Val Loss: nan\n",
      "Epoch [1522/10000] Train Loss: 0.000188 Val Loss: nan\n",
      "Epoch [1523/10000] Train Loss: 0.000244 Val Loss: nan\n",
      "Epoch [1524/10000] Train Loss: 0.000246 Val Loss: nan\n",
      "Epoch [1525/10000] Train Loss: 0.000238 Val Loss: nan\n",
      "Epoch [1526/10000] Train Loss: 0.000203 Val Loss: nan\n",
      "Epoch [1527/10000] Train Loss: 0.000165 Val Loss: nan\n",
      "Epoch [1528/10000] Train Loss: 0.000156 Val Loss: nan\n",
      "Epoch [1529/10000] Train Loss: 0.000191 Val Loss: nan\n",
      "Epoch [1530/10000] Train Loss: 0.000295 Val Loss: nan\n",
      "Epoch [1531/10000] Train Loss: 0.000924 Val Loss: nan\n",
      "Epoch [1532/10000] Train Loss: 0.001312 Val Loss: nan\n",
      "Epoch [1533/10000] Train Loss: 0.000930 Val Loss: nan\n",
      "Epoch [1534/10000] Train Loss: 0.001249 Val Loss: nan\n",
      "Epoch [1535/10000] Train Loss: 0.001770 Val Loss: nan\n",
      "Epoch [1536/10000] Train Loss: 0.002512 Val Loss: nan\n",
      "Epoch [1537/10000] Train Loss: 0.004344 Val Loss: nan\n",
      "Epoch [1538/10000] Train Loss: 0.003858 Val Loss: nan\n",
      "Epoch [1539/10000] Train Loss: 0.003345 Val Loss: nan\n",
      "Epoch [1540/10000] Train Loss: 0.002274 Val Loss: nan\n",
      "Epoch [1541/10000] Train Loss: 0.001152 Val Loss: nan\n",
      "Epoch [1542/10000] Train Loss: 0.001093 Val Loss: nan\n",
      "Epoch [1543/10000] Train Loss: 0.002099 Val Loss: nan\n",
      "Epoch [1544/10000] Train Loss: 0.001208 Val Loss: nan\n",
      "Epoch [1545/10000] Train Loss: 0.000771 Val Loss: nan\n",
      "Epoch [1546/10000] Train Loss: 0.000487 Val Loss: nan\n",
      "Epoch [1547/10000] Train Loss: 0.000431 Val Loss: nan\n",
      "Epoch [1548/10000] Train Loss: 0.000413 Val Loss: nan\n",
      "Epoch [1549/10000] Train Loss: 0.000282 Val Loss: nan\n",
      "Epoch [1550/10000] Train Loss: 0.000232 Val Loss: nan\n",
      "Epoch [1551/10000] Train Loss: 0.000184 Val Loss: nan\n",
      "Epoch [1552/10000] Train Loss: 0.000172 Val Loss: nan\n",
      "Epoch [1553/10000] Train Loss: 0.000153 Val Loss: nan\n",
      "Epoch [1554/10000] Train Loss: 0.000143 Val Loss: nan\n",
      "Epoch [1555/10000] Train Loss: 0.000135 Val Loss: nan\n",
      "Epoch [1556/10000] Train Loss: 0.000126 Val Loss: nan\n",
      "Epoch [1557/10000] Train Loss: 0.000119 Val Loss: nan\n",
      "Epoch [1558/10000] Train Loss: 0.000119 Val Loss: nan\n",
      "Epoch [1559/10000] Train Loss: 0.000115 Val Loss: nan\n",
      "Epoch [1560/10000] Train Loss: 0.000110 Val Loss: nan\n",
      "Epoch [1561/10000] Train Loss: 0.000114 Val Loss: nan\n",
      "Epoch [1562/10000] Train Loss: 0.000118 Val Loss: nan\n",
      "Epoch [1563/10000] Train Loss: 0.000118 Val Loss: nan\n",
      "Epoch [1564/10000] Train Loss: 0.000115 Val Loss: nan\n",
      "Epoch [1565/10000] Train Loss: 0.000111 Val Loss: nan\n",
      "Epoch [1566/10000] Train Loss: 0.000110 Val Loss: nan\n",
      "Epoch [1567/10000] Train Loss: 0.000111 Val Loss: nan\n",
      "Epoch [1568/10000] Train Loss: 0.000110 Val Loss: nan\n",
      "Epoch [1569/10000] Train Loss: 0.000109 Val Loss: nan\n",
      "Epoch [1570/10000] Train Loss: 0.000107 Val Loss: nan\n",
      "Epoch [1571/10000] Train Loss: 0.000105 Val Loss: nan\n",
      "Epoch [1572/10000] Train Loss: 0.000106 Val Loss: nan\n",
      "Epoch [1573/10000] Train Loss: 0.000105 Val Loss: nan\n",
      "Epoch [1574/10000] Train Loss: 0.000106 Val Loss: nan\n",
      "Epoch [1575/10000] Train Loss: 0.000107 Val Loss: nan\n",
      "Epoch [1576/10000] Train Loss: 0.000108 Val Loss: nan\n",
      "Epoch [1577/10000] Train Loss: 0.000105 Val Loss: nan\n",
      "Epoch [1578/10000] Train Loss: 0.000104 Val Loss: nan\n",
      "Epoch [1579/10000] Train Loss: 0.000101 Val Loss: nan\n",
      "Epoch [1580/10000] Train Loss: 0.000100 Val Loss: nan\n",
      "Epoch [1581/10000] Train Loss: 0.000098 Val Loss: nan\n",
      "Epoch [1582/10000] Train Loss: 0.000101 Val Loss: nan\n",
      "Epoch [1583/10000] Train Loss: 0.000103 Val Loss: nan\n",
      "Epoch [1584/10000] Train Loss: 0.000104 Val Loss: nan\n",
      "Epoch [1585/10000] Train Loss: 0.000109 Val Loss: nan\n",
      "Epoch [1586/10000] Train Loss: 0.000107 Val Loss: nan\n",
      "Epoch [1587/10000] Train Loss: 0.000100 Val Loss: nan\n",
      "Epoch [1588/10000] Train Loss: 0.000102 Val Loss: nan\n",
      "Epoch [1589/10000] Train Loss: 0.000102 Val Loss: nan\n",
      "Epoch [1590/10000] Train Loss: 0.000106 Val Loss: nan\n",
      "Epoch [1591/10000] Train Loss: 0.000109 Val Loss: nan\n",
      "Epoch [1592/10000] Train Loss: 0.000101 Val Loss: nan\n",
      "Epoch [1593/10000] Train Loss: 0.000103 Val Loss: nan\n",
      "Epoch [1594/10000] Train Loss: 0.000097 Val Loss: nan\n",
      "Epoch [1595/10000] Train Loss: 0.000094 Val Loss: nan\n",
      "Epoch [1596/10000] Train Loss: 0.000096 Val Loss: nan\n",
      "Epoch [1597/10000] Train Loss: 0.000091 Val Loss: nan\n",
      "Epoch [1598/10000] Train Loss: 0.000096 Val Loss: nan\n",
      "Epoch [1599/10000] Train Loss: 0.000095 Val Loss: nan\n",
      "Epoch [1600/10000] Train Loss: 0.000093 Val Loss: nan\n",
      "Epoch [1601/10000] Train Loss: 0.000095 Val Loss: nan\n",
      "Epoch [1602/10000] Train Loss: 0.000094 Val Loss: nan\n",
      "Epoch [1603/10000] Train Loss: 0.000090 Val Loss: nan\n",
      "Epoch [1604/10000] Train Loss: 0.000099 Val Loss: nan\n",
      "Epoch [1605/10000] Train Loss: 0.000104 Val Loss: nan\n",
      "Epoch [1606/10000] Train Loss: 0.000097 Val Loss: nan\n",
      "Epoch [1607/10000] Train Loss: 0.000098 Val Loss: nan\n",
      "Epoch [1608/10000] Train Loss: 0.000101 Val Loss: nan\n",
      "Epoch [1609/10000] Train Loss: 0.000106 Val Loss: nan\n",
      "Epoch [1610/10000] Train Loss: 0.000100 Val Loss: nan\n",
      "Epoch [1611/10000] Train Loss: 0.000101 Val Loss: nan\n",
      "Epoch [1612/10000] Train Loss: 0.000100 Val Loss: nan\n",
      "Epoch [1613/10000] Train Loss: 0.000096 Val Loss: nan\n",
      "Epoch [1614/10000] Train Loss: 0.000097 Val Loss: nan\n",
      "Epoch [1615/10000] Train Loss: 0.000093 Val Loss: nan\n",
      "Epoch [1616/10000] Train Loss: 0.000098 Val Loss: nan\n",
      "Epoch [1617/10000] Train Loss: 0.000099 Val Loss: nan\n",
      "Epoch [1618/10000] Train Loss: 0.000099 Val Loss: nan\n",
      "Epoch [1619/10000] Train Loss: 0.000100 Val Loss: nan\n",
      "Epoch [1620/10000] Train Loss: 0.000100 Val Loss: nan\n",
      "Epoch [1621/10000] Train Loss: 0.000096 Val Loss: nan\n",
      "Epoch [1622/10000] Train Loss: 0.000103 Val Loss: nan\n",
      "Epoch [1623/10000] Train Loss: 0.000102 Val Loss: nan\n",
      "Epoch [1624/10000] Train Loss: 0.000096 Val Loss: nan\n",
      "Epoch [1625/10000] Train Loss: 0.000095 Val Loss: nan\n",
      "Epoch [1626/10000] Train Loss: 0.000096 Val Loss: nan\n",
      "Epoch [1627/10000] Train Loss: 0.000092 Val Loss: nan\n",
      "Epoch [1628/10000] Train Loss: 0.000091 Val Loss: nan\n",
      "Epoch [1629/10000] Train Loss: 0.000090 Val Loss: nan\n",
      "Epoch [1630/10000] Train Loss: 0.000086 Val Loss: nan\n",
      "Epoch [1631/10000] Train Loss: 0.000087 Val Loss: nan\n",
      "Epoch [1632/10000] Train Loss: 0.000088 Val Loss: nan\n",
      "Epoch [1633/10000] Train Loss: 0.000089 Val Loss: nan\n",
      "Epoch [1634/10000] Train Loss: 0.000091 Val Loss: nan\n",
      "Epoch [1635/10000] Train Loss: 0.000096 Val Loss: nan\n",
      "Epoch [1636/10000] Train Loss: 0.000091 Val Loss: nan\n",
      "Epoch [1637/10000] Train Loss: 0.000091 Val Loss: nan\n",
      "Epoch [1638/10000] Train Loss: 0.000089 Val Loss: nan\n",
      "Epoch [1639/10000] Train Loss: 0.000089 Val Loss: nan\n",
      "Epoch [1640/10000] Train Loss: 0.000090 Val Loss: nan\n",
      "Epoch [1641/10000] Train Loss: 0.000087 Val Loss: nan\n",
      "Epoch [1642/10000] Train Loss: 0.000087 Val Loss: nan\n",
      "Epoch [1643/10000] Train Loss: 0.000087 Val Loss: nan\n",
      "Epoch [1644/10000] Train Loss: 0.000092 Val Loss: nan\n",
      "Epoch [1645/10000] Train Loss: 0.000098 Val Loss: nan\n",
      "Epoch [1646/10000] Train Loss: 0.000096 Val Loss: nan\n",
      "Epoch [1647/10000] Train Loss: 0.000107 Val Loss: nan\n",
      "Epoch [1648/10000] Train Loss: 0.000098 Val Loss: nan\n",
      "Epoch [1649/10000] Train Loss: 0.000099 Val Loss: nan\n",
      "Epoch [1650/10000] Train Loss: 0.000095 Val Loss: nan\n",
      "Epoch [1651/10000] Train Loss: 0.000098 Val Loss: nan\n",
      "Epoch [1652/10000] Train Loss: 0.000100 Val Loss: nan\n",
      "Epoch [1653/10000] Train Loss: 0.000094 Val Loss: nan\n",
      "Epoch [1654/10000] Train Loss: 0.000099 Val Loss: nan\n",
      "Epoch [1655/10000] Train Loss: 0.000097 Val Loss: nan\n",
      "Epoch [1656/10000] Train Loss: 0.000092 Val Loss: nan\n",
      "Epoch [1657/10000] Train Loss: 0.000094 Val Loss: nan\n",
      "Epoch [1658/10000] Train Loss: 0.000091 Val Loss: nan\n",
      "Epoch [1659/10000] Train Loss: 0.000091 Val Loss: nan\n",
      "Epoch [1660/10000] Train Loss: 0.000094 Val Loss: nan\n",
      "Epoch [1661/10000] Train Loss: 0.000095 Val Loss: nan\n",
      "Epoch [1662/10000] Train Loss: 0.000094 Val Loss: nan\n",
      "Epoch [1663/10000] Train Loss: 0.000090 Val Loss: nan\n",
      "Epoch [1664/10000] Train Loss: 0.000089 Val Loss: nan\n",
      "Epoch [1665/10000] Train Loss: 0.000091 Val Loss: nan\n",
      "Epoch [1666/10000] Train Loss: 0.000096 Val Loss: nan\n",
      "Epoch [1667/10000] Train Loss: 0.000095 Val Loss: nan\n",
      "Epoch [1668/10000] Train Loss: 0.000096 Val Loss: nan\n",
      "Epoch [1669/10000] Train Loss: 0.000094 Val Loss: nan\n",
      "Epoch [1670/10000] Train Loss: 0.000093 Val Loss: nan\n",
      "Epoch [1671/10000] Train Loss: 0.000097 Val Loss: nan\n",
      "Epoch [1672/10000] Train Loss: 0.000096 Val Loss: nan\n",
      "Epoch [1673/10000] Train Loss: 0.000091 Val Loss: nan\n",
      "Epoch [1674/10000] Train Loss: 0.000092 Val Loss: nan\n",
      "Epoch [1675/10000] Train Loss: 0.000093 Val Loss: nan\n",
      "Epoch [1676/10000] Train Loss: 0.000091 Val Loss: nan\n",
      "Epoch [1677/10000] Train Loss: 0.000096 Val Loss: nan\n",
      "Epoch [1678/10000] Train Loss: 0.000104 Val Loss: nan\n",
      "Epoch [1679/10000] Train Loss: 0.000100 Val Loss: nan\n",
      "Epoch [1680/10000] Train Loss: 0.000093 Val Loss: nan\n",
      "Epoch [1681/10000] Train Loss: 0.000097 Val Loss: nan\n",
      "Epoch [1682/10000] Train Loss: 0.000100 Val Loss: nan\n",
      "Epoch [1683/10000] Train Loss: 0.000097 Val Loss: nan\n",
      "Epoch [1684/10000] Train Loss: 0.000091 Val Loss: nan\n",
      "Epoch [1685/10000] Train Loss: 0.000087 Val Loss: nan\n",
      "Epoch [1686/10000] Train Loss: 0.000096 Val Loss: nan\n",
      "Epoch [1687/10000] Train Loss: 0.000093 Val Loss: nan\n",
      "Epoch [1688/10000] Train Loss: 0.000090 Val Loss: nan\n",
      "Epoch [1689/10000] Train Loss: 0.000090 Val Loss: nan\n",
      "Epoch [1690/10000] Train Loss: 0.000090 Val Loss: nan\n",
      "Epoch [1691/10000] Train Loss: 0.000089 Val Loss: nan\n",
      "Epoch [1692/10000] Train Loss: 0.000094 Val Loss: nan\n",
      "Epoch [1693/10000] Train Loss: 0.000105 Val Loss: nan\n",
      "Epoch [1694/10000] Train Loss: 0.000100 Val Loss: nan\n",
      "Epoch [1695/10000] Train Loss: 0.000103 Val Loss: nan\n",
      "Epoch [1696/10000] Train Loss: 0.000100 Val Loss: nan\n",
      "Epoch [1697/10000] Train Loss: 0.000096 Val Loss: nan\n",
      "Epoch [1698/10000] Train Loss: 0.000091 Val Loss: nan\n",
      "Epoch [1699/10000] Train Loss: 0.000090 Val Loss: nan\n",
      "Epoch [1700/10000] Train Loss: 0.000087 Val Loss: nan\n",
      "Epoch [1701/10000] Train Loss: 0.000085 Val Loss: nan\n",
      "Epoch [1702/10000] Train Loss: 0.000088 Val Loss: nan\n",
      "Epoch [1703/10000] Train Loss: 0.000088 Val Loss: nan\n",
      "Epoch [1704/10000] Train Loss: 0.000089 Val Loss: nan\n",
      "Epoch [1705/10000] Train Loss: 0.000086 Val Loss: nan\n",
      "Epoch [1706/10000] Train Loss: 0.000082 Val Loss: nan\n",
      "Epoch [1707/10000] Train Loss: 0.000083 Val Loss: nan\n",
      "Epoch [1708/10000] Train Loss: 0.000086 Val Loss: nan\n",
      "Epoch [1709/10000] Train Loss: 0.000092 Val Loss: nan\n",
      "Epoch [1710/10000] Train Loss: 0.000090 Val Loss: nan\n",
      "Epoch [1711/10000] Train Loss: 0.000093 Val Loss: nan\n",
      "Epoch [1712/10000] Train Loss: 0.000091 Val Loss: nan\n",
      "Epoch [1713/10000] Train Loss: 0.000100 Val Loss: nan\n",
      "Epoch [1714/10000] Train Loss: 0.000104 Val Loss: nan\n",
      "Epoch [1715/10000] Train Loss: 0.000105 Val Loss: nan\n",
      "Epoch [1716/10000] Train Loss: 0.000102 Val Loss: nan\n",
      "Epoch [1717/10000] Train Loss: 0.000105 Val Loss: nan\n",
      "Epoch [1718/10000] Train Loss: 0.000117 Val Loss: nan\n",
      "Epoch [1719/10000] Train Loss: 0.000126 Val Loss: nan\n",
      "Epoch [1720/10000] Train Loss: 0.000115 Val Loss: nan\n",
      "Epoch [1721/10000] Train Loss: 0.000105 Val Loss: nan\n",
      "Epoch [1722/10000] Train Loss: 0.000104 Val Loss: nan\n",
      "Epoch [1723/10000] Train Loss: 0.000109 Val Loss: nan\n",
      "Epoch [1724/10000] Train Loss: 0.000106 Val Loss: nan\n",
      "Epoch [1725/10000] Train Loss: 0.000105 Val Loss: nan\n",
      "Epoch [1726/10000] Train Loss: 0.000106 Val Loss: nan\n",
      "Epoch [1727/10000] Train Loss: 0.000107 Val Loss: nan\n",
      "Epoch [1728/10000] Train Loss: 0.000095 Val Loss: nan\n",
      "Epoch [1729/10000] Train Loss: 0.000090 Val Loss: nan\n",
      "Epoch [1730/10000] Train Loss: 0.000091 Val Loss: nan\n",
      "Epoch [1731/10000] Train Loss: 0.000093 Val Loss: nan\n",
      "Epoch [1732/10000] Train Loss: 0.000091 Val Loss: nan\n",
      "Epoch [1733/10000] Train Loss: 0.000091 Val Loss: nan\n",
      "Epoch [1734/10000] Train Loss: 0.000094 Val Loss: nan\n",
      "Epoch [1735/10000] Train Loss: 0.000096 Val Loss: nan\n",
      "Epoch [1736/10000] Train Loss: 0.000095 Val Loss: nan\n",
      "Epoch [1737/10000] Train Loss: 0.000093 Val Loss: nan\n",
      "Epoch [1738/10000] Train Loss: 0.000089 Val Loss: nan\n",
      "Epoch [1739/10000] Train Loss: 0.000087 Val Loss: nan\n",
      "Epoch [1740/10000] Train Loss: 0.000081 Val Loss: nan\n",
      "Epoch [1741/10000] Train Loss: 0.000088 Val Loss: nan\n",
      "Epoch [1742/10000] Train Loss: 0.000092 Val Loss: nan\n",
      "Epoch [1743/10000] Train Loss: 0.000091 Val Loss: nan\n",
      "Epoch [1744/10000] Train Loss: 0.000099 Val Loss: nan\n",
      "Epoch [1745/10000] Train Loss: 0.000092 Val Loss: nan\n",
      "Epoch [1746/10000] Train Loss: 0.000094 Val Loss: nan\n",
      "Epoch [1747/10000] Train Loss: 0.000096 Val Loss: nan\n",
      "Epoch [1748/10000] Train Loss: 0.000092 Val Loss: nan\n",
      "Epoch [1749/10000] Train Loss: 0.000089 Val Loss: nan\n",
      "Epoch [1750/10000] Train Loss: 0.000096 Val Loss: nan\n",
      "Epoch [1751/10000] Train Loss: 0.000091 Val Loss: nan\n",
      "Epoch [1752/10000] Train Loss: 0.000102 Val Loss: nan\n",
      "Epoch [1753/10000] Train Loss: 0.000123 Val Loss: nan\n",
      "Epoch [1754/10000] Train Loss: 0.000136 Val Loss: nan\n",
      "Epoch [1755/10000] Train Loss: 0.000108 Val Loss: nan\n",
      "Epoch [1756/10000] Train Loss: 0.000107 Val Loss: nan\n",
      "Epoch [1757/10000] Train Loss: 0.000129 Val Loss: nan\n",
      "Epoch [1758/10000] Train Loss: 0.000112 Val Loss: nan\n",
      "Epoch [1759/10000] Train Loss: 0.000091 Val Loss: nan\n",
      "Epoch [1760/10000] Train Loss: 0.000097 Val Loss: nan\n",
      "Epoch [1761/10000] Train Loss: 0.000092 Val Loss: nan\n",
      "Epoch [1762/10000] Train Loss: 0.000086 Val Loss: nan\n",
      "Epoch [1763/10000] Train Loss: 0.000091 Val Loss: nan\n",
      "Epoch [1764/10000] Train Loss: 0.000085 Val Loss: nan\n",
      "Epoch [1765/10000] Train Loss: 0.000078 Val Loss: nan\n",
      "Epoch [1766/10000] Train Loss: 0.000079 Val Loss: nan\n",
      "Epoch [1767/10000] Train Loss: 0.000082 Val Loss: nan\n",
      "Epoch [1768/10000] Train Loss: 0.000077 Val Loss: nan\n",
      "Epoch [1769/10000] Train Loss: 0.000075 Val Loss: nan\n",
      "Epoch [1770/10000] Train Loss: 0.000083 Val Loss: nan\n",
      "Epoch [1771/10000] Train Loss: 0.000086 Val Loss: nan\n",
      "Epoch [1772/10000] Train Loss: 0.000091 Val Loss: nan\n",
      "Epoch [1773/10000] Train Loss: 0.000126 Val Loss: nan\n",
      "Epoch [1774/10000] Train Loss: 0.000103 Val Loss: nan\n",
      "Epoch [1775/10000] Train Loss: 0.000094 Val Loss: nan\n",
      "Epoch [1776/10000] Train Loss: 0.000080 Val Loss: nan\n",
      "Epoch [1777/10000] Train Loss: 0.000090 Val Loss: nan\n",
      "Epoch [1778/10000] Train Loss: 0.000088 Val Loss: nan\n",
      "Epoch [1779/10000] Train Loss: 0.000091 Val Loss: nan\n",
      "Epoch [1780/10000] Train Loss: 0.000098 Val Loss: nan\n",
      "Epoch [1781/10000] Train Loss: 0.000100 Val Loss: nan\n",
      "Epoch [1782/10000] Train Loss: 0.000097 Val Loss: nan\n",
      "Epoch [1783/10000] Train Loss: 0.000076 Val Loss: nan\n",
      "Epoch [1784/10000] Train Loss: 0.000079 Val Loss: nan\n",
      "Epoch [1785/10000] Train Loss: 0.000079 Val Loss: nan\n",
      "Epoch [1786/10000] Train Loss: 0.000074 Val Loss: nan\n",
      "Epoch [1787/10000] Train Loss: 0.000069 Val Loss: nan\n",
      "Epoch [1788/10000] Train Loss: 0.000073 Val Loss: nan\n",
      "Epoch [1789/10000] Train Loss: 0.000086 Val Loss: nan\n",
      "Epoch [1790/10000] Train Loss: 0.000082 Val Loss: nan\n",
      "Epoch [1791/10000] Train Loss: 0.000081 Val Loss: nan\n",
      "Epoch [1792/10000] Train Loss: 0.000085 Val Loss: nan\n",
      "Epoch [1793/10000] Train Loss: 0.000097 Val Loss: nan\n",
      "Epoch [1794/10000] Train Loss: 0.000110 Val Loss: nan\n",
      "Epoch [1795/10000] Train Loss: 0.000122 Val Loss: nan\n",
      "Epoch [1796/10000] Train Loss: 0.000119 Val Loss: nan\n",
      "Epoch [1797/10000] Train Loss: 0.000099 Val Loss: nan\n",
      "Epoch [1798/10000] Train Loss: 0.000095 Val Loss: nan\n",
      "Epoch [1799/10000] Train Loss: 0.000082 Val Loss: nan\n",
      "Epoch [1800/10000] Train Loss: 0.000085 Val Loss: nan\n",
      "Epoch [1801/10000] Train Loss: 0.000085 Val Loss: nan\n",
      "Epoch [1802/10000] Train Loss: 0.000080 Val Loss: nan\n",
      "Epoch [1803/10000] Train Loss: 0.000075 Val Loss: nan\n",
      "Epoch [1804/10000] Train Loss: 0.000073 Val Loss: nan\n",
      "Epoch [1805/10000] Train Loss: 0.000074 Val Loss: nan\n",
      "Epoch [1806/10000] Train Loss: 0.000082 Val Loss: nan\n",
      "Epoch [1807/10000] Train Loss: 0.000109 Val Loss: nan\n",
      "Epoch [1808/10000] Train Loss: 0.000095 Val Loss: nan\n",
      "Epoch [1809/10000] Train Loss: 0.000092 Val Loss: nan\n",
      "Epoch [1810/10000] Train Loss: 0.000081 Val Loss: nan\n",
      "Epoch [1811/10000] Train Loss: 0.000073 Val Loss: nan\n",
      "Epoch [1812/10000] Train Loss: 0.000073 Val Loss: nan\n",
      "Epoch [1813/10000] Train Loss: 0.000067 Val Loss: nan\n",
      "Epoch [1814/10000] Train Loss: 0.000063 Val Loss: nan\n",
      "Epoch [1815/10000] Train Loss: 0.000066 Val Loss: nan\n",
      "Epoch [1816/10000] Train Loss: 0.000074 Val Loss: nan\n",
      "Epoch [1817/10000] Train Loss: 0.000082 Val Loss: nan\n",
      "Epoch [1818/10000] Train Loss: 0.000080 Val Loss: nan\n",
      "Epoch [1819/10000] Train Loss: 0.000083 Val Loss: nan\n",
      "Epoch [1820/10000] Train Loss: 0.000086 Val Loss: nan\n",
      "Epoch [1821/10000] Train Loss: 0.000100 Val Loss: nan\n",
      "Epoch [1822/10000] Train Loss: 0.000093 Val Loss: nan\n",
      "Epoch [1823/10000] Train Loss: 0.000110 Val Loss: nan\n",
      "Epoch [1824/10000] Train Loss: 0.000093 Val Loss: nan\n",
      "Epoch [1825/10000] Train Loss: 0.000091 Val Loss: nan\n",
      "Epoch [1826/10000] Train Loss: 0.000093 Val Loss: nan\n",
      "Epoch [1827/10000] Train Loss: 0.000096 Val Loss: nan\n",
      "Epoch [1828/10000] Train Loss: 0.000085 Val Loss: nan\n",
      "Epoch [1829/10000] Train Loss: 0.000097 Val Loss: nan\n",
      "Epoch [1830/10000] Train Loss: 0.000097 Val Loss: nan\n",
      "Epoch [1831/10000] Train Loss: 0.000075 Val Loss: nan\n",
      "Epoch [1832/10000] Train Loss: 0.000077 Val Loss: nan\n",
      "Epoch [1833/10000] Train Loss: 0.000077 Val Loss: nan\n",
      "Epoch [1834/10000] Train Loss: 0.000083 Val Loss: nan\n",
      "Epoch [1835/10000] Train Loss: 0.000074 Val Loss: nan\n",
      "Epoch [1836/10000] Train Loss: 0.000073 Val Loss: nan\n",
      "Epoch [1837/10000] Train Loss: 0.000078 Val Loss: nan\n",
      "Epoch [1838/10000] Train Loss: 0.000079 Val Loss: nan\n",
      "Epoch [1839/10000] Train Loss: 0.000078 Val Loss: nan\n",
      "Epoch [1840/10000] Train Loss: 0.000076 Val Loss: nan\n",
      "Epoch [1841/10000] Train Loss: 0.000075 Val Loss: nan\n",
      "Epoch [1842/10000] Train Loss: 0.000071 Val Loss: nan\n",
      "Epoch [1843/10000] Train Loss: 0.000075 Val Loss: nan\n",
      "Epoch [1844/10000] Train Loss: 0.000080 Val Loss: nan\n",
      "Epoch [1845/10000] Train Loss: 0.000083 Val Loss: nan\n",
      "Epoch [1846/10000] Train Loss: 0.000087 Val Loss: nan\n",
      "Epoch [1847/10000] Train Loss: 0.000076 Val Loss: nan\n",
      "Epoch [1848/10000] Train Loss: 0.000073 Val Loss: nan\n",
      "Epoch [1849/10000] Train Loss: 0.000077 Val Loss: nan\n",
      "Epoch [1850/10000] Train Loss: 0.000069 Val Loss: nan\n",
      "Epoch [1851/10000] Train Loss: 0.000068 Val Loss: nan\n",
      "Epoch [1852/10000] Train Loss: 0.000068 Val Loss: nan\n",
      "Epoch [1853/10000] Train Loss: 0.000071 Val Loss: nan\n",
      "Epoch [1854/10000] Train Loss: 0.000079 Val Loss: nan\n",
      "Epoch [1855/10000] Train Loss: 0.000104 Val Loss: nan\n",
      "Epoch [1856/10000] Train Loss: 0.000129 Val Loss: nan\n",
      "Epoch [1857/10000] Train Loss: 0.000134 Val Loss: nan\n",
      "Epoch [1858/10000] Train Loss: 0.000130 Val Loss: nan\n",
      "Epoch [1859/10000] Train Loss: 0.000163 Val Loss: nan\n",
      "Epoch [1860/10000] Train Loss: 0.000135 Val Loss: nan\n",
      "Epoch [1861/10000] Train Loss: 0.000137 Val Loss: nan\n",
      "Epoch [1862/10000] Train Loss: 0.000139 Val Loss: nan\n",
      "Epoch [1863/10000] Train Loss: 0.000129 Val Loss: nan\n",
      "Epoch [1864/10000] Train Loss: 0.000130 Val Loss: nan\n",
      "Epoch [1865/10000] Train Loss: 0.000138 Val Loss: nan\n",
      "Epoch [1866/10000] Train Loss: 0.000171 Val Loss: nan\n",
      "Epoch [1867/10000] Train Loss: 0.000238 Val Loss: nan\n",
      "Epoch [1868/10000] Train Loss: 0.000323 Val Loss: nan\n",
      "Epoch [1869/10000] Train Loss: 0.001019 Val Loss: nan\n",
      "Epoch [1870/10000] Train Loss: 0.001387 Val Loss: nan\n",
      "Epoch [1871/10000] Train Loss: 0.001845 Val Loss: nan\n",
      "Epoch [1872/10000] Train Loss: 0.004072 Val Loss: nan\n",
      "Epoch [1873/10000] Train Loss: 0.004421 Val Loss: nan\n",
      "Epoch [1874/10000] Train Loss: 0.003868 Val Loss: nan\n",
      "Epoch [1875/10000] Train Loss: 0.004143 Val Loss: nan\n",
      "Epoch [1876/10000] Train Loss: 0.003287 Val Loss: nan\n",
      "Epoch [1877/10000] Train Loss: 0.002289 Val Loss: nan\n",
      "Epoch [1878/10000] Train Loss: 0.001123 Val Loss: nan\n",
      "Epoch [1879/10000] Train Loss: 0.000616 Val Loss: nan\n",
      "Epoch [1880/10000] Train Loss: 0.000372 Val Loss: nan\n",
      "Epoch [1881/10000] Train Loss: 0.000236 Val Loss: nan\n",
      "Epoch [1882/10000] Train Loss: 0.000158 Val Loss: nan\n",
      "Epoch [1883/10000] Train Loss: 0.000129 Val Loss: nan\n",
      "Epoch [1884/10000] Train Loss: 0.000113 Val Loss: nan\n",
      "Epoch [1885/10000] Train Loss: 0.000098 Val Loss: nan\n",
      "Epoch [1886/10000] Train Loss: 0.000088 Val Loss: nan\n",
      "Epoch [1887/10000] Train Loss: 0.000081 Val Loss: nan\n",
      "Epoch [1888/10000] Train Loss: 0.000080 Val Loss: nan\n",
      "Epoch [1889/10000] Train Loss: 0.000079 Val Loss: nan\n",
      "Epoch [1890/10000] Train Loss: 0.000074 Val Loss: nan\n",
      "Epoch [1891/10000] Train Loss: 0.000069 Val Loss: nan\n",
      "Epoch [1892/10000] Train Loss: 0.000067 Val Loss: nan\n",
      "Epoch [1893/10000] Train Loss: 0.000065 Val Loss: nan\n",
      "Epoch [1894/10000] Train Loss: 0.000064 Val Loss: nan\n",
      "Epoch [1895/10000] Train Loss: 0.000063 Val Loss: nan\n",
      "Epoch [1896/10000] Train Loss: 0.000062 Val Loss: nan\n",
      "Epoch [1897/10000] Train Loss: 0.000059 Val Loss: nan\n",
      "Epoch [1898/10000] Train Loss: 0.000058 Val Loss: nan\n",
      "Epoch [1899/10000] Train Loss: 0.000057 Val Loss: nan\n",
      "Epoch [1900/10000] Train Loss: 0.000056 Val Loss: nan\n",
      "Epoch [1901/10000] Train Loss: 0.000057 Val Loss: nan\n",
      "Epoch [1902/10000] Train Loss: 0.000056 Val Loss: nan\n",
      "Epoch [1903/10000] Train Loss: 0.000053 Val Loss: nan\n",
      "Epoch [1904/10000] Train Loss: 0.000051 Val Loss: nan\n",
      "Epoch [1905/10000] Train Loss: 0.000053 Val Loss: nan\n",
      "Epoch [1906/10000] Train Loss: 0.000052 Val Loss: nan\n",
      "Epoch [1907/10000] Train Loss: 0.000051 Val Loss: nan\n",
      "Epoch [1908/10000] Train Loss: 0.000049 Val Loss: nan\n",
      "Epoch [1909/10000] Train Loss: 0.000049 Val Loss: nan\n",
      "Epoch [1910/10000] Train Loss: 0.000050 Val Loss: nan\n",
      "Epoch [1911/10000] Train Loss: 0.000050 Val Loss: nan\n",
      "Epoch [1912/10000] Train Loss: 0.000051 Val Loss: nan\n",
      "Epoch [1913/10000] Train Loss: 0.000049 Val Loss: nan\n",
      "Epoch [1914/10000] Train Loss: 0.000051 Val Loss: nan\n",
      "Epoch [1915/10000] Train Loss: 0.000051 Val Loss: nan\n",
      "Epoch [1916/10000] Train Loss: 0.000049 Val Loss: nan\n",
      "Epoch [1917/10000] Train Loss: 0.000048 Val Loss: nan\n",
      "Epoch [1918/10000] Train Loss: 0.000049 Val Loss: nan\n",
      "Epoch [1919/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [1920/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [1921/10000] Train Loss: 0.000045 Val Loss: nan\n",
      "Epoch [1922/10000] Train Loss: 0.000045 Val Loss: nan\n",
      "Epoch [1923/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [1924/10000] Train Loss: 0.000047 Val Loss: nan\n",
      "Epoch [1925/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [1926/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [1927/10000] Train Loss: 0.000045 Val Loss: nan\n",
      "Epoch [1928/10000] Train Loss: 0.000048 Val Loss: nan\n",
      "Epoch [1929/10000] Train Loss: 0.000053 Val Loss: nan\n",
      "Epoch [1930/10000] Train Loss: 0.000048 Val Loss: nan\n",
      "Epoch [1931/10000] Train Loss: 0.000048 Val Loss: nan\n",
      "Epoch [1932/10000] Train Loss: 0.000047 Val Loss: nan\n",
      "Epoch [1933/10000] Train Loss: 0.000047 Val Loss: nan\n",
      "Epoch [1934/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [1935/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [1936/10000] Train Loss: 0.000047 Val Loss: nan\n",
      "Epoch [1937/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [1938/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [1939/10000] Train Loss: 0.000047 Val Loss: nan\n",
      "Epoch [1940/10000] Train Loss: 0.000044 Val Loss: nan\n",
      "Epoch [1941/10000] Train Loss: 0.000043 Val Loss: nan\n",
      "Epoch [1942/10000] Train Loss: 0.000042 Val Loss: nan\n",
      "Epoch [1943/10000] Train Loss: 0.000042 Val Loss: nan\n",
      "Epoch [1944/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [1945/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [1946/10000] Train Loss: 0.000048 Val Loss: nan\n",
      "Epoch [1947/10000] Train Loss: 0.000051 Val Loss: nan\n",
      "Epoch [1948/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [1949/10000] Train Loss: 0.000045 Val Loss: nan\n",
      "Epoch [1950/10000] Train Loss: 0.000044 Val Loss: nan\n",
      "Epoch [1951/10000] Train Loss: 0.000047 Val Loss: nan\n",
      "Epoch [1952/10000] Train Loss: 0.000045 Val Loss: nan\n",
      "Epoch [1953/10000] Train Loss: 0.000044 Val Loss: nan\n",
      "Epoch [1954/10000] Train Loss: 0.000042 Val Loss: nan\n",
      "Epoch [1955/10000] Train Loss: 0.000043 Val Loss: nan\n",
      "Epoch [1956/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [1957/10000] Train Loss: 0.000047 Val Loss: nan\n",
      "Epoch [1958/10000] Train Loss: 0.000041 Val Loss: nan\n",
      "Epoch [1959/10000] Train Loss: 0.000039 Val Loss: nan\n",
      "Epoch [1960/10000] Train Loss: 0.000038 Val Loss: nan\n",
      "Epoch [1961/10000] Train Loss: 0.000039 Val Loss: nan\n",
      "Epoch [1962/10000] Train Loss: 0.000039 Val Loss: nan\n",
      "Epoch [1963/10000] Train Loss: 0.000041 Val Loss: nan\n",
      "Epoch [1964/10000] Train Loss: 0.000040 Val Loss: nan\n",
      "Epoch [1965/10000] Train Loss: 0.000039 Val Loss: nan\n",
      "Epoch [1966/10000] Train Loss: 0.000039 Val Loss: nan\n",
      "Epoch [1967/10000] Train Loss: 0.000043 Val Loss: nan\n",
      "Epoch [1968/10000] Train Loss: 0.000044 Val Loss: nan\n",
      "Epoch [1969/10000] Train Loss: 0.000042 Val Loss: nan\n",
      "Epoch [1970/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [1971/10000] Train Loss: 0.000055 Val Loss: nan\n",
      "Epoch [1972/10000] Train Loss: 0.000058 Val Loss: nan\n",
      "Epoch [1973/10000] Train Loss: 0.000054 Val Loss: nan\n",
      "Epoch [1974/10000] Train Loss: 0.000060 Val Loss: nan\n",
      "Epoch [1975/10000] Train Loss: 0.000069 Val Loss: nan\n",
      "Epoch [1976/10000] Train Loss: 0.000055 Val Loss: nan\n",
      "Epoch [1977/10000] Train Loss: 0.000059 Val Loss: nan\n",
      "Epoch [1978/10000] Train Loss: 0.000068 Val Loss: nan\n",
      "Epoch [1979/10000] Train Loss: 0.000057 Val Loss: nan\n",
      "Epoch [1980/10000] Train Loss: 0.000053 Val Loss: nan\n",
      "Epoch [1981/10000] Train Loss: 0.000051 Val Loss: nan\n",
      "Epoch [1982/10000] Train Loss: 0.000051 Val Loss: nan\n",
      "Epoch [1983/10000] Train Loss: 0.000048 Val Loss: nan\n",
      "Epoch [1984/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [1985/10000] Train Loss: 0.000043 Val Loss: nan\n",
      "Epoch [1986/10000] Train Loss: 0.000042 Val Loss: nan\n",
      "Epoch [1987/10000] Train Loss: 0.000041 Val Loss: nan\n",
      "Epoch [1988/10000] Train Loss: 0.000040 Val Loss: nan\n",
      "Epoch [1989/10000] Train Loss: 0.000040 Val Loss: nan\n",
      "Epoch [1990/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [1991/10000] Train Loss: 0.000052 Val Loss: nan\n",
      "Epoch [1992/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [1993/10000] Train Loss: 0.000050 Val Loss: nan\n",
      "Epoch [1994/10000] Train Loss: 0.000050 Val Loss: nan\n",
      "Epoch [1995/10000] Train Loss: 0.000058 Val Loss: nan\n",
      "Epoch [1996/10000] Train Loss: 0.000057 Val Loss: nan\n",
      "Epoch [1997/10000] Train Loss: 0.000055 Val Loss: nan\n",
      "Epoch [1998/10000] Train Loss: 0.000051 Val Loss: nan\n",
      "Epoch [1999/10000] Train Loss: 0.000047 Val Loss: nan\n",
      "Epoch [2000/10000] Train Loss: 0.000043 Val Loss: nan\n",
      "Epoch [2001/10000] Train Loss: 0.000043 Val Loss: nan\n",
      "Epoch [2002/10000] Train Loss: 0.000042 Val Loss: nan\n",
      "Epoch [2003/10000] Train Loss: 0.000042 Val Loss: nan\n",
      "Epoch [2004/10000] Train Loss: 0.000043 Val Loss: nan\n",
      "Epoch [2005/10000] Train Loss: 0.000041 Val Loss: nan\n",
      "Epoch [2006/10000] Train Loss: 0.000038 Val Loss: nan\n",
      "Epoch [2007/10000] Train Loss: 0.000038 Val Loss: nan\n",
      "Epoch [2008/10000] Train Loss: 0.000039 Val Loss: nan\n",
      "Epoch [2009/10000] Train Loss: 0.000039 Val Loss: nan\n",
      "Epoch [2010/10000] Train Loss: 0.000037 Val Loss: nan\n",
      "Epoch [2011/10000] Train Loss: 0.000038 Val Loss: nan\n",
      "Epoch [2012/10000] Train Loss: 0.000038 Val Loss: nan\n",
      "Epoch [2013/10000] Train Loss: 0.000041 Val Loss: nan\n",
      "Epoch [2014/10000] Train Loss: 0.000041 Val Loss: nan\n",
      "Epoch [2015/10000] Train Loss: 0.000040 Val Loss: nan\n",
      "Epoch [2016/10000] Train Loss: 0.000042 Val Loss: nan\n",
      "Epoch [2017/10000] Train Loss: 0.000044 Val Loss: nan\n",
      "Epoch [2018/10000] Train Loss: 0.000044 Val Loss: nan\n",
      "Epoch [2019/10000] Train Loss: 0.000044 Val Loss: nan\n",
      "Epoch [2020/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [2021/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [2022/10000] Train Loss: 0.000049 Val Loss: nan\n",
      "Epoch [2023/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [2024/10000] Train Loss: 0.000045 Val Loss: nan\n",
      "Epoch [2025/10000] Train Loss: 0.000052 Val Loss: nan\n",
      "Epoch [2026/10000] Train Loss: 0.000051 Val Loss: nan\n",
      "Epoch [2027/10000] Train Loss: 0.000047 Val Loss: nan\n",
      "Epoch [2028/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [2029/10000] Train Loss: 0.000044 Val Loss: nan\n",
      "Epoch [2030/10000] Train Loss: 0.000044 Val Loss: nan\n",
      "Epoch [2031/10000] Train Loss: 0.000041 Val Loss: nan\n",
      "Epoch [2032/10000] Train Loss: 0.000043 Val Loss: nan\n",
      "Epoch [2033/10000] Train Loss: 0.000041 Val Loss: nan\n",
      "Epoch [2034/10000] Train Loss: 0.000044 Val Loss: nan\n",
      "Epoch [2035/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [2036/10000] Train Loss: 0.000053 Val Loss: nan\n",
      "Epoch [2037/10000] Train Loss: 0.000050 Val Loss: nan\n",
      "Epoch [2038/10000] Train Loss: 0.000054 Val Loss: nan\n",
      "Epoch [2039/10000] Train Loss: 0.000060 Val Loss: nan\n",
      "Epoch [2040/10000] Train Loss: 0.000056 Val Loss: nan\n",
      "Epoch [2041/10000] Train Loss: 0.000051 Val Loss: nan\n",
      "Epoch [2042/10000] Train Loss: 0.000066 Val Loss: nan\n",
      "Epoch [2043/10000] Train Loss: 0.000066 Val Loss: nan\n",
      "Epoch [2044/10000] Train Loss: 0.000088 Val Loss: nan\n",
      "Epoch [2045/10000] Train Loss: 0.000069 Val Loss: nan\n",
      "Epoch [2046/10000] Train Loss: 0.000066 Val Loss: nan\n",
      "Epoch [2047/10000] Train Loss: 0.000067 Val Loss: nan\n",
      "Epoch [2048/10000] Train Loss: 0.000065 Val Loss: nan\n",
      "Epoch [2049/10000] Train Loss: 0.000071 Val Loss: nan\n",
      "Epoch [2050/10000] Train Loss: 0.000066 Val Loss: nan\n",
      "Epoch [2051/10000] Train Loss: 0.000054 Val Loss: nan\n",
      "Epoch [2052/10000] Train Loss: 0.000059 Val Loss: nan\n",
      "Epoch [2053/10000] Train Loss: 0.000052 Val Loss: nan\n",
      "Epoch [2054/10000] Train Loss: 0.000049 Val Loss: nan\n",
      "Epoch [2055/10000] Train Loss: 0.000045 Val Loss: nan\n",
      "Epoch [2056/10000] Train Loss: 0.000041 Val Loss: nan\n",
      "Epoch [2057/10000] Train Loss: 0.000042 Val Loss: nan\n",
      "Epoch [2058/10000] Train Loss: 0.000041 Val Loss: nan\n",
      "Epoch [2059/10000] Train Loss: 0.000040 Val Loss: nan\n",
      "Epoch [2060/10000] Train Loss: 0.000039 Val Loss: nan\n",
      "Epoch [2061/10000] Train Loss: 0.000041 Val Loss: nan\n",
      "Epoch [2062/10000] Train Loss: 0.000039 Val Loss: nan\n",
      "Epoch [2063/10000] Train Loss: 0.000042 Val Loss: nan\n",
      "Epoch [2064/10000] Train Loss: 0.000045 Val Loss: nan\n",
      "Epoch [2065/10000] Train Loss: 0.000043 Val Loss: nan\n",
      "Epoch [2066/10000] Train Loss: 0.000040 Val Loss: nan\n",
      "Epoch [2067/10000] Train Loss: 0.000042 Val Loss: nan\n",
      "Epoch [2068/10000] Train Loss: 0.000039 Val Loss: nan\n",
      "Epoch [2069/10000] Train Loss: 0.000039 Val Loss: nan\n",
      "Epoch [2070/10000] Train Loss: 0.000038 Val Loss: nan\n",
      "Epoch [2071/10000] Train Loss: 0.000036 Val Loss: nan\n",
      "Epoch [2072/10000] Train Loss: 0.000039 Val Loss: nan\n",
      "Epoch [2073/10000] Train Loss: 0.000047 Val Loss: nan\n",
      "Epoch [2074/10000] Train Loss: 0.000047 Val Loss: nan\n",
      "Epoch [2075/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [2076/10000] Train Loss: 0.000047 Val Loss: nan\n",
      "Epoch [2077/10000] Train Loss: 0.000043 Val Loss: nan\n",
      "Epoch [2078/10000] Train Loss: 0.000044 Val Loss: nan\n",
      "Epoch [2079/10000] Train Loss: 0.000044 Val Loss: nan\n",
      "Epoch [2080/10000] Train Loss: 0.000040 Val Loss: nan\n",
      "Epoch [2081/10000] Train Loss: 0.000039 Val Loss: nan\n",
      "Epoch [2082/10000] Train Loss: 0.000036 Val Loss: nan\n",
      "Epoch [2083/10000] Train Loss: 0.000034 Val Loss: nan\n",
      "Epoch [2084/10000] Train Loss: 0.000033 Val Loss: nan\n",
      "Epoch [2085/10000] Train Loss: 0.000032 Val Loss: nan\n",
      "Epoch [2086/10000] Train Loss: 0.000033 Val Loss: nan\n",
      "Epoch [2087/10000] Train Loss: 0.000033 Val Loss: nan\n",
      "Epoch [2088/10000] Train Loss: 0.000036 Val Loss: nan\n",
      "Epoch [2089/10000] Train Loss: 0.000036 Val Loss: nan\n",
      "Epoch [2090/10000] Train Loss: 0.000039 Val Loss: nan\n",
      "Epoch [2091/10000] Train Loss: 0.000041 Val Loss: nan\n",
      "Epoch [2092/10000] Train Loss: 0.000044 Val Loss: nan\n",
      "Epoch [2093/10000] Train Loss: 0.000047 Val Loss: nan\n",
      "Epoch [2094/10000] Train Loss: 0.000050 Val Loss: nan\n",
      "Epoch [2095/10000] Train Loss: 0.000052 Val Loss: nan\n",
      "Epoch [2096/10000] Train Loss: 0.000053 Val Loss: nan\n",
      "Epoch [2097/10000] Train Loss: 0.000057 Val Loss: nan\n",
      "Epoch [2098/10000] Train Loss: 0.000083 Val Loss: nan\n",
      "Epoch [2099/10000] Train Loss: 0.000144 Val Loss: nan\n",
      "Epoch [2100/10000] Train Loss: 0.000173 Val Loss: nan\n",
      "Epoch [2101/10000] Train Loss: 0.000241 Val Loss: nan\n",
      "Epoch [2102/10000] Train Loss: 0.000153 Val Loss: nan\n",
      "Epoch [2103/10000] Train Loss: 0.000121 Val Loss: nan\n",
      "Epoch [2104/10000] Train Loss: 0.000105 Val Loss: nan\n",
      "Epoch [2105/10000] Train Loss: 0.000081 Val Loss: nan\n",
      "Epoch [2106/10000] Train Loss: 0.000073 Val Loss: nan\n",
      "Epoch [2107/10000] Train Loss: 0.000060 Val Loss: nan\n",
      "Epoch [2108/10000] Train Loss: 0.000064 Val Loss: nan\n",
      "Epoch [2109/10000] Train Loss: 0.000075 Val Loss: nan\n",
      "Epoch [2110/10000] Train Loss: 0.000069 Val Loss: nan\n",
      "Epoch [2111/10000] Train Loss: 0.000060 Val Loss: nan\n",
      "Epoch [2112/10000] Train Loss: 0.000065 Val Loss: nan\n",
      "Epoch [2113/10000] Train Loss: 0.000065 Val Loss: nan\n",
      "Epoch [2114/10000] Train Loss: 0.000066 Val Loss: nan\n",
      "Epoch [2115/10000] Train Loss: 0.000063 Val Loss: nan\n",
      "Epoch [2116/10000] Train Loss: 0.000057 Val Loss: nan\n",
      "Epoch [2117/10000] Train Loss: 0.000047 Val Loss: nan\n",
      "Epoch [2118/10000] Train Loss: 0.000042 Val Loss: nan\n",
      "Epoch [2119/10000] Train Loss: 0.000043 Val Loss: nan\n",
      "Epoch [2120/10000] Train Loss: 0.000041 Val Loss: nan\n",
      "Epoch [2121/10000] Train Loss: 0.000042 Val Loss: nan\n",
      "Epoch [2122/10000] Train Loss: 0.000044 Val Loss: nan\n",
      "Epoch [2123/10000] Train Loss: 0.000039 Val Loss: nan\n",
      "Epoch [2124/10000] Train Loss: 0.000036 Val Loss: nan\n",
      "Epoch [2125/10000] Train Loss: 0.000036 Val Loss: nan\n",
      "Epoch [2126/10000] Train Loss: 0.000036 Val Loss: nan\n",
      "Epoch [2127/10000] Train Loss: 0.000041 Val Loss: nan\n",
      "Epoch [2128/10000] Train Loss: 0.000053 Val Loss: nan\n",
      "Epoch [2129/10000] Train Loss: 0.000053 Val Loss: nan\n",
      "Epoch [2130/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [2131/10000] Train Loss: 0.000041 Val Loss: nan\n",
      "Epoch [2132/10000] Train Loss: 0.000039 Val Loss: nan\n",
      "Epoch [2133/10000] Train Loss: 0.000036 Val Loss: nan\n",
      "Epoch [2134/10000] Train Loss: 0.000034 Val Loss: nan\n",
      "Epoch [2135/10000] Train Loss: 0.000034 Val Loss: nan\n",
      "Epoch [2136/10000] Train Loss: 0.000034 Val Loss: nan\n",
      "Epoch [2137/10000] Train Loss: 0.000033 Val Loss: nan\n",
      "Epoch [2138/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [2139/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [2140/10000] Train Loss: 0.000031 Val Loss: nan\n",
      "Epoch [2141/10000] Train Loss: 0.000033 Val Loss: nan\n",
      "Epoch [2142/10000] Train Loss: 0.000037 Val Loss: nan\n",
      "Epoch [2143/10000] Train Loss: 0.000035 Val Loss: nan\n",
      "Epoch [2144/10000] Train Loss: 0.000036 Val Loss: nan\n",
      "Epoch [2145/10000] Train Loss: 0.000034 Val Loss: nan\n",
      "Epoch [2146/10000] Train Loss: 0.000038 Val Loss: nan\n",
      "Epoch [2147/10000] Train Loss: 0.000035 Val Loss: nan\n",
      "Epoch [2148/10000] Train Loss: 0.000031 Val Loss: nan\n",
      "Epoch [2149/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [2150/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [2151/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [2152/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [2153/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [2154/10000] Train Loss: 0.000032 Val Loss: nan\n",
      "Epoch [2155/10000] Train Loss: 0.000031 Val Loss: nan\n",
      "Epoch [2156/10000] Train Loss: 0.000032 Val Loss: nan\n",
      "Epoch [2157/10000] Train Loss: 0.000033 Val Loss: nan\n",
      "Epoch [2158/10000] Train Loss: 0.000041 Val Loss: nan\n",
      "Epoch [2159/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [2160/10000] Train Loss: 0.000053 Val Loss: nan\n",
      "Epoch [2161/10000] Train Loss: 0.000077 Val Loss: nan\n",
      "Epoch [2162/10000] Train Loss: 0.000083 Val Loss: nan\n",
      "Epoch [2163/10000] Train Loss: 0.000074 Val Loss: nan\n",
      "Epoch [2164/10000] Train Loss: 0.000083 Val Loss: nan\n",
      "Epoch [2165/10000] Train Loss: 0.000094 Val Loss: nan\n",
      "Epoch [2166/10000] Train Loss: 0.000081 Val Loss: nan\n",
      "Epoch [2167/10000] Train Loss: 0.000071 Val Loss: nan\n",
      "Epoch [2168/10000] Train Loss: 0.000077 Val Loss: nan\n",
      "Epoch [2169/10000] Train Loss: 0.000082 Val Loss: nan\n",
      "Epoch [2170/10000] Train Loss: 0.000080 Val Loss: nan\n",
      "Epoch [2171/10000] Train Loss: 0.000065 Val Loss: nan\n",
      "Epoch [2172/10000] Train Loss: 0.000052 Val Loss: nan\n",
      "Epoch [2173/10000] Train Loss: 0.000052 Val Loss: nan\n",
      "Epoch [2174/10000] Train Loss: 0.000051 Val Loss: nan\n",
      "Epoch [2175/10000] Train Loss: 0.000079 Val Loss: nan\n",
      "Epoch [2176/10000] Train Loss: 0.000097 Val Loss: nan\n",
      "Epoch [2177/10000] Train Loss: 0.000109 Val Loss: nan\n",
      "Epoch [2178/10000] Train Loss: 0.000137 Val Loss: nan\n",
      "Epoch [2179/10000] Train Loss: 0.000141 Val Loss: nan\n",
      "Epoch [2180/10000] Train Loss: 0.000125 Val Loss: nan\n",
      "Epoch [2181/10000] Train Loss: 0.000113 Val Loss: nan\n",
      "Epoch [2182/10000] Train Loss: 0.000113 Val Loss: nan\n",
      "Epoch [2183/10000] Train Loss: 0.000100 Val Loss: nan\n",
      "Epoch [2184/10000] Train Loss: 0.000115 Val Loss: nan\n",
      "Epoch [2185/10000] Train Loss: 0.000122 Val Loss: nan\n",
      "Epoch [2186/10000] Train Loss: 0.000099 Val Loss: nan\n",
      "Epoch [2187/10000] Train Loss: 0.000101 Val Loss: nan\n",
      "Epoch [2188/10000] Train Loss: 0.000086 Val Loss: nan\n",
      "Epoch [2189/10000] Train Loss: 0.000081 Val Loss: nan\n",
      "Epoch [2190/10000] Train Loss: 0.000084 Val Loss: nan\n",
      "Epoch [2191/10000] Train Loss: 0.000078 Val Loss: nan\n",
      "Epoch [2192/10000] Train Loss: 0.000059 Val Loss: nan\n",
      "Epoch [2193/10000] Train Loss: 0.000061 Val Loss: nan\n",
      "Epoch [2194/10000] Train Loss: 0.000056 Val Loss: nan\n",
      "Epoch [2195/10000] Train Loss: 0.000048 Val Loss: nan\n",
      "Epoch [2196/10000] Train Loss: 0.000043 Val Loss: nan\n",
      "Epoch [2197/10000] Train Loss: 0.000041 Val Loss: nan\n",
      "Epoch [2198/10000] Train Loss: 0.000043 Val Loss: nan\n",
      "Epoch [2199/10000] Train Loss: 0.000045 Val Loss: nan\n",
      "Epoch [2200/10000] Train Loss: 0.000044 Val Loss: nan\n",
      "Epoch [2201/10000] Train Loss: 0.000038 Val Loss: nan\n",
      "Epoch [2202/10000] Train Loss: 0.000035 Val Loss: nan\n",
      "Epoch [2203/10000] Train Loss: 0.000036 Val Loss: nan\n",
      "Epoch [2204/10000] Train Loss: 0.000033 Val Loss: nan\n",
      "Epoch [2205/10000] Train Loss: 0.000031 Val Loss: nan\n",
      "Epoch [2206/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [2207/10000] Train Loss: 0.000032 Val Loss: nan\n",
      "Epoch [2208/10000] Train Loss: 0.000032 Val Loss: nan\n",
      "Epoch [2209/10000] Train Loss: 0.000031 Val Loss: nan\n",
      "Epoch [2210/10000] Train Loss: 0.000031 Val Loss: nan\n",
      "Epoch [2211/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [2212/10000] Train Loss: 0.000032 Val Loss: nan\n",
      "Epoch [2213/10000] Train Loss: 0.000033 Val Loss: nan\n",
      "Epoch [2214/10000] Train Loss: 0.000034 Val Loss: nan\n",
      "Epoch [2215/10000] Train Loss: 0.000033 Val Loss: nan\n",
      "Epoch [2216/10000] Train Loss: 0.000033 Val Loss: nan\n",
      "Epoch [2217/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [2218/10000] Train Loss: 0.000031 Val Loss: nan\n",
      "Epoch [2219/10000] Train Loss: 0.000031 Val Loss: nan\n",
      "Epoch [2220/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [2221/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [2222/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [2223/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [2224/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [2225/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [2226/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [2227/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [2228/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [2229/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [2230/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [2231/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [2232/10000] Train Loss: 0.000034 Val Loss: nan\n",
      "Epoch [2233/10000] Train Loss: 0.000037 Val Loss: nan\n",
      "Epoch [2234/10000] Train Loss: 0.000049 Val Loss: nan\n",
      "Epoch [2235/10000] Train Loss: 0.000049 Val Loss: nan\n",
      "Epoch [2236/10000] Train Loss: 0.000072 Val Loss: nan\n",
      "Epoch [2237/10000] Train Loss: 0.000355 Val Loss: nan\n",
      "Epoch [2238/10000] Train Loss: 0.000502 Val Loss: nan\n",
      "Epoch [2239/10000] Train Loss: 0.000934 Val Loss: nan\n",
      "Epoch [2240/10000] Train Loss: 0.003081 Val Loss: nan\n",
      "Epoch [2241/10000] Train Loss: 0.003019 Val Loss: nan\n",
      "Epoch [2242/10000] Train Loss: 0.003219 Val Loss: nan\n",
      "Epoch [2243/10000] Train Loss: 0.004529 Val Loss: nan\n",
      "Epoch [2244/10000] Train Loss: 0.003328 Val Loss: nan\n",
      "Epoch [2245/10000] Train Loss: 0.001864 Val Loss: nan\n",
      "Epoch [2246/10000] Train Loss: 0.001074 Val Loss: nan\n",
      "Epoch [2247/10000] Train Loss: 0.000807 Val Loss: nan\n",
      "Epoch [2248/10000] Train Loss: 0.000752 Val Loss: nan\n",
      "Epoch [2249/10000] Train Loss: 0.000492 Val Loss: nan\n",
      "Epoch [2250/10000] Train Loss: 0.000342 Val Loss: nan\n",
      "Epoch [2251/10000] Train Loss: 0.000199 Val Loss: nan\n",
      "Epoch [2252/10000] Train Loss: 0.000155 Val Loss: nan\n",
      "Epoch [2253/10000] Train Loss: 0.000117 Val Loss: nan\n",
      "Epoch [2254/10000] Train Loss: 0.000090 Val Loss: nan\n",
      "Epoch [2255/10000] Train Loss: 0.000075 Val Loss: nan\n",
      "Epoch [2256/10000] Train Loss: 0.000068 Val Loss: nan\n",
      "Epoch [2257/10000] Train Loss: 0.000062 Val Loss: nan\n",
      "Epoch [2258/10000] Train Loss: 0.000057 Val Loss: nan\n",
      "Epoch [2259/10000] Train Loss: 0.000053 Val Loss: nan\n",
      "Epoch [2260/10000] Train Loss: 0.000050 Val Loss: nan\n",
      "Epoch [2261/10000] Train Loss: 0.000047 Val Loss: nan\n",
      "Epoch [2262/10000] Train Loss: 0.000044 Val Loss: nan\n",
      "Epoch [2263/10000] Train Loss: 0.000043 Val Loss: nan\n",
      "Epoch [2264/10000] Train Loss: 0.000040 Val Loss: nan\n",
      "Epoch [2265/10000] Train Loss: 0.000039 Val Loss: nan\n",
      "Epoch [2266/10000] Train Loss: 0.000038 Val Loss: nan\n",
      "Epoch [2267/10000] Train Loss: 0.000037 Val Loss: nan\n",
      "Epoch [2268/10000] Train Loss: 0.000037 Val Loss: nan\n",
      "Epoch [2269/10000] Train Loss: 0.000035 Val Loss: nan\n",
      "Epoch [2270/10000] Train Loss: 0.000037 Val Loss: nan\n",
      "Epoch [2271/10000] Train Loss: 0.000037 Val Loss: nan\n",
      "Epoch [2272/10000] Train Loss: 0.000035 Val Loss: nan\n",
      "Epoch [2273/10000] Train Loss: 0.000035 Val Loss: nan\n",
      "Epoch [2274/10000] Train Loss: 0.000035 Val Loss: nan\n",
      "Epoch [2275/10000] Train Loss: 0.000033 Val Loss: nan\n",
      "Epoch [2276/10000] Train Loss: 0.000031 Val Loss: nan\n",
      "Epoch [2277/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [2278/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [2279/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [2280/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [2281/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [2282/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [2283/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [2284/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [2285/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [2286/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [2287/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [2288/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [2289/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [2290/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [2291/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [2292/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [2293/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [2294/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [2295/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [2296/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [2297/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [2298/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [2299/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [2300/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [2301/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [2302/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [2303/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [2304/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [2305/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [2306/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [2307/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [2308/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [2309/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [2310/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [2311/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2312/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2313/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2314/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2315/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2316/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [2317/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [2318/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [2319/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [2320/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [2321/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [2322/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [2323/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2324/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2325/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2326/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2327/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [2328/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2329/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2330/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2331/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2332/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2333/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2334/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2335/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2336/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2337/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2338/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2339/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2340/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2341/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2342/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [2343/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [2344/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [2345/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [2346/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [2347/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2348/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2349/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2350/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2351/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2352/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2353/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2354/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2355/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2356/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2357/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2358/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2359/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2360/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2361/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [2362/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [2363/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [2364/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2365/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2366/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [2367/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [2368/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [2369/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [2370/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [2371/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [2372/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2373/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2374/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [2375/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [2376/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [2377/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [2378/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [2379/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [2380/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2381/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2382/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2383/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [2384/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [2385/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [2386/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2387/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2388/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2389/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [2390/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [2391/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [2392/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [2393/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [2394/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [2395/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [2396/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [2397/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [2398/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [2399/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [2400/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [2401/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2402/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [2403/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [2404/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [2405/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [2406/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2407/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2408/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2409/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2410/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2411/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [2412/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [2413/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [2414/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [2415/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [2416/10000] Train Loss: 0.000034 Val Loss: nan\n",
      "Epoch [2417/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [2418/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [2419/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [2420/10000] Train Loss: 0.000032 Val Loss: nan\n",
      "Epoch [2421/10000] Train Loss: 0.000033 Val Loss: nan\n",
      "Epoch [2422/10000] Train Loss: 0.000032 Val Loss: nan\n",
      "Epoch [2423/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [2424/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [2425/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [2426/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [2427/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [2428/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [2429/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [2430/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [2431/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [2432/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [2433/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [2434/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [2435/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [2436/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [2437/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [2438/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [2439/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [2440/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2441/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2442/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2443/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [2444/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [2445/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [2446/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [2447/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [2448/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2449/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2450/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [2451/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [2452/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2453/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [2454/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [2455/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2456/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [2457/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [2458/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [2459/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [2460/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [2461/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [2462/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [2463/10000] Train Loss: 0.000034 Val Loss: nan\n",
      "Epoch [2464/10000] Train Loss: 0.000032 Val Loss: nan\n",
      "Epoch [2465/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [2466/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [2467/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [2468/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [2469/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [2470/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [2471/10000] Train Loss: 0.000031 Val Loss: nan\n",
      "Epoch [2472/10000] Train Loss: 0.000034 Val Loss: nan\n",
      "Epoch [2473/10000] Train Loss: 0.000039 Val Loss: nan\n",
      "Epoch [2474/10000] Train Loss: 0.000040 Val Loss: nan\n",
      "Epoch [2475/10000] Train Loss: 0.000044 Val Loss: nan\n",
      "Epoch [2476/10000] Train Loss: 0.000041 Val Loss: nan\n",
      "Epoch [2477/10000] Train Loss: 0.000044 Val Loss: nan\n",
      "Epoch [2478/10000] Train Loss: 0.000042 Val Loss: nan\n",
      "Epoch [2479/10000] Train Loss: 0.000045 Val Loss: nan\n",
      "Epoch [2480/10000] Train Loss: 0.000050 Val Loss: nan\n",
      "Epoch [2481/10000] Train Loss: 0.000051 Val Loss: nan\n",
      "Epoch [2482/10000] Train Loss: 0.000052 Val Loss: nan\n",
      "Epoch [2483/10000] Train Loss: 0.000055 Val Loss: nan\n",
      "Epoch [2484/10000] Train Loss: 0.000063 Val Loss: nan\n",
      "Epoch [2485/10000] Train Loss: 0.000081 Val Loss: nan\n",
      "Epoch [2486/10000] Train Loss: 0.000105 Val Loss: nan\n",
      "Epoch [2487/10000] Train Loss: 0.000094 Val Loss: nan\n",
      "Epoch [2488/10000] Train Loss: 0.000069 Val Loss: nan\n",
      "Epoch [2489/10000] Train Loss: 0.000058 Val Loss: nan\n",
      "Epoch [2490/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [2491/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [2492/10000] Train Loss: 0.000040 Val Loss: nan\n",
      "Epoch [2493/10000] Train Loss: 0.000037 Val Loss: nan\n",
      "Epoch [2494/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [2495/10000] Train Loss: 0.000049 Val Loss: nan\n",
      "Epoch [2496/10000] Train Loss: 0.000052 Val Loss: nan\n",
      "Epoch [2497/10000] Train Loss: 0.000053 Val Loss: nan\n",
      "Epoch [2498/10000] Train Loss: 0.000069 Val Loss: nan\n",
      "Epoch [2499/10000] Train Loss: 0.000086 Val Loss: nan\n",
      "Epoch [2500/10000] Train Loss: 0.000093 Val Loss: nan\n",
      "Epoch [2501/10000] Train Loss: 0.000100 Val Loss: nan\n",
      "Epoch [2502/10000] Train Loss: 0.000103 Val Loss: nan\n",
      "Epoch [2503/10000] Train Loss: 0.000139 Val Loss: nan\n",
      "Epoch [2504/10000] Train Loss: 0.000150 Val Loss: nan\n",
      "Epoch [2505/10000] Train Loss: 0.000099 Val Loss: nan\n",
      "Epoch [2506/10000] Train Loss: 0.000079 Val Loss: nan\n",
      "Epoch [2507/10000] Train Loss: 0.000073 Val Loss: nan\n",
      "Epoch [2508/10000] Train Loss: 0.000063 Val Loss: nan\n",
      "Epoch [2509/10000] Train Loss: 0.000068 Val Loss: nan\n",
      "Epoch [2510/10000] Train Loss: 0.000069 Val Loss: nan\n",
      "Epoch [2511/10000] Train Loss: 0.000068 Val Loss: nan\n",
      "Epoch [2512/10000] Train Loss: 0.000073 Val Loss: nan\n",
      "Epoch [2513/10000] Train Loss: 0.000048 Val Loss: nan\n",
      "Epoch [2514/10000] Train Loss: 0.000039 Val Loss: nan\n",
      "Epoch [2515/10000] Train Loss: 0.000033 Val Loss: nan\n",
      "Epoch [2516/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [2517/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [2518/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [2519/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [2520/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [2521/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [2522/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [2523/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [2524/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [2525/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [2526/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [2527/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [2528/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [2529/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [2530/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [2531/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2532/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [2533/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [2534/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [2535/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [2536/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [2537/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [2538/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [2539/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2540/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [2541/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [2542/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [2543/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2544/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [2545/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [2546/10000] Train Loss: 0.000032 Val Loss: nan\n",
      "Epoch [2547/10000] Train Loss: 0.000037 Val Loss: nan\n",
      "Epoch [2548/10000] Train Loss: 0.000041 Val Loss: nan\n",
      "Epoch [2549/10000] Train Loss: 0.000033 Val Loss: nan\n",
      "Epoch [2550/10000] Train Loss: 0.000034 Val Loss: nan\n",
      "Epoch [2551/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [2552/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [2553/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [2554/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [2555/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [2556/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [2557/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2558/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [2559/10000] Train Loss: 0.000039 Val Loss: nan\n",
      "Epoch [2560/10000] Train Loss: 0.000048 Val Loss: nan\n",
      "Epoch [2561/10000] Train Loss: 0.000045 Val Loss: nan\n",
      "Epoch [2562/10000] Train Loss: 0.000058 Val Loss: nan\n",
      "Epoch [2563/10000] Train Loss: 0.000059 Val Loss: nan\n",
      "Epoch [2564/10000] Train Loss: 0.000050 Val Loss: nan\n",
      "Epoch [2565/10000] Train Loss: 0.000051 Val Loss: nan\n",
      "Epoch [2566/10000] Train Loss: 0.000058 Val Loss: nan\n",
      "Epoch [2567/10000] Train Loss: 0.000070 Val Loss: nan\n",
      "Epoch [2568/10000] Train Loss: 0.000072 Val Loss: nan\n",
      "Epoch [2569/10000] Train Loss: 0.000066 Val Loss: nan\n",
      "Epoch [2570/10000] Train Loss: 0.000054 Val Loss: nan\n",
      "Epoch [2571/10000] Train Loss: 0.000044 Val Loss: nan\n",
      "Epoch [2572/10000] Train Loss: 0.000049 Val Loss: nan\n",
      "Epoch [2573/10000] Train Loss: 0.000051 Val Loss: nan\n",
      "Epoch [2574/10000] Train Loss: 0.000048 Val Loss: nan\n",
      "Epoch [2575/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [2576/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [2577/10000] Train Loss: 0.000042 Val Loss: nan\n",
      "Epoch [2578/10000] Train Loss: 0.000053 Val Loss: nan\n",
      "Epoch [2579/10000] Train Loss: 0.000061 Val Loss: nan\n",
      "Epoch [2580/10000] Train Loss: 0.000083 Val Loss: nan\n",
      "Epoch [2581/10000] Train Loss: 0.000101 Val Loss: nan\n",
      "Epoch [2582/10000] Train Loss: 0.000083 Val Loss: nan\n",
      "Epoch [2583/10000] Train Loss: 0.000070 Val Loss: nan\n",
      "Epoch [2584/10000] Train Loss: 0.000051 Val Loss: nan\n",
      "Epoch [2585/10000] Train Loss: 0.000049 Val Loss: nan\n",
      "Epoch [2586/10000] Train Loss: 0.000052 Val Loss: nan\n",
      "Epoch [2587/10000] Train Loss: 0.000052 Val Loss: nan\n",
      "Epoch [2588/10000] Train Loss: 0.000045 Val Loss: nan\n",
      "Epoch [2589/10000] Train Loss: 0.000043 Val Loss: nan\n",
      "Epoch [2590/10000] Train Loss: 0.000035 Val Loss: nan\n",
      "Epoch [2591/10000] Train Loss: 0.000035 Val Loss: nan\n",
      "Epoch [2592/10000] Train Loss: 0.000036 Val Loss: nan\n",
      "Epoch [2593/10000] Train Loss: 0.000038 Val Loss: nan\n",
      "Epoch [2594/10000] Train Loss: 0.000041 Val Loss: nan\n",
      "Epoch [2595/10000] Train Loss: 0.000039 Val Loss: nan\n",
      "Epoch [2596/10000] Train Loss: 0.000038 Val Loss: nan\n",
      "Epoch [2597/10000] Train Loss: 0.000033 Val Loss: nan\n",
      "Epoch [2598/10000] Train Loss: 0.000048 Val Loss: nan\n",
      "Epoch [2599/10000] Train Loss: 0.000052 Val Loss: nan\n",
      "Epoch [2600/10000] Train Loss: 0.000041 Val Loss: nan\n",
      "Epoch [2601/10000] Train Loss: 0.000032 Val Loss: nan\n",
      "Epoch [2602/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [2603/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [2604/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [2605/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [2606/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [2607/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [2608/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [2609/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2610/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2611/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [2612/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [2613/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [2614/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [2615/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [2616/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [2617/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [2618/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [2619/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [2620/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [2621/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [2622/10000] Train Loss: 0.000034 Val Loss: nan\n",
      "Epoch [2623/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [2624/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [2625/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [2626/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [2627/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [2628/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [2629/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [2630/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [2631/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2632/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [2633/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [2634/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [2635/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [2636/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [2637/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [2638/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2639/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2640/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [2641/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [2642/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [2643/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [2644/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [2645/10000] Train Loss: 0.000035 Val Loss: nan\n",
      "Epoch [2646/10000] Train Loss: 0.000041 Val Loss: nan\n",
      "Epoch [2647/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [2648/10000] Train Loss: 0.000057 Val Loss: nan\n",
      "Epoch [2649/10000] Train Loss: 0.000056 Val Loss: nan\n",
      "Epoch [2650/10000] Train Loss: 0.000052 Val Loss: nan\n",
      "Epoch [2651/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [2652/10000] Train Loss: 0.000043 Val Loss: nan\n",
      "Epoch [2653/10000] Train Loss: 0.000042 Val Loss: nan\n",
      "Epoch [2654/10000] Train Loss: 0.000040 Val Loss: nan\n",
      "Epoch [2655/10000] Train Loss: 0.000032 Val Loss: nan\n",
      "Epoch [2656/10000] Train Loss: 0.000031 Val Loss: nan\n",
      "Epoch [2657/10000] Train Loss: 0.000036 Val Loss: nan\n",
      "Epoch [2658/10000] Train Loss: 0.000038 Val Loss: nan\n",
      "Epoch [2659/10000] Train Loss: 0.000033 Val Loss: nan\n",
      "Epoch [2660/10000] Train Loss: 0.000044 Val Loss: nan\n",
      "Epoch [2661/10000] Train Loss: 0.000047 Val Loss: nan\n",
      "Epoch [2662/10000] Train Loss: 0.000048 Val Loss: nan\n",
      "Epoch [2663/10000] Train Loss: 0.000049 Val Loss: nan\n",
      "Epoch [2664/10000] Train Loss: 0.000049 Val Loss: nan\n",
      "Epoch [2665/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [2666/10000] Train Loss: 0.000041 Val Loss: nan\n",
      "Epoch [2667/10000] Train Loss: 0.000037 Val Loss: nan\n",
      "Epoch [2668/10000] Train Loss: 0.000034 Val Loss: nan\n",
      "Epoch [2669/10000] Train Loss: 0.000039 Val Loss: nan\n",
      "Epoch [2670/10000] Train Loss: 0.000050 Val Loss: nan\n",
      "Epoch [2671/10000] Train Loss: 0.000064 Val Loss: nan\n",
      "Epoch [2672/10000] Train Loss: 0.000064 Val Loss: nan\n",
      "Epoch [2673/10000] Train Loss: 0.000066 Val Loss: nan\n",
      "Epoch [2674/10000] Train Loss: 0.000078 Val Loss: nan\n",
      "Epoch [2675/10000] Train Loss: 0.000079 Val Loss: nan\n",
      "Epoch [2676/10000] Train Loss: 0.000069 Val Loss: nan\n",
      "Epoch [2677/10000] Train Loss: 0.000059 Val Loss: nan\n",
      "Epoch [2678/10000] Train Loss: 0.000049 Val Loss: nan\n",
      "Epoch [2679/10000] Train Loss: 0.000056 Val Loss: nan\n",
      "Epoch [2680/10000] Train Loss: 0.000053 Val Loss: nan\n",
      "Epoch [2681/10000] Train Loss: 0.000054 Val Loss: nan\n",
      "Epoch [2682/10000] Train Loss: 0.000051 Val Loss: nan\n",
      "Epoch [2683/10000] Train Loss: 0.000048 Val Loss: nan\n",
      "Epoch [2684/10000] Train Loss: 0.000052 Val Loss: nan\n",
      "Epoch [2685/10000] Train Loss: 0.000053 Val Loss: nan\n",
      "Epoch [2686/10000] Train Loss: 0.000052 Val Loss: nan\n",
      "Epoch [2687/10000] Train Loss: 0.000060 Val Loss: nan\n",
      "Epoch [2688/10000] Train Loss: 0.000069 Val Loss: nan\n",
      "Epoch [2689/10000] Train Loss: 0.000065 Val Loss: nan\n",
      "Epoch [2690/10000] Train Loss: 0.000055 Val Loss: nan\n",
      "Epoch [2691/10000] Train Loss: 0.000057 Val Loss: nan\n",
      "Epoch [2692/10000] Train Loss: 0.000051 Val Loss: nan\n",
      "Epoch [2693/10000] Train Loss: 0.000045 Val Loss: nan\n",
      "Epoch [2694/10000] Train Loss: 0.000037 Val Loss: nan\n",
      "Epoch [2695/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [2696/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [2697/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [2698/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [2699/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2700/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [2701/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [2702/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [2703/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [2704/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [2705/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [2706/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [2707/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [2708/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [2709/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [2710/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [2711/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [2712/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [2713/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [2714/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [2715/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [2716/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [2717/10000] Train Loss: 0.000034 Val Loss: nan\n",
      "Epoch [2718/10000] Train Loss: 0.000058 Val Loss: nan\n",
      "Epoch [2719/10000] Train Loss: 0.000084 Val Loss: nan\n",
      "Epoch [2720/10000] Train Loss: 0.000114 Val Loss: nan\n",
      "Epoch [2721/10000] Train Loss: 0.000126 Val Loss: nan\n",
      "Epoch [2722/10000] Train Loss: 0.000100 Val Loss: nan\n",
      "Epoch [2723/10000] Train Loss: 0.000091 Val Loss: nan\n",
      "Epoch [2724/10000] Train Loss: 0.000079 Val Loss: nan\n",
      "Epoch [2725/10000] Train Loss: 0.000081 Val Loss: nan\n",
      "Epoch [2726/10000] Train Loss: 0.000087 Val Loss: nan\n",
      "Epoch [2727/10000] Train Loss: 0.000067 Val Loss: nan\n",
      "Epoch [2728/10000] Train Loss: 0.000071 Val Loss: nan\n",
      "Epoch [2729/10000] Train Loss: 0.000062 Val Loss: nan\n",
      "Epoch [2730/10000] Train Loss: 0.000064 Val Loss: nan\n",
      "Epoch [2731/10000] Train Loss: 0.000059 Val Loss: nan\n",
      "Epoch [2732/10000] Train Loss: 0.000069 Val Loss: nan\n",
      "Epoch [2733/10000] Train Loss: 0.000088 Val Loss: nan\n",
      "Epoch [2734/10000] Train Loss: 0.000077 Val Loss: nan\n",
      "Epoch [2735/10000] Train Loss: 0.000059 Val Loss: nan\n",
      "Epoch [2736/10000] Train Loss: 0.000050 Val Loss: nan\n",
      "Epoch [2737/10000] Train Loss: 0.000056 Val Loss: nan\n",
      "Epoch [2738/10000] Train Loss: 0.000051 Val Loss: nan\n",
      "Epoch [2739/10000] Train Loss: 0.000051 Val Loss: nan\n",
      "Epoch [2740/10000] Train Loss: 0.000057 Val Loss: nan\n",
      "Epoch [2741/10000] Train Loss: 0.000045 Val Loss: nan\n",
      "Epoch [2742/10000] Train Loss: 0.000036 Val Loss: nan\n",
      "Epoch [2743/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [2744/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2745/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [2746/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [2747/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [2748/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [2749/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [2750/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [2751/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [2752/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [2753/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [2754/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [2755/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [2756/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [2757/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [2758/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [2759/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [2760/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [2761/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [2762/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [2763/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [2764/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [2765/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [2766/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [2767/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2768/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2769/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [2770/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [2771/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [2772/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [2773/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [2774/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [2775/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [2776/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [2777/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [2778/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [2779/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [2780/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [2781/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [2782/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [2783/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [2784/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [2785/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [2786/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [2787/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [2788/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [2789/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [2790/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [2791/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [2792/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [2793/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [2794/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [2795/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [2796/10000] Train Loss: 0.000036 Val Loss: nan\n",
      "Epoch [2797/10000] Train Loss: 0.000037 Val Loss: nan\n",
      "Epoch [2798/10000] Train Loss: 0.000057 Val Loss: nan\n",
      "Epoch [2799/10000] Train Loss: 0.000081 Val Loss: nan\n",
      "Epoch [2800/10000] Train Loss: 0.000088 Val Loss: nan\n",
      "Epoch [2801/10000] Train Loss: 0.000100 Val Loss: nan\n",
      "Epoch [2802/10000] Train Loss: 0.000082 Val Loss: nan\n",
      "Epoch [2803/10000] Train Loss: 0.000106 Val Loss: nan\n",
      "Epoch [2804/10000] Train Loss: 0.000109 Val Loss: nan\n",
      "Epoch [2805/10000] Train Loss: 0.000102 Val Loss: nan\n",
      "Epoch [2806/10000] Train Loss: 0.000350 Val Loss: nan\n",
      "Epoch [2807/10000] Train Loss: 0.001900 Val Loss: nan\n",
      "Epoch [2808/10000] Train Loss: 0.001610 Val Loss: nan\n",
      "Epoch [2809/10000] Train Loss: 0.001503 Val Loss: nan\n",
      "Epoch [2810/10000] Train Loss: 0.002801 Val Loss: nan\n",
      "Epoch [2811/10000] Train Loss: 0.001984 Val Loss: nan\n",
      "Epoch [2812/10000] Train Loss: 0.001587 Val Loss: nan\n",
      "Epoch [2813/10000] Train Loss: 0.002048 Val Loss: nan\n",
      "Epoch [2814/10000] Train Loss: 0.001686 Val Loss: nan\n",
      "Epoch [2815/10000] Train Loss: 0.000887 Val Loss: nan\n",
      "Epoch [2816/10000] Train Loss: 0.001498 Val Loss: nan\n",
      "Epoch [2817/10000] Train Loss: 0.001142 Val Loss: nan\n",
      "Epoch [2818/10000] Train Loss: 0.000836 Val Loss: nan\n",
      "Epoch [2819/10000] Train Loss: 0.000483 Val Loss: nan\n",
      "Epoch [2820/10000] Train Loss: 0.000256 Val Loss: nan\n",
      "Epoch [2821/10000] Train Loss: 0.000176 Val Loss: nan\n",
      "Epoch [2822/10000] Train Loss: 0.000095 Val Loss: nan\n",
      "Epoch [2823/10000] Train Loss: 0.000066 Val Loss: nan\n",
      "Epoch [2824/10000] Train Loss: 0.000054 Val Loss: nan\n",
      "Epoch [2825/10000] Train Loss: 0.000044 Val Loss: nan\n",
      "Epoch [2826/10000] Train Loss: 0.000037 Val Loss: nan\n",
      "Epoch [2827/10000] Train Loss: 0.000032 Val Loss: nan\n",
      "Epoch [2828/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [2829/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [2830/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [2831/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [2832/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [2833/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2834/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [2835/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [2836/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [2837/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [2838/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [2839/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [2840/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [2841/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [2842/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [2843/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [2844/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [2845/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [2846/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [2847/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [2848/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [2849/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [2850/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [2851/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [2852/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [2853/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [2854/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [2855/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [2856/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [2857/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [2858/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [2859/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [2860/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [2861/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [2862/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [2863/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [2864/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [2865/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [2866/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [2867/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [2868/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [2869/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [2870/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [2871/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [2872/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2873/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [2874/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2875/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [2876/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2877/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2878/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2879/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2880/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2881/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2882/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2883/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2884/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2885/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2886/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2887/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2888/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2889/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2890/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2891/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2892/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2893/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2894/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2895/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2896/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2897/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2898/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2899/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2900/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2901/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2902/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2903/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2904/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2905/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2906/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2907/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2908/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2909/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2910/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2911/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2912/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2913/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2914/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2915/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2916/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2917/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2918/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2919/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2920/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2921/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2922/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2923/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2924/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2925/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2926/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2927/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2928/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2929/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2930/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2931/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2932/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2933/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2934/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2935/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2936/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2937/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2938/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2939/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2940/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2941/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [2942/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [2943/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [2944/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [2945/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [2946/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2947/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2948/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [2949/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [2950/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [2951/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [2952/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [2953/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [2954/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [2955/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [2956/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [2957/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2958/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [2959/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [2960/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [2961/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [2962/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2963/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2964/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2965/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2966/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2967/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2968/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2969/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2970/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2971/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [2972/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2973/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2974/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2975/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2976/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2977/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2978/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2979/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2980/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2981/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2982/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2983/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2984/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2985/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2986/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2987/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2988/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2989/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [2990/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2991/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2992/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2993/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [2994/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2995/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2996/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2997/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2998/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [2999/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3000/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3001/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3002/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3003/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3004/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3005/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3006/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3007/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3008/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3009/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3010/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3011/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3012/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3013/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3014/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3015/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3016/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3017/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3018/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3019/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3020/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3021/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3022/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3023/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3024/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3025/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3026/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3027/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [3028/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [3029/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [3030/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [3031/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3032/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3033/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [3034/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [3035/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [3036/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3037/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [3038/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3039/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [3040/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3041/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3042/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3043/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3044/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3045/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3046/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3047/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3048/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3049/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [3050/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [3051/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [3052/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [3053/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [3054/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [3055/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [3056/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [3057/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [3058/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3059/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3060/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3061/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3062/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [3063/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3064/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3065/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [3066/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [3067/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [3068/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3069/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3070/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3071/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3072/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3073/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3074/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3075/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3076/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3077/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3078/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3079/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3080/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3081/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3082/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3083/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [3084/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [3085/10000] Train Loss: 0.000032 Val Loss: nan\n",
      "Epoch [3086/10000] Train Loss: 0.000031 Val Loss: nan\n",
      "Epoch [3087/10000] Train Loss: 0.000035 Val Loss: nan\n",
      "Epoch [3088/10000] Train Loss: 0.000047 Val Loss: nan\n",
      "Epoch [3089/10000] Train Loss: 0.000049 Val Loss: nan\n",
      "Epoch [3090/10000] Train Loss: 0.000062 Val Loss: nan\n",
      "Epoch [3091/10000] Train Loss: 0.000088 Val Loss: nan\n",
      "Epoch [3092/10000] Train Loss: 0.000075 Val Loss: nan\n",
      "Epoch [3093/10000] Train Loss: 0.000090 Val Loss: nan\n",
      "Epoch [3094/10000] Train Loss: 0.000091 Val Loss: nan\n",
      "Epoch [3095/10000] Train Loss: 0.000106 Val Loss: nan\n",
      "Epoch [3096/10000] Train Loss: 0.000126 Val Loss: nan\n",
      "Epoch [3097/10000] Train Loss: 0.000125 Val Loss: nan\n",
      "Epoch [3098/10000] Train Loss: 0.000096 Val Loss: nan\n",
      "Epoch [3099/10000] Train Loss: 0.000127 Val Loss: nan\n",
      "Epoch [3100/10000] Train Loss: 0.000113 Val Loss: nan\n",
      "Epoch [3101/10000] Train Loss: 0.000084 Val Loss: nan\n",
      "Epoch [3102/10000] Train Loss: 0.000061 Val Loss: nan\n",
      "Epoch [3103/10000] Train Loss: 0.000043 Val Loss: nan\n",
      "Epoch [3104/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [3105/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [3106/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [3107/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [3108/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [3109/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [3110/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3111/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3112/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3113/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3114/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3115/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3116/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3117/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3118/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3119/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3120/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3121/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3122/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3123/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3124/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3125/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3126/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3127/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3128/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3129/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3130/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3131/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3132/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3133/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3134/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3135/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3136/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3137/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3138/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3139/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3140/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3141/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3142/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3143/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3144/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3145/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3146/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3147/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3148/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3149/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3150/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3151/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3152/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3153/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3154/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3155/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3156/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3157/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3158/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3159/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3160/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3161/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3162/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3163/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3164/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [3165/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [3166/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [3167/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [3168/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [3169/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [3170/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [3171/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [3172/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [3173/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [3174/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [3175/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [3176/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [3177/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [3178/10000] Train Loss: 0.000032 Val Loss: nan\n",
      "Epoch [3179/10000] Train Loss: 0.000038 Val Loss: nan\n",
      "Epoch [3180/10000] Train Loss: 0.000043 Val Loss: nan\n",
      "Epoch [3181/10000] Train Loss: 0.000041 Val Loss: nan\n",
      "Epoch [3182/10000] Train Loss: 0.000035 Val Loss: nan\n",
      "Epoch [3183/10000] Train Loss: 0.000033 Val Loss: nan\n",
      "Epoch [3184/10000] Train Loss: 0.000031 Val Loss: nan\n",
      "Epoch [3185/10000] Train Loss: 0.000038 Val Loss: nan\n",
      "Epoch [3186/10000] Train Loss: 0.000050 Val Loss: nan\n",
      "Epoch [3187/10000] Train Loss: 0.000075 Val Loss: nan\n",
      "Epoch [3188/10000] Train Loss: 0.000527 Val Loss: nan\n",
      "Epoch [3189/10000] Train Loss: 0.001531 Val Loss: nan\n",
      "Epoch [3190/10000] Train Loss: 0.002988 Val Loss: nan\n",
      "Epoch [3191/10000] Train Loss: 0.001922 Val Loss: nan\n",
      "Epoch [3192/10000] Train Loss: 0.001892 Val Loss: nan\n",
      "Epoch [3193/10000] Train Loss: 0.001641 Val Loss: nan\n",
      "Epoch [3194/10000] Train Loss: 0.001506 Val Loss: nan\n",
      "Epoch [3195/10000] Train Loss: 0.000837 Val Loss: nan\n",
      "Epoch [3196/10000] Train Loss: 0.000415 Val Loss: nan\n",
      "Epoch [3197/10000] Train Loss: 0.000246 Val Loss: nan\n",
      "Epoch [3198/10000] Train Loss: 0.000140 Val Loss: nan\n",
      "Epoch [3199/10000] Train Loss: 0.000088 Val Loss: nan\n",
      "Epoch [3200/10000] Train Loss: 0.000064 Val Loss: nan\n",
      "Epoch [3201/10000] Train Loss: 0.000054 Val Loss: nan\n",
      "Epoch [3202/10000] Train Loss: 0.000043 Val Loss: nan\n",
      "Epoch [3203/10000] Train Loss: 0.000033 Val Loss: nan\n",
      "Epoch [3204/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [3205/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [3206/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [3207/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [3208/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [3209/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [3210/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [3211/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [3212/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [3213/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3214/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3215/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3216/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3217/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3218/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3219/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3220/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3221/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3222/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3223/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3224/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3225/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3226/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3227/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3228/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3229/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3230/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3231/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3232/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3233/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3234/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3235/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3236/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3237/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3238/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3239/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3240/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3241/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3242/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3243/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3244/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3245/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3246/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3247/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3248/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3249/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3250/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3251/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3252/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3253/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3254/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3255/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3256/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3257/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3258/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3259/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3260/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3261/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3262/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3263/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3264/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3265/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3266/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3267/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3268/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3269/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3270/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3271/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3272/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3273/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3274/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3275/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3276/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3277/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3278/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3279/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3280/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3281/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3282/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3283/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3284/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3285/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3286/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3287/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3288/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3289/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3290/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3291/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3292/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3293/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3294/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3295/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3296/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3297/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3298/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3299/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3300/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3301/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3302/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3303/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3304/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3305/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3306/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3307/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3308/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3309/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3310/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3311/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3312/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3313/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3314/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3315/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3316/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3317/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3318/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3319/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3320/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3321/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3322/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3323/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3324/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3325/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3326/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3327/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3328/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3329/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3330/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3331/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3332/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3333/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3334/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3335/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3336/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3337/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3338/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3339/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3340/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3341/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3342/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3343/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3344/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3345/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3346/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3347/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3348/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3349/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3350/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3351/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3352/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3353/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3354/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3355/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3356/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3357/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3358/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3359/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3360/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3361/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3362/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3363/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3364/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3365/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3366/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [3367/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [3368/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [3369/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [3370/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [3371/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [3372/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [3373/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [3374/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [3375/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [3376/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3377/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3378/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3379/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3380/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3381/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3382/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3383/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3384/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3385/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3386/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3387/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3388/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3389/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3390/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3391/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3392/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3393/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3394/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3395/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3396/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3397/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3398/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3399/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3400/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3401/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3402/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3403/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3404/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3405/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3406/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3407/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3408/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3409/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3410/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3411/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3412/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3413/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3414/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3415/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3416/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3417/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3418/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3419/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3420/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3421/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3422/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3423/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3424/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3425/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3426/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3427/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3428/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3429/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3430/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3431/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3432/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3433/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3434/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3435/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3436/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3437/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3438/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [3439/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [3440/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [3441/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [3442/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [3443/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [3444/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [3445/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [3446/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [3447/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [3448/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [3449/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [3450/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [3451/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [3452/10000] Train Loss: 0.000031 Val Loss: nan\n",
      "Epoch [3453/10000] Train Loss: 0.000034 Val Loss: nan\n",
      "Epoch [3454/10000] Train Loss: 0.000035 Val Loss: nan\n",
      "Epoch [3455/10000] Train Loss: 0.000034 Val Loss: nan\n",
      "Epoch [3456/10000] Train Loss: 0.000031 Val Loss: nan\n",
      "Epoch [3457/10000] Train Loss: 0.000036 Val Loss: nan\n",
      "Epoch [3458/10000] Train Loss: 0.000048 Val Loss: nan\n",
      "Epoch [3459/10000] Train Loss: 0.000043 Val Loss: nan\n",
      "Epoch [3460/10000] Train Loss: 0.000035 Val Loss: nan\n",
      "Epoch [3461/10000] Train Loss: 0.000035 Val Loss: nan\n",
      "Epoch [3462/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [3463/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [3464/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [3465/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [3466/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [3467/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [3468/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [3469/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [3470/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [3471/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [3472/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [3473/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [3474/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3475/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3476/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3477/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3478/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3479/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3480/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3481/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3482/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3483/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3484/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3485/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3486/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3487/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3488/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3489/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3490/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3491/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3492/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3493/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3494/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3495/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3496/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3497/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3498/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3499/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3500/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3501/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3502/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3503/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3504/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3505/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3506/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3507/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [3508/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [3509/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [3510/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [3511/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [3512/10000] Train Loss: 0.000039 Val Loss: nan\n",
      "Epoch [3513/10000] Train Loss: 0.000043 Val Loss: nan\n",
      "Epoch [3514/10000] Train Loss: 0.000042 Val Loss: nan\n",
      "Epoch [3515/10000] Train Loss: 0.000034 Val Loss: nan\n",
      "Epoch [3516/10000] Train Loss: 0.000031 Val Loss: nan\n",
      "Epoch [3517/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [3518/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [3519/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [3520/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [3521/10000] Train Loss: 0.000033 Val Loss: nan\n",
      "Epoch [3522/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [3523/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [3524/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [3525/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [3526/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [3527/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [3528/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [3529/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [3530/10000] Train Loss: 0.000032 Val Loss: nan\n",
      "Epoch [3531/10000] Train Loss: 0.000031 Val Loss: nan\n",
      "Epoch [3532/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [3533/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [3534/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [3535/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [3536/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [3537/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [3538/10000] Train Loss: 0.000037 Val Loss: nan\n",
      "Epoch [3539/10000] Train Loss: 0.000048 Val Loss: nan\n",
      "Epoch [3540/10000] Train Loss: 0.000061 Val Loss: nan\n",
      "Epoch [3541/10000] Train Loss: 0.000088 Val Loss: nan\n",
      "Epoch [3542/10000] Train Loss: 0.000090 Val Loss: nan\n",
      "Epoch [3543/10000] Train Loss: 0.000088 Val Loss: nan\n",
      "Epoch [3544/10000] Train Loss: 0.000065 Val Loss: nan\n",
      "Epoch [3545/10000] Train Loss: 0.000056 Val Loss: nan\n",
      "Epoch [3546/10000] Train Loss: 0.000047 Val Loss: nan\n",
      "Epoch [3547/10000] Train Loss: 0.000045 Val Loss: nan\n",
      "Epoch [3548/10000] Train Loss: 0.000040 Val Loss: nan\n",
      "Epoch [3549/10000] Train Loss: 0.000040 Val Loss: nan\n",
      "Epoch [3550/10000] Train Loss: 0.000034 Val Loss: nan\n",
      "Epoch [3551/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [3552/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [3553/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [3554/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [3555/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [3556/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3557/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3558/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3559/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3560/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3561/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3562/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3563/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3564/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3565/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3566/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3567/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3568/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3569/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3570/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3571/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3572/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3573/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3574/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3575/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3576/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3577/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3578/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3579/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3580/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3581/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3582/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3583/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3584/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3585/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3586/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3587/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3588/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3589/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3590/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3591/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3592/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [3593/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [3594/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [3595/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [3596/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [3597/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [3598/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [3599/10000] Train Loss: 0.000040 Val Loss: nan\n",
      "Epoch [3600/10000] Train Loss: 0.000035 Val Loss: nan\n",
      "Epoch [3601/10000] Train Loss: 0.000045 Val Loss: nan\n",
      "Epoch [3602/10000] Train Loss: 0.000050 Val Loss: nan\n",
      "Epoch [3603/10000] Train Loss: 0.000063 Val Loss: nan\n",
      "Epoch [3604/10000] Train Loss: 0.000083 Val Loss: nan\n",
      "Epoch [3605/10000] Train Loss: 0.000149 Val Loss: nan\n",
      "Epoch [3606/10000] Train Loss: 0.000540 Val Loss: nan\n",
      "Epoch [3607/10000] Train Loss: 0.001850 Val Loss: nan\n",
      "Epoch [3608/10000] Train Loss: 0.001447 Val Loss: nan\n",
      "Epoch [3609/10000] Train Loss: 0.002038 Val Loss: nan\n",
      "Epoch [3610/10000] Train Loss: 0.001648 Val Loss: nan\n",
      "Epoch [3611/10000] Train Loss: 0.003476 Val Loss: nan\n",
      "Epoch [3612/10000] Train Loss: 0.002760 Val Loss: nan\n",
      "Epoch [3613/10000] Train Loss: 0.002604 Val Loss: nan\n",
      "Epoch [3614/10000] Train Loss: 0.001734 Val Loss: nan\n",
      "Epoch [3615/10000] Train Loss: 0.000739 Val Loss: nan\n",
      "Epoch [3616/10000] Train Loss: 0.000387 Val Loss: nan\n",
      "Epoch [3617/10000] Train Loss: 0.000216 Val Loss: nan\n",
      "Epoch [3618/10000] Train Loss: 0.000125 Val Loss: nan\n",
      "Epoch [3619/10000] Train Loss: 0.000087 Val Loss: nan\n",
      "Epoch [3620/10000] Train Loss: 0.000065 Val Loss: nan\n",
      "Epoch [3621/10000] Train Loss: 0.000050 Val Loss: nan\n",
      "Epoch [3622/10000] Train Loss: 0.000039 Val Loss: nan\n",
      "Epoch [3623/10000] Train Loss: 0.000034 Val Loss: nan\n",
      "Epoch [3624/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [3625/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [3626/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [3627/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [3628/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [3629/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [3630/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [3631/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [3632/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [3633/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [3634/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3635/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3636/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3637/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3638/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3639/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3640/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3641/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3642/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3643/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3644/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3645/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3646/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3647/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3648/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3649/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3650/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3651/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3652/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3653/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3654/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3655/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3656/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3657/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3658/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3659/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3660/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3661/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3662/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3663/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3664/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3665/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3666/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3667/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3668/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3669/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3670/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3671/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3672/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3673/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3674/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3675/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3676/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3677/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3678/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3679/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3680/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3681/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3682/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3683/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3684/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3685/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3686/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3687/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3688/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3689/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3690/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3691/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3692/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3693/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3694/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3695/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3696/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3697/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3698/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3699/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3700/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3701/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3702/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3703/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3704/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3705/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3706/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3707/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3708/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3709/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3710/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3711/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3712/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3713/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3714/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3715/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3716/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3717/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3718/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3719/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3720/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3721/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3722/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3723/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3724/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3725/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3726/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3727/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3728/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3729/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3730/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3731/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3732/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3733/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3734/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3735/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3736/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3737/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [3738/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [3739/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3740/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3741/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3742/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3743/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3744/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3745/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3746/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3747/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3748/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3749/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3750/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3751/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3752/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [3753/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3754/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3755/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3756/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3757/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3758/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [3759/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [3760/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [3761/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [3762/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [3763/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [3764/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [3765/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3766/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3767/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3768/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3769/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3770/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [3771/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3772/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3773/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3774/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3775/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [3776/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3777/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3778/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3779/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3780/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3781/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3782/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3783/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3784/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3785/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3786/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3787/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3788/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3789/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3790/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3791/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3792/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3793/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3794/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3795/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3796/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3797/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3798/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [3799/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3800/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3801/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3802/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3803/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3804/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3805/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3806/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3807/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3808/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3809/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3810/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3811/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3812/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3813/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3814/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3815/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3816/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3817/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3818/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3819/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3820/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3821/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3822/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3823/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3824/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3825/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3826/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3827/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3828/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3829/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3830/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3831/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3832/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [3833/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3834/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3835/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3836/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3837/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3838/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3839/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3840/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3841/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3842/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3843/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3844/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3845/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3846/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3847/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3848/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3849/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3850/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [3851/10000] Train Loss: 0.000032 Val Loss: nan\n",
      "Epoch [3852/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [3853/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [3854/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [3855/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [3856/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [3857/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [3858/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [3859/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [3860/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [3861/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [3862/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [3863/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [3864/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [3865/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [3866/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [3867/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [3868/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [3869/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3870/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3871/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3872/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3873/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3874/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3875/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3876/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3877/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3878/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3879/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3880/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3881/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3882/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3883/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3884/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3885/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3886/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3887/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3888/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [3889/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3890/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [3891/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3892/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3893/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3894/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3895/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [3896/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [3897/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [3898/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [3899/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [3900/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [3901/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [3902/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [3903/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [3904/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [3905/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3906/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3907/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3908/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3909/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3910/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [3911/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3912/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3913/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3914/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3915/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3916/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3917/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3918/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3919/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3920/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3921/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3922/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3923/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [3924/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [3925/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [3926/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3927/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [3928/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3929/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [3930/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [3931/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [3932/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [3933/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [3934/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [3935/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [3936/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [3937/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [3938/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [3939/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [3940/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [3941/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [3942/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [3943/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [3944/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [3945/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [3946/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [3947/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [3948/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [3949/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [3950/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [3951/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [3952/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [3953/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [3954/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [3955/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [3956/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [3957/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [3958/10000] Train Loss: 0.000045 Val Loss: nan\n",
      "Epoch [3959/10000] Train Loss: 0.000045 Val Loss: nan\n",
      "Epoch [3960/10000] Train Loss: 0.000049 Val Loss: nan\n",
      "Epoch [3961/10000] Train Loss: 0.000059 Val Loss: nan\n",
      "Epoch [3962/10000] Train Loss: 0.000063 Val Loss: nan\n",
      "Epoch [3963/10000] Train Loss: 0.000055 Val Loss: nan\n",
      "Epoch [3964/10000] Train Loss: 0.000045 Val Loss: nan\n",
      "Epoch [3965/10000] Train Loss: 0.000035 Val Loss: nan\n",
      "Epoch [3966/10000] Train Loss: 0.000033 Val Loss: nan\n",
      "Epoch [3967/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [3968/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [3969/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [3970/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [3971/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [3972/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [3973/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [3974/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [3975/10000] Train Loss: 0.000033 Val Loss: nan\n",
      "Epoch [3976/10000] Train Loss: 0.000034 Val Loss: nan\n",
      "Epoch [3977/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [3978/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [3979/10000] Train Loss: 0.000035 Val Loss: nan\n",
      "Epoch [3980/10000] Train Loss: 0.000034 Val Loss: nan\n",
      "Epoch [3981/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [3982/10000] Train Loss: 0.000033 Val Loss: nan\n",
      "Epoch [3983/10000] Train Loss: 0.000042 Val Loss: nan\n",
      "Epoch [3984/10000] Train Loss: 0.000032 Val Loss: nan\n",
      "Epoch [3985/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [3986/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [3987/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [3988/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [3989/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [3990/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [3991/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [3992/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [3993/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [3994/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [3995/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [3996/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [3997/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [3998/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [3999/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [4000/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [4001/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [4002/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [4003/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [4004/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [4005/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [4006/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [4007/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [4008/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [4009/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [4010/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [4011/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [4012/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [4013/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [4014/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [4015/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [4016/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [4017/10000] Train Loss: 0.000032 Val Loss: nan\n",
      "Epoch [4018/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [4019/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [4020/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [4021/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [4022/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [4023/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [4024/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [4025/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [4026/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [4027/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [4028/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [4029/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [4030/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [4031/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [4032/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [4033/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4034/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4035/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4036/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4037/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4038/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4039/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4040/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4041/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4042/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4043/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4044/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [4045/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [4046/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [4047/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [4048/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [4049/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [4050/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [4051/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [4052/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [4053/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [4054/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [4055/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [4056/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4057/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4058/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [4059/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [4060/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [4061/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [4062/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [4063/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [4064/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [4065/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4066/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [4067/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4068/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4069/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4070/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [4071/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [4072/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [4073/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [4074/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [4075/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [4076/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [4077/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [4078/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [4079/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [4080/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [4081/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [4082/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [4083/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [4084/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [4085/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [4086/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [4087/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [4088/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [4089/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [4090/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [4091/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [4092/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [4093/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [4094/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [4095/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [4096/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [4097/10000] Train Loss: 0.000039 Val Loss: nan\n",
      "Epoch [4098/10000] Train Loss: 0.000057 Val Loss: nan\n",
      "Epoch [4099/10000] Train Loss: 0.000094 Val Loss: nan\n",
      "Epoch [4100/10000] Train Loss: 0.000094 Val Loss: nan\n",
      "Epoch [4101/10000] Train Loss: 0.000134 Val Loss: nan\n",
      "Epoch [4102/10000] Train Loss: 0.000777 Val Loss: nan\n",
      "Epoch [4103/10000] Train Loss: 0.002817 Val Loss: nan\n",
      "Epoch [4104/10000] Train Loss: 0.003752 Val Loss: nan\n",
      "Epoch [4105/10000] Train Loss: 0.001684 Val Loss: nan\n",
      "Epoch [4106/10000] Train Loss: 0.001278 Val Loss: nan\n",
      "Epoch [4107/10000] Train Loss: 0.000489 Val Loss: nan\n",
      "Epoch [4108/10000] Train Loss: 0.000540 Val Loss: nan\n",
      "Epoch [4109/10000] Train Loss: 0.000355 Val Loss: nan\n",
      "Epoch [4110/10000] Train Loss: 0.000225 Val Loss: nan\n",
      "Epoch [4111/10000] Train Loss: 0.000127 Val Loss: nan\n",
      "Epoch [4112/10000] Train Loss: 0.000088 Val Loss: nan\n",
      "Epoch [4113/10000] Train Loss: 0.000057 Val Loss: nan\n",
      "Epoch [4114/10000] Train Loss: 0.000040 Val Loss: nan\n",
      "Epoch [4115/10000] Train Loss: 0.000032 Val Loss: nan\n",
      "Epoch [4116/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [4117/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [4118/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [4119/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [4120/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [4121/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [4122/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [4123/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [4124/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [4125/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [4126/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [4127/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [4128/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [4129/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [4130/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4131/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4132/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4133/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4134/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4135/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4136/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4137/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4138/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4139/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4140/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4141/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4142/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4143/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4144/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4145/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4146/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4147/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4148/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4149/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4150/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4151/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4152/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4153/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4154/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4155/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4156/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4157/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4158/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4159/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4160/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4161/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4162/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4163/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4164/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4165/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4166/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4167/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4168/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4169/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4170/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4171/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4172/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4173/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4174/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4175/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4176/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4177/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4178/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4179/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4180/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4181/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4182/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4183/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4184/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4185/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4186/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4187/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4188/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4189/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4190/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4191/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4192/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4193/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4194/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4195/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4196/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4197/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4198/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4199/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4200/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4201/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4202/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4203/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4204/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4205/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4206/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4207/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4208/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4209/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4210/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4211/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4212/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4213/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4214/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4215/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4216/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4217/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4218/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4219/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4220/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4221/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4222/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4223/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4224/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4225/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4226/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4227/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4228/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4229/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4230/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4231/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4232/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4233/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4234/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4235/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4236/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4237/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4238/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4239/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4240/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4241/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4242/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4243/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4244/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4245/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4246/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4247/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4248/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4249/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4250/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4251/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4252/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4253/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4254/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4255/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4256/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4257/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4258/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4259/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4260/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4261/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4262/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4263/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4264/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4265/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4266/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4267/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4268/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4269/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4270/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4271/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4272/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4273/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4274/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4275/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4276/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4277/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4278/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4279/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4280/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4281/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4282/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4283/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4284/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4285/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4286/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4287/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4288/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4289/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4290/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4291/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4292/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4293/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4294/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4295/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4296/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4297/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4298/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4299/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4300/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4301/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4302/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4303/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4304/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4305/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4306/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4307/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4308/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4309/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4310/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4311/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4312/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4313/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4314/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4315/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4316/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4317/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4318/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4319/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4320/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [4321/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [4322/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [4323/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [4324/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [4325/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [4326/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [4327/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [4328/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [4329/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [4330/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [4331/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [4332/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [4333/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [4334/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4335/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4336/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4337/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [4338/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [4339/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [4340/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [4341/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [4342/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [4343/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [4344/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [4345/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [4346/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [4347/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [4348/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [4349/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [4350/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [4351/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [4352/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [4353/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [4354/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [4355/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [4356/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [4357/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [4358/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [4359/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [4360/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [4361/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [4362/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [4363/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [4364/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [4365/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4366/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4367/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4368/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4369/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4370/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4371/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4372/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4373/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4374/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4375/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4376/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4377/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4378/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4379/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4380/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4381/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4382/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4383/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4384/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4385/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4386/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4387/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4388/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4389/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4390/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4391/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [4392/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [4393/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [4394/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [4395/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [4396/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [4397/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [4398/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [4399/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [4400/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [4401/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [4402/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [4403/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [4404/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [4405/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [4406/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [4407/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [4408/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [4409/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [4410/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [4411/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [4412/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [4413/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [4414/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [4415/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [4416/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [4417/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [4418/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [4419/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [4420/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [4421/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [4422/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4423/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4424/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4425/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4426/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4427/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4428/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4429/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4430/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4431/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4432/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4433/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4434/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [4435/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4436/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4437/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4438/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4439/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4440/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4441/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4442/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4443/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4444/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4445/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [4446/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [4447/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [4448/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [4449/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [4450/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [4451/10000] Train Loss: 0.000038 Val Loss: nan\n",
      "Epoch [4452/10000] Train Loss: 0.000050 Val Loss: nan\n",
      "Epoch [4453/10000] Train Loss: 0.000051 Val Loss: nan\n",
      "Epoch [4454/10000] Train Loss: 0.000053 Val Loss: nan\n",
      "Epoch [4455/10000] Train Loss: 0.000043 Val Loss: nan\n",
      "Epoch [4456/10000] Train Loss: 0.000056 Val Loss: nan\n",
      "Epoch [4457/10000] Train Loss: 0.000072 Val Loss: nan\n",
      "Epoch [4458/10000] Train Loss: 0.000080 Val Loss: nan\n",
      "Epoch [4459/10000] Train Loss: 0.000088 Val Loss: nan\n",
      "Epoch [4460/10000] Train Loss: 0.000094 Val Loss: nan\n",
      "Epoch [4461/10000] Train Loss: 0.000073 Val Loss: nan\n",
      "Epoch [4462/10000] Train Loss: 0.000053 Val Loss: nan\n",
      "Epoch [4463/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [4464/10000] Train Loss: 0.000035 Val Loss: nan\n",
      "Epoch [4465/10000] Train Loss: 0.000033 Val Loss: nan\n",
      "Epoch [4466/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [4467/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [4468/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [4469/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [4470/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [4471/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [4472/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [4473/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [4474/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4475/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4476/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4477/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4478/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4479/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4480/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4481/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4482/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4483/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4484/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4485/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4486/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4487/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4488/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4489/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4490/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4491/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4492/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4493/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4494/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4495/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4496/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4497/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4498/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4499/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4500/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4501/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4502/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4503/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4504/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4505/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4506/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4507/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4508/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4509/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4510/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4511/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4512/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4513/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4514/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4515/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4516/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4517/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4518/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4519/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4520/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4521/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4522/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4523/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [4524/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [4525/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [4526/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [4527/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [4528/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [4529/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [4530/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [4531/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [4532/10000] Train Loss: 0.000038 Val Loss: nan\n",
      "Epoch [4533/10000] Train Loss: 0.000035 Val Loss: nan\n",
      "Epoch [4534/10000] Train Loss: 0.000037 Val Loss: nan\n",
      "Epoch [4535/10000] Train Loss: 0.000034 Val Loss: nan\n",
      "Epoch [4536/10000] Train Loss: 0.000032 Val Loss: nan\n",
      "Epoch [4537/10000] Train Loss: 0.000034 Val Loss: nan\n",
      "Epoch [4538/10000] Train Loss: 0.000032 Val Loss: nan\n",
      "Epoch [4539/10000] Train Loss: 0.000032 Val Loss: nan\n",
      "Epoch [4540/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [4541/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [4542/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [4543/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [4544/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [4545/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [4546/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [4547/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [4548/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [4549/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [4550/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [4551/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [4552/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [4553/10000] Train Loss: 0.000031 Val Loss: nan\n",
      "Epoch [4554/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [4555/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [4556/10000] Train Loss: 0.000034 Val Loss: nan\n",
      "Epoch [4557/10000] Train Loss: 0.000036 Val Loss: nan\n",
      "Epoch [4558/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [4559/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [4560/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [4561/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [4562/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [4563/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [4564/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [4565/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [4566/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [4567/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [4568/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [4569/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [4570/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [4571/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [4572/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [4573/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [4574/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [4575/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [4576/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [4577/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [4578/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [4579/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [4580/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [4581/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [4582/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4583/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [4584/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [4585/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [4586/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [4587/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [4588/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [4589/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [4590/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [4591/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [4592/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [4593/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [4594/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [4595/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [4596/10000] Train Loss: 0.000032 Val Loss: nan\n",
      "Epoch [4597/10000] Train Loss: 0.000035 Val Loss: nan\n",
      "Epoch [4598/10000] Train Loss: 0.000035 Val Loss: nan\n",
      "Epoch [4599/10000] Train Loss: 0.000033 Val Loss: nan\n",
      "Epoch [4600/10000] Train Loss: 0.000031 Val Loss: nan\n",
      "Epoch [4601/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [4602/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [4603/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [4604/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [4605/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [4606/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [4607/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [4608/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [4609/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [4610/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [4611/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [4612/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [4613/10000] Train Loss: 0.000032 Val Loss: nan\n",
      "Epoch [4614/10000] Train Loss: 0.000036 Val Loss: nan\n",
      "Epoch [4615/10000] Train Loss: 0.000044 Val Loss: nan\n",
      "Epoch [4616/10000] Train Loss: 0.000041 Val Loss: nan\n",
      "Epoch [4617/10000] Train Loss: 0.000038 Val Loss: nan\n",
      "Epoch [4618/10000] Train Loss: 0.000034 Val Loss: nan\n",
      "Epoch [4619/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [4620/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [4621/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [4622/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [4623/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [4624/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [4625/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [4626/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [4627/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4628/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4629/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4630/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4631/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4632/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4633/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4634/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4635/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4636/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4637/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4638/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4639/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4640/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4641/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4642/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4643/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4644/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4645/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4646/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4647/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4648/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4649/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4650/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4651/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4652/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4653/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4654/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4655/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4656/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4657/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4658/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4659/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4660/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4661/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4662/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4663/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4664/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4665/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4666/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4667/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4668/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4669/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4670/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4671/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4672/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4673/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [4674/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [4675/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [4676/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [4677/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [4678/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [4679/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [4680/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [4681/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [4682/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [4683/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [4684/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [4685/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [4686/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [4687/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [4688/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [4689/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [4690/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [4691/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [4692/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [4693/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [4694/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [4695/10000] Train Loss: 0.000034 Val Loss: nan\n",
      "Epoch [4696/10000] Train Loss: 0.000033 Val Loss: nan\n",
      "Epoch [4697/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [4698/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [4699/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [4700/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [4701/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [4702/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [4703/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [4704/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [4705/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [4706/10000] Train Loss: 0.000041 Val Loss: nan\n",
      "Epoch [4707/10000] Train Loss: 0.000055 Val Loss: nan\n",
      "Epoch [4708/10000] Train Loss: 0.000054 Val Loss: nan\n",
      "Epoch [4709/10000] Train Loss: 0.000063 Val Loss: nan\n",
      "Epoch [4710/10000] Train Loss: 0.000062 Val Loss: nan\n",
      "Epoch [4711/10000] Train Loss: 0.000063 Val Loss: nan\n",
      "Epoch [4712/10000] Train Loss: 0.000140 Val Loss: nan\n",
      "Epoch [4713/10000] Train Loss: 0.000269 Val Loss: nan\n",
      "Epoch [4714/10000] Train Loss: 0.000857 Val Loss: nan\n",
      "Epoch [4715/10000] Train Loss: 0.000684 Val Loss: nan\n",
      "Epoch [4716/10000] Train Loss: 0.001602 Val Loss: nan\n",
      "Epoch [4717/10000] Train Loss: 0.002321 Val Loss: nan\n",
      "Epoch [4718/10000] Train Loss: 0.004218 Val Loss: nan\n",
      "Epoch [4719/10000] Train Loss: 0.002355 Val Loss: nan\n",
      "Epoch [4720/10000] Train Loss: 0.002955 Val Loss: nan\n",
      "Epoch [4721/10000] Train Loss: 0.003039 Val Loss: nan\n",
      "Epoch [4722/10000] Train Loss: 0.003193 Val Loss: nan\n",
      "Epoch [4723/10000] Train Loss: 0.001059 Val Loss: nan\n",
      "Epoch [4724/10000] Train Loss: 0.000592 Val Loss: nan\n",
      "Epoch [4725/10000] Train Loss: 0.000256 Val Loss: nan\n",
      "Epoch [4726/10000] Train Loss: 0.000151 Val Loss: nan\n",
      "Epoch [4727/10000] Train Loss: 0.000098 Val Loss: nan\n",
      "Epoch [4728/10000] Train Loss: 0.000067 Val Loss: nan\n",
      "Epoch [4729/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [4730/10000] Train Loss: 0.000036 Val Loss: nan\n",
      "Epoch [4731/10000] Train Loss: 0.000031 Val Loss: nan\n",
      "Epoch [4732/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [4733/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [4734/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [4735/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [4736/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [4737/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [4738/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [4739/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [4740/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [4741/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [4742/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [4743/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [4744/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [4745/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [4746/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [4747/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [4748/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [4749/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [4750/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4751/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4752/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4753/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4754/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4755/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4756/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4757/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4758/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4759/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4760/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4761/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4762/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4763/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4764/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4765/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4766/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4767/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4768/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4769/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4770/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4771/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4772/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4773/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4774/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4775/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4776/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4777/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4778/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4779/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4780/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4781/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4782/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4783/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4784/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4785/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4786/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4787/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4788/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4789/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4790/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4791/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4792/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4793/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4794/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4795/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4796/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4797/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4798/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4799/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4800/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4801/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4802/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4803/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4804/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4805/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4806/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4807/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4808/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4809/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4810/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4811/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4812/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4813/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4814/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4815/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4816/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4817/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4818/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4819/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4820/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4821/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4822/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4823/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4824/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4825/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4826/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4827/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4828/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4829/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4830/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4831/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4832/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4833/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4834/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4835/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4836/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4837/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4838/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4839/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4840/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4841/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4842/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4843/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4844/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4845/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4846/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4847/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4848/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4849/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4850/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4851/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4852/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4853/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4854/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4855/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4856/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4857/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4858/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4859/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4860/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4861/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4862/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4863/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4864/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4865/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4866/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4867/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4868/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4869/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4870/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4871/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4872/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4873/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4874/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4875/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4876/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4877/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4878/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4879/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4880/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4881/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4882/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4883/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4884/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4885/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4886/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4887/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4888/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4889/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4890/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4891/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4892/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4893/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4894/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4895/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4896/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4897/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4898/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4899/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4900/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4901/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4902/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4903/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4904/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4905/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4906/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4907/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4908/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4909/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4910/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4911/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4912/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4913/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4914/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4915/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4916/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4917/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4918/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4919/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4920/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4921/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4922/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4923/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4924/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4925/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4926/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4927/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4928/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4929/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4930/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4931/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4932/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4933/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4934/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4935/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4936/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4937/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4938/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4939/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4940/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [4941/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4942/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4943/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4944/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4945/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4946/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4947/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4948/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4949/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4950/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4951/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4952/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4953/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4954/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4955/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4956/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4957/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4958/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4959/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4960/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4961/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4962/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4963/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4964/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4965/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4966/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4967/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4968/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4969/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4970/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4971/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4972/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4973/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4974/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4975/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4976/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4977/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4978/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4979/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4980/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4981/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [4982/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [4983/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4984/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4985/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4986/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [4987/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4988/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4989/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4990/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [4991/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4992/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [4993/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [4994/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4995/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4996/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [4997/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [4998/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [4999/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5000/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5001/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5002/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5003/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5004/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5005/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5006/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5007/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5008/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5009/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5010/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5011/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5012/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5013/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5014/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5015/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5016/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5017/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5018/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5019/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5020/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5021/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5022/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5023/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5024/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5025/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5026/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5027/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5028/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5029/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5030/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5031/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5032/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5033/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5034/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [5035/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [5036/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [5037/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [5038/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [5039/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [5040/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [5041/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5042/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5043/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5044/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5045/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5046/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5047/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [5048/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [5049/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [5050/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [5051/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [5052/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [5053/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5054/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5055/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5056/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5057/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5058/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5059/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5060/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5061/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5062/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5063/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5064/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5065/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5066/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5067/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5068/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5069/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5070/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5071/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5072/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5073/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5074/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5075/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5076/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5077/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5078/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5079/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5080/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5081/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5082/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5083/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5084/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5085/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5086/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [5087/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [5088/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [5089/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [5090/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [5091/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [5092/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [5093/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [5094/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [5095/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [5096/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [5097/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [5098/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [5099/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [5100/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [5101/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [5102/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [5103/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [5104/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [5105/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [5106/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [5107/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [5108/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [5109/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [5110/10000] Train Loss: 0.000037 Val Loss: nan\n",
      "Epoch [5111/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [5112/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [5113/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [5114/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [5115/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [5116/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [5117/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [5118/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [5119/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [5120/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [5121/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [5122/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [5123/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [5124/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [5125/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [5126/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [5127/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [5128/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [5129/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [5130/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [5131/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [5132/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [5133/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [5134/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [5135/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [5136/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [5137/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [5138/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [5139/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5140/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5141/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5142/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5143/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5144/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5145/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5146/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5147/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5148/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5149/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5150/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5151/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5152/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5153/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5154/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5155/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [5156/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [5157/10000] Train Loss: 0.000035 Val Loss: nan\n",
      "Epoch [5158/10000] Train Loss: 0.000033 Val Loss: nan\n",
      "Epoch [5159/10000] Train Loss: 0.000038 Val Loss: nan\n",
      "Epoch [5160/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [5161/10000] Train Loss: 0.000048 Val Loss: nan\n",
      "Epoch [5162/10000] Train Loss: 0.000040 Val Loss: nan\n",
      "Epoch [5163/10000] Train Loss: 0.000035 Val Loss: nan\n",
      "Epoch [5164/10000] Train Loss: 0.000036 Val Loss: nan\n",
      "Epoch [5165/10000] Train Loss: 0.000034 Val Loss: nan\n",
      "Epoch [5166/10000] Train Loss: 0.000035 Val Loss: nan\n",
      "Epoch [5167/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [5168/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [5169/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [5170/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [5171/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [5172/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [5173/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [5174/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [5175/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [5176/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [5177/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [5178/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [5179/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [5180/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [5181/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [5182/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [5183/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [5184/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [5185/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [5186/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [5187/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [5188/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [5189/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [5190/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [5191/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5192/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5193/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5194/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5195/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5196/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5197/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5198/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5199/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5200/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5201/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5202/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5203/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5204/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5205/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5206/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5207/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5208/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5209/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5210/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5211/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5212/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5213/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5214/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5215/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5216/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5217/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5218/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5219/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5220/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5221/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5222/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5223/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5224/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5225/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5226/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5227/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5228/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5229/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5230/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5231/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5232/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5233/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5234/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5235/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5236/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5237/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5238/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5239/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5240/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5241/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5242/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5243/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5244/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5245/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5246/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [5247/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [5248/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [5249/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [5250/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [5251/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [5252/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [5253/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [5254/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [5255/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [5256/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [5257/10000] Train Loss: 0.000032 Val Loss: nan\n",
      "Epoch [5258/10000] Train Loss: 0.000047 Val Loss: nan\n",
      "Epoch [5259/10000] Train Loss: 0.000042 Val Loss: nan\n",
      "Epoch [5260/10000] Train Loss: 0.000035 Val Loss: nan\n",
      "Epoch [5261/10000] Train Loss: 0.000035 Val Loss: nan\n",
      "Epoch [5262/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [5263/10000] Train Loss: 0.000033 Val Loss: nan\n",
      "Epoch [5264/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [5265/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [5266/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [5267/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [5268/10000] Train Loss: 0.000041 Val Loss: nan\n",
      "Epoch [5269/10000] Train Loss: 0.000032 Val Loss: nan\n",
      "Epoch [5270/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [5271/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [5272/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [5273/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [5274/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [5275/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [5276/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [5277/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [5278/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [5279/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [5280/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [5281/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [5282/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [5283/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5284/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [5285/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [5286/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [5287/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [5288/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [5289/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [5290/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [5291/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5292/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5293/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5294/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [5295/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [5296/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [5297/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [5298/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [5299/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [5300/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5301/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5302/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5303/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5304/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [5305/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [5306/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [5307/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [5308/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [5309/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [5310/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [5311/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [5312/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [5313/10000] Train Loss: 0.000035 Val Loss: nan\n",
      "Epoch [5314/10000] Train Loss: 0.000033 Val Loss: nan\n",
      "Epoch [5315/10000] Train Loss: 0.000036 Val Loss: nan\n",
      "Epoch [5316/10000] Train Loss: 0.000034 Val Loss: nan\n",
      "Epoch [5317/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [5318/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [5319/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [5320/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [5321/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [5322/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [5323/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [5324/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [5325/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [5326/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [5327/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [5328/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [5329/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [5330/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [5331/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [5332/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [5333/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [5334/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5335/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5336/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5337/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5338/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5339/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5340/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5341/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5342/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5343/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5344/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5345/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5346/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5347/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5348/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5349/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5350/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5351/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5352/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5353/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5354/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5355/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5356/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5357/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5358/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5359/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5360/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5361/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5362/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5363/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5364/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5365/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5366/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5367/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5368/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5369/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5370/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5371/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5372/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5373/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5374/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5375/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5376/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5377/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5378/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5379/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5380/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5381/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [5382/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [5383/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [5384/10000] Train Loss: 0.000038 Val Loss: nan\n",
      "Epoch [5385/10000] Train Loss: 0.000050 Val Loss: nan\n",
      "Epoch [5386/10000] Train Loss: 0.000053 Val Loss: nan\n",
      "Epoch [5387/10000] Train Loss: 0.000076 Val Loss: nan\n",
      "Epoch [5388/10000] Train Loss: 0.000078 Val Loss: nan\n",
      "Epoch [5389/10000] Train Loss: 0.000087 Val Loss: nan\n",
      "Epoch [5390/10000] Train Loss: 0.000090 Val Loss: nan\n",
      "Epoch [5391/10000] Train Loss: 0.000088 Val Loss: nan\n",
      "Epoch [5392/10000] Train Loss: 0.000074 Val Loss: nan\n",
      "Epoch [5393/10000] Train Loss: 0.000075 Val Loss: nan\n",
      "Epoch [5394/10000] Train Loss: 0.000076 Val Loss: nan\n",
      "Epoch [5395/10000] Train Loss: 0.000070 Val Loss: nan\n",
      "Epoch [5396/10000] Train Loss: 0.000051 Val Loss: nan\n",
      "Epoch [5397/10000] Train Loss: 0.000054 Val Loss: nan\n",
      "Epoch [5398/10000] Train Loss: 0.000050 Val Loss: nan\n",
      "Epoch [5399/10000] Train Loss: 0.000032 Val Loss: nan\n",
      "Epoch [5400/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [5401/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [5402/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [5403/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [5404/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [5405/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [5406/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [5407/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [5408/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [5409/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [5410/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5411/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5412/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5413/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5414/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5415/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5416/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5417/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5418/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5419/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5420/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5421/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5422/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5423/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5424/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5425/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5426/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5427/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5428/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5429/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5430/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5431/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5432/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5433/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5434/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5435/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5436/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5437/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5438/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5439/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5440/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5441/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5442/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5443/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5444/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5445/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5446/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5447/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5448/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5449/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5450/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5451/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5452/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5453/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5454/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5455/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5456/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5457/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5458/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5459/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5460/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5461/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5462/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5463/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5464/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5465/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5466/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5467/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5468/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5469/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5470/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5471/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5472/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5473/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5474/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5475/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5476/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5477/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5478/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5479/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5480/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5481/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5482/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5483/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5484/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5485/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5486/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5487/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5488/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5489/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5490/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5491/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5492/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5493/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5494/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5495/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5496/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5497/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [5498/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [5499/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [5500/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [5501/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [5502/10000] Train Loss: 0.000031 Val Loss: nan\n",
      "Epoch [5503/10000] Train Loss: 0.000036 Val Loss: nan\n",
      "Epoch [5504/10000] Train Loss: 0.000038 Val Loss: nan\n",
      "Epoch [5505/10000] Train Loss: 0.000050 Val Loss: nan\n",
      "Epoch [5506/10000] Train Loss: 0.000070 Val Loss: nan\n",
      "Epoch [5507/10000] Train Loss: 0.000060 Val Loss: nan\n",
      "Epoch [5508/10000] Train Loss: 0.000057 Val Loss: nan\n",
      "Epoch [5509/10000] Train Loss: 0.000045 Val Loss: nan\n",
      "Epoch [5510/10000] Train Loss: 0.000056 Val Loss: nan\n",
      "Epoch [5511/10000] Train Loss: 0.000051 Val Loss: nan\n",
      "Epoch [5512/10000] Train Loss: 0.000071 Val Loss: nan\n",
      "Epoch [5513/10000] Train Loss: 0.000294 Val Loss: nan\n",
      "Epoch [5514/10000] Train Loss: 0.000372 Val Loss: nan\n",
      "Epoch [5515/10000] Train Loss: 0.000670 Val Loss: nan\n",
      "Epoch [5516/10000] Train Loss: 0.001837 Val Loss: nan\n",
      "Epoch [5517/10000] Train Loss: 0.004470 Val Loss: nan\n",
      "Epoch [5518/10000] Train Loss: 0.002984 Val Loss: nan\n",
      "Epoch [5519/10000] Train Loss: 0.002817 Val Loss: nan\n",
      "Epoch [5520/10000] Train Loss: 0.001522 Val Loss: nan\n",
      "Epoch [5521/10000] Train Loss: 0.000844 Val Loss: nan\n",
      "Epoch [5522/10000] Train Loss: 0.001547 Val Loss: nan\n",
      "Epoch [5523/10000] Train Loss: 0.002223 Val Loss: nan\n",
      "Epoch [5524/10000] Train Loss: 0.000757 Val Loss: nan\n",
      "Epoch [5525/10000] Train Loss: 0.000397 Val Loss: nan\n",
      "Epoch [5526/10000] Train Loss: 0.000188 Val Loss: nan\n",
      "Epoch [5527/10000] Train Loss: 0.000107 Val Loss: nan\n",
      "Epoch [5528/10000] Train Loss: 0.000066 Val Loss: nan\n",
      "Epoch [5529/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [5530/10000] Train Loss: 0.000032 Val Loss: nan\n",
      "Epoch [5531/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [5532/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [5533/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [5534/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [5535/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [5536/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [5537/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [5538/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [5539/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [5540/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [5541/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [5542/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5543/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5544/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5545/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5546/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5547/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5548/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5549/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5550/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5551/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5552/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5553/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5554/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5555/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5556/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5557/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5558/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5559/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5560/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5561/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5562/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5563/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5564/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5565/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5566/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5567/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5568/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5569/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5570/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5571/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5572/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5573/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5574/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5575/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5576/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5577/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5578/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5579/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5580/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5581/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5582/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5583/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5584/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5585/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5586/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5587/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5588/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5589/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5590/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5591/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5592/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5593/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5594/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5595/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5596/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5597/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5598/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5599/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5600/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5601/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5602/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5603/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5604/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5605/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5606/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5607/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5608/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5609/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5610/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5611/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5612/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5613/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5614/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5615/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5616/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5617/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5618/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5619/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5620/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5621/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5622/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5623/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5624/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5625/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5626/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5627/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5628/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5629/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5630/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5631/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5632/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5633/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5634/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5635/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5636/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5637/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5638/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5639/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5640/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5641/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5642/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5643/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5644/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5645/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5646/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5647/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5648/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5649/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5650/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5651/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5652/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5653/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5654/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5655/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5656/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5657/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5658/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5659/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5660/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5661/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5662/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5663/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5664/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5665/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5666/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5667/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5668/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5669/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5670/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5671/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5672/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5673/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5674/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5675/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5676/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5677/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5678/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5679/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5680/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5681/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5682/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5683/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5684/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5685/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5686/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5687/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5688/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5689/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5690/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5691/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5692/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5693/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5694/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5695/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5696/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5697/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5698/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5699/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5700/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5701/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5702/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5703/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5704/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5705/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5706/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5707/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5708/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5709/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5710/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5711/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5712/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5713/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5714/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5715/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5716/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5717/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5718/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5719/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5720/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5721/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5722/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5723/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5724/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5725/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5726/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5727/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5728/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5729/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5730/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5731/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5732/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5733/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5734/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5735/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5736/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5737/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5738/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5739/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5740/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5741/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5742/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5743/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5744/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5745/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5746/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5747/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5748/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5749/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5750/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5751/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5752/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5753/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5754/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5755/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5756/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5757/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5758/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5759/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5760/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5761/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5762/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5763/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5764/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5765/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5766/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5767/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5768/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5769/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5770/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5771/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5772/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5773/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5774/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5775/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5776/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5777/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5778/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5779/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5780/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5781/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5782/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5783/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5784/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5785/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [5786/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5787/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5788/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5789/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5790/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5791/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5792/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5793/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5794/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5795/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5796/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5797/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5798/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5799/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5800/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5801/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5802/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5803/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5804/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5805/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5806/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5807/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5808/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5809/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5810/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5811/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5812/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5813/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5814/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5815/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5816/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [5817/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5818/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5819/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5820/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5821/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5822/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5823/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5824/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5825/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [5826/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [5827/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [5828/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [5829/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [5830/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [5831/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [5832/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [5833/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5834/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5835/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5836/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5837/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5838/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5839/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5840/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5841/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5842/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5843/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5844/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5845/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5846/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5847/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5848/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5849/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5850/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5851/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5852/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5853/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5854/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5855/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5856/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5857/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5858/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5859/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5860/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5861/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5862/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5863/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5864/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5865/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5866/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5867/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5868/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5869/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5870/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5871/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5872/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5873/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5874/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5875/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5876/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5877/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5878/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5879/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5880/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5881/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5882/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5883/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5884/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5885/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5886/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5887/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5888/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5889/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5890/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5891/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5892/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5893/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5894/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5895/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5896/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5897/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5898/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5899/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5900/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5901/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5902/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5903/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5904/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5905/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5906/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5907/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5908/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5909/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5910/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5911/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5912/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5913/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [5914/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5915/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [5916/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [5917/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [5918/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [5919/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [5920/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [5921/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [5922/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [5923/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [5924/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [5925/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [5926/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [5927/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [5928/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [5929/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [5930/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [5931/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [5932/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [5933/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5934/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5935/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5936/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5937/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5938/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5939/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5940/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5941/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5942/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5943/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5944/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5945/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5946/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5947/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5948/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5949/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5950/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5951/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5952/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5953/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5954/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5955/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5956/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5957/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5958/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5959/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5960/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [5961/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5962/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [5963/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5964/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5965/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [5966/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [5967/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5968/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5969/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5970/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5971/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5972/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5973/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5974/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5975/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5976/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5977/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [5978/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5979/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5980/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [5981/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [5982/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5983/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [5984/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [5985/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [5986/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5987/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5988/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5989/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5990/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [5991/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [5992/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [5993/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [5994/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [5995/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [5996/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [5997/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [5998/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [5999/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [6000/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6001/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6002/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6003/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6004/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [6005/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [6006/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [6007/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [6008/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [6009/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [6010/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [6011/10000] Train Loss: 0.000034 Val Loss: nan\n",
      "Epoch [6012/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [6013/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [6014/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [6015/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [6016/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [6017/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [6018/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [6019/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [6020/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [6021/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6022/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [6023/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [6024/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6025/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6026/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6027/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6028/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6029/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6030/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6031/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6032/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6033/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6034/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6035/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6036/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6037/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6038/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6039/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [6040/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6041/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6042/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [6043/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [6044/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [6045/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [6046/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6047/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6048/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6049/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6050/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6051/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6052/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6053/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6054/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6055/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6056/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6057/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [6058/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [6059/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [6060/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6061/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6062/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6063/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [6064/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6065/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6066/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [6067/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [6068/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6069/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6070/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6071/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [6072/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [6073/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6074/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6075/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6076/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [6077/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [6078/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6079/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [6080/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6081/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [6082/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6083/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6084/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [6085/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6086/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6087/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6088/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6089/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6090/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6091/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6092/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6093/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6094/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6095/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6096/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6097/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6098/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6099/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6100/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6101/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6102/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6103/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [6104/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [6105/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [6106/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6107/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6108/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6109/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [6110/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [6111/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [6112/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [6113/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [6114/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [6115/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [6116/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6117/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [6118/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6119/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [6120/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [6121/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [6122/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6123/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6124/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [6125/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6126/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [6127/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [6128/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6129/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [6130/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [6131/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6132/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [6133/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [6134/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [6135/10000] Train Loss: 0.000031 Val Loss: nan\n",
      "Epoch [6136/10000] Train Loss: 0.000060 Val Loss: nan\n",
      "Epoch [6137/10000] Train Loss: 0.000058 Val Loss: nan\n",
      "Epoch [6138/10000] Train Loss: 0.000047 Val Loss: nan\n",
      "Epoch [6139/10000] Train Loss: 0.000071 Val Loss: nan\n",
      "Epoch [6140/10000] Train Loss: 0.000109 Val Loss: nan\n",
      "Epoch [6141/10000] Train Loss: 0.000088 Val Loss: nan\n",
      "Epoch [6142/10000] Train Loss: 0.000072 Val Loss: nan\n",
      "Epoch [6143/10000] Train Loss: 0.000058 Val Loss: nan\n",
      "Epoch [6144/10000] Train Loss: 0.000058 Val Loss: nan\n",
      "Epoch [6145/10000] Train Loss: 0.000054 Val Loss: nan\n",
      "Epoch [6146/10000] Train Loss: 0.000070 Val Loss: nan\n",
      "Epoch [6147/10000] Train Loss: 0.000051 Val Loss: nan\n",
      "Epoch [6148/10000] Train Loss: 0.000053 Val Loss: nan\n",
      "Epoch [6149/10000] Train Loss: 0.000045 Val Loss: nan\n",
      "Epoch [6150/10000] Train Loss: 0.000036 Val Loss: nan\n",
      "Epoch [6151/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [6152/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [6153/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [6154/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [6155/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6156/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6157/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6158/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [6159/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6160/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6161/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6162/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6163/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6164/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6165/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6166/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6167/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6168/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6169/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6170/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6171/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6172/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6173/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6174/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6175/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6176/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6177/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6178/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6179/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6180/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6181/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6182/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6183/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6184/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6185/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6186/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6187/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6188/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6189/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6190/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6191/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6192/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6193/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6194/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6195/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6196/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6197/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6198/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6199/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6200/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6201/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6202/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6203/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6204/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6205/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6206/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6207/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6208/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6209/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6210/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6211/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6212/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6213/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6214/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6215/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6216/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6217/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6218/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6219/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6220/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6221/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6222/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6223/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6224/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6225/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6226/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6227/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6228/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6229/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6230/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6231/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6232/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6233/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6234/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6235/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6236/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [6237/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6238/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [6239/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [6240/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [6241/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [6242/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [6243/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [6244/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [6245/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [6246/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [6247/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [6248/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [6249/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [6250/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [6251/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [6252/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [6253/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [6254/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6255/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [6256/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [6257/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [6258/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [6259/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [6260/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [6261/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6262/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [6263/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6264/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [6265/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6266/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [6267/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [6268/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [6269/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [6270/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [6271/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6272/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [6273/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6274/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6275/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [6276/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [6277/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6278/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [6279/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6280/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [6281/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [6282/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [6283/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [6284/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [6285/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6286/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6287/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6288/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6289/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6290/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6291/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6292/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6293/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6294/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6295/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6296/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6297/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6298/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6299/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6300/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6301/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6302/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6303/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6304/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [6305/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [6306/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [6307/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [6308/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6309/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6310/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [6311/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6312/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [6313/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6314/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [6315/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6316/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [6317/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [6318/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6319/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [6320/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [6321/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [6322/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [6323/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [6324/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [6325/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [6326/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [6327/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [6328/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6329/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6330/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6331/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [6332/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6333/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [6334/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6335/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6336/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [6337/10000] Train Loss: 0.000035 Val Loss: nan\n",
      "Epoch [6338/10000] Train Loss: 0.000045 Val Loss: nan\n",
      "Epoch [6339/10000] Train Loss: 0.000043 Val Loss: nan\n",
      "Epoch [6340/10000] Train Loss: 0.000047 Val Loss: nan\n",
      "Epoch [6341/10000] Train Loss: 0.000041 Val Loss: nan\n",
      "Epoch [6342/10000] Train Loss: 0.000035 Val Loss: nan\n",
      "Epoch [6343/10000] Train Loss: 0.000037 Val Loss: nan\n",
      "Epoch [6344/10000] Train Loss: 0.000036 Val Loss: nan\n",
      "Epoch [6345/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [6346/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [6347/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [6348/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [6349/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6350/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [6351/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [6352/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6353/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6354/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6355/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6356/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6357/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6358/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6359/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6360/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6361/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6362/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6363/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6364/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6365/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6366/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6367/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6368/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6369/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6370/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6371/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6372/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6373/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6374/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6375/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6376/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6377/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6378/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6379/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6380/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6381/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6382/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6383/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6384/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6385/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6386/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6387/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6388/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6389/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6390/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6391/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6392/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6393/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6394/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6395/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6396/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6397/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6398/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6399/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [6400/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [6401/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6402/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [6403/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [6404/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [6405/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [6406/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [6407/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6408/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6409/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6410/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6411/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6412/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6413/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6414/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6415/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6416/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6417/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6418/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6419/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6420/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6421/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6422/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6423/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6424/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6425/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6426/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6427/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6428/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [6429/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6430/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [6431/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6432/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [6433/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [6434/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6435/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [6436/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6437/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6438/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6439/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6440/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6441/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [6442/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [6443/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [6444/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [6445/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [6446/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [6447/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [6448/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [6449/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [6450/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [6451/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [6452/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [6453/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [6454/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [6455/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [6456/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [6457/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [6458/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [6459/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [6460/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [6461/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [6462/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [6463/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [6464/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [6465/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6466/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [6467/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [6468/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [6469/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6470/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [6471/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6472/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6473/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [6474/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [6475/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [6476/10000] Train Loss: 0.000037 Val Loss: nan\n",
      "Epoch [6477/10000] Train Loss: 0.000050 Val Loss: nan\n",
      "Epoch [6478/10000] Train Loss: 0.000042 Val Loss: nan\n",
      "Epoch [6479/10000] Train Loss: 0.000059 Val Loss: nan\n",
      "Epoch [6480/10000] Train Loss: 0.000056 Val Loss: nan\n",
      "Epoch [6481/10000] Train Loss: 0.000049 Val Loss: nan\n",
      "Epoch [6482/10000] Train Loss: 0.000041 Val Loss: nan\n",
      "Epoch [6483/10000] Train Loss: 0.000033 Val Loss: nan\n",
      "Epoch [6484/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [6485/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [6486/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [6487/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6488/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6489/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6490/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6491/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6492/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6493/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6494/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6495/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6496/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6497/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6498/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6499/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6500/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6501/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6502/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6503/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6504/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6505/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6506/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6507/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6508/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6509/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6510/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6511/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6512/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6513/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6514/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6515/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6516/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6517/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6518/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6519/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6520/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6521/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6522/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6523/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6524/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6525/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6526/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6527/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6528/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6529/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6530/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6531/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6532/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6533/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6534/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6535/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6536/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6537/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6538/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6539/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6540/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6541/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6542/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6543/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6544/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6545/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6546/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6547/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6548/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6549/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6550/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6551/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6552/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6553/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6554/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6555/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6556/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6557/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6558/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6559/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6560/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6561/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6562/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6563/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6564/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6565/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6566/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6567/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6568/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6569/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6570/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6571/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6572/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6573/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6574/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6575/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6576/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6577/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6578/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6579/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6580/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6581/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [6582/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [6583/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [6584/10000] Train Loss: 0.000039 Val Loss: nan\n",
      "Epoch [6585/10000] Train Loss: 0.000073 Val Loss: nan\n",
      "Epoch [6586/10000] Train Loss: 0.000075 Val Loss: nan\n",
      "Epoch [6587/10000] Train Loss: 0.000093 Val Loss: nan\n",
      "Epoch [6588/10000] Train Loss: 0.000102 Val Loss: nan\n",
      "Epoch [6589/10000] Train Loss: 0.000109 Val Loss: nan\n",
      "Epoch [6590/10000] Train Loss: 0.000094 Val Loss: nan\n",
      "Epoch [6591/10000] Train Loss: 0.000076 Val Loss: nan\n",
      "Epoch [6592/10000] Train Loss: 0.000066 Val Loss: nan\n",
      "Epoch [6593/10000] Train Loss: 0.000062 Val Loss: nan\n",
      "Epoch [6594/10000] Train Loss: 0.000058 Val Loss: nan\n",
      "Epoch [6595/10000] Train Loss: 0.000047 Val Loss: nan\n",
      "Epoch [6596/10000] Train Loss: 0.000034 Val Loss: nan\n",
      "Epoch [6597/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [6598/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [6599/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [6600/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6601/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [6602/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6603/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6604/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6605/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6606/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6607/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6608/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6609/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6610/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6611/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6612/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6613/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6614/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6615/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6616/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6617/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6618/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6619/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6620/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6621/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6622/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6623/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6624/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6625/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6626/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6627/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6628/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6629/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6630/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6631/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6632/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6633/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6634/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6635/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6636/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6637/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6638/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6639/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6640/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6641/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6642/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6643/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6644/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6645/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6646/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6647/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6648/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6649/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6650/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6651/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6652/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6653/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6654/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6655/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6656/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6657/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6658/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6659/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6660/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6661/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6662/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6663/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6664/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6665/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6666/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6667/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6668/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6669/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6670/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6671/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6672/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6673/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6674/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6675/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6676/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6677/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6678/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6679/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6680/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6681/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6682/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6683/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6684/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6685/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6686/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6687/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6688/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6689/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6690/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6691/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6692/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6693/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6694/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6695/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6696/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6697/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6698/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6699/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6700/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6701/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6702/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6703/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [6704/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [6705/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6706/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6707/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6708/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6709/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [6710/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [6711/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [6712/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [6713/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [6714/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [6715/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [6716/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [6717/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [6718/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [6719/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [6720/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6721/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [6722/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [6723/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [6724/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [6725/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [6726/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6727/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6728/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [6729/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [6730/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6731/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [6732/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [6733/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [6734/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [6735/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [6736/10000] Train Loss: 0.000031 Val Loss: nan\n",
      "Epoch [6737/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [6738/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [6739/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [6740/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [6741/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [6742/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [6743/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [6744/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6745/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [6746/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6747/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6748/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6749/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6750/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6751/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6752/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6753/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6754/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6755/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6756/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6757/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6758/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6759/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6760/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6761/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6762/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6763/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6764/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6765/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6766/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6767/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6768/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6769/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6770/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6771/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6772/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6773/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6774/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6775/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6776/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6777/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6778/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6779/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6780/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6781/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6782/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6783/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6784/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6785/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6786/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6787/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6788/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6789/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6790/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6791/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6792/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6793/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6794/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6795/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6796/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6797/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6798/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6799/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6800/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6801/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6802/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6803/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6804/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6805/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6806/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6807/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [6808/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6809/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [6810/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6811/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6812/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [6813/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [6814/10000] Train Loss: 0.000057 Val Loss: nan\n",
      "Epoch [6815/10000] Train Loss: 0.000049 Val Loss: nan\n",
      "Epoch [6816/10000] Train Loss: 0.000051 Val Loss: nan\n",
      "Epoch [6817/10000] Train Loss: 0.000073 Val Loss: nan\n",
      "Epoch [6818/10000] Train Loss: 0.000072 Val Loss: nan\n",
      "Epoch [6819/10000] Train Loss: 0.000076 Val Loss: nan\n",
      "Epoch [6820/10000] Train Loss: 0.000093 Val Loss: nan\n",
      "Epoch [6821/10000] Train Loss: 0.000079 Val Loss: nan\n",
      "Epoch [6822/10000] Train Loss: 0.000057 Val Loss: nan\n",
      "Epoch [6823/10000] Train Loss: 0.000041 Val Loss: nan\n",
      "Epoch [6824/10000] Train Loss: 0.000037 Val Loss: nan\n",
      "Epoch [6825/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [6826/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [6827/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6828/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6829/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6830/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6831/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6832/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6833/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6834/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6835/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6836/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6837/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6838/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6839/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6840/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6841/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6842/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6843/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6844/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6845/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6846/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6847/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6848/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6849/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6850/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6851/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6852/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6853/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6854/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6855/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6856/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6857/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6858/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6859/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6860/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6861/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6862/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6863/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6864/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6865/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6866/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6867/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6868/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6869/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6870/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6871/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6872/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6873/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6874/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [6875/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6876/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6877/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6878/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6879/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6880/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6881/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6882/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6883/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6884/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6885/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6886/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6887/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6888/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6889/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6890/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6891/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6892/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6893/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6894/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6895/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6896/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6897/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6898/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6899/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6900/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6901/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6902/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6903/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6904/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6905/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6906/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6907/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6908/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6909/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6910/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6911/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6912/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6913/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6914/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6915/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6916/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6917/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6918/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [6919/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6920/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6921/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6922/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6923/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [6924/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6925/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [6926/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6927/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6928/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6929/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [6930/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6931/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6932/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [6933/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [6934/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6935/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6936/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [6937/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6938/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6939/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6940/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6941/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6942/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [6943/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [6944/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [6945/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [6946/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [6947/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [6948/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [6949/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [6950/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [6951/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [6952/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [6953/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [6954/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [6955/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [6956/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6957/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [6958/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [6959/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6960/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6961/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [6962/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [6963/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [6964/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [6965/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6966/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6967/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6968/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6969/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6970/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6971/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6972/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6973/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6974/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6975/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6976/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6977/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6978/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6979/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6980/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6981/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6982/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6983/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6984/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6985/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6986/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [6987/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6988/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [6989/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6990/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6991/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6992/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [6993/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6994/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6995/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6996/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6997/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6998/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [6999/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7000/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7001/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7002/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7003/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7004/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7005/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [7006/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [7007/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [7008/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [7009/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [7010/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [7011/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [7012/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [7013/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [7014/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [7015/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [7016/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [7017/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [7018/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [7019/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [7020/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [7021/10000] Train Loss: 0.000031 Val Loss: nan\n",
      "Epoch [7022/10000] Train Loss: 0.000038 Val Loss: nan\n",
      "Epoch [7023/10000] Train Loss: 0.000039 Val Loss: nan\n",
      "Epoch [7024/10000] Train Loss: 0.000040 Val Loss: nan\n",
      "Epoch [7025/10000] Train Loss: 0.000038 Val Loss: nan\n",
      "Epoch [7026/10000] Train Loss: 0.000044 Val Loss: nan\n",
      "Epoch [7027/10000] Train Loss: 0.000039 Val Loss: nan\n",
      "Epoch [7028/10000] Train Loss: 0.000043 Val Loss: nan\n",
      "Epoch [7029/10000] Train Loss: 0.000178 Val Loss: nan\n",
      "Epoch [7030/10000] Train Loss: 0.000132 Val Loss: nan\n",
      "Epoch [7031/10000] Train Loss: 0.000123 Val Loss: nan\n",
      "Epoch [7032/10000] Train Loss: 0.000115 Val Loss: nan\n",
      "Epoch [7033/10000] Train Loss: 0.000111 Val Loss: nan\n",
      "Epoch [7034/10000] Train Loss: 0.000076 Val Loss: nan\n",
      "Epoch [7035/10000] Train Loss: 0.000063 Val Loss: nan\n",
      "Epoch [7036/10000] Train Loss: 0.000059 Val Loss: nan\n",
      "Epoch [7037/10000] Train Loss: 0.000043 Val Loss: nan\n",
      "Epoch [7038/10000] Train Loss: 0.000031 Val Loss: nan\n",
      "Epoch [7039/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [7040/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [7041/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [7042/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7043/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7044/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7045/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7046/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7047/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7048/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7049/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7050/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7051/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7052/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7053/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7054/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7055/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7056/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7057/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7058/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7059/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7060/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7061/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7062/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7063/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7064/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7065/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7066/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7067/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7068/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7069/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7070/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7071/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7072/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7073/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7074/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7075/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7076/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7077/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7078/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7079/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7080/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7081/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7082/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7083/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7084/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7085/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7086/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7087/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7088/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7089/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7090/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7091/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7092/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7093/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7094/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7095/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7096/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7097/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7098/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7099/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7100/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7101/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7102/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7103/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7104/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7105/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7106/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7107/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7108/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7109/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7110/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7111/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7112/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7113/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7114/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7115/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7116/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7117/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7118/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7119/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7120/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7121/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7122/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7123/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7124/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7125/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7126/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7127/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7128/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7129/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7130/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7131/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7132/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7133/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7134/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7135/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7136/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7137/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7138/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7139/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7140/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7141/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7142/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [7143/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [7144/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [7145/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [7146/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7147/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7148/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7149/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7150/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7151/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [7152/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7153/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7154/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [7155/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [7156/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [7157/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [7158/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [7159/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [7160/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [7161/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [7162/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [7163/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [7164/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [7165/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [7166/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [7167/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [7168/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [7169/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [7170/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [7171/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [7172/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [7173/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7174/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7175/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7176/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7177/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7178/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7179/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7180/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7181/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [7182/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [7183/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [7184/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [7185/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [7186/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [7187/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [7188/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [7189/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [7190/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [7191/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [7192/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [7193/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7194/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7195/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7196/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7197/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7198/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7199/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7200/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7201/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7202/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7203/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7204/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7205/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7206/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7207/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7208/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [7209/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7210/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7211/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [7212/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [7213/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [7214/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [7215/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [7216/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [7217/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [7218/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [7219/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [7220/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [7221/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [7222/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [7223/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [7224/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [7225/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [7226/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [7227/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [7228/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [7229/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [7230/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [7231/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7232/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [7233/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [7234/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [7235/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [7236/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [7237/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [7238/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [7239/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7240/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7241/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7242/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7243/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7244/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7245/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7246/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7247/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7248/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7249/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7250/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7251/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7252/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7253/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7254/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7255/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [7256/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [7257/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [7258/10000] Train Loss: 0.000070 Val Loss: nan\n",
      "Epoch [7259/10000] Train Loss: 0.000294 Val Loss: nan\n",
      "Epoch [7260/10000] Train Loss: 0.000211 Val Loss: nan\n",
      "Epoch [7261/10000] Train Loss: 0.000307 Val Loss: nan\n",
      "Epoch [7262/10000] Train Loss: 0.000548 Val Loss: nan\n",
      "Epoch [7263/10000] Train Loss: 0.000346 Val Loss: nan\n",
      "Epoch [7264/10000] Train Loss: 0.000543 Val Loss: nan\n",
      "Epoch [7265/10000] Train Loss: 0.001915 Val Loss: nan\n",
      "Epoch [7266/10000] Train Loss: 0.002207 Val Loss: nan\n",
      "Epoch [7267/10000] Train Loss: 0.002511 Val Loss: nan\n",
      "Epoch [7268/10000] Train Loss: 0.002003 Val Loss: nan\n",
      "Epoch [7269/10000] Train Loss: 0.001483 Val Loss: nan\n",
      "Epoch [7270/10000] Train Loss: 0.000688 Val Loss: nan\n",
      "Epoch [7271/10000] Train Loss: 0.000426 Val Loss: nan\n",
      "Epoch [7272/10000] Train Loss: 0.000321 Val Loss: nan\n",
      "Epoch [7273/10000] Train Loss: 0.000252 Val Loss: nan\n",
      "Epoch [7274/10000] Train Loss: 0.000126 Val Loss: nan\n",
      "Epoch [7275/10000] Train Loss: 0.000078 Val Loss: nan\n",
      "Epoch [7276/10000] Train Loss: 0.000048 Val Loss: nan\n",
      "Epoch [7277/10000] Train Loss: 0.000034 Val Loss: nan\n",
      "Epoch [7278/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [7279/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [7280/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [7281/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [7282/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [7283/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [7284/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [7285/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [7286/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [7287/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [7288/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [7289/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7290/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7291/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7292/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7293/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7294/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7295/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7296/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7297/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7298/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7299/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7300/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7301/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7302/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7303/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7304/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7305/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7306/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7307/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7308/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7309/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7310/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7311/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7312/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7313/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7314/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7315/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7316/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7317/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7318/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7319/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7320/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7321/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7322/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7323/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7324/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7325/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7326/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7327/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7328/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7329/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7330/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7331/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7332/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7333/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7334/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7335/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7336/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7337/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7338/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7339/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7340/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7341/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7342/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7343/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7344/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7345/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7346/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7347/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7348/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7349/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7350/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7351/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7352/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7353/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7354/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7355/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7356/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7357/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7358/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7359/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7360/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7361/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7362/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7363/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7364/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7365/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7366/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7367/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7368/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7369/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7370/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7371/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7372/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7373/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7374/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7375/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7376/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7377/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7378/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7379/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7380/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7381/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7382/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7383/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7384/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7385/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7386/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7387/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7388/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7389/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7390/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7391/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7392/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7393/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7394/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7395/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7396/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7397/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7398/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7399/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7400/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7401/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7402/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7403/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7404/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7405/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7406/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7407/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7408/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7409/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7410/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7411/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7412/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7413/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7414/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7415/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7416/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7417/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7418/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7419/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7420/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7421/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7422/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7423/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7424/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7425/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7426/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7427/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7428/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7429/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7430/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7431/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7432/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7433/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7434/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7435/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7436/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7437/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7438/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7439/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7440/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7441/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7442/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7443/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7444/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7445/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7446/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7447/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7448/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7449/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7450/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7451/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7452/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7453/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7454/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7455/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7456/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7457/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7458/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7459/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7460/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7461/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7462/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7463/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7464/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7465/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7466/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7467/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7468/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7469/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7470/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7471/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7472/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7473/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7474/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7475/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7476/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7477/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7478/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7479/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7480/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7481/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7482/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7483/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7484/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7485/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7486/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7487/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7488/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7489/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7490/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7491/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7492/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7493/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7494/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7495/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7496/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7497/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7498/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7499/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7500/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7501/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7502/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7503/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7504/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7505/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7506/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7507/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7508/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7509/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7510/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7511/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7512/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7513/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7514/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7515/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7516/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7517/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7518/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7519/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7520/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7521/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7522/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7523/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7524/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7525/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7526/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7527/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7528/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7529/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7530/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7531/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7532/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7533/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7534/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7535/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7536/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7537/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7538/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7539/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7540/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7541/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7542/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7543/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7544/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7545/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7546/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7547/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7548/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7549/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7550/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7551/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7552/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7553/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7554/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7555/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7556/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7557/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7558/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7559/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7560/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7561/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7562/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7563/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7564/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7565/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7566/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7567/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7568/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7569/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7570/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7571/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7572/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7573/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7574/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7575/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7576/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7577/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7578/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7579/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7580/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7581/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7582/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7583/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7584/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7585/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7586/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7587/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7588/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7589/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7590/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7591/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7592/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7593/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7594/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7595/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7596/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7597/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7598/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7599/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7600/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7601/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7602/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7603/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7604/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7605/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7606/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7607/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7608/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7609/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7610/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7611/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7612/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7613/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7614/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7615/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7616/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7617/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7618/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7619/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7620/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7621/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7622/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7623/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7624/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7625/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7626/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7627/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7628/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7629/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7630/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7631/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7632/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7633/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7634/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7635/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7636/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7637/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7638/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7639/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7640/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7641/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7642/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7643/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7644/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7645/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7646/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7647/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7648/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7649/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7650/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7651/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7652/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7653/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7654/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7655/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7656/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7657/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7658/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7659/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7660/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7661/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7662/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7663/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7664/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7665/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7666/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7667/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7668/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7669/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7670/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7671/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7672/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7673/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7674/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7675/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7676/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7677/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7678/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7679/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7680/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7681/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7682/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7683/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7684/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7685/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7686/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7687/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7688/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7689/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7690/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7691/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [7692/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [7693/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [7694/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [7695/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [7696/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [7697/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [7698/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [7699/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [7700/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [7701/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [7702/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [7703/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [7704/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [7705/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [7706/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [7707/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [7708/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [7709/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [7710/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7711/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7712/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7713/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7714/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7715/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7716/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7717/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7718/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7719/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7720/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7721/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7722/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7723/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7724/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7725/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7726/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7727/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7728/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7729/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7730/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7731/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7732/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7733/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7734/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7735/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7736/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7737/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7738/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7739/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7740/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7741/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7742/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7743/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7744/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7745/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7746/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7747/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7748/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7749/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7750/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7751/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7752/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7753/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7754/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7755/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7756/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7757/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7758/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7759/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [7760/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [7761/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7762/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7763/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7764/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [7765/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [7766/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [7767/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [7768/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [7769/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [7770/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [7771/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [7772/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [7773/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [7774/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [7775/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [7776/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [7777/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [7778/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [7779/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [7780/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [7781/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [7782/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [7783/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7784/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7785/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7786/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7787/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7788/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7789/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7790/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7791/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7792/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7793/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7794/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7795/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7796/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7797/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7798/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7799/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7800/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7801/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7802/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7803/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7804/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7805/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7806/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7807/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7808/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7809/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7810/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7811/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7812/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7813/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7814/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7815/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7816/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7817/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7818/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7819/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7820/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7821/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7822/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7823/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7824/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7825/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7826/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7827/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7828/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [7829/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7830/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7831/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [7832/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [7833/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [7834/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [7835/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [7836/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [7837/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [7838/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [7839/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [7840/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [7841/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [7842/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [7843/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [7844/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [7845/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [7846/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [7847/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [7848/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [7849/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [7850/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [7851/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [7852/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [7853/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [7854/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [7855/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [7856/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [7857/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7858/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7859/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7860/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7861/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7862/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7863/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7864/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7865/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7866/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7867/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7868/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7869/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7870/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7871/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7872/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7873/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7874/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7875/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7876/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7877/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7878/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7879/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7880/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7881/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7882/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7883/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7884/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7885/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7886/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7887/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7888/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7889/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7890/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7891/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7892/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7893/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7894/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7895/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7896/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7897/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7898/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7899/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7900/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7901/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7902/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7903/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7904/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7905/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7906/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7907/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7908/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7909/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7910/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7911/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7912/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7913/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7914/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7915/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7916/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7917/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7918/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7919/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7920/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7921/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7922/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7923/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7924/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7925/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7926/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [7927/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [7928/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [7929/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [7930/10000] Train Loss: 0.000030 Val Loss: nan\n",
      "Epoch [7931/10000] Train Loss: 0.000059 Val Loss: nan\n",
      "Epoch [7932/10000] Train Loss: 0.000043 Val Loss: nan\n",
      "Epoch [7933/10000] Train Loss: 0.000054 Val Loss: nan\n",
      "Epoch [7934/10000] Train Loss: 0.000058 Val Loss: nan\n",
      "Epoch [7935/10000] Train Loss: 0.000046 Val Loss: nan\n",
      "Epoch [7936/10000] Train Loss: 0.000035 Val Loss: nan\n",
      "Epoch [7937/10000] Train Loss: 0.000032 Val Loss: nan\n",
      "Epoch [7938/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [7939/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [7940/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [7941/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [7942/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [7943/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [7944/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [7945/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [7946/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [7947/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [7948/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7949/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [7950/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7951/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7952/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7953/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7954/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7955/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7956/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7957/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7958/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7959/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7960/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7961/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7962/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7963/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7964/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7965/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7966/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7967/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7968/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7969/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7970/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7971/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7972/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7973/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7974/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7975/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7976/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7977/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7978/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7979/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7980/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7981/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7982/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7983/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7984/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7985/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7986/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7987/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7988/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7989/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7990/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7991/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7992/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7993/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7994/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7995/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7996/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7997/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [7998/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [7999/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8000/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8001/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8002/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8003/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8004/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8005/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8006/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8007/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8008/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8009/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8010/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8011/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8012/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8013/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8014/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8015/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8016/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8017/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8018/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8019/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8020/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [8021/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8022/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8023/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [8024/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [8025/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8026/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [8027/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [8028/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [8029/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [8030/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [8031/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [8032/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [8033/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [8034/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [8035/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [8036/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [8037/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [8038/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [8039/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [8040/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [8041/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [8042/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [8043/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [8044/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [8045/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [8046/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [8047/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [8048/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [8049/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [8050/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [8051/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [8052/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8053/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [8054/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [8055/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [8056/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [8057/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8058/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [8059/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8060/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8061/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8062/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8063/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8064/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8065/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8066/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8067/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8068/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8069/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8070/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8071/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8072/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8073/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8074/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8075/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8076/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8077/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8078/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8079/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8080/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8081/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8082/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8083/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8084/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8085/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8086/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8087/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8088/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8089/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8090/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8091/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8092/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8093/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8094/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8095/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8096/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8097/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8098/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8099/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8100/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8101/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8102/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8103/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8104/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8105/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8106/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8107/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8108/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8109/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8110/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8111/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8112/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8113/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8114/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8115/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8116/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8117/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8118/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8119/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8120/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8121/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8122/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8123/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8124/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8125/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8126/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8127/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [8128/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [8129/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8130/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [8131/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8132/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [8133/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [8134/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [8135/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [8136/10000] Train Loss: 0.000033 Val Loss: nan\n",
      "Epoch [8137/10000] Train Loss: 0.000049 Val Loss: nan\n",
      "Epoch [8138/10000] Train Loss: 0.000058 Val Loss: nan\n",
      "Epoch [8139/10000] Train Loss: 0.000067 Val Loss: nan\n",
      "Epoch [8140/10000] Train Loss: 0.000082 Val Loss: nan\n",
      "Epoch [8141/10000] Train Loss: 0.000082 Val Loss: nan\n",
      "Epoch [8142/10000] Train Loss: 0.000430 Val Loss: nan\n",
      "Epoch [8143/10000] Train Loss: 0.003395 Val Loss: nan\n",
      "Epoch [8144/10000] Train Loss: 0.002700 Val Loss: nan\n",
      "Epoch [8145/10000] Train Loss: 0.002741 Val Loss: nan\n",
      "Epoch [8146/10000] Train Loss: 0.001813 Val Loss: nan\n",
      "Epoch [8147/10000] Train Loss: 0.001322 Val Loss: nan\n",
      "Epoch [8148/10000] Train Loss: 0.000549 Val Loss: nan\n",
      "Epoch [8149/10000] Train Loss: 0.000450 Val Loss: nan\n",
      "Epoch [8150/10000] Train Loss: 0.000384 Val Loss: nan\n",
      "Epoch [8151/10000] Train Loss: 0.001444 Val Loss: nan\n",
      "Epoch [8152/10000] Train Loss: 0.001944 Val Loss: nan\n",
      "Epoch [8153/10000] Train Loss: 0.000429 Val Loss: nan\n",
      "Epoch [8154/10000] Train Loss: 0.000188 Val Loss: nan\n",
      "Epoch [8155/10000] Train Loss: 0.000088 Val Loss: nan\n",
      "Epoch [8156/10000] Train Loss: 0.000059 Val Loss: nan\n",
      "Epoch [8157/10000] Train Loss: 0.000037 Val Loss: nan\n",
      "Epoch [8158/10000] Train Loss: 0.000029 Val Loss: nan\n",
      "Epoch [8159/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [8160/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [8161/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [8162/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [8163/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [8164/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [8165/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [8166/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [8167/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [8168/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8169/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [8170/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [8171/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [8172/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [8173/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8174/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8175/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8176/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8177/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8178/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8179/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8180/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8181/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8182/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8183/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8184/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8185/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8186/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8187/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8188/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8189/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8190/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8191/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8192/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8193/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8194/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8195/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8196/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8197/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8198/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8199/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8200/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8201/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8202/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8203/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8204/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8205/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8206/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8207/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8208/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8209/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8210/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8211/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8212/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8213/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8214/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8215/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8216/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8217/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8218/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8219/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8220/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8221/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8222/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8223/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8224/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8225/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8226/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8227/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8228/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8229/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8230/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8231/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8232/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8233/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8234/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8235/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8236/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8237/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8238/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8239/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8240/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8241/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8242/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8243/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8244/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8245/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8246/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8247/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8248/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8249/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8250/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8251/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8252/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8253/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8254/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8255/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8256/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8257/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8258/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8259/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8260/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8261/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8262/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8263/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8264/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8265/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8266/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8267/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8268/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8269/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8270/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8271/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8272/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8273/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8274/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8275/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8276/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8277/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8278/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8279/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8280/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8281/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8282/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8283/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8284/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8285/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8286/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8287/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8288/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8289/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8290/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8291/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8292/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8293/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8294/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8295/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8296/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8297/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8298/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8299/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8300/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8301/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8302/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8303/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8304/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8305/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8306/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8307/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8308/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8309/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8310/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8311/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8312/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8313/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8314/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8315/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8316/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8317/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8318/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8319/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8320/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8321/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8322/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8323/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8324/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8325/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8326/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8327/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8328/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8329/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8330/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8331/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8332/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8333/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8334/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8335/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8336/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8337/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8338/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8339/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8340/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8341/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8342/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8343/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8344/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8345/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8346/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8347/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8348/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8349/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8350/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8351/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8352/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8353/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8354/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8355/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8356/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8357/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8358/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8359/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8360/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8361/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8362/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8363/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8364/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8365/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8366/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8367/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8368/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8369/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8370/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8371/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8372/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8373/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8374/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8375/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8376/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8377/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8378/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8379/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8380/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8381/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8382/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8383/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8384/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8385/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8386/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8387/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8388/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8389/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8390/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8391/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8392/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8393/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8394/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8395/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8396/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8397/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8398/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8399/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8400/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8401/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8402/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8403/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8404/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8405/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8406/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8407/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8408/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8409/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8410/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8411/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8412/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8413/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8414/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8415/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8416/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8417/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8418/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8419/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8420/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8421/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8422/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8423/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8424/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8425/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8426/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8427/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8428/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8429/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8430/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8431/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8432/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8433/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8434/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8435/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8436/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8437/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8438/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8439/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8440/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8441/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8442/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8443/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8444/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8445/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8446/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8447/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8448/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8449/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8450/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8451/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8452/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8453/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8454/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8455/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8456/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8457/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8458/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8459/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8460/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8461/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8462/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8463/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8464/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8465/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8466/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8467/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [8468/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8469/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8470/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [8471/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8472/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8473/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8474/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8475/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8476/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8477/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8478/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8479/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8480/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8481/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8482/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8483/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8484/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8485/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8486/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8487/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8488/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8489/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8490/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8491/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8492/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8493/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8494/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8495/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [8496/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [8497/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [8498/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [8499/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [8500/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [8501/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [8502/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8503/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [8504/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8505/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8506/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8507/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8508/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8509/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8510/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8511/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8512/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8513/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8514/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8515/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8516/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8517/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8518/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8519/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8520/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8521/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8522/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8523/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8524/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8525/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8526/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8527/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8528/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8529/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8530/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8531/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8532/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8533/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8534/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8535/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8536/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8537/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8538/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8539/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8540/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8541/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8542/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8543/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8544/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8545/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8546/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8547/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8548/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8549/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8550/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8551/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8552/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8553/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8554/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8555/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8556/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8557/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8558/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8559/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8560/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8561/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8562/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8563/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8564/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8565/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8566/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8567/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8568/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8569/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8570/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8571/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8572/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8573/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8574/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8575/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8576/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8577/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8578/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8579/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8580/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8581/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8582/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8583/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8584/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8585/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8586/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8587/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8588/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8589/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8590/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8591/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8592/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8593/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8594/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8595/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8596/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8597/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8598/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8599/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8600/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [8601/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [8602/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [8603/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8604/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8605/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8606/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8607/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8608/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8609/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8610/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8611/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8612/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8613/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8614/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8615/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8616/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8617/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8618/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8619/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8620/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8621/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8622/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8623/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8624/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8625/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8626/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8627/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8628/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8629/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8630/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8631/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8632/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8633/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8634/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8635/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8636/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8637/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8638/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8639/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8640/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8641/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8642/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8643/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8644/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8645/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8646/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8647/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8648/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8649/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [8650/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [8651/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [8652/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8653/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8654/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [8655/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8656/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [8657/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [8658/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [8659/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [8660/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [8661/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [8662/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8663/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [8664/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [8665/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8666/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8667/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [8668/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [8669/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8670/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [8671/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [8672/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8673/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8674/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8675/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8676/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8677/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8678/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8679/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8680/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8681/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8682/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8683/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8684/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8685/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8686/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8687/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8688/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8689/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8690/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8691/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8692/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8693/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8694/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8695/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8696/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8697/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8698/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8699/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8700/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8701/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8702/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8703/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8704/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8705/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8706/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8707/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8708/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8709/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8710/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8711/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8712/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8713/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8714/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8715/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8716/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8717/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8718/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8719/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8720/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8721/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8722/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8723/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8724/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8725/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8726/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8727/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [8728/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [8729/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8730/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [8731/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [8732/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [8733/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [8734/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [8735/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [8736/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [8737/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [8738/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [8739/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8740/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [8741/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8742/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8743/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8744/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8745/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8746/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8747/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8748/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [8749/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [8750/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [8751/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8752/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8753/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8754/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8755/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8756/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8757/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8758/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8759/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8760/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [8761/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8762/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [8763/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8764/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8765/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8766/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [8767/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [8768/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [8769/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [8770/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8771/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8772/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8773/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8774/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8775/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8776/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8777/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8778/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8779/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8780/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8781/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8782/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8783/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8784/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8785/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8786/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8787/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8788/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8789/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8790/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8791/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8792/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8793/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8794/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8795/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8796/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8797/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8798/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8799/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8800/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8801/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8802/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8803/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8804/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8805/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8806/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8807/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8808/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8809/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8810/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8811/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8812/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8813/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8814/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8815/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8816/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8817/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8818/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8819/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8820/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8821/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8822/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8823/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8824/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [8825/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [8826/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [8827/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [8828/10000] Train Loss: 0.000032 Val Loss: nan\n",
      "Epoch [8829/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [8830/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [8831/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [8832/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [8833/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [8834/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [8835/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [8836/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [8837/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8838/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [8839/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8840/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8841/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8842/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8843/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8844/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8845/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8846/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8847/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8848/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8849/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8850/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8851/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8852/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8853/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8854/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8855/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8856/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8857/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8858/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8859/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8860/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8861/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8862/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8863/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8864/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8865/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8866/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8867/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8868/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8869/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8870/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8871/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8872/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8873/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8874/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8875/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8876/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8877/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8878/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8879/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8880/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8881/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8882/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8883/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8884/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8885/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8886/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8887/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8888/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8889/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8890/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8891/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8892/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8893/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8894/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8895/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8896/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8897/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8898/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8899/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8900/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8901/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8902/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8903/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8904/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8905/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8906/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [8907/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8908/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [8909/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [8910/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [8911/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [8912/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [8913/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [8914/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8915/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [8916/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8917/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8918/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [8919/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [8920/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8921/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8922/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [8923/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8924/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8925/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8926/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [8927/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [8928/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [8929/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [8930/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [8931/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [8932/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8933/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [8934/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [8935/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [8936/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [8937/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [8938/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8939/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [8940/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [8941/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [8942/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [8943/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [8944/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [8945/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [8946/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [8947/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8948/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [8949/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [8950/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8951/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8952/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8953/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8954/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8955/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8956/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8957/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8958/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8959/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8960/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8961/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8962/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8963/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8964/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8965/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8966/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [8967/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8968/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8969/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8970/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8971/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8972/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8973/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8974/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8975/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8976/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8977/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8978/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8979/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8980/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8981/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8982/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [8983/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8984/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8985/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8986/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8987/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8988/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8989/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [8990/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8991/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8992/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8993/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8994/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [8995/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8996/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8997/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8998/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [8999/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9000/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9001/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9002/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9003/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9004/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9005/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9006/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9007/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9008/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9009/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9010/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9011/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9012/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9013/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9014/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9015/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9016/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9017/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9018/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9019/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9020/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9021/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9022/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9023/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9024/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9025/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9026/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9027/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9028/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9029/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9030/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9031/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9032/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [9033/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [9034/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [9035/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [9036/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9037/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9038/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9039/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9040/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9041/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9042/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9043/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9044/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9045/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9046/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9047/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9048/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9049/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9050/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9051/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9052/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9053/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9054/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [9055/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [9056/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [9057/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [9058/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [9059/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [9060/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [9061/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [9062/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [9063/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9064/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9065/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9066/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9067/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9068/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9069/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9070/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9071/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9072/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9073/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9074/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9075/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9076/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9077/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9078/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9079/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9080/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9081/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9082/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9083/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9084/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9085/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9086/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9087/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9088/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9089/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9090/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9091/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9092/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9093/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9094/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9095/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9096/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9097/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9098/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9099/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9100/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9101/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9102/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9103/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9104/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [9105/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [9106/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [9107/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [9108/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [9109/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [9110/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [9111/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [9112/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [9113/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [9114/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9115/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9116/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [9117/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9118/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [9119/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9120/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9121/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9122/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9123/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9124/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [9125/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [9126/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [9127/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [9128/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [9129/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [9130/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [9131/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [9132/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9133/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9134/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [9135/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [9136/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [9137/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [9138/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9139/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9140/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9141/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9142/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9143/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9144/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9145/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9146/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9147/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9148/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9149/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9150/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9151/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9152/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9153/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9154/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9155/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9156/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9157/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9158/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9159/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9160/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9161/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9162/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9163/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9164/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9165/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9166/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9167/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9168/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9169/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9170/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9171/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9172/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9173/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9174/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9175/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9176/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9177/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9178/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9179/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9180/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9181/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9182/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9183/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9184/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9185/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9186/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9187/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9188/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9189/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9190/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9191/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9192/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9193/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9194/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9195/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9196/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9197/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9198/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9199/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9200/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9201/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9202/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9203/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9204/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9205/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9206/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9207/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9208/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9209/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9210/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9211/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9212/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9213/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9214/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9215/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9216/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9217/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9218/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9219/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [9220/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9221/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9222/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9223/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9224/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9225/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9226/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9227/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9228/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9229/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9230/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9231/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9232/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9233/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9234/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9235/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9236/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9237/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9238/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9239/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9240/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9241/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9242/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9243/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9244/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9245/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9246/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9247/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9248/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [9249/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [9250/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [9251/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [9252/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [9253/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [9254/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9255/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9256/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9257/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [9258/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [9259/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [9260/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [9261/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [9262/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [9263/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [9264/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [9265/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [9266/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [9267/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [9268/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [9269/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [9270/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [9271/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [9272/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [9273/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [9274/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9275/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9276/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [9277/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [9278/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [9279/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [9280/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [9281/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [9282/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [9283/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9284/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9285/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9286/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9287/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9288/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9289/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9290/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9291/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9292/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9293/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9294/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9295/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9296/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9297/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9298/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9299/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9300/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9301/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9302/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9303/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9304/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9305/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9306/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9307/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9308/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9309/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9310/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9311/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9312/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9313/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9314/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9315/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9316/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9317/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9318/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9319/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9320/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9321/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9322/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9323/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9324/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9325/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9326/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9327/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9328/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9329/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9330/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9331/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9332/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9333/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9334/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9335/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9336/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9337/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9338/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9339/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9340/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9341/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9342/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9343/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9344/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9345/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9346/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9347/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9348/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9349/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9350/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9351/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9352/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9353/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9354/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9355/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9356/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9357/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9358/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9359/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9360/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9361/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9362/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9363/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9364/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9365/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9366/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9367/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9368/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9369/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9370/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9371/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9372/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [9373/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [9374/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [9375/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [9376/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [9377/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [9378/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [9379/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [9380/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [9381/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [9382/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [9383/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [9384/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [9385/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [9386/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [9387/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [9388/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [9389/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [9390/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [9391/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [9392/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [9393/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [9394/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9395/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9396/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9397/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9398/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9399/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9400/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9401/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9402/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9403/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9404/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9405/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9406/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9407/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9408/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9409/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9410/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9411/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9412/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9413/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9414/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9415/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9416/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9417/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9418/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9419/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9420/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9421/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [9422/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9423/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [9424/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9425/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9426/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9427/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9428/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9429/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9430/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9431/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9432/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [9433/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [9434/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [9435/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [9436/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9437/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9438/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9439/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9440/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9441/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9442/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9443/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9444/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9445/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9446/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9447/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9448/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9449/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9450/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9451/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9452/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9453/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9454/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9455/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9456/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9457/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9458/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9459/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9460/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9461/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9462/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9463/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9464/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9465/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9466/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9467/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9468/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9469/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9470/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9471/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9472/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9473/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9474/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9475/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9476/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9477/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9478/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9479/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9480/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9481/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9482/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9483/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [9484/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [9485/10000] Train Loss: 0.000024 Val Loss: nan\n",
      "Epoch [9486/10000] Train Loss: 0.000026 Val Loss: nan\n",
      "Epoch [9487/10000] Train Loss: 0.000028 Val Loss: nan\n",
      "Epoch [9488/10000] Train Loss: 0.000025 Val Loss: nan\n",
      "Epoch [9489/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [9490/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [9491/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [9492/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [9493/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [9494/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [9495/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [9496/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [9497/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [9498/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [9499/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [9500/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [9501/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [9502/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [9503/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [9504/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9505/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9506/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9507/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9508/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9509/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9510/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9511/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9512/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9513/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9514/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9515/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9516/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9517/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9518/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9519/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9520/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9521/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9522/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9523/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9524/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9525/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9526/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9527/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9528/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9529/10000] Train Loss: 0.000000 Val Loss: nan\n",
      "Epoch [9530/10000] Train Loss: 0.000000 Val Loss: nan\n",
      "Epoch [9531/10000] Train Loss: 0.000000 Val Loss: nan\n",
      "Epoch [9532/10000] Train Loss: 0.000000 Val Loss: nan\n",
      "Epoch [9533/10000] Train Loss: 0.000000 Val Loss: nan\n",
      "Epoch [9534/10000] Train Loss: 0.000000 Val Loss: nan\n",
      "Epoch [9535/10000] Train Loss: 0.000000 Val Loss: nan\n",
      "Epoch [9536/10000] Train Loss: 0.000000 Val Loss: nan\n",
      "Epoch [9537/10000] Train Loss: 0.000000 Val Loss: nan\n",
      "Epoch [9538/10000] Train Loss: 0.000000 Val Loss: nan\n",
      "Epoch [9539/10000] Train Loss: 0.000000 Val Loss: nan\n",
      "Epoch [9540/10000] Train Loss: 0.000000 Val Loss: nan\n",
      "Epoch [9541/10000] Train Loss: 0.000000 Val Loss: nan\n",
      "Epoch [9542/10000] Train Loss: 0.000000 Val Loss: nan\n",
      "Epoch [9543/10000] Train Loss: 0.000000 Val Loss: nan\n",
      "Epoch [9544/10000] Train Loss: 0.000000 Val Loss: nan\n",
      "Epoch [9545/10000] Train Loss: 0.000000 Val Loss: nan\n",
      "Epoch [9546/10000] Train Loss: 0.000000 Val Loss: nan\n",
      "Epoch [9547/10000] Train Loss: 0.000000 Val Loss: nan\n",
      "Epoch [9548/10000] Train Loss: 0.000000 Val Loss: nan\n",
      "Epoch [9549/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9550/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9551/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9552/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9553/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9554/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9555/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9556/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9557/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9558/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9559/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9560/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9561/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9562/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9563/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9564/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9565/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9566/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9567/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9568/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9569/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9570/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9571/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9572/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9573/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9574/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9575/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9576/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9577/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9578/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9579/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9580/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9581/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9582/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9583/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9584/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9585/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9586/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9587/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [9588/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9589/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9590/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [9591/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [9592/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [9593/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [9594/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [9595/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [9596/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [9597/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [9598/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [9599/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [9600/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9601/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [9602/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [9603/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [9604/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [9605/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [9606/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [9607/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [9608/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9609/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9610/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9611/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9612/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9613/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9614/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9615/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9616/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9617/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9618/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9619/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9620/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9621/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9622/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9623/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9624/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9625/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9626/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9627/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9628/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9629/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9630/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9631/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9632/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9633/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9634/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9635/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9636/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9637/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9638/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9639/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9640/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9641/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9642/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9643/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9644/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9645/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9646/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9647/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9648/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9649/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9650/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9651/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9652/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9653/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9654/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9655/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9656/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9657/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9658/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9659/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9660/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9661/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9662/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9663/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9664/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9665/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9666/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [9667/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [9668/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [9669/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [9670/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [9671/10000] Train Loss: 0.000018 Val Loss: nan\n",
      "Epoch [9672/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [9673/10000] Train Loss: 0.000015 Val Loss: nan\n",
      "Epoch [9674/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [9675/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9676/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9677/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9678/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9679/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9680/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9681/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9682/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9683/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9684/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9685/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9686/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9687/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9688/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9689/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9690/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9691/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9692/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9693/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9694/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9695/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9696/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9697/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9698/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9699/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9700/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9701/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9702/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9703/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9704/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9705/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9706/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9707/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9708/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9709/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9710/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9711/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9712/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9713/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9714/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9715/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9716/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9717/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9718/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9719/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9720/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9721/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9722/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9723/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9724/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9725/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9726/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9727/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9728/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9729/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9730/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9731/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9732/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9733/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9734/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9735/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9736/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9737/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9738/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9739/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9740/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9741/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9742/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9743/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9744/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9745/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9746/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9747/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9748/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9749/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9750/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9751/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9752/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9753/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9754/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9755/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9756/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9757/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9758/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9759/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9760/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9761/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9762/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9763/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [9764/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [9765/10000] Train Loss: 0.000021 Val Loss: nan\n",
      "Epoch [9766/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [9767/10000] Train Loss: 0.000023 Val Loss: nan\n",
      "Epoch [9768/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [9769/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [9770/10000] Train Loss: 0.000027 Val Loss: nan\n",
      "Epoch [9771/10000] Train Loss: 0.000022 Val Loss: nan\n",
      "Epoch [9772/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [9773/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [9774/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [9775/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [9776/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9777/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9778/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9779/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9780/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9781/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9782/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9783/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9784/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9785/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9786/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9787/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9788/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9789/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9790/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9791/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9792/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9793/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9794/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9795/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9796/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9797/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9798/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9799/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9800/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9801/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9802/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9803/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9804/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9805/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9806/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9807/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9808/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9809/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9810/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9811/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9812/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9813/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9814/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9815/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9816/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9817/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9818/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9819/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9820/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9821/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9822/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9823/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9824/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9825/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9826/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9827/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9828/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9829/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9830/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9831/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9832/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9833/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9834/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9835/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9836/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9837/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9838/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9839/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9840/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9841/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9842/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9843/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9844/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9845/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9846/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9847/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9848/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9849/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9850/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9851/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [9852/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9853/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9854/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9855/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [9856/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9857/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [9858/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [9859/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [9860/10000] Train Loss: 0.000020 Val Loss: nan\n",
      "Epoch [9861/10000] Train Loss: 0.000019 Val Loss: nan\n",
      "Epoch [9862/10000] Train Loss: 0.000017 Val Loss: nan\n",
      "Epoch [9863/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [9864/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [9865/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [9866/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [9867/10000] Train Loss: 0.000012 Val Loss: nan\n",
      "Epoch [9868/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [9869/10000] Train Loss: 0.000014 Val Loss: nan\n",
      "Epoch [9870/10000] Train Loss: 0.000016 Val Loss: nan\n",
      "Epoch [9871/10000] Train Loss: 0.000013 Val Loss: nan\n",
      "Epoch [9872/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [9873/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9874/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9875/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9876/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9877/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9878/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9879/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9880/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9881/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9882/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9883/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9884/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9885/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9886/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9887/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9888/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9889/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9890/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9891/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9892/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9893/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9894/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9895/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9896/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9897/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9898/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9899/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9900/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9901/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9902/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9903/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9904/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9905/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9906/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9907/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9908/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9909/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9910/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9911/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9912/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9913/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9914/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9915/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9916/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9917/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9918/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9919/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9920/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9921/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9922/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9923/10000] Train Loss: 0.000001 Val Loss: nan\n",
      "Epoch [9924/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9925/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9926/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9927/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9928/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9929/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9930/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9931/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9932/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9933/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9934/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9935/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9936/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9937/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9938/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9939/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9940/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9941/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9942/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9943/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9944/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9945/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9946/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9947/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9948/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9949/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9950/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9951/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9952/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9953/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9954/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9955/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9956/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9957/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9958/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9959/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9960/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9961/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9962/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [9963/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [9964/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9965/10000] Train Loss: 0.000011 Val Loss: nan\n",
      "Epoch [9966/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9967/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9968/10000] Train Loss: 0.000010 Val Loss: nan\n",
      "Epoch [9969/10000] Train Loss: 0.000009 Val Loss: nan\n",
      "Epoch [9970/10000] Train Loss: 0.000008 Val Loss: nan\n",
      "Epoch [9971/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9972/10000] Train Loss: 0.000007 Val Loss: nan\n",
      "Epoch [9973/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9974/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9975/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9976/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9977/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9978/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9979/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9980/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9981/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9982/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9983/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9984/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9985/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9986/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9987/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9988/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9989/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9990/10000] Train Loss: 0.000006 Val Loss: nan\n",
      "Epoch [9991/10000] Train Loss: 0.000005 Val Loss: nan\n",
      "Epoch [9992/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9993/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9994/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9995/10000] Train Loss: 0.000004 Val Loss: nan\n",
      "Epoch [9996/10000] Train Loss: 0.000003 Val Loss: nan\n",
      "Epoch [9997/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9998/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [9999/10000] Train Loss: 0.000002 Val Loss: nan\n",
      "Epoch [10000/10000] Train Loss: 0.000002 Val Loss: nan\n"
     ]
    }
   ],
   "source": [
    "# model, history = train(\n",
    "#     model,\n",
    "#     train_loader=train_dl,\n",
    "#     val_loader=None,\n",
    "#     epochs=EPOCHS,\n",
    "#     patience=PATIENCE\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "70c3f2c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history[\"train_loss\"], label=\"train loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8069c9c1",
   "metadata": {},
   "source": [
    "# Compare with textures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f21295e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralSH()\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f7630277",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_sh = true_sh\n",
    "with torch.no_grad():\n",
    "    pred_sh = model(torch.tensor(pos_grid, dtype=torch.float32), torch.tensor(light_angle, dtype=torch.float32))\n",
    "pred_sh = pred_sh.numpy()\n",
    "assert true_sh.shape == pred_sh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "19f6e0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_sh_tex = SH3ToTex(true_sh)\n",
    "pred_sh_tex = SH3ToTex(pred_sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2c0ec055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that nearest sampling yields the same result as loading a raw texture\n",
    "raw_sh_tex = LoadTextures()\n",
    "for field in fields(SHTextures):\n",
    "    raw = getattr(raw_sh_tex, field.name).data\n",
    "    true = getattr(true_sh_tex, field.name).data\n",
    "    assert np.all(raw == true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "abfc89a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0033960417..4.4410677].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..4.4375].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0033504143..9.0027685].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..9.0].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0039718..1.5586581].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..1.5625].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.002643548..2.8744562].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..2.875].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-0.0031840873..4.4442616].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.0..4.4375].\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.061495893..1.0190964].\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for field in fields(SHTextures):\n",
    "    pred = getattr(pred_sh_tex, field.name).data\n",
    "    true = getattr(true_sh_tex, field.name).data\n",
    "    mse = np.mean((true - pred) ** 2, axis=-1)\n",
    "\n",
    "    fig, axes = plt.subplots(5, 3, figsize=(8, 3))\n",
    "\n",
    "    # Column titles\n",
    "    column_titles = [\"Predicted\", \"True\", \"MSE\"]\n",
    "    for col, title in enumerate(column_titles):\n",
    "        axes[0, col].set_title(title, fontsize=12)\n",
    "\n",
    "    # Plot images\n",
    "    for row in range(5):\n",
    "        for col in range(3):\n",
    "            axes[row, col].axis(\"off\")\n",
    "\n",
    "        axes[row, 0].imshow(pred[row])\n",
    "        axes[row, 1].imshow(true[row])\n",
    "        im = axes[row, 2].imshow(mse[row], cmap='hot')  # Save handle for colorbar\n",
    "\n",
    "    # Add colorbar for the last MSE image\n",
    "    cbar = fig.colorbar(im, ax=axes[:, 2])\n",
    "\n",
    "    fig.suptitle(f\"{field.name}\")\n",
    "    fig.savefig(f\"img/{field.name}.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453109f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
